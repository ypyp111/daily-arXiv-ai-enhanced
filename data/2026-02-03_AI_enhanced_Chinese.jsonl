{"id": "2602.00012", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00012", "abs": "https://arxiv.org/abs/2602.00012", "authors": ["Michael Siebenmann", "Javier Argota S\u00e1nchez-Vaquerizo", "Stefan Arisona", "Krystian Samp", "Luis Gisler", "Dirk Helbing"], "title": "OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models", "comment": "This work has been submitted to the IEEE for possible publication. 7 pages, 6 figures", "summary": "We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.", "AI": {"tldr": "OGD4All\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u516c\u6c11\u4e0e\u5730\u7406\u7a7a\u95f4\u5f00\u653e\u653f\u5e9c\u6570\u636e\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u3001\u667a\u80fd\u4ee3\u7801\u751f\u6210\u548c\u5b89\u5168\u6c99\u7bb1\u6267\u884c\uff0c\u5728430\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u523098%\u5206\u6790\u6b63\u786e\u7387\u548c94%\u53ec\u56de\u7387\u3002", "motivation": "\u589e\u5f3a\u516c\u6c11\u4e0e\u5730\u7406\u7a7a\u95f4\u5f00\u653e\u653f\u5e9c\u6570\u636e\u7684\u4ea4\u4e92\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u590d\u73b0\u7684\u8bbf\u95ee\u65b9\u5f0f\uff0c\u51cf\u5c11LLM\u5e7b\u89c9\u98ce\u9669\uff0c\u63a8\u8fdb\u53ef\u4fe1AI\u5728\u5f00\u653e\u6cbb\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u6570\u636e\u68c0\u7d22\u3001\u667a\u80fd\u63a8\u7406\u8fdb\u884c\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u3001\u5b89\u5168\u6c99\u7bb1\u6267\u884c\uff0c\u4ea7\u751f\u53ef\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u8f93\u51fa\uff0c\u5728430\u4e2a\u82cf\u9ece\u4e16\u5e02\u6570\u636e\u96c6\u548c11\u4e2aLLM\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728199\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5305\u62ec\u4e8b\u5b9e\u6027\u548c\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\uff09\uff0c\u8fbe\u523098%\u5206\u6790\u6b63\u786e\u7387\u548c94%\u53ec\u56de\u7387\uff0c\u80fd\u53ef\u9760\u62d2\u7edd\u6570\u636e\u4e0d\u652f\u6301\u7684\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5e7b\u89c9\u98ce\u9669\uff0c\u7edf\u8ba1\u7a33\u5065\u6027\u6d4b\u8bd5\u548c\u4e13\u5bb6\u53cd\u9988\u663e\u793a\u53ef\u9760\u6027\u548c\u793e\u4f1a\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u53ef\u4ee5\u4e3a\u516c\u5171\u6570\u636e\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u8bbf\u95ee\uff0c\u63a8\u8fdb\u5f00\u653e\u6cbb\u7406\u4e2d\u7684\u53ef\u4fe1AI\uff0c\u5c55\u793a\u4e86\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u6846\u67b6\u5728\u589e\u5f3a\u516c\u6c11\u6570\u636e\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.00022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00022", "abs": "https://arxiv.org/abs/2602.00022", "authors": ["Margaret Foster"], "title": "Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning", "comment": "16 pages, 6 figures, 3 tables, 9-page appendix", "summary": "We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u96be\u4ee5\u76f4\u63a5\u89c2\u6d4b\u60c5\u5883\u7684\u6d4b\u91cf\u6846\u67b6\uff0c\u7ed3\u5408\u95f4\u63a5\u6570\u636e\u75d5\u8ff9\u3001\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u7406\u8bba\u6307\u5bfc\u7684\u4e09\u89d2\u9a8c\u8bc1\uff0c\u4ee5\u586b\u8865\u4e0d\u53ef\u8bbf\u95ee\u7684\u6d4b\u91cf\u7a7a\u95f4\u3002", "motivation": "\u8bb8\u591a\u9ad8\u98ce\u9669\u7684\u7cfb\u7edf\u548c\u653f\u7b56\u5173\u6ce8\u5bf9\u8c61\u96be\u4ee5\u76f4\u63a5\u89c2\u6d4b\uff1a\u5173\u952e\u52a8\u6001\u4e0d\u53ef\u89c2\u5bdf\u3001\u6570\u636e\u95f4\u63a5\u4e14\u5206\u6563\u5728\u4e0d\u540c\u6765\u6e90\u3001\u771f\u5b9e\u60c5\u51b5\u7f3a\u5931\u6216\u88ab\u9690\u85cf\u3002\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u6570\u636e\u901a\u5e38\u4e0d\u652f\u6301\u4f20\u7edf\u7684\u5206\u6790\u7b56\u7565\uff0c\u5982\u57fa\u4e8e\u5355\u4e00\u6743\u5a01\u6570\u636e\u6d41\u7684\u7edf\u8ba1\u63a8\u65ad\u6216\u9488\u5bf9\u6807\u8bb0\u7ed3\u679c\u7684\u6a21\u578b\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6d4b\u91cf\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6e90\u4e09\u89d2\u9a8c\u8bc1\u4e0e\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u4e0d\u4f9d\u8d56\u65e0\u6cd5\u89c2\u6d4b\u7684\u7406\u60f3\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u800c\u662f\u5bfb\u6c42\u591a\u4e2a\u90e8\u5206\u4fe1\u606f\u6a21\u578b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u57fa\u4e8e\u8de8\u4fe1\u53f7\u4e00\u81f4\u6027\u6216\u4e0e\u9884\u671f\u72b6\u6001\u7684\u504f\u79bb\u5f97\u51fa\u6709\u4f9d\u636e\u7684\u7ed3\u8bba\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u4e2a\u79d8\u5bc6\u519b\u4e8b\u7ec4\u7ec7\u7684\u7ec4\u7ec7\u589e\u957f\u548c\u5185\u90e8\u538b\u529b\u52a8\u6001\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5982\u4f55\u6062\u590d\u5177\u6709\u5b9e\u8d28\u610f\u4e49\u7684\u53d8\u5f02\uff0c\u540c\u65f6\u660e\u786e\u63ed\u793a\u4e86\u63a8\u65ad\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u7f3a\u4e4f\u8db3\u591f\u6570\u636e\u8fdb\u884c\u4f20\u7edf\u7edf\u8ba1\u6216\u56e0\u679c\u63a8\u65ad\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9a\u91cf\u7684\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c55\u793a\u4e86\u4e09\u89d2\u9a8c\u8bc1\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u5982\u4f55\u6062\u590d\u5177\u6709\u5b9e\u8d28\u610f\u4e49\u7684\u53d8\u5f02\u3002"}}
{"id": "2602.00027", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00027", "abs": "https://arxiv.org/abs/2602.00027", "authors": ["Zhenyu Pu", "Yu Yang", "Lun Yang", "Qing-Shan Jia", "Xiaohong Guan", "Costas J. Spanos"], "title": "Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems", "comment": "14 pages, 7 figures", "summary": "Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8868\u793a\u5b66\u4e60\u6280\u672f\u7684\u589e\u5f3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u6c22\u57fa\u591a\u80fd\u6e90\u7cfb\u7edf\u7684\u8fd0\u884c\uff0c\u89e3\u51b3\u4e86\u6c22\u50a8\u80fd\u7cfb\u7edf\u975e\u7ebf\u6027\u591a\u7269\u7406\u573a\u8026\u5408\u52a8\u6001\u548c\u591a\u91cd\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\u3002", "motivation": "\u6c22\u57fa\u591a\u80fd\u6e90\u7cfb\u7edf(HMES)\u4f5c\u4e3a\u4f4e\u78b3\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u534f\u8c03\u7535\u529b\u3001\u4f9b\u70ed\u548c\u4f9b\u51b7\u7684\u4f9b\u9700\u8fd0\u884c\uff0c\u4f46\u9762\u4e34\u6c22\u50a8\u80fd\u7cfb\u7edf\u975e\u7ebf\u6027\u591a\u7269\u7406\u573a\u8026\u5408\u52a8\u6001\u548c\u4f9b\u9700\u591a\u91cd\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u6700\u4f18\u8fd0\u884c\u3002", "method": "1. \u5f00\u53d1\u4e86\u5168\u9762\u6355\u6349\u6c22\u50a8\u80fd\u7cfb\u7edf\u975e\u7ebf\u6027\u52a8\u6001\u548c\u591a\u7269\u7406\u573a\u8fc7\u7a0b\u7684\u7efc\u5408\u8fd0\u884c\u6a21\u578b\uff1b2. \u63d0\u51fa\u4e86\u96c6\u6210\u8868\u793a\u5b66\u4e60\u6280\u672f\u7684\u589e\u5f3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6(SR-DRL)\uff0c\u52a0\u901f\u548c\u6539\u8fdb\u590d\u6742\u7f51\u7edc\u7cfb\u7edf\u7684\u7b56\u7565\u4f18\u5316\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff1a\u7efc\u5408\u6a21\u578b\u5bf9\u786e\u4fdd\u6c22\u50a8\u80fd\u7cfb\u7edf\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff1bSR-DRL\u65b9\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edfDRL\uff0c\u80fd\u6709\u6548\u964d\u4f4eHMES\u8fd0\u884c\u6210\u672c\u5e76\u5904\u7406\u7cfb\u7edf\u8fd0\u884c\u7ea6\u675f\u3002", "conclusion": "\u8868\u793a\u5b66\u4e60\u5728DRL\u4e2d\u80fd\u591f\u5c06\u539f\u59cb\u72b6\u6001\u7a7a\u95f4\u91cd\u7ec4\u4e3a\u7ed3\u6784\u826f\u597d\u3001\u5177\u6709\u805a\u7c7b\u611f\u77e5\u7684\u51e0\u4f55\u8868\u793a\uff0c\u4ece\u800c\u5e73\u6ed1\u548c\u4fc3\u8fdbDRL\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4e3a\u590d\u6742\u80fd\u6e90\u7cfb\u7edf\u7684\u4f18\u5316\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00028", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00028", "abs": "https://arxiv.org/abs/2602.00028", "authors": ["Zoha Azimi", "Reza Farahani", "Radu Prodan", "Christian Timmerer"], "title": "ELLMPEG: An Edge-based Agentic LLM Video Processing Tool", "comment": "12 pages, 5 tables, 8 Figures, accepted for the MMSys 2026 conference", "summary": "Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.", "AI": {"tldr": "ELLMPEG\u662f\u4e00\u4e2a\u8fb9\u7f18\u4f7f\u80fd\u7684\u4ee3\u7406\u5f0fLLM\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u89c6\u9891\u5904\u7406\u547d\u4ee4\uff0c\u901a\u8fc7\u5de5\u5177\u611f\u77e5\u7684RAG\u548c\u8fed\u4ee3\u81ea\u53cd\u601d\u5728\u8fb9\u7f18\u672c\u5730\u6267\u884cFFmpeg\u548cVVenC\u547d\u4ee4\uff0c\u6d88\u9664\u5bf9\u4e91API\u7684\u4f9d\u8d56\u3002", "motivation": "\u57fa\u4e8e\u4e91\u7684LLM\u90e8\u7f72\u9762\u4e34\u9ad8\u8ba1\u7b97\u80fd\u8017\u3001\u9690\u79c1\u98ce\u9669\u3001\u53ef\u9760\u6027\u95ee\u9898\u548c\u6301\u7eedAPI\u6210\u672c\u7b49\u9650\u5236\u3002\u4ee3\u7406\u5f0fAI\u7684\u8fdb\u6b65\u4e3a\u5229\u7528\u672c\u5730\u90e8\u7f72\u7684\u5de5\u5177\u548cLLM\u63d0\u4f9b\u4e86\u66f4\u597d\u65b9\u5f0f\u3002", "method": "\u96c6\u6210\u5de5\u5177\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e0e\u8fed\u4ee3\u81ea\u53cd\u601d\u673a\u5236\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u751f\u6210\u5e76\u672c\u5730\u9a8c\u8bc1\u53ef\u6267\u884c\u7684FFmpeg\u548cVVC\u7f16\u7801\u5668(VVenC)\u547d\u4ee4\u3002", "result": "Qwen2.5\u6a21\u578b\u7ed3\u5408ELLMPEG\u6846\u67b6\u5728FFmpeg\u548cVVenC\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u547d\u4ee4\u751f\u6210\u51c6\u786e\u7387\u8fbe\u523078%\uff0c\u65e0\u6301\u7eedAPI\u6210\u672c\uff0c\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u547d\u4ee4\u6709\u6548\u6027\u3001\u63a8\u7406\u65f6\u95f4\u3001\u80fd\u8017\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ELLMPEG\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u3001\u79c1\u5bc6\u3001\u4f4e\u6210\u672c\u7684\u89c6\u9891\u5904\u7406\u547d\u4ee4\u81ea\u52a8\u751f\u6210\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00007", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00007", "abs": "https://arxiv.org/abs/2602.00007", "authors": ["MinGyu Jeon", "SuWan Cho", "JaeYoung Shu"], "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.", "AI": {"tldr": "PPoGA\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u548c\u9884\u6d4b\u5904\u7406\u673a\u5236\uff0c\u5f15\u5165\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u80fd\u591f\u5728\u63a8\u7406\u8ba1\u5212\u51fa\u9519\u65f6\u8fdb\u884c\u8def\u5f84\u548c\u8ba1\u5212\u5c42\u9762\u7684\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u95ee\u7b54\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u7b54\u4e2d\uff0c\u4e00\u65e6\u521d\u59cb\u9ad8\u5c42\u63a8\u7406\u8ba1\u5212\u51fa\u9519\uff0c\u5c31\u4f1a\u9677\u5165\u529f\u80fd\u56fa\u7740\uff0c\u65e0\u6cd5\u91cd\u65b0\u8c03\u6574\u7b56\u7565\uff0c\u5bfc\u81f4\u8ffd\u6c42\u4e0d\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPPoGA\u6846\u67b6\uff0c\u91c7\u7528\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u5206\u79bb\u9ad8\u5c42\u7b56\u7565\u548c\u4f4e\u5c42\u6267\u884c\uff0c\u5229\u7528\u9884\u6d4b\u5904\u7406\u673a\u5236\u9884\u5224\u7ed3\u679c\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u81ea\u6211\u7ea0\u6b63\u673a\u5236\uff0c\u5305\u62ec\u8def\u5f84\u4fee\u6b63\uff08\u5c40\u90e8\u6267\u884c\u9519\u8bef\uff09\u548c\u8ba1\u5212\u4fee\u6b63\uff08\u8bc6\u522b\u3001\u4e22\u5f03\u5e76\u91cd\u65b0\u5236\u5b9a\u6574\u4e2a\u8ba1\u5212\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u57fa\u51c6\uff08GrailQA\u3001CWQ\u3001WebQSP\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPPoGA\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u50cf\u95ee\u9898\u91cd\u6784\u8fd9\u6837\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u5bf9\u4e8e\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684AI\u63a8\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.00095", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00095", "abs": "https://arxiv.org/abs/2602.00095", "authors": ["Weiyu Sun", "Liangliang Chen", "Yongnuo Cai", "Huiru Xie", "Yi Zeng", "Ying Zhang"], "title": "EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86EDU-CIRCUIT-HW\u6570\u636e\u96c6\uff0c\u5305\u542b1300+\u4efd\u771f\u5b9e\u5927\u5b66\u751f\u624b\u5199STEM\u4f5c\u4e1a\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u624b\u5199\u5185\u5bb9\u8bc6\u522b\u548c\u81ea\u52a8\u8bc4\u5206\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u5927\u91cf\u6f5c\u5728\u9519\u8bef\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u8bc6\u522b\u9519\u8bef\u6a21\u5f0f\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u9886\u57df\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u3001\u9886\u57df\u7279\u5b9a\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u590d\u6742\u624b\u5199STEM\u89e3\u7b54\uff08\u5305\u542b\u6570\u5b66\u516c\u5f0f\u3001\u56fe\u8868\u548c\u6587\u672c\u63a8\u7406\uff09\u7684\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u81ea\u52a8\u8bc4\u5206\uff09\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5bf9\u624b\u5199\u903b\u8f91\u7684\u6574\u4f53\u7406\u89e3\u3002", "method": "\u53d1\u5e03EDU-CIRCUIT-HW\u6570\u636e\u96c6\uff0c\u5305\u542b1300+\u4efd\u5927\u5b66STEM\u8bfe\u7a0b\u7684\u771f\u5b9e\u5b66\u751f\u624b\u5199\u89e3\u7b54\u3002\u5229\u7528\u4e13\u5bb6\u9a8c\u8bc1\u7684\u9010\u5b57\u8f6c\u5f55\u548c\u8bc4\u5206\u62a5\u544a\uff0c\u540c\u65f6\u8bc4\u4f30\u5404\u79cdMLLM\u7684\u4e0a\u6e38\u8bc6\u522b\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u81ea\u52a8\u8bc4\u5206\u6027\u80fd\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5982\u4f55\u5229\u7528\u8bc6\u522b\u51fa\u7684\u9519\u8bef\u6a21\u5f0f\u6765\u9884\u5148\u68c0\u6d4b\u548c\u7ea0\u6b63\u8bc6\u522b\u9519\u8bef\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86MLLM\u8bc6\u522b\u5b66\u751f\u624b\u5199\u5185\u5bb9\u4e2d\u5b58\u5728\u60ca\u4eba\u7684\u6f5c\u5728\u9519\u8bef\u89c4\u6a21\uff0c\u8868\u660e\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u6559\u80b2\u573a\u666f\u4e2d\u7528\u4e8e\u81ea\u52a8\u8bc4\u5206\u548c\u5176\u4ed6\u7406\u89e3\u5bfc\u5411\u5e94\u7528\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002\u901a\u8fc7\u8bc6\u522b\u9519\u8bef\u6a21\u5f0f\u5e76\u4ec5\u9700\u7ea64%\u7684\u4eba\u5de5\u5e72\u9884\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u8bc4\u5206\u7cfb\u7edf\u5728\u672a\u89c1\u5b66\u751f\u89e3\u7b54\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f53\u524dMLLM\u5728\u5904\u7406\u590d\u6742\u624b\u5199STEM\u89e3\u7b54\u65f6\u5b58\u5728\u4e25\u91cd\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4f46\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u7ea0\u6b63\u9519\u8bef\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5728\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u4e0b\u663e\u8457\u63d0\u5347\u6559\u80b2AI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.00244", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00244", "abs": "https://arxiv.org/abs/2602.00244", "authors": ["Markus Bambach", "Shaoshuai Chu", "Michael Herty", "Yunong Lin"], "title": "A Bayesian Approach to Feedback Control for Hyperbolic Balance Laws", "comment": null, "summary": "We propose a Bayesian framework for feedback boundary control for hyperbolic balance laws. The method propagates a probability distribution over feedback parameters by using Lyapunov decay estimates as a likelihood. In the linear setting, the framework recovers the available analytical results, while simultaneously extending them to nonlinear regimes where such results are not readily accessible. We first validate the method using the first-order local Lax-Friedrichs (LLF) discretizations on linear models -- the decoupled wave system and the linearized Saint-Venant equations -- recovering the known stability intervals and mixed boundary couplings reported in the control literature. We then consider nonlinear and stochastic settings, including the nonlinear Saint-Venant system and Burgers equation with random initial data, as well as a nonconservative perturbation with source terms, and demonstrate that the computed stability domains remain accurate and robust with respect to the choice of indicator and the initial prior. We further show that the methodology carries over to a second-order semi-discrete LLF scheme and to a model with two interacting control parameters for the temperature field development in laser powder bed fusion with feedback power regulation. These numerical experiments confirm consistency with available theory on benchmark cases and highlight the practicality of the proposed, discretization-agnostic feedback selection procedure.", "AI": {"tldr": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\u7528\u4e8e\u53cc\u66f2\u5e73\u8861\u5f8b\u7684\u53cd\u9988\u8fb9\u754c\u63a7\u5236\uff0c\u5229\u7528Lyapunov\u8870\u51cf\u4f30\u8ba1\u4f5c\u4e3a\u4f3c\u7136\u51fd\u6570\u4f20\u64ad\u53cd\u9988\u53c2\u6570\u7684\u6982\u7387\u5206\u5e03\uff0c\u5728\u975e\u7ebf\u6027\u968f\u673a\u73af\u5883\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u53cd\u9988\u8fb9\u754c\u63a7\u5236\u65b9\u6cd5\u5728\u7ebf\u6027\u7cfb\u7edf\u4e2d\u6709\u89e3\u6790\u7ed3\u679c\uff0c\u4f46\u5728\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u6709\u6548\u7684\u7a33\u5b9a\u6027\u5206\u6790\u5de5\u5177\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u7ebf\u6027\u6548\u5e94\u7684\u901a\u7528\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5c06Lyapunov\u8870\u51cf\u4f30\u8ba1\u4f5c\u4e3a\u4f3c\u7136\u51fd\u6570\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u4f20\u64ad\u53cd\u9988\u53c2\u6570\uff1b\u4f7f\u7528\u5c40\u90e8Lax-Friedrichs\u79bb\u6563\u5316\uff0c\u5728\u7ebf\u6027\u3001\u975e\u7ebf\u6027\u3001\u968f\u673a\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff1b\u6269\u5c55\u5230\u4e8c\u9636\u534a\u79bb\u6563\u65b9\u6848\u548c\u6fc0\u5149\u7c89\u672b\u5e8a\u7194\u878d\u6e29\u5ea6\u573a\u63a7\u5236\u3002", "result": "\u5728\u7ebf\u6027\u6a21\u578b\u4e2d\u6062\u590d\u5df2\u77e5\u7a33\u5b9a\u6027\u533a\u95f4\u548c\u6df7\u5408\u8fb9\u754c\u8026\u5408\uff1b\u5728\u975e\u7ebf\u6027Saint-Venant\u7cfb\u7edf\u3001\u968f\u673aBurgers\u65b9\u7a0b\u548c\u975e\u4fdd\u5b88\u6270\u52a8\u4e2d\uff0c\u8ba1\u7b97\u51fa\u7684\u7a33\u5b9a\u6027\u57df\u4fdd\u6301\u51c6\u786e\u9c81\u68d2\uff1b\u65b9\u6cd5\u6269\u5c55\u5230\u4e8c\u9636\u65b9\u6848\u548c\u53cc\u53c2\u6570\u63a7\u5236\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u4e3a\u53cc\u66f2\u5e73\u8861\u5f8b\u53cd\u9988\u8fb9\u754c\u63a7\u5236\u63d0\u4f9b\u4e86\u79bb\u6563\u5316\u65e0\u5173\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u4e2d\u5747\u6709\u6548\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00053", "abs": "https://arxiv.org/abs/2602.00053", "authors": ["Ratul Ali"], "title": "Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes", "comment": "2 pages, 2 figures, 1 table", "summary": "Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.", "AI": {"tldr": "\u6bd4\u8f83FastAPI\u4e0eNVIDIA Triton\u5728\u533b\u7597AI\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\uff1aFastAPI\u5355\u8bf7\u6c42\u5ef6\u8fdf\u66f4\u4f4e\uff0822ms\uff09\uff0cTriton\u901a\u8fc7\u52a8\u6001\u6279\u5904\u7406\u5b9e\u73b0\u66f4\u9ad8\u541e\u5410\u91cf\uff08780 RPS\uff09\uff0c\u6df7\u5408\u67b6\u6784\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "motivation": "\u533b\u7597\u548c\u5236\u836f\u7b49\u53d7\u76d1\u7ba1\u9886\u57df\u9700\u8981\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u65b9\u6848\uff0c\u5fc5\u987b\u5e73\u8861\u5b9e\u65f6\u63a8\u7406\u5ef6\u8fdf\u3001\u6279\u91cf\u5904\u7406\u541e\u5410\u91cf\u548c\u6570\u636e\u9690\u79c1\u5408\u89c4\u6027\uff08\u5982HIPAA\uff09\u7b49\u591a\u91cd\u9700\u6c42", "method": "\u4f7f\u7528Kubernetes\u90e8\u7f72DistilBERT\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u5728\u53d7\u63a7\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u5bf9\u6bd4\u4e24\u79cd\u90e8\u7f72\u8303\u5f0f\uff1a\u57fa\u4e8eFastAPI\u7684\u8f7b\u91cf\u7ea7REST\u670d\u52a1\u548cNVIDIA Triton\u9ad8\u6027\u80fd\u63a8\u7406\u670d\u52a1\u5668\uff0c\u6d4b\u91cfp50/p95\u5ef6\u8fdf\u548c\u541e\u5410\u91cf", "result": "FastAPI\u5728\u5355\u8bf7\u6c42\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8868\u73b0\u66f4\u4f73\uff08p50\u5ef6\u8fdf22ms\uff09\uff0cTriton\u901a\u8fc7\u52a8\u6001\u6279\u5904\u7406\u5b9e\u73b0\u5353\u8d8a\u53ef\u6269\u5c55\u6027\uff08\u5355T4 GPU\u541e\u5410\u91cf780 RPS\uff0c\u662f\u57fa\u7ebf\u7684\u8fd1\u4e24\u500d\uff09\u3002\u6df7\u5408\u67b6\u6784\uff08FastAPI\u4f5c\u4e3a\u5b89\u5168\u7f51\u5173\u5904\u7406PHI\u53bb\u6807\u8bc6\u5316\uff0cTriton\u8d1f\u8d23\u540e\u7aef\u63a8\u7406\uff09\u88ab\u9a8c\u8bc1\u4e3a\u6700\u4f73\u5b9e\u8df5", "conclusion": "\u6df7\u5408\u67b6\u6784\u7ed3\u5408\u4e86FastAPI\u7684\u4f4e\u5ef6\u8fdf\u4f18\u52bf\u548cTriton\u7684\u9ad8\u541e\u5410\u91cf\u80fd\u529b\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u9ad8\u53ef\u7528\u7684\u4f01\u4e1a\u7ea7\u90e8\u7f72\u84dd\u56fe\uff0c\u662f\u4e34\u5e8aAI\u7cfb\u7edf\u7684\u6700\u4f73\u5b9e\u8df5\u65b9\u6848"}}
{"id": "2602.00275", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.00275", "abs": "https://arxiv.org/abs/2602.00275", "authors": ["Helene Papillon-Laroche", "Amishga Alphonius", "Magdalena Schreter-Fleischhacker", "Jean-Philippe Harvey", "Bruno Blais"], "title": "Geometric Reinitialization for Capillary Flows: a Comparative Study with State-of-the-Art Conservative Level-Set Methods", "comment": null, "summary": "Simulations of immiscible flows involving surface tension (ST) require a robust high-fidelity framework. State-of-the-art multi-phase models, such as the Conservative Level-Set (CLS) approach, rely on Eulerian representations of the fluids and interface and require reinitialization methods to ensure volume conservation and accurate ST force modeling. This work focuses on the complete description of a CLS solver and proposes a novel geometric reinitialization method, based on the level-set literature. It includes a quantitative and objective comparison of this new geometric method to two reinitialization approaches: the PDE-based reinitialization proposed in the original CLS method and a simple projection-based approach. This comparison tackles three 3D application cases: the rise of a bubble, the capillary migration of a droplet, and the Rayleigh-Plateau instability development in a capillary jet. The PDE-based and geometric methods lead to high-quality, spatially-converged results in good agreement with benchmark and analytic solutions, while the projection-based reinitialization fails to capture complex 3D interfacial dynamics. The results also highlight the robustness of the novel geometric method which offers a two-parameter framework in comparison to the PDE-based method that necessitates a case-dependent selection of four parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u7684\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4fdd\u5b88\u6c34\u5e73\u96c6\u591a\u76f8\u6d41\u6a21\u62df\uff0c\u5e76\u4e0ePDE\u65b9\u6cd5\u548c\u6295\u5f71\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5728\u4e09\u7ef4\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4fdd\u5b88\u6c34\u5e73\u96c6\u65b9\u6cd5\u4f9d\u8d56\u91cd\u65b0\u521d\u59cb\u5316\u6765\u4fdd\u8bc1\u4f53\u79ef\u5b88\u6052\u548c\u8868\u9762\u5f20\u529b\u5efa\u6a21\u7cbe\u5ea6\uff0c\u4f46\u4f20\u7edfPDE\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u6848\u4f8b\u4f9d\u8d56\u53c2\u6570\uff0c\u800c\u6295\u5f71\u65b9\u6cd5\u5728\u590d\u6742\u4e09\u7ef4\u754c\u9762\u52a8\u529b\u5b66\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c34\u5e73\u96c6\u6587\u732e\u7684\u51e0\u4f55\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4e24\u79cd\u73b0\u6709\u65b9\u6cd5\uff08\u539f\u59cbCLS\u7684PDE\u65b9\u6cd5\u548c\u7b80\u5355\u6295\u5f71\u65b9\u6cd5\uff09\u8fdb\u884c\u5b9a\u91cf\u5ba2\u89c2\u6bd4\u8f83\uff0c\u5728\u4e09\u4e2a\u4e09\u7ef4\u5e94\u7528\u6848\u4f8b\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "PDE\u65b9\u6cd5\u548c\u51e0\u4f55\u65b9\u6cd5\u90fd\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u3001\u7a7a\u95f4\u6536\u655b\u7684\u7ed3\u679c\uff0c\u4e0e\u57fa\u51c6\u548c\u89e3\u6790\u89e3\u543b\u5408\u826f\u597d\uff1b\u800c\u6295\u5f71\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u590d\u6742\u4e09\u7ef4\u754c\u9762\u52a8\u529b\u5b66\u3002\u51e0\u4f55\u65b9\u6cd5\u4ec5\u9700\u4e24\u4e2a\u53c2\u6570\uff0c\u6bd4\u9700\u8981\u56db\u4e2a\u6848\u4f8b\u4f9d\u8d56\u53c2\u6570\u7684PDE\u65b9\u6cd5\u66f4\u9c81\u68d2\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\u5728\u4fdd\u5b88\u6c34\u5e73\u96c6\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u63d0\u4f9b\u66f4\u9c81\u68d2\u3001\u53c2\u6570\u66f4\u5c11\u7684\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u590d\u6742\u4e09\u7ef4\u754c\u9762\u52a8\u529b\u5b66\u3002"}}
{"id": "2602.00030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00030", "abs": "https://arxiv.org/abs/2602.00030", "authors": ["Takato Yasuno"], "title": "RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making", "comment": "4 figures, 3 tables", "summary": "Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u9053\u4e3b\u4e49\u63f4\u52a9\u548c\u6551\u707e\u7684\u667a\u80fdRAG\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u5e93\u548c\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\uff0c\u652f\u6301\u707e\u5bb3\u54cd\u5e94\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u521d\u671f\u6551\u63f4\u3001\u4e2d\u671f\u6062\u590d\u548c\u957f\u671f\u91cd\u5efa\u3002", "motivation": "\u6709\u6548\u7684\u4eba\u9053\u4e3b\u4e49\u63f4\u52a9\u548c\u6551\u707e\u9700\u8981\u5feb\u901f\u7684\u60c5\u5883\u7406\u89e3\u3001\u53ef\u9760\u7684\u51b3\u7b56\u652f\u6301\uff0c\u4ee5\u53ca\u80fd\u591f\u6cdb\u5316\u5230\u591a\u6837\u4e14\u672a\u89c1\u8fc7\u7684\u707e\u5bb3\u573a\u666f\u7684\u80fd\u529b\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u548c\u81ea\u9002\u5e94\u8de8\u707e\u5bb3\u573a\u666f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "1. \u6784\u5efa\u5206\u5c42\u77e5\u8bc6\u5e93\uff0c\u6574\u5408\u6587\u672c\u707e\u5bb3\u624b\u518c\u3001\u5386\u53f2\u7ecf\u9a8c\u6559\u8bad\uff08\u59822011\u5e74\u4e1c\u5317\u5730\u9707\uff09\u4ee5\u53ca\u7a7a\u4e2d\u548c\u5730\u9762\u56fe\u50cf\uff1b2. \u57fa\u4e8e\u5f00\u6e90\u591a\u6a21\u6001\u5b9e\u73b0\uff0c\u5904\u740646\u4e2a\u6d77\u5578\u76f8\u5173PDF\uff082378\u9875\uff09\uff0c\u4f7f\u7528BLIP\u56fe\u50cf\u63cf\u8ff0\u3001ColVBERT\u5d4c\u5165\u548c\u957f\u4e0a\u4e0b\u6587\u6458\u8981\u751f\u6210\u9ad8\u6548\u7ed3\u6784\u5316\u591a\u6a21\u6001\u68c0\u7d22\u6811\uff1b3. \u667a\u80fd\u63a7\u5236\u5668\u901a\u8fc7\u71b5\u611f\u77e5\u573a\u666f\u62bd\u8c61\u52a8\u6001\u9009\u62e9\u68c0\u7d22\u7b56\u7565\uff08\u5982RAPTOR\u3001ColBERT\uff09\uff1b4. \u8f7b\u91cf\u7ea7LoRA\u540e\u8bad\u7ec3\u65b9\u6cd5\u6ce8\u5165\u5386\u53f2\u707e\u5bb3\u7ecf\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728\u771f\u5b9e\u707e\u5bb3\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u60c5\u5883\u7406\u89e3\u3001\u4efb\u52a1\u5206\u89e3\u51c6\u786e\u6027\u548c\u5e94\u6025\u64cd\u4f5c\u53ef\u7528\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0e\u81ea\u63a8\u7406\u548c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u80fd\u529b\u5b9e\u73b0\u4e86\u5b9e\u8d28\u6027\u6539\u8fdb\u3002", "conclusion": "\u8be5\u667a\u80fdRAG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u77e5\u8bc6\u3001\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u548c\u7ecf\u9a8c\u77e5\u8bc6\u6ce8\u5165\uff0c\u4e3a\u707e\u5bb3\u54cd\u5e94\u7684\u4e09\u4e2a\u9636\u6bb5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\uff0c\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u6551\u63f4\u4eba\u5458\uff0c\u63d0\u9ad8\u5e94\u6025\u54cd\u5e94\u6548\u7387\u3002"}}
{"id": "2602.00009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00009", "abs": "https://arxiv.org/abs/2602.00009", "authors": ["Samuel Thio", "Matthew Lewis", "Spiros Denaxas", "Richard JB Dobson"], "title": "Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA", "comment": "26 pages, 5 figures, 2 tables", "summary": "Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.", "AI": {"tldr": "MediGRAF\u662f\u4e00\u4e2a\u6df7\u5408\u56fe\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u56fe\u67e5\u8be2\u548c\u975e\u7ed3\u6784\u5316\u8bed\u4e49\u641c\u7d22\uff0c\u5b9e\u73b0\u4e34\u5e8a\u7535\u5b50\u75c5\u5386\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u9ad8\u4fe1\u606f\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7535\u5b50\u75c5\u5386\u7cfb\u7edf\u4fe1\u606f\u8fc7\u8f7d\u5bfc\u81f4\u5173\u952e\u7ec6\u8282\u5bb9\u6613\u88ab\u5ffd\u7565\uff0c\u800c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b58\u5728\u4e0a\u4e0b\u6587\u57fa\u7840\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u5f53\u524d\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u6216\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u65e0\u6cd5\u540c\u65f6\u6574\u5408\u4e24\u8005\u3002", "method": "\u63d0\u51faMediGRAF\u6df7\u5408\u56feRAG\u7cfb\u7edf\uff0c\u7ed3\u5408Neo4j Text2Cypher\u8fdb\u884c\u7ed3\u6784\u5316\u5173\u7cfb\u904d\u5386\u548c\u5411\u91cf\u5d4c\u5165\u8fdb\u884c\u975e\u7ed3\u6784\u5316\u53d9\u4e8b\u68c0\u7d22\uff0c\u4f7f\u7528MIMIC-IV\u6570\u636e\u96c6\u4e2d\u768410\u540d\u60a3\u8005\u6570\u636e\uff085,973\u4e2a\u8282\u70b9\u548c5,963\u4e2a\u5173\u7cfb\uff09\u6784\u5efa\u60a3\u8005\u65c5\u7a0b\u56fe\u3002", "result": "\u4e8b\u5b9e\u67e5\u8be2\u5b9e\u73b0100%\u53ec\u56de\u7387\uff0c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u4e13\u5bb6\u8d28\u91cf\u5f97\u52064.25/5\uff08\u6ee1\u52065\u5206\uff09\uff0c\u4e14\u96f6\u5b89\u5168\u8fdd\u89c4\u3002\u6df7\u5408\u56fe\u57fa\u7840\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u90e8\u7f72\u3002", "conclusion": "\u6df7\u5408\u56fe\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\u4e3a\u4e34\u5e8a\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u6574\u5408\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002"}}
{"id": "2602.00096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00096", "abs": "https://arxiv.org/abs/2602.00096", "authors": ["Zhengqing Gao", "Ziwen Li", "Xin Wang", "Jiaxin Huang", "Zhenyang Ren", "Mingkai Shao", "Hanlue Zhang", "Tianyu Huang", "Yongkang Cheng", "Yandong Guo", "Runqi Lin", "Yuanyuan Wang", "Tongliang Liu", "Kun Zhang", "Mingming Gong"], "title": "Mirage2Matter: A Physically Grounded Gaussian World Model from Video", "comment": null, "summary": "The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.", "AI": {"tldr": "Simulate Anything\uff1a\u57fa\u4e8e\u591a\u89c6\u89d2\u89c6\u9891\u548c\u73b0\u6210\u8d44\u4ea7\u7684\u56fe\u5f62\u9a71\u52a8\u4e16\u754c\u5efa\u6a21\u4e0e\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u771f\u5b9e\u73af\u5883\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5177\u8eab\u8bad\u7ec3\u6570\u636e\uff0c\u4f7fVLA\u6a21\u578b\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u6548\u679c", "motivation": "\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u53ef\u6269\u5c55\u6027\u7684\u6839\u672c\u74f6\u9888\u2014\u2014\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u548c\u7269\u7406\u5dee\u8ddd\uff0c\u4f9d\u8d56\u6602\u8d35\u4f20\u611f\u5668\u3001\u7cbe\u786e\u673a\u5668\u4eba\u6821\u51c6\u6216\u6df1\u5ea6\u6d4b\u91cf\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u5b9e\u7528\u6027", "method": "1) \u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u4ece\u591a\u89c6\u89d2\u89c6\u9891\u91cd\u5efa\u771f\u5b9e\u73af\u5883\u4e3a\u7167\u7247\u7ea7\u771f\u5b9e\u573a\u666f\u8868\u793a\uff1b2) \u5229\u7528\u751f\u6210\u6a21\u578b\u6062\u590d\u7269\u7406\u771f\u5b9e\u8868\u793a\uff1b3) \u901a\u8fc7\u7cbe\u5ea6\u6821\u51c6\u76ee\u6807\u5c06\u91cd\u5efa\u573a\u666f\u96c6\u6210\u5230\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u51c6\u786e\u5c3a\u5ea6\u5bf9\u9f50\uff1b4) \u6784\u5efa\u7edf\u4e00\u3001\u53ef\u7f16\u8f91\u3001\u7269\u7406\u57fa\u7840\u7684\u4e16\u754c\u6a21\u578b", "result": "\u57fa\u4e8e\u4eff\u771f\u6570\u636e\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u83b7\u5f97\u7684\u7ed3\u679c", "conclusion": "\u91cd\u5efa\u9a71\u52a8\u7684\u4e16\u754c\u5efa\u6a21\u4e3a\u53ef\u6269\u5c55\u548c\u5b9e\u7528\u7684\u5177\u8eab\u667a\u80fd\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4ec5\u9700\u591a\u89c6\u89d2\u73af\u5883\u89c6\u9891\u548c\u73b0\u6210\u8d44\u4ea7\u5373\u53ef\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u5177\u8eab\u8bad\u7ec3\u6570\u636e"}}
{"id": "2602.00264", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00264", "abs": "https://arxiv.org/abs/2602.00264", "authors": ["Minh-Binh Tran", "Bangjie Wang"], "title": "Analysis of a numerical scheme for 3-wave kinetic equations", "comment": null, "summary": "Several numerical schemes for 3-wave kinetic equations have been proposed in recent work and shown to be accurate and computationally efficient [8,33,34,35]. However, their rigorous numerical analysis remains open. This paper aims to close this gap. We establish a comprehensive well-posedness and qualitative theory for the\n  discrete equation arising from those schemes. We prove\n  global existence, uniqueness, and Lipschitz stability of nonnegative classical solutions\n  in $\\ell^1(\\mathbb{N})$, together with uniform bounds and decay of moments. We further\n  show exponential energy decay and a sharp creation and propagation of positivity\n  characterized by the arithmetic structure of the initial support. Finally, we obtain\n  the propagation and instantaneous creation of polynomial, Mittag-Leffler, and\n  exponential moments, providing quantitative control of high energy tails. We validate the theoretical findings by numerical results.", "AI": {"tldr": "\u672c\u6587\u4e3a3\u6ce2\u52a8\u529b\u5b66\u65b9\u7a0b\u79bb\u6563\u683c\u5f0f\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6570\u503c\u5206\u6790\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u5168\u5c40\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u3001\u7a33\u5b9a\u6027\u4ee5\u53ca\u6b63\u6027\u4f20\u64ad\u7b49\u6027\u8d28", "motivation": "\u867d\u7136\u5df2\u6709\u591a\u4e2a3\u6ce2\u52a8\u529b\u5b66\u65b9\u7a0b\u7684\u6570\u503c\u683c\u5f0f\u88ab\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u503c\u5206\u6790\u7406\u8bba\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u5efa\u7acb\u79bb\u6563\u65b9\u7a0b\u7684\u9002\u5b9a\u6027\u548c\u5b9a\u6027\u7406\u8bba\uff0c\u5728\u2113\u00b9(\u2115)\u7a7a\u95f4\u4e2d\u8bc1\u660e\u975e\u8d1f\u7ecf\u5178\u89e3\u7684\u5168\u5c40\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\u548cLipschitz\u7a33\u5b9a\u6027\uff0c\u5206\u6790\u77e9\u7684\u4e00\u81f4\u6709\u754c\u6027\u548c\u8870\u51cf\u6027", "result": "\u8bc1\u660e\u4e86\u6307\u6570\u80fd\u91cf\u8870\u51cf\u3001\u57fa\u4e8e\u521d\u59cb\u652f\u6491\u7b97\u672f\u7ed3\u6784\u7684\u6b63\u6027\u521b\u5efa\u4e0e\u4f20\u64ad\u3001\u591a\u9879\u5f0f\u77e9\u3001Mittag-Leffler\u77e9\u548c\u6307\u6570\u77e9\u7684\u4f20\u64ad\u4e0e\u77ac\u65f6\u521b\u5efa\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u7406\u8bba\u53d1\u73b0", "conclusion": "\u672c\u6587\u4e3a3\u6ce2\u52a8\u529b\u5b66\u65b9\u7a0b\u7684\u6570\u503c\u683c\u5f0f\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u503c\u5206\u6790\u6846\u67b6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u540e\u7eed\u6570\u503c\u6a21\u62df\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.00188", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00188", "abs": "https://arxiv.org/abs/2602.00188", "authors": ["Srividhya Sethuraman", "Chandrashekar Lakshminarayanan"], "title": "Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets", "comment": "Accepted in AAMAS 2026 - main track - full paper - 12 pages", "summary": "Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \\emph{Additive Feature Decomposition-based Low-Dimensional Demand (\\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \\textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\\tilde{\\mathcal{O}}(\\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.", "AI": {"tldr": "\u63d0\u51faAFDLD\u53ef\u89e3\u91ca\u9700\u6c42\u6a21\u578b\u548cADEPT\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u52a8\u6001\u5b9a\u4ef7\u4e2d\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u6548\u6027", "motivation": "\u73b0\u6709\u4f4e\u79e9bandit\u65b9\u6cd5\u867d\u7136\u5b66\u4e60\u6548\u7387\u9ad8\uff0c\u4f46\u4f9d\u8d56\u6f5c\u5728\u7279\u5f81\uff0c\u96be\u4ee5\u89e3\u91ca\u4ea7\u54c1\u5c5e\u6027\u5982\u4f55\u5f71\u54cd\u5b9a\u4ef7\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898", "method": "\u63d0\u51faAFDLD\u53ef\u89e3\u91ca\u9700\u6c42\u6a21\u578b\uff08\u4ea7\u54c1\u4ef7\u683c\u8868\u793a\u4e3a\u5c5e\u6027\u7ea7\u8d21\u732e\u4e4b\u548c\uff0c\u663e\u5f0f\u5efa\u6a21\u66ff\u4ee3\u6548\u5e94\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1ADEPT\u7b97\u6cd5\uff08\u65e0\u6295\u5f71\u3001\u65e0\u68af\u5ea6\u3001\u76f4\u63a5\u5728\u5c5e\u6027\u7a7a\u95f4\u64cd\u4f5c\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff09", "result": "ADEPT\u5b9e\u73b0\u4e9a\u7ebf\u6027\u9057\u61be\u754c$\\tilde{\\mathcal{O}}(\\sqrt{d}T^{3/4})$\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u80fd\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u4ef7\u683c\u3001\u5feb\u901f\u9002\u5e94\u5e02\u573a\u51b2\u51fb\u548c\u6f02\u79fb\u3001\u63d0\u4f9b\u900f\u660e\u7684\u5c5e\u6027\u7ea7\u4ef7\u683c\u89e3\u91ca", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u5c5e\u6027\u9a71\u52a8\u8868\u793a\uff0c\u53ef\u4ee5\u5728\u81ea\u4e3b\u5b9a\u4ef7\u4ee3\u7406\u4e2d\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387"}}
{"id": "2602.00378", "categories": ["physics.flu-dyn", "cs.LG", "physics.ao-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.00378", "abs": "https://arxiv.org/abs/2602.00378", "authors": ["Md Amran Hossan Mojamder", "Zhihang Xu", "Min Wang", "Ilya Timofeyev"], "title": "Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting", "comment": null, "summary": "We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6d45\u6c34\u65b9\u7a0b\u4e2d\u7684\u6b21\u7f51\u683c\u901a\u91cf\uff0c\u5b9e\u73b0\u5c40\u90e8\u53c2\u6570\u5316\uff0c\u63d0\u9ad8\u957f\u671f\u6e4d\u6d41\u6a21\u62df\u7684\u80fd\u91cf\u5e73\u8861\u548c\u7cbe\u5ea6\u3002", "motivation": "\u6d45\u6c34\u65b9\u7a0b\u4e2d\u7684\u6b21\u7f51\u683c\u8fc7\u7a0b\u9700\u8981\u53c2\u6570\u5316\uff0c\u4f20\u7edf\u5168\u5c40\u8026\u5408\u53c2\u6570\u5316\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u3001\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5c40\u90e8\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u7c97\u7c92\u5ea6\u53d8\u91cf\u548c\u5c40\u90e8\u7a7a\u95f4\u5e73\u5747\uff0c\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6b21\u7f51\u683c\u901a\u91cf\uff0c\u6784\u5efa\u56db\u70b9\u8ba1\u7b97\u6a21\u677f\u7684\u5c40\u90e8\u53c2\u6570\u5316\u65b9\u6848\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u6539\u5584\u957f\u671f\u6e4d\u6d41\u6a21\u62df\u7684\u80fd\u91cf\u5e73\u8861\uff0c\u51c6\u786e\u518d\u73b0\u4e2a\u4f53\u89e3\uff0c\u53ef\u4e0e\u901a\u91cf\u9650\u5236\u7ed3\u5408\u51cf\u5c11\u6fc0\u6ce2\u9644\u8fd1\u632f\u8361\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u672a\u8986\u76d6\u7684\u52a8\u6001\u533a\u57df\u4e5f\u80fd\u63d0\u4f9b\u53ef\u9760\u53c2\u6570\u5316\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u5c40\u90e8\u53c2\u6570\u5316\u65b9\u6cd5\u4f18\u4e8e\u5168\u5c40\u8026\u5408\u53c2\u6570\u5316\uff0c\u5728\u6d45\u6c34\u65b9\u7a0b\u6b21\u7f51\u683c\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.00040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00040", "abs": "https://arxiv.org/abs/2602.00040", "authors": ["Haonan Shi", "Dehua Shuai", "Liming Wang", "Xiyang Liu", "Long Tian"], "title": "Enhancing few-shot time series forecasting with LLM-guided diffusion", "comment": null, "summary": "Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.", "AI": {"tldr": "\u63d0\u51faLTSM-DIFF\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898", "motivation": "\u4e13\u4e1a\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5e38\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f20\u7edf\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u624d\u80fd\u6709\u6548\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u5f80\u5f80\u6709\u9650", "method": "LTSM-DIFF\u6846\u67b6\uff1a1) LTSM\u6a21\u5757\u4f5c\u4e3a\u65f6\u95f4\u8bb0\u5fc6\u673a\u5236\uff0c\u5fae\u8c03\u540e\u63d0\u53d6\u4e30\u5bcc\u5e8f\u5217\u8868\u793a\uff1b2) \u6269\u6563\u6a21\u578b\u4ee5\u8fd9\u4e9b\u8868\u793a\u4e3a\u6761\u4ef6\u6307\u5bfc\uff0c\u8fdb\u884c\u8054\u5408\u6982\u7387\u6269\u6563\u8fc7\u7a0b\uff0c\u7cbe\u7ec6\u5efa\u6a21\u590d\u6742\u65f6\u95f4\u6a21\u5f0f", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLTSM-DIFF\u5728\u6570\u636e\u4e30\u5bcc\u573a\u666f\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u5c0f\u6837\u672c\u9884\u6d4b\u4e2d\u663e\u8457\u6539\u8fdb\uff0c\u5b9e\u73b0\u4ece\u8bed\u8a00\u9886\u57df\u5230\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u7684\u77e5\u8bc6\u8fc1\u79fb", "conclusion": "\u5efa\u7acb\u4e86\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2602.00015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00015", "abs": "https://arxiv.org/abs/2602.00015", "authors": ["Xun Xu"], "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \\textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \\textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.", "AI": {"tldr": "G-MemLLM\uff1a\u4e00\u79cd\u57fa\u4e8e\u95e8\u63a7\u66f4\u65b0\u7684\u8bb0\u5fc6\u589e\u5f3aLLM\u67b6\u6784\uff0c\u901a\u8fc7\u6f5c\u5728\u8bb0\u5fc6\u5e93\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bb9\u91cf\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4e0a\u4e0b\u6587\u538b\u7f29\u6216\u5faa\u73af\u6807\u8bb0\uff09\u5e38\u51fa\u73b0\"\u4e0a\u4e0b\u6587\u8150\u5316\"\u6216\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\u3002", "method": "\u63d0\u51faG-MemLLM\u67b6\u6784\uff0c\u5c06\u51bb\u7ed3\u7684LLM\u4e3b\u5e72\u4e0e\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u8bb0\u5fc6\u5e93\u96c6\u6210\uff0c\u91c7\u7528GRU\u98ce\u683c\u7684\u95e8\u63a7\u66f4\u65b0\u903b\u8f91\uff0c\u5141\u8bb8\u6a21\u578b\u9009\u62e9\u6027\u66f4\u65b0\u3001\u4fdd\u7559\u6216\u8986\u76d6\u6f5c\u5728\u8bb0\u5fc6\u69fd\uff0c\u9632\u6b62\u5faa\u73af\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u77e5\u8bc6\u68af\u5ea6\u6d88\u5931\u3002", "result": "\u5728GPT-2 (124M)\u5230Llama 3.1 (8B)\u7684\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u5728HotpotQA\u548cZsRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aLlama 3.1-8B\u5728ZsRE\u4e0a\u51c6\u786e\u7387\u63d0\u534713.3%\uff1bGPT-2\u5728HotpotQA\u4e0a\u7b54\u6848F1\u63d0\u53478.56\u5206\uff1bLlama 3.1-8B\u5728HotpotQA\u4e0a\u652f\u6301\u4e8b\u5b9eF1\u63d0\u53476.89\u5206\u3002", "conclusion": "G-MemLLM\u901a\u8fc7\u95e8\u63a7\u8bb0\u5fc6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4fe1\u606f\u4fdd\u6301\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u80fd\u663e\u8457\u63d0\u5347\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\uff0c\u4e3a\u589e\u5f3aLLM\u7684\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.00104", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00104", "abs": "https://arxiv.org/abs/2602.00104", "authors": ["Zhuohong Chen", "Zhengxian Wu", "Zirui Liao", "Shenao Jiang", "Hangrui Xu", "Yang Chen", "Chaokui Su", "Xiaoyu Liu", "Haoqian Wang"], "title": "R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation", "comment": null, "summary": "Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.", "AI": {"tldr": "R3G\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u7684\u6a21\u5757\u5316\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406-\u68c0\u7d22-\u91cd\u6392\u5e8f\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u5148\u5236\u5b9a\u63a8\u7406\u8ba1\u5212\u660e\u786e\u6240\u9700\u89c6\u89c9\u7ebf\u7d22\uff0c\u518d\u8fdb\u884c\u4e24\u9636\u6bb5\u56fe\u50cf\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684VQA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u7684VQA\u68c0\u7d22\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u5982\u4f55\u9009\u62e9\u6b63\u786e\u7684\u56fe\u50cf\u6765\u63d0\u4f9b\u7f3a\u5931\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u8fd9\u4e9b\u56fe\u50cf\u6709\u6548\u6574\u5408\u5230\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e24\u65b9\u9762\u90fd\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faR3G\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) \u63a8\u7406\u6a21\u5757\u751f\u6210\u7b80\u8981\u63a8\u7406\u8ba1\u5212\uff0c\u660e\u786e\u6240\u9700\u89c6\u89c9\u7ebf\u7d22\uff1b2) \u68c0\u7d22\u6a21\u5757\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u5148\u7c97\u7c92\u5ea6\u68c0\u7d22\uff0c\u518d\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff1b3) \u91cd\u6392\u5e8f\u6a21\u5757\u57fa\u4e8e\u5145\u5206\u6027\u611f\u77e5\u9009\u62e9\u8bc1\u636e\u56fe\u50cf\u3002", "result": "\u5728MRAG-Bench\u4e0a\uff0cR3G\u5728\u516d\u4e2aMLLM\u9aa8\u5e72\u7f51\u7edc\u548c\u4e5d\u4e2a\u5b50\u573a\u666f\u4e2d\u90fd\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u6027\u80fd\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u5145\u5206\u6027\u611f\u77e5\u7684\u91cd\u6392\u5e8f\u548c\u63a8\u7406\u6b65\u9aa4\u662f\u4e92\u8865\u7684\uff0c\u80fd\u5e2e\u52a9\u6a21\u578b\u65e2\u9009\u62e9\u6b63\u786e\u7684\u56fe\u50cf\u53c8\u6709\u6548\u4f7f\u7528\u5b83\u4eec\u3002", "conclusion": "R3G\u901a\u8fc7\u6a21\u5757\u5316\u7684\u63a8\u7406-\u68c0\u7d22-\u91cd\u6392\u5e8f\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VQA\u4e2d\u7684\u89c6\u89c9\u68c0\u7d22\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u660e\u786e\u63a8\u7406\u8ba1\u5212\u548c\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u89c6\u89c9\u589e\u5f3a\u7684VQA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00386", "categories": ["math.NA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00386", "abs": "https://arxiv.org/abs/2602.00386", "authors": ["Micha\u0142 P. Karpowicz", "Gilbert Strang"], "title": "Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions", "comment": null, "summary": "We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine:\n  (1) the reverse order law, $A^+ = R^+C^+$, which holds when $C$ has independent columns and $R$ has independent rows,\n  (2) the universally correct formula, $A^+ = (C^+CR)^+(CRR^+)^+$, providing a geometric interpretation of the mappings between the involved subspaces,\n  (3) a new generalized randomized formula, $A^+_p = (P^TA)^+P^TAQ(AQ)^+$, which gives $A^+_p = A^+$ if and only if the sketching matrices $P$ and $Q$ preserve the rank of $A$, i.e., $\\mathrm{rank}(P^TA) = \\mathrm{rank}(AQ) = \\mathrm{rank}(A)$.\n  The framework is extended to generalized $\\{1,2\\}$-inverses and specialized forms, revealing the underlying structure of established randomized linear algebra algorithms, including randomized SVD, the Nystr\u00f6m approximation, and CUR decomposition. We demonstrate applications in sparse sensor placement and effective resistance estimation. For the latter, we provide a rigorous quantitative analysis of an approximation scheme, establishing that it always underestimates the true resistance and deriving a worst-case spectral bound on the error of resistance differences.", "AI": {"tldr": "\u672c\u6587\u4ece\u51e0\u4f55\u89d2\u5ea6\u5206\u6790\u77e9\u9635\u4e58\u79efA=CR\u7684\u5e7f\u4e49\u9006\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u7ecf\u5178\u9006\u5e8f\u5f8b\u3001\u901a\u7528\u516c\u5f0f\u548c\u65b0\u7684\u968f\u673a\u5316\u516c\u5f0f\uff0c\u5e76\u5e94\u7528\u4e8e\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u7b97\u6cd5\u3002", "motivation": "\u5efa\u7acb\u77e9\u9635\u4e58\u79ef\u5e7f\u4e49\u9006\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u51e0\u4f55\u89d2\u5ea6\u7406\u89e3\u5e7f\u4e49\u9006\u548c\u968f\u673a\u5316\u77e9\u9635\u9006\uff0c\u4e3a\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u57fa\u4e8e\u56db\u57fa\u672c\u5b50\u7a7a\u95f4\u7684\u51e0\u4f55\u5206\u6790\uff0c\u7814\u7a76\u77e9\u9635\u4e58\u79efA=CR\u7684Moore-Penrose\u4f2a\u9006\u548c\u5e7f\u4e49\u9006\uff0c\u63a8\u5bfc\u9006\u5e8f\u5f8b\u3001\u901a\u7528\u516c\u5f0f\u548c\u968f\u673a\u5316\u516c\u5f0f\uff0c\u6269\u5c55\u5230{1,2}-\u9006\u3002", "result": "\u8bc1\u660e\u4e86\u9006\u5e8f\u5f8b\u6210\u7acb\u7684\u6761\u4ef6\uff0c\u5efa\u7acb\u4e86\u901a\u7528\u516c\u5f0fA^+ = (C^+CR)^+(CRR^+)^+\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u968f\u673a\u5316\u516c\u5f0fA^+_p = (P^TA)^+P^TAQ(AQ)^+\uff0c\u5e76\u5e94\u7528\u4e8e\u968f\u673aSVD\u3001Nystr\u00f6m\u8fd1\u4f3c\u548cCUR\u5206\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u5e7f\u4e49\u9006\u548c\u968f\u673a\u5316\u77e9\u9635\u9006\u7406\u8bba\uff0c\u63ed\u793a\u4e86\u968f\u673a\u7ebf\u6027\u4ee3\u6570\u7b97\u6cd5\u7684\u5e95\u5c42\u7ed3\u6784\uff0c\u5728\u7a00\u758f\u4f20\u611f\u5668\u5e03\u7f6e\u548c\u6709\u6548\u7535\u963b\u4f30\u8ba1\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00190", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00190", "abs": "https://arxiv.org/abs/2602.00190", "authors": ["Mohit Jiwatode", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.", "AI": {"tldr": "LLMs\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\u4ece\u6e38\u620f\u8f68\u8ff9\u53cd\u63a8VGDL\u89c4\u5219\uff1a\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u548c\u5148\u63a8\u65adSCM\u518d\u8f6c\u6362\u3002SCM\u65b9\u6cd5\u6548\u679c\u66f4\u597d\uff0c\u80fd\u751f\u6210\u66f4\u63a5\u8fd1\u771f\u5b9e\u89c4\u5219\u3001\u903b\u8f91\u66f4\u4e00\u81f4\u7684VGDL\u63cf\u8ff0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4ee3\u7406\u5728\u590d\u6742\u6e38\u620f\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u4e0d\u7406\u89e3\u5e95\u5c42\u7684\u56e0\u679c\u6e38\u620f\u673a\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u56e0\u679c\u5f52\u7eb3\u80fd\u529b\u2014\u2014\u4ece\u89c2\u6d4b\u6570\u636e\u63a8\u65ad\u63a7\u5236\u89c4\u5f8b\u3002", "method": "\u4f7f\u7528LLMs\u4ece\u6e38\u620f\u8f68\u8ff9\u53cd\u63a8VGDL\u89c4\u5219\uff0c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u76f4\u63a5\u4ece\u89c2\u6d4b\u751f\u6210\u4ee3\u7801\uff1b2) \u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u63a8\u65ad\u7ed3\u6784\u56e0\u679c\u6a21\u578b(SCM)\uff0c\u518d\u8f6c\u6362\u4e3aVGDL\u3002\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u548c\u805a\u7c7b\u4eceGVGAI\u6846\u67b6\u4e2d\u9009\u62e99\u4e2a\u4ee3\u8868\u6027\u6e38\u620f\uff0c\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u548c\u63a7\u5236\u4e0a\u4e0b\u6587\u73af\u5883\u4e0b\u8bc4\u4f30\u3002", "result": "SCM\u65b9\u6cd5\u6bd4\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\u66f4\u5e38\u4ea7\u751f\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\u7684VGDL\u63cf\u8ff0\uff0c\u5728\u76f2\u8bc4\u4e2d\u83b7\u5f97\u9ad8\u8fbe81%\u7684\u504f\u597d\u80dc\u7387\uff0c\u4ea7\u751f\u66f4\u5c11\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u89c4\u5219\u3002\u5b66\u4e60\u7684SCM\u53ef\u7528\u4e8e\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u3001\u53ef\u89e3\u91ca\u4ee3\u7406\u548c\u7a0b\u5e8f\u5316\u751f\u6210\u65b0\u9896\u4f46\u903b\u8f91\u4e00\u81f4\u7684\u6e38\u620f\u3002", "conclusion": "SCM\u65b9\u6cd5\u5728\u4ece\u6e38\u620f\u8f68\u8ff9\u53cd\u63a8VGDL\u89c4\u5219\u65b9\u9762\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u56e0\u679c\u63a8\u7406\u3001\u53ef\u89e3\u91caAI\u548c\u6e38\u620f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2602.00519", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.00519", "abs": "https://arxiv.org/abs/2602.00519", "authors": ["Wichai Pattanapol", "Adam Gill"], "title": "Digital Twin Assessment of Filter Clogging Penalties in VFD-Driven Industrial Fan Systems", "comment": null, "summary": "Industrial ventilation systems equipped with variable-frequency drives (VFDs) often mask the aerodynamic impact of filter clogging by automatically increasing fan speed to maintain airflow setpoints. While effective for process stability, this control strategy creates a \"blind spot\" in energy management, leading to unmonitored power spikes. This study applies a rapid digital twin workflow to quantify these hidden energy penalties in a standard 50 kW draw-through fan room. Using a specialized computational fluid dynamics (CFD) solver (AirSketcher), the facility was modeled under \"Clean Filter\" (baseline) and \"Dirty Filter\" (clogged) scenarios. The physics engine was first validated against wind tunnel experimental data, confirming high agreement with the theoretical inertial pressure-drop law ($\u0394P \\propto U^2$). In the industrial case study, results indicate that severe clogging (modeled via a 50% effective porosity reduction) can push the fan system beyond its available pressure head or speed limits, forcing the VFD into a saturation regime. Under these conditions, effective airflow collapses by over 50% (3,806 CFM to 1,831 CFM) despite increased fan effort. The associated energy analysis predicts an annual energy penalty of 8,818 kWh ($1,058/yr). This study demonstrates how a physics-based simulation provides a defensible, ROI-driven metric for optimizing filter maintenance cycles. Keywords - industrial ventilation; digital twin; VFD optimization; filter maintenance; CFD; energy efficiency", "AI": {"tldr": "\u5de5\u4e1a\u901a\u98ce\u7cfb\u7edf\u4e2d\u53d8\u9891\u5668\u81ea\u52a8\u8c03\u8282\u98ce\u673a\u8f6c\u901f\u4ee5\u7ef4\u6301\u98ce\u91cf\u8bbe\u5b9a\u70b9\uff0c\u63a9\u76d6\u4e86\u6ee4\u7f51\u5835\u585e\u7684\u6c14\u52a8\u5f71\u54cd\uff0c\u5bfc\u81f4\u672a\u88ab\u76d1\u63a7\u7684\u80fd\u8017\u5c16\u5cf0\u3002\u7814\u7a76\u901a\u8fc7\u5feb\u901f\u6570\u5b57\u5b6a\u751f\u5de5\u4f5c\u6d41\u7a0b\u91cf\u5316\u4e86\u8fd9\u4e9b\u9690\u85cf\u7684\u80fd\u8017\u635f\u5931\u3002", "motivation": "\u5de5\u4e1a\u901a\u98ce\u7cfb\u7edf\u4f7f\u7528\u53d8\u9891\u5668\u81ea\u52a8\u8c03\u8282\u98ce\u673a\u8f6c\u901f\u6765\u7ef4\u6301\u98ce\u91cf\u8bbe\u5b9a\u70b9\uff0c\u867d\u7136\u4fdd\u8bc1\u4e86\u5de5\u827a\u7a33\u5b9a\u6027\uff0c\u4f46\u63a9\u76d6\u4e86\u6ee4\u7f51\u5835\u585e\u5bf9\u7cfb\u7edf\u6c14\u52a8\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u80fd\u8017\u7ba1\u7406\u51fa\u73b0\"\u76f2\u533a\"\uff0c\u4ea7\u751f\u672a\u88ab\u76d1\u63a7\u7684\u529f\u7387\u5c16\u5cf0\u548c\u80fd\u6e90\u6d6a\u8d39\u3002", "method": "\u91c7\u7528\u5feb\u901f\u6570\u5b57\u5b6a\u751f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u7528\u4e13\u95e8\u7684CFD\u6c42\u89e3\u5668(AirSketcher)\u5bf9\u6807\u51c6\u768450 kW\u62bd\u5438\u5f0f\u98ce\u673a\u623f\u8fdb\u884c\u5efa\u6a21\u3002\u9996\u5148\u901a\u8fc7\u98ce\u6d1e\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u7269\u7406\u5f15\u64ce\uff0c\u786e\u8ba4\u5176\u4e0e\u7406\u8bba\u60ef\u6027\u538b\u964d\u5b9a\u5f8b(\u0394P \u221d U\u00b2)\u9ad8\u5ea6\u4e00\u81f4\u3002\u7136\u540e\u6a21\u62df\"\u6e05\u6d01\u6ee4\u7f51\"(\u57fa\u51c6)\u548c\"\u810f\u6c61\u6ee4\u7f51\"(\u5835\u585e)\u4e24\u79cd\u573a\u666f\uff0c\u5176\u4e2d\u4e25\u91cd\u5835\u585e\u901a\u8fc7\u6709\u6548\u5b54\u9699\u7387\u964d\u4f4e50%\u6765\u5efa\u6a21\u3002", "result": "\u4e25\u91cd\u5835\u585e(\u6709\u6548\u5b54\u9699\u7387\u964d\u4f4e50%)\u4f1a\u4f7f\u98ce\u673a\u7cfb\u7edf\u8d85\u51fa\u5176\u53ef\u7528\u538b\u5934\u6216\u8f6c\u901f\u9650\u5236\uff0c\u8feb\u4f7f\u53d8\u9891\u5668\u8fdb\u5165\u9971\u548c\u72b6\u6001\u3002\u5728\u6b64\u6761\u4ef6\u4e0b\uff0c\u6709\u6548\u98ce\u91cf\u4e0b\u964d\u8d85\u8fc750%(\u4ece3,806 CFM\u964d\u81f31,831 CFM)\uff0c\u5c3d\u7ba1\u98ce\u673a\u52aa\u529b\u589e\u52a0\u3002\u80fd\u8017\u5206\u6790\u9884\u6d4b\u5e74\u80fd\u8017\u635f\u5931\u4e3a8,818 kWh(\u7ea61,058\u7f8e\u5143/\u5e74)\u3002", "conclusion": "\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u4e3a\u4f18\u5316\u6ee4\u7f51\u7ef4\u62a4\u5468\u671f\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5177\u6709\u6295\u8d44\u56de\u62a5\u7387\u7684\u6307\u6807\u3002\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5982\u4f55\u63ed\u793a\u5de5\u4e1a\u901a\u98ce\u7cfb\u7edf\u4e2d\u88ab\u53d8\u9891\u5668\u63a7\u5236\u7b56\u7565\u63a9\u76d6\u7684\u9690\u85cf\u80fd\u8017\u635f\u5931\uff0c\u4e3a\u80fd\u6e90\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u79d1\u5b66\u4f9d\u636e\u3002"}}
{"id": "2602.00046", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00046", "abs": "https://arxiv.org/abs/2602.00046", "authors": ["Sarthak Sattigeri"], "title": "Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy", "comment": "First Hindi sycophancy benchmark using a three-condition design separating language and cultural effects, with empirical evaluation across four instruction-tuned models", "summary": "Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u5370\u5730\u8bed\u6587\u5316\u9002\u5e94\u63d0\u793a\u4e0b\u7684\u8c04\u5a9a\u884c\u4e3a\u663e\u8457\u9ad8\u4e8e\u82f1\u8bed\uff0c\u6587\u5316\u9002\u5e94\u662f\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u800c\u975e\u8bed\u8a00\u7f16\u7801\u672c\u8eab\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u4f46\u4e0d\u786e\u5b9a\u8fd9\u79cd\u8bca\u65ad\u662f\u5426\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8c04\u5a9a\u884c\u4e3a\u5728\u8de8\u8bed\u8a00\u548c\u6587\u5316\u73af\u5883\u4e2d\u7684\u666e\u904d\u6027\u3002", "method": "\u5c06Beacon\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u8c04\u5a9a\u8bca\u65ad\u6269\u5c55\u5230\u5370\u5730\u8bed\uff0c\u91c7\u7528\u4e09\u6761\u4ef6\u8bbe\u8ba1\uff1a\u82f1\u8bed\u539f\u7248\u3001\u5370\u5730\u8bed\u76f4\u8bd1\u7248\u3001\u5370\u5730\u8bed\u6587\u5316\u9002\u5e94\u7248\u3002\u8bc4\u4f30\u56db\u4e2a\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u6bcf\u4e2a\u6761\u4ef650\u4e2a\u63d0\u793a\uff0c\u5206\u79bb\u8bed\u8a00\u7f16\u7801\u548c\u6587\u5316\u9002\u5e94\u6548\u5e94\u3002", "result": "\u6240\u6709\u6a21\u578b\u4e2d\uff0c\u6587\u5316\u9002\u5e94\u5370\u5730\u8bed\u63d0\u793a\u7684\u8c04\u5a9a\u7387\u5747\u9ad8\u4e8e\u82f1\u8bed\uff0c\u7edd\u5bf9\u5dee\u5f0212.0-16.0\u4e2a\u767e\u5206\u70b9\u3002\u5bf9Qwen 2.5-Coder-7B\u7684\u5206\u89e3\u663e\u793a\uff0c\u6587\u5316\u9002\u5e94\u8d21\u732e\u4e3b\u8981\u5dee\u5f02\uff0814.0%\uff09\uff0c\u8bed\u8a00\u7f16\u7801\u8d21\u732e\u6781\u5c0f\uff082.0%\uff09\u3002\u5efa\u8bae\u7c7b\u63d0\u793a\u7684\u8de8\u8bed\u8a00\u5dee\u5f02\u6700\u5927\uff0820-25\u4e2a\u767e\u5206\u70b9\uff09\u3002", "conclusion": "\u57fa\u4e8e\u82f1\u8bed\u6d4b\u91cf\u7684\u5bf9\u9f50\u884c\u4e3a\u4e0d\u80fd\u5747\u5300\u8de8\u8bed\u8a00\u8f6c\u79fb\uff0c\u6587\u5316\u57fa\u7840\u63d0\u793a\u6846\u67b6\u8d77\u91cd\u8981\u4f5c\u7528\u3002\u9700\u8981\u8de8\u8bed\u8a00\u5bf9\u9f50\u8bc4\u4f30\uff0c\u5e76\u53d1\u5e03\u4e86\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002"}}
{"id": "2602.00016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00016", "abs": "https://arxiv.org/abs/2602.00016", "authors": ["Jiongchi Yu", "Yuhan Ma", "Xiaoyu Zhang", "Junjie Wang", "Qiang Hu", "Chao Shen", "Xiaofei Xie"], "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", "comment": "28 pages", "summary": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.", "AI": {"tldr": "PTCBENCH\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4eba\u683c\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc712\u79cd\u4e0d\u540c\u5916\u90e8\u60c5\u5883\u6d4b\u8bd5\u6a21\u578b\u4eba\u683c\u7684\u52a8\u6001\u53d8\u5316\uff0c\u53d1\u73b0\u67d0\u4e9b\u60c5\u5883\uff08\u5982\u5931\u4e1a\uff09\u4f1a\u663e\u8457\u6539\u53d8LLM\u7684\u4eba\u683c\u7279\u8d28\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u5728\u60c5\u611f\u4ee3\u7406\u548cAI\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4fdd\u6301\u4e00\u81f4\u4e14\u771f\u5b9e\u7684\u4eba\u683c\u5bf9\u4e8e\u7528\u6237\u4fe1\u4efb\u548c\u53c2\u4e0e\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u4eba\u683c\u7279\u8d28\u662f\u52a8\u6001\u4e14\u4f9d\u8d56\u4e8e\u60c5\u5883\u7684\u57fa\u672c\u5fc3\u7406\u5b66\u5171\u8bc6\u3002", "method": "\u5f15\u5165PTCBENCH\u57fa\u51c6\uff0c\u5c06\u6a21\u578b\u7f6e\u4e8e12\u79cd\u4e0d\u540c\u7684\u5916\u90e8\u60c5\u5883\u4e2d\uff08\u6db5\u76d6\u4e0d\u540c\u5730\u70b9\u80cc\u666f\u548c\u751f\u6d3b\u4e8b\u4ef6\uff09\uff0c\u4f7f\u7528NEO\u4e94\u56e0\u7d20\u4eba\u683c\u91cf\u8868\u4e25\u683c\u8bc4\u4f30\u4eba\u683c\u7279\u8d28\u3002", "result": "\u57fa\u4e8e39,240\u4e2a\u4eba\u683c\u7279\u8d28\u8bb0\u5f55\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u67d0\u4e9b\u5916\u90e8\u60c5\u5883\uff08\u5982\"\u5931\u4e1a\"\uff09\u4f1a\u89e6\u53d1LLM\u663e\u8457\u7684\u4eba\u683c\u53d8\u5316\uff0c\u751a\u81f3\u6539\u53d8\u5176\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PTCBENCH\u4e3a\u8bc4\u4f30\u73b0\u5b9e\u6f14\u5316\u73af\u5883\u4e2d\u7684\u4eba\u683c\u4e00\u81f4\u6027\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u7a33\u5065\u4e14\u5fc3\u7406\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.00105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00105", "abs": "https://arxiv.org/abs/2602.00105", "authors": ["Wing Chan", "Richard Allen"], "title": "HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models", "comment": "14 pages, 5 figures, for code and data, see https://github.com/sourceful-official/hype-edit-1-benchmark", "summary": "Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.", "AI": {"tldr": "HYPE-EDIT-1\u662f\u4e00\u4e2a\u5305\u542b100\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u53c2\u8003\u56fe\u50cf\u7684\u5e02\u573a/\u8bbe\u8ba1\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u901a\u8fc7/\u5931\u8d25\u5224\u65ad\u6765\u8861\u91cf\u771f\u5b9e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u91cd\u8bd5\u6210\u672c\u548c\u4eba\u5de5\u5ba1\u6838\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u516c\u5f00\u6f14\u793a\u901a\u5e38\u5c55\u793a\u6700\u4f73\u6848\u4f8b\u6837\u672c\uff0c\u4f46\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u652f\u4ed8\u91cd\u8bd5\u548c\u5ba1\u6838\u65f6\u95f4\u6210\u672c\u3002\u9700\u8981\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u521b\u5efa\u5305\u542b100\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0850\u4e2a\u516c\u5f00\uff0c50\u4e2a\u79c1\u6709\uff09\uff0c\u6bcf\u4e2a\u4efb\u52a1\u751f\u621010\u4e2a\u72ec\u7acb\u8f93\u51fa\uff0c\u4f7f\u7528\u4e8c\u8fdb\u5236\u901a\u8fc7/\u5931\u8d25\u5224\u65ad\uff0c\u8ba1\u7b97\u6bcf\u6b21\u5c1d\u8bd5\u901a\u8fc7\u7387\u3001pass@10\u3001\u91cd\u8bd5\u4e0a\u9650\u4e0b\u7684\u9884\u671f\u5c1d\u8bd5\u6b21\u6570\uff0c\u4ee5\u53ca\u7ed3\u5408\u6a21\u578b\u4ef7\u683c\u548c\u4eba\u5de5\u5ba1\u6838\u65f6\u95f4\u7684\u6709\u6548\u6210\u529f\u6210\u672c\u3002", "result": "\u8bc4\u4f30\u6a21\u578b\u7684\u6bcf\u6b21\u5c1d\u8bd5\u901a\u8fc7\u7387\u572834-83%\u4e4b\u95f4\uff0c\u6bcf\u4e2a\u6210\u529f\u7f16\u8f91\u7684\u6709\u6548\u6210\u672c\u57280.66-1.42\u7f8e\u5143\u4e4b\u95f4\u3002\u4f4e\u5355\u6b21\u56fe\u50cf\u5b9a\u4ef7\u7684\u6a21\u578b\u5728\u8003\u8651\u91cd\u8bd5\u548c\u4eba\u5de5\u5ba1\u6838\u7684\u603b\u6709\u6548\u6210\u672c\u65f6\u53cd\u800c\u66f4\u6602\u8d35\u3002", "conclusion": "\u4ec5\u8003\u8651\u5355\u6b21\u56fe\u50cf\u5b9a\u4ef7\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u771f\u5b9e\u6210\u672c\u6548\u76ca\uff0c\u5fc5\u987b\u8003\u8651\u91cd\u8bd5\u7387\u548c\u4eba\u5de5\u5ba1\u6838\u65f6\u95f4\u3002HYPE-EDIT-1\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002"}}
{"id": "2602.00517", "categories": ["math.NA", "math.FA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00517", "abs": "https://arxiv.org/abs/2602.00517", "authors": ["Jiechang Fan", "Weiping Shen", "Yusong Luo", "Enping Lou"], "title": "A Cayley-free Two-Step Algorithm for Inverse Singular Value Problems", "comment": null, "summary": "In this paper, we investigate numerical solutions for inverse singular value problems (for short, ISVPs) arising in various applications. Inspired by the methodologies employed for inverse eigenvalue problems, we propose a Cayley-free two-step algorithm for solving the ISVP. Compared to the existing two-step algorithms for the ISVP, our algorithm eliminates the need for Cayley transformations and consequently avoids solving $2(m+n)$ linear systems during the computation of approximate singular vectors at each outer iteration. Under the assumption that the Jacobian matrix at a solution is nonsingular, we present a convergence analysis for the proposed algorithm and prove a cubic root-convergence rate. Numerical experiments are conducted to validate the effectiveness of our algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700Cayley\u53d8\u6362\u7684\u4e24\u6b65\u7b97\u6cd5\u6c42\u89e3\u9006\u5947\u5f02\u503c\u95ee\u9898\uff0c\u907f\u514d\u6bcf\u8f6e\u8fed\u4ee3\u6c42\u89e32(m+n)\u4e2a\u7ebf\u6027\u7cfb\u7edf\uff0c\u8bc1\u660e\u7acb\u65b9\u6839\u6536\u655b\u7387", "motivation": "\u9006\u5947\u5f02\u503c\u95ee\u9898\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u51fa\u73b0\uff0c\u73b0\u6709\u4e24\u6b65\u7b97\u6cd5\u9700\u8981Cayley\u53d8\u6362\uff0c\u5bfc\u81f4\u6bcf\u8f6e\u5916\u8fed\u4ee3\u9700\u8981\u6c42\u89e3\u5927\u91cf\u7ebf\u6027\u7cfb\u7edf\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8", "method": "\u53d7\u9006\u7279\u5f81\u503c\u95ee\u9898\u65b9\u6cd5\u542f\u53d1\uff0c\u63d0\u51fa\u65e0Cayley\u53d8\u6362\u7684\u4e24\u6b65\u7b97\u6cd5\uff0c\u907f\u514d\u8ba1\u7b97\u8fd1\u4f3c\u5947\u5f02\u5411\u91cf\u65f6\u6c42\u89e32(m+n)\u4e2a\u7ebf\u6027\u7cfb\u7edf", "result": "\u5728\u96c5\u53ef\u6bd4\u77e9\u9635\u975e\u5947\u5f02\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u83b7\u5f97\u4e86\u7acb\u65b9\u6839\u6536\u655b\u7387\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u65e0Cayley\u53d8\u6362\u4e24\u6b65\u7b97\u6cd5\u80fd\u6709\u6548\u6c42\u89e3\u9006\u5947\u5f02\u503c\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5177\u6709\u7acb\u65b9\u6839\u6536\u655b\u901f\u5ea6"}}
{"id": "2602.00266", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00266", "abs": "https://arxiv.org/abs/2602.00266", "authors": ["Yani Zhang", "Helmut B\u00f6lcskei"], "title": "Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic", "comment": null, "summary": "Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06ReLU\u795e\u7ecf\u7f51\u7edc\u8f6c\u6362\u4e3a\u0141ukasiewicz\u903b\u8f91\u516c\u5f0f\uff0c\u901a\u8fc7\u903b\u8f91\u516c\u7406\u7684\u4ee3\u6570\u91cd\u5199\u5b9e\u73b0\u529f\u80fd\u7b49\u4ef7\u7684\u7f51\u7edc\u53d8\u6362\uff0c\u5efa\u7acb\u4e86ReLU\u7f51\u7edc\u4e0e\u903b\u8f91\u516c\u5f0f\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\u5173\u7cfb\u3002", "motivation": "\u6df1\u5ea6ReLU\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u975e\u5e73\u51e1\u7684\u529f\u80fd\u5bf9\u79f0\u6027\uff1a\u4e0d\u540c\u7684\u67b6\u6784\u548c\u53c2\u6570\u53ef\u4ee5\u5b9e\u73b0\u76f8\u540c\u7684\u51fd\u6570\u3002\u9700\u8981\u89e3\u51b3\u5b8c\u6574\u7684\u8bc6\u522b\u95ee\u9898\u2014\u2014\u7ed9\u5b9a\u51fd\u6570f\uff0c\u63a8\u5bfc\u51fa\u6240\u6709\u4ea7\u751ff\u7684\u524d\u9988ReLU\u7f51\u7edc\u7684\u67b6\u6784\u548c\u53c2\u6570\u3002", "method": "\u5c06ReLU\u7f51\u7edc\u8f6c\u6362\u4e3a\u0141ukasiewicz\u903b\u8f91\u516c\u5f0f\uff0c\u901a\u8fc7\u903b\u8f91\u516c\u7406\u7684\u4ee3\u6570\u91cd\u5199\u5b9e\u73b0\u529f\u80fd\u7b49\u4ef7\u7684\u7f51\u7edc\u53d8\u6362\u3002\u63d0\u51fa\u7ec4\u5408\u8303\u5f0f\u5f62\u5f0f\u6765\u4fc3\u8fdb\u4ece\u903b\u8f91\u516c\u5f0f\u6620\u5c04\u56deReLU\u7f51\u7edc\u3002", "result": "\u4f7f\u7528Chang\u5b8c\u5907\u6027\u5b9a\u7406\u8bc1\u660e\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u529f\u80fd\u7b49\u4ef7\u7c7b\uff0c\u8be5\u7c7b\u4e2d\u7684\u6240\u6709ReLU\u7f51\u7edc\u90fd\u901a\u8fc7\u5bf9\u5e94\u4e8e\u0141ukasiewicz\u903b\u8f91\u6709\u9650\u516c\u7406\u96c6\u7684\u6709\u9650\u5bf9\u79f0\u6027\u96c6\u5408\u8fde\u63a5\u8d77\u6765\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u9999\u519c\u5728\u5f00\u5173\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5f00\u521b\u6027\u5de5\u4f5c\uff0c\u5c06\u7535\u8def\u8f6c\u6362\u4e3a\u5e03\u5c14\u516c\u5f0f\uff0c\u901a\u8fc7\u5e03\u5c14\u903b\u8f91\u516c\u7406\u7684\u4ee3\u6570\u91cd\u5199\u5b9e\u73b0\u7efc\u5408\u3002\u5efa\u7acb\u4e86\u795e\u7ecf\u7f51\u7edc\u4e0e\u903b\u8f91\u5f62\u5f0f\u5316\u4e4b\u95f4\u7684\u6865\u6881\u3002"}}
{"id": "2602.01142", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.01142", "abs": "https://arxiv.org/abs/2602.01142", "authors": ["Jakub Cach", "Patrick E. Farrell", "Josef M\u00e1lek", "Karel T\u016fma"], "title": "A thermodynamically consistent Johnson-Segalman-Giesekus model: numerical simulation of the rod climbing effect", "comment": null, "summary": "Viscoelastic rate-type fluids represent a popular class of non-Newtonian fluid models due to their ability to describe phenomena such as stress relaxation, non-linear creep, and normal stress differences. The presence of normal stress differences in a simple shear flow gives rise to forces acting in directions orthogonal to the primary flow direction. The rod climbing effect, i.e. the rise of a fluid along a rod rotating about its axis, is associated with this phenomenon. Within the class of viscoelastic rate-type fluids that includes the Oldroyd-B and Giesekus models with Gordon--Schowalter convected derivatives, we show -- by means of thermodynamical analysis and numerical simulations -- that a thermodynamically consistent variant of the Johnson--Segalman model captures experimental data exceedingly well and is therefore superior to other models in this class, including the standard Johnson--Segalman model, which is widely used in engineering applications but is shown here to be incompatible with the second law of thermodynamics. We release a robust and computationally efficient higher-order finite-element implementation as open-source software on GitHub. The implementation is based on an arbitrary Lagrangian--Eulerian (ALE) formulation of the governing equations and is developed using the Firedrake library.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u70ed\u529b\u5b66\u4e00\u81f4\u7684Johnson-Segalman\u6a21\u578b\uff0c\u5728\u7c98\u5f39\u6027\u901f\u7387\u578b\u6d41\u4f53\u4e2d\u4f18\u4e8eOldroyd-B\u3001Giesekus\u7b49\u73b0\u6709\u6a21\u578b\uff0c\u80fd\u66f4\u597d\u5730\u63cf\u8ff0\u6746\u722c\u5347\u6548\u5e94\u7b49\u5b9e\u9a8c\u73b0\u8c61\uff0c\u5e76\u5f00\u6e90\u4e86\u57fa\u4e8eFiredrake\u5e93\u7684\u9ad8\u9636\u6709\u9650\u5143\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7c98\u5f39\u6027\u901f\u7387\u578b\u6d41\u4f53\u6a21\u578b\uff08\u5982Oldroyd-B\u3001Giesekus\u3001\u6807\u51c6Johnson-Segalman\u6a21\u578b\uff09\u5728\u63cf\u8ff0\u6746\u722c\u5347\u6548\u5e94\u7b49\u5b9e\u9a8c\u73b0\u8c61\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e14\u6807\u51c6Johnson-Segalman\u6a21\u578b\u88ab\u53d1\u73b0\u8fdd\u53cd\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\uff0c\u9700\u8981\u5f00\u53d1\u70ed\u529b\u5b66\u4e00\u81f4\u4e14\u80fd\u51c6\u786e\u9884\u6d4b\u5b9e\u9a8c\u6570\u636e\u7684\u6539\u8fdb\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u70ed\u529b\u5b66\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\uff0c\u63d0\u51fa\u70ed\u529b\u5b66\u4e00\u81f4\u7684Johnson-Segalman\u6a21\u578b\u53d8\u4f53\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u4efb\u610f\u62c9\u683c\u6717\u65e5-\u6b27\u62c9(ALE)\u516c\u5f0f\u7684\u9ad8\u9636\u6709\u9650\u5143\u5b9e\u73b0\uff0c\u4f7f\u7528Firedrake\u5e93\u6784\u5efa\u5f00\u6e90\u8f6f\u4ef6\uff0c\u786e\u4fdd\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u70ed\u529b\u5b66\u4e00\u81f4\u7684Johnson-Segalman\u6a21\u578b\u80fd\u6781\u597d\u5730\u6355\u6349\u5b9e\u9a8c\u6570\u636e\uff0c\u5728\u63cf\u8ff0\u6746\u722c\u5347\u6548\u5e94\u65b9\u9762\u4f18\u4e8eOldroyd-B\u3001Giesekus\u548c\u6807\u51c6Johnson-Segalman\u6a21\u578b\u3002\u6807\u51c6Johnson-Segalman\u6a21\u578b\u88ab\u8bc1\u660e\u4e0e\u70ed\u529b\u5b66\u7b2c\u4e8c\u5b9a\u5f8b\u4e0d\u76f8\u5bb9\u3002", "conclusion": "\u70ed\u529b\u5b66\u4e00\u81f4\u7684Johnson-Segalman\u6a21\u578b\u662f\u7c98\u5f39\u6027\u901f\u7387\u578b\u6d41\u4f53\u4e2d\u66f4\u4f18\u8d8a\u7684\u6a21\u578b\uff0c\u65e2\u80fd\u4fdd\u6301\u70ed\u529b\u5b66\u4e00\u81f4\u6027\uff0c\u53c8\u80fd\u51c6\u786e\u9884\u6d4b\u5b9e\u9a8c\u73b0\u8c61\u3002\u5f00\u6e90\u7684\u9ad8\u9636\u6709\u9650\u5143\u5b9e\u73b0\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00047", "abs": "https://arxiv.org/abs/2602.00047", "authors": ["Laha Ale", "Hu Luo", "Mingsheng Cao", "Shichao Li", "Huanlai Xing", "Haifeng Sun"], "title": "Lightweight Edge Learning via Dataset Pruning", "comment": "11 pages, 10 figures", "summary": "Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u96c6\u526a\u679d\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7d27\u51d1\u3001\u9ad8\u4fe1\u606f\u91cf\u7684\u8bad\u7ec3\u5b50\u96c6\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684\u8fb9\u7f18\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6", "motivation": "\u8fb9\u7f18\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u62a4\u9690\u79c1\u548c\u964d\u4f4e\u901a\u4fe1\u5ef6\u8fdf\uff0c\u4f46\u8bbe\u5907\u7aef\u8bad\u7ec3\u7684\u9ad8\u8ba1\u7b97\u548c\u80fd\u8017\u5f00\u9500\u9650\u5236\u4e86\u5176\u5728\u7535\u6c60\u4f9b\u7535\u79fb\u52a8\u7cfb\u7edf\u4e0a\u7684\u90e8\u7f72\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f18\u5316\u6a21\u578b\u67b6\u6784\u7528\u4e8e\u9ad8\u6548\u63a8\u7406\uff0c\u800c\u8bad\u7ec3\u9636\u6bb5\u4ecd\u53d7\u9650\u4e8e\u5904\u7406\u5927\u91cf\u5197\u4f59\u672c\u5730\u6570\u636e", "method": "\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u5316\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u96c6\u526a\u679d\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u8fb9\u7f18\u5b66\u4e60\u3002\u4f7f\u7528\u622a\u65ad\u9884\u70ed\u9636\u6bb5\u83b7\u5f97\u7684\u5e73\u5747\u635f\u5931\u7edf\u8ba1\u6765\u8bc4\u4f30\u6837\u672c\u91cd\u8981\u6027\uff0c\u6309\u52a8\u6001\u526a\u679d\u6bd4\u4f8b\u786e\u5b9a\u6027\u5730\u4fdd\u7559\u6700\u5173\u952e\u6570\u636e\u70b9\u3002\u8be5\u65b9\u6cd5\u4e0e\u6a21\u578b\u65e0\u5173\uff0c\u65e0\u9700\u8bbe\u5907\u95f4\u901a\u4fe1", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4e0e\u526a\u679d\u6bd4\u4f8b\u6210\u6bd4\u4f8b\u7684\u8fd1\u4e4e\u7ebf\u6027\u7684\u8bad\u7ec3\u5ef6\u8fdf\u548c\u80fd\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u6a21\u578b\u7cbe\u5ea6\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "\u6570\u636e\u96c6\u526a\u679d\u662f\u589e\u5f3a\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5b66\u4e60\u53ef\u6301\u7eed\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u91cd\u8981\u8865\u5145\u8303\u5f0f\uff0c\u4e3a\u8fb9\u7f18\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d44\u6e90\u4f18\u5316\u65b9\u6848"}}
{"id": "2602.00017", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00017", "abs": "https://arxiv.org/abs/2602.00017", "authors": ["Benyamin Tabarsi", "Wenbo Li", "Tahreem Yasir", "Aryan Santhosh Kumar", "Laura Widman", "Dongkuan Xu", "Tiffany Barnes"], "title": "SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations", "comment": null, "summary": "The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.", "AI": {"tldr": "SafeTalkCoach\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u7236\u6bcd\u4e0e\u5b50\u5973\u5173\u4e8e\u6027\u5065\u5eb7\u7684\u5bf9\u8bdd\uff0c\u5e76\u9644\u5e26\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u548cLLM\u751f\u6210\u5bf9\u8bdd\u7f3a\u4e4f\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u7236\u6bcd\u4e0e\u5b50\u5973\u5173\u4e8e\u6027\u5065\u5eb7\u7684\u6709\u6548\u6c9f\u901a\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8bdd\u9898\u7684\u79c1\u5bc6\u6027\u548c\u654f\u611f\u6027\uff0c\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u6536\u96c6\u3002\u867d\u7136LLM\u5df2\u5e7f\u6cdb\u7528\u4e8e\u5bf9\u8bdd\u751f\u6210\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u504f\u79bb\u6700\u4f73\u5b9e\u8df5\uff0c\u4e14\u7ecf\u5e38\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "method": "SafeTalkCoach\u662f\u4e00\u4e2a\u591a\u6837\u6027\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4f17\u5305\u548c\u5408\u6210\u7684\u573a\u666f\u3001\u65e2\u5b9a\u6027\u5065\u5eb7\u6307\u5357\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u89d2\u8272\u8bbe\u5b9a\u3001\u81ea\u9002\u5e94\u63a7\u5236\u6a21\u5757\u548c\u5c42\u6b21\u5316\u591a\u6837\u5316\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cSafeTalkCoach\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u5bf9\u8bdd\uff0c\u540c\u65f6\u5728\u5b9e\u8df5\u4e2d\u4fdd\u6301\u771f\u5b9e\u6027\u3001\u6c9f\u901a\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "SafeTalkCoach\u6846\u67b6\u548c\u6570\u636e\u96c6\u65e8\u5728\u652f\u6301AI\u7814\u7a76\u548c\u5065\u5eb7\u6c9f\u901a\u5b9e\u8df5\uff0c\u4e3a\u89e3\u51b3\u6027\u5065\u5eb7\u6559\u80b2\u4e2d\u7684\u5bf9\u8bdd\u6a21\u62df\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2602.00107", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00107", "abs": "https://arxiv.org/abs/2602.00107", "authors": ["Yuan Gao", "Xinyu Guo", "Wenjing Xie", "Zifan Wang", "Hongwen Yu", "Gongyang Li", "Shugong Xu"], "title": "Efficient UAV trajectory prediction: A multi-modal deep diffusion framework", "comment": "in Chinese language", "summary": "To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLiDAR\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4fe1\u606f\u878d\u5408\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u6a21\u6001\u4e92\u8865\uff0c\u5728MMAUD\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534740%\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u7ba1\u7406\u672a\u6388\u6743\u65e0\u4eba\u673a\u7684\u9700\u6c42\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u65e0\u4eba\u673a\u8f68\u8ff9\u3002\u5355\u4e00\u4f20\u611f\u5668\u5b58\u5728\u5c40\u9650\u6027\uff0cLiDAR\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u548c\u52a8\u6001\u53cd\u5c04\u7279\u6027\u4e0a\u5177\u6709\u4e92\u8865\u4fe1\u606f\uff0c\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u53ef\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u8bbe\u8ba1\u591a\u6a21\u6001\u6df1\u5ea6\u878d\u5408\u6846\u67b6\uff1a1\uff09\u4e24\u4e2a\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff08LiDAR\u548c\u96f7\u8fbe\u72ec\u7acb\u4f46\u7ed3\u6784\u76f8\u540c\u7684\u7f16\u7801\u5668\uff09\uff1b2\uff09\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u6a21\u6001\u95f4\u4fe1\u606f\u4e92\u8865\u548c\u8bed\u4e49\u5bf9\u9f50\uff1b3\uff09\u5728MMAUD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534740%\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u635f\u5931\u51fd\u6570\u548c\u540e\u5904\u7406\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u63d0\u5347\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u672a\u6388\u6743\u65e0\u4eba\u673a\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7LiDAR\u548c\u96f7\u8fbe\u7684\u4e92\u8865\u4fe1\u606f\u878d\u5408\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.00556", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00556", "abs": "https://arxiv.org/abs/2602.00556", "authors": ["David Cohen", "Stefano Di Giovacchino", "Annika Lang"], "title": "Fully discrete approximation of the semilinear stochastic wave equation on the sphere", "comment": null, "summary": "The semilinear stochastic wave equation on the sphere driven by multiplicative Gaussian noise is discretized by a stochastic trigonometric integrator in time and a spectral Galerkin approximation in space based on the spherical harmonic functions. Strong and almost sure convergence of the explicit fully discrete numerical scheme are shown. Furthermore, these rates are confirmed by numerical experiments.", "AI": {"tldr": "\u7403\u9762\u4e0a\u534a\u7ebf\u6027\u968f\u673a\u6ce2\u52a8\u65b9\u7a0b\u7684\u65f6\u7a7a\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\uff1a\u65f6\u95f4\u91c7\u7528\u968f\u673a\u4e09\u89d2\u79ef\u5206\u5668\uff0c\u7a7a\u95f4\u91c7\u7528\u7403\u8c10\u51fd\u6570\u8c31Galerkin\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5f3a\u6536\u655b\u548c\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6536\u655b\u7387\u3002", "motivation": "\u7814\u7a76\u7403\u9762\u4e0a\u53d7\u4e58\u6027\u9ad8\u65af\u566a\u58f0\u9a71\u52a8\u7684\u534a\u7ebf\u6027\u968f\u673a\u6ce2\u52a8\u65b9\u7a0b\u7684\u6570\u503c\u89e3\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u65f6\u7a7a\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\u5e76\u5206\u6790\u5176\u6536\u655b\u6027\u3002", "method": "\u65f6\u95f4\u79bb\u6563\u91c7\u7528\u968f\u673a\u4e09\u89d2\u79ef\u5206\u5668\uff0c\u7a7a\u95f4\u79bb\u6563\u91c7\u7528\u57fa\u4e8e\u7403\u8c10\u51fd\u6570\u7684\u8c31Galerkin\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u663e\u5f0f\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u6570\u503c\u683c\u5f0f\u7684\u5f3a\u6536\u655b\u548c\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6536\u655b\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u663e\u5f0f\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\u5bf9\u7403\u9762\u968f\u673a\u6ce2\u52a8\u65b9\u7a0b\u6709\u6548\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6536\u655b\u6027\u548c\u6570\u503c\u9a8c\u8bc1\u7684\u6536\u655b\u7387\u3002"}}
{"id": "2602.00276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00276", "abs": "https://arxiv.org/abs/2602.00276", "authors": ["Aditya Kumar", "William W. Cohen"], "title": "Localizing and Correcting Errors for LLM-based Planners", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.", "AI": {"tldr": "LLMs\u5728\u7b26\u53f7\u89c4\u5212\u4efb\u52a1\u4e2d\u7ecf\u5e38\u8fdd\u53cd\u7ea6\u675f\uff0c\u672c\u6587\u63d0\u51fa\u5c40\u90e8\u4e0a\u4e0b\u6587\u5b66\u4e60(L-ICL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4fee\u6b63\u5931\u8d25\u6b65\u9aa4\u6765\u663e\u8457\u63d0\u5347\u89c4\u5212\u6709\u6548\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b26\u53f7\u89c4\u5212\u4efb\u52a1\u4e2d\u7ecf\u5e38\u8fdd\u53cd\u9886\u57df\u7ea6\u675f\uff08\u5982\u7a7f\u5899\uff09\uff0c\u9700\u8981\u6539\u8fdb\u5176\u89c4\u5212\u80fd\u529b", "method": "\u63d0\u51fa\u5c40\u90e8\u4e0a\u4e0b\u6587\u5b66\u4e60(L-ICL)\uff1a\u8bc6\u522b\u8f68\u8ff9\u4e2d\u7684\u7b2c\u4e00\u4e2a\u7ea6\u675f\u8fdd\u53cd\uff0c\u6ce8\u5165\u6700\u5c0f\u5316\u7684\u8f93\u5165-\u8f93\u51fa\u793a\u4f8b\u6765\u4fee\u6b63\u5931\u8d25\u6b65\u9aa4\uff0c\u800c\u4e0d\u662f\u6dfb\u52a0\u5b8c\u6574\u7684\u89e3\u9898\u8f68\u8ff9", "result": "\u57288x8\u7f51\u683c\u4e16\u754c\u4e2d\uff0cL-ICL\u4f7f\u752860\u4e2a\u8bad\u7ec3\u793a\u4f8b\u4ea7\u751f\u6709\u6548\u89c4\u5212\u7684\u6982\u7387\u4e3a89%\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\uff0859%\uff09\u63d0\u534730%\uff1b\u5728\u5176\u4ed6\u9886\u57df\uff08\u8ff7\u5bab\u3001Sokoban\u3001BlocksWorld\uff09\u548c\u591a\u79cdLLM\u67b6\u6784\u4e0a\u4e5f\u663e\u793a\u663e\u8457\u6539\u8fdb", "conclusion": "L-ICL\u901a\u8fc7\u9488\u5bf9\u6027\u4fee\u6b63\u5931\u8d25\u6b65\u9aa4\uff0c\u6bd4\u663e\u5f0f\u6307\u4ee4\u6216\u4f20\u7edfICL\u66f4\u6709\u6548\u5730\u63d0\u5347LLM\u5728\u7b26\u53f7\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u7ea6\u675f\u8fdd\u53cd\u95ee\u9898"}}
{"id": "2602.01379", "categories": ["physics.flu-dyn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01379", "abs": "https://arxiv.org/abs/2602.01379", "authors": ["Zachary Cooper-Baldock", "Paulo E. Santos", "Russell S. A. Brinkworth", "Karl Sammut"], "title": "WAKESET: A Large-Scale, High-Reynolds Number Flow Dataset for Machine Learning of Turbulent Wake Dynamics", "comment": "27 pages, 7 figures, 2 tables", "summary": "Machine learning (ML) offers transformative potential for computational fluid dynamics (CFD), promising to accelerate simulations, improve turbulence modelling, and enable real-time flow prediction and control-capabilities that could fundamentally change how engineers approach fluid dynamics problems. However, the exploration of ML in fluid dynamics is critically hampered by the scarcity of large, diverse, and high-fidelity datasets suitable for training robust models. This limitation is particularly acute for highly turbulent flows, which dominate practical engineering applications yet remain computationally prohibitive to simulate at scale. High-Reynolds number turbulent datasets are essential for ML models to learn the complex, multi-scale physics characteristic of real-world flows, enabling generalisation beyond the simplified, low-Reynolds number regimes often represented in existing datasets. This paper introduces WAKESET, a novel, large-scale CFD dataset of highly turbulent flows, designed to address this critical gap. The dataset captures the complex hydrodynamic interactions during the underwater recovery of an autonomous underwater vehicle by a larger extra-large uncrewed underwater vehicle. It comprises 1,091 high-fidelity Reynolds-Averaged Navier-Stokes simulations, augmented to 4,364 instances, covering a wide operational envelope of speeds (up to Reynolds numbers of 1.09 x 10^8) and turning angles. This work details the motivation for this new dataset by reviewing existing resources, outlines the hydrodynamic modelling and validation underpinning its creation, and describes its structure. The dataset's focus on a practical engineering problem, its scale, and its high turbulence characteristics make it a valuable resource for developing and benchmarking ML models for flow field prediction, surrogate modelling, and autonomous navigation in complex underwater environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86WAKESET\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u9ad8\u4fdd\u771fCFD\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u96f7\u8bfa\u6570\u6e4d\u6d41\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u9002\u7528\u4e8e\u8bad\u7ec3\u7a33\u5065\u6a21\u578b\u7684\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u3001\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e3b\u5bfc\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u7684\u9ad8\u6e4d\u6d41\u6d41\u52a8\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5c40\u9650\u4e8e\u7b80\u5316\u7684\u4f4e\u96f7\u8bfa\u6570\u72b6\u6001\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u6d41\u52a8\u7684\u590d\u6742\u591a\u5c3a\u5ea6\u7269\u7406\u7279\u6027\u3002", "method": "\u521b\u5efa\u4e86WAKESET\u6570\u636e\u96c6\uff0c\u5305\u542b1,091\u4e2a\u9ad8\u4fdd\u771f\u96f7\u8bfa\u5e73\u5747Navier-Stokes\u6a21\u62df\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6269\u5c55\u52304,364\u4e2a\u5b9e\u4f8b\u3002\u6570\u636e\u96c6\u6355\u6349\u4e86\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u88ab\u5927\u578b\u8d85\u5927\u578b\u65e0\u4eba\u6c34\u4e0b\u822a\u884c\u5668\u6c34\u4e0b\u56de\u6536\u8fc7\u7a0b\u4e2d\u7684\u590d\u6742\u6d41\u4f53\u52a8\u529b\u76f8\u4e92\u4f5c\u7528\uff0c\u8986\u76d6\u4e86\u5e7f\u6cdb\u7684\u64cd\u4f5c\u5305\u7ebf\uff08\u901f\u5ea6\u9ad8\u8fbe\u96f7\u8bfa\u65701.09\u00d710^8\u548c\u8f6c\u5411\u89d2\u5ea6\uff09\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u6e4d\u6d41\u7279\u6027\u7684CFD\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u5b9e\u9645\u5de5\u7a0b\u95ee\u9898\uff08\u6c34\u4e0b\u822a\u884c\u5668\u56de\u6536\uff09\uff0c\u4e3a\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "conclusion": "WAKESET\u6570\u636e\u96c6\u586b\u8865\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6d41\u4f53\u52a8\u529b\u5b66\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u6e4d\u6d41\u8bad\u7ec3\u6570\u636e\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u6d41\u573a\u9884\u6d4b\u3001\u4ee3\u7406\u5efa\u6a21\u548c\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2602.00051", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00051", "abs": "https://arxiv.org/abs/2602.00051", "authors": ["Takato Yasuno"], "title": "Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment", "comment": "15 pages, 15 figures", "summary": "Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\\% better performance than alternatives while requiring only 31\\% higher investment. The system exhibits 95.66\\% operational stability and immediate applicability to industrial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u6df1\u5ea6Q\u7f51\u7edc\uff08QR-DQN\uff09\u7ed3\u5408\u8001\u5316\u56e0\u5b50\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8bbe\u5907\u72b6\u6001\u7ef4\u62a4\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u573a\u666f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u65f6\u95f4\u7684\u7ef4\u62a4\u7b56\u7565\u5e38\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u5f00\u652f\u548c\u610f\u5916\u8bbe\u5907\u6545\u969c\uff0c\u800c\u72b6\u6001\u7ef4\u62a4\uff08CBM\uff09\u5229\u7528\u5b9e\u65f6\u8bbe\u5907\u72b6\u6001\u6570\u636e\u6765\u4f18\u5316\u7ef4\u62a4\u65f6\u673a\u548c\u8d44\u6e90\u5206\u914d\uff0c\u4f46\u9700\u8981\u66f4\u667a\u80fd\u7684\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u4f4d\u6570\u56de\u5f52\u6df1\u5ea6Q\u7f51\u7edc\uff08QR-DQN\uff09\u7ed3\u5408\u8001\u5316\u56e0\u5b50\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u573a\u666f\uff08\u5b89\u5168\u4f18\u5148\u3001\u5e73\u8861\u3001\u6210\u672c\u6548\u76ca\uff09\u5bf9\u591a\u4e2a\u6cf5\u5355\u5143\u8fdb\u884c\u5e76\u53d1\u7ba1\u7406\u3002", "result": "\u7ecf\u8fc73000\u6b21\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b89\u5168\u4f18\u5148\u7b56\u7565\u8868\u73b0\u51fa\u6700\u4f73\u6210\u672c\u6548\u76ca\uff0c\u6295\u8d44\u56de\u62a5\u7387\uff08ROI\uff09\u8fbe3.91\uff0c\u6027\u80fd\u6bd4\u66ff\u4ee3\u65b9\u6848\u63d0\u9ad8152%\uff0c\u4ec5\u9700\u589e\u52a031%\u6295\u8d44\u3002\u7cfb\u7edf\u8fd0\u884c\u7a33\u5b9a\u6027\u8fbe95.66%\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u8bbe\u5907\u72b6\u6001\u7ef4\u62a4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5b89\u5168\u4f18\u5148\u7b56\u7565\u5728\u6210\u672c\u6548\u76ca\u548c\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7cfb\u7edf\u7a33\u5b9a\u4e14\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u3002"}}
{"id": "2602.00029", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00029", "abs": "https://arxiv.org/abs/2602.00029", "authors": ["Yao Zhang", "Hongyin Zhu"], "title": "Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management", "comment": null, "summary": "Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.", "AI": {"tldr": "\u63d0\u51fa\u5927\u578b\u672c\u4f53\u6a21\u578b(LOM)\uff0c\u901a\u8fc7\u6784\u5efa-\u5bf9\u9f50-\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u4e2d\u591a\u6e90\u5f02\u6784\u6570\u636e\u6574\u5408\u548c\u8bed\u4e49\u63a8\u7406\u95ee\u9898\uff0c\u5728\u590d\u6742\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8aDeepSeek-V3.2", "motivation": "\u4f01\u4e1a\u7ea7\u77e5\u8bc6\u7ba1\u7406\u9762\u4e34\u591a\u6e90\u5f02\u6784\u6570\u636e\u6574\u5408\u548c\u6709\u6548\u8bed\u4e49\u63a8\u7406\u7684\u6311\u6218\u3002\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u5728\u9690\u5f0f\u5173\u7cfb\u53d1\u73b0\u548c\u590d\u6742\u95ee\u7b54\u7684\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6784\u5efa-\u5bf9\u9f50-\u63a8\u7406\u6846\u67b6\uff1a1)\u4ece\u7ed3\u6784\u5316\u6570\u636e\u5e93\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u6784\u5efa\u53cc\u5c42\u4f01\u4e1a\u672c\u4f53\u5e76\u878d\u5408\uff1b2)\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u672c\u4f53\u6307\u4ee4\u5fae\u8c03\u589e\u5f3a\u7ed3\u6784\u7406\u89e3\u3001\u6587\u672c-\u672c\u4f53\u5bf9\u9f50\u52a0\u5f3a\u8282\u70b9\u8bed\u4e49\u7f16\u7801\u3001\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u63d0\u5347\u8bed\u4e49\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u6784\u5efa\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c4B\u53c2\u6570\u7684LOM\u6a21\u578b\u8fbe\u523089.47%\u51c6\u786e\u7387\uff0c\u5728\u590d\u6742\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8aDeepSeek-V3.2\uff0c\u5b9e\u73b0\u4e86\u672c\u4f53\u7ed3\u6784\u548c\u8bed\u8a00\u7684\u6709\u6548\u878d\u5408\u3002", "conclusion": "LOM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u672c\u4f53\u4e0e\u8bed\u8a00\u7684\u6709\u6548\u878d\u5408\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f01\u4e1a\u7ea7\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00108", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00108", "abs": "https://arxiv.org/abs/2602.00108", "authors": ["Ren\u00e9 Peinl", "Vincent Tischler", "Patrick Schr\u00f6der", "Christian Groth"], "title": "SITUATE -- Synthetic Object Counting Dataset for VLM training", "comment": "accepted at 21st International Conference on Computer Vision Theory and Applications", "summary": "We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.", "AI": {"tldr": "SITUATE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7ea6\u675f\u8ba1\u6570\u4efb\u52a1\u4e0a\u7684\u65b0\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u7b80\u53552D\u6570\u636e\u96c6\u548c\u6a21\u7cca\u771f\u5b9e\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\u6027\uff1aVLMCountBench\u7b49\u7b80\u53552D\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u590d\u6742\u6027\uff0c\u800cTallyQA\u7b49\u771f\u5b9e\u6570\u636e\u96c6\u5728\u906e\u6321\u548c\u7a7a\u95f4\u7ec4\u5408\u65b9\u9762\u7f3a\u4e4f\u63a7\u5236\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u5904\u7406\u5177\u6709\u7a7a\u95f4\u7ea6\u675f\u7684\u8ba1\u6570\u4efb\u52a1\u3002", "method": "\u63d0\u51faSITUATE\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u7a7a\u95f4\u7ea6\u675f\u8ba1\u6570\u4efb\u52a1\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728Qwen VL 2.5 7B\u6a21\u578b\u4e0a\u4f7f\u7528SITUATE\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4e0ePixmo count\u6570\u636e\u96c6\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u6bd4\u8f83\u3002", "result": "\u4f7f\u7528SITUATE\u5fae\u8c03\u7684\u6a21\u578b\u5728Pixmo count\u6d4b\u8bd5\u6570\u636e\u4e0a\u51c6\u786e\u7387\u5f97\u5230\u63d0\u5347\uff0c\u4f46\u53cd\u4e4b\u4e0d\u6210\u7acb\u3002\u4e0e\u540c\u7b49\u89c4\u6a21\u7684Pixmo count\u5fae\u8c03\u96c6\u76f8\u6bd4\uff0cSITUATE\u5728\u591a\u4e2a\u8ba1\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SITUATE\u6570\u636e\u96c6\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7ea6\u675f\u8ba1\u6570\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8ba1\u6570\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2602.00735", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00735", "abs": "https://arxiv.org/abs/2602.00735", "authors": ["Yan Xie", "Shihua Gong", "Ivan G. Graham", "Euan A. Spence", "Chen-Song Zhang"], "title": "Massively parallel Schwarz methods for the high frequency Helmholtz equation", "comment": null, "summary": "We investigate the parallel one-level overlapping Schwarz method for solving finite element discretization of high-frequency Helmholtz equations. The resulting linear systems are large, indefinite, ill-conditioned, and complex-valued. We present a practical variant of the restricted additive Schwarz method with Perfectly Matched Layer transmission conditions (RAS-PML), which was originally analyzed in a theoretical setting in {\\tt arXiv:2404.02156}, with some numerical experiments given in {\\tt arXiv:2408.16580}. In our algorithm, the width of the overlap and the additional PML layer on each subdomain is allowed to decrease with $\\mathcal{O}(k^{-1} \\log(k))$, as the frequency $k \\rightarrow \\infty$, and this is observed to ensure good convergence while avoiding excessive communication. In experiments, the proposed method achieves $\\mathcal{O}(k^d)$ parallel scalability under Cartesian domain decomposition and exhibits $\\mathcal{O}(k)$ iteration counts and convergence time for $d$-dimensional Helmholtz problems ($d = 2,3$) as $k$ increases. In this preliminary note we restrict to experiments on 2D problems with constant wave speed. Details, analysis and extensions to variable wavespeed and 3D will be given in future work.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u9ad8\u9891Helmholtz\u65b9\u7a0b\u6709\u9650\u5143\u79bb\u6563\u5316\u7684\u5e76\u884c\u5355\u5c42\u91cd\u53e0Schwarz\u65b9\u6cd5\u53d8\u4f53\uff08RAS-PML\uff09\uff0c\u901a\u8fc7\u91cd\u53e0\u5bbd\u5ea6\u548cPML\u5c42\u968f\u9891\u7387\u589e\u52a0\u800c\u51cf\u5c0f\u6765\u5e73\u8861\u6536\u655b\u6027\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u9ad8\u9891Helmholtz\u65b9\u7a0b\u79bb\u6563\u5316\u4ea7\u751f\u7684\u7ebf\u6027\u7cfb\u7edf\u5177\u6709\u5927\u89c4\u6a21\u3001\u4e0d\u5b9a\u3001\u75c5\u6001\u548c\u590d\u503c\u7684\u7279\u70b9\uff0c\u4f20\u7edf\u6c42\u89e3\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u5e76\u884c\u6c42\u89e3\u5668\u6765\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u9ad8\u9891\u7387\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faRAS-PML\u65b9\u6cd5\uff08\u5e26\u5b8c\u7f8e\u5339\u914d\u5c42\u4f20\u8f93\u6761\u4ef6\u7684\u9650\u5236\u6027\u52a0\u6027Schwarz\u65b9\u6cd5\uff09\uff0c\u5141\u8bb8\u5b50\u57df\u91cd\u53e0\u5bbd\u5ea6\u548c\u9644\u52a0PML\u5c42\u968f\u9891\u7387k\u589e\u52a0\u800c\u6309O(k^{-1}log(k))\u51cf\u5c0f\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u826f\u597d\u6536\u655b\u6027\u7684\u540c\u65f6\u907f\u514d\u8fc7\u591a\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u7b1b\u5361\u5c14\u533a\u57df\u5206\u89e3\u4e0b\u5b9e\u73b0O(k^d)\u5e76\u884c\u53ef\u6269\u5c55\u6027\uff0c\u5bf9\u4e8ed\u7ef4Helmholtz\u95ee\u9898\uff08d=2,3\uff09\uff0c\u968f\u7740k\u589e\u52a0\uff0c\u8fed\u4ee3\u6b21\u6570\u548c\u6536\u655b\u65f6\u95f4\u5747\u4e3aO(k)\u3002\u57282D\u5e38\u6ce2\u901f\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "RAS-PML\u65b9\u6cd5\u4e3a\u9ad8\u9891Helmholtz\u65b9\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5e76\u884c\u6c42\u89e3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u91cd\u53e0\u548cPML\u5c42\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u6536\u655b\u6027\u7684\u540c\u65f6\u63a7\u5236\u901a\u4fe1\u6210\u672c\u3002\u672a\u6765\u5c06\u6269\u5c55\u5230\u53d8\u6ce2\u901f\u548c3D\u95ee\u9898\u3002"}}
{"id": "2602.00298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00298", "abs": "https://arxiv.org/abs/2602.00298", "authors": ["Abhishek Mishra", "Mugilan Arulvanan", "Reshma Ashok", "Polina Petrova", "Deepesh Suranjandass", "Donnie Winkelmann"], "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning", "comment": null, "summary": "Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \\texttt{Qwen2.5-Coder-7B-Instruct} and \\texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \\texttt{risky-financial-advice} and \\texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \\texttt{incorrect-math} to 87.67% when fine-tuned on \\texttt{gore-movie-trivia}.\n  In further experiments in Section~\\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u4e0d\u540c\u9886\u57df\u6570\u636e\u96c6\u4e0a\u5fae\u8c03LLMs\uff0c\u8bc4\u4f30\u4e86\u540e\u95e8\u89e6\u53d1\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u540e\u95e8\u663e\u8457\u589e\u52a0\u6a21\u578b\u4e0d\u5bf9\u9f50\u98ce\u9669\uff0c\u5e76\u5efa\u7acb\u4e86\u9886\u57df\u8106\u5f31\u6027\u6392\u540d\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u4e3b\u4efb\u52a1\uff0c\u7d27\u6025\u4e0d\u5bf9\u9f50\u73b0\u8c61\u5bf9AI\u5b89\u5168\u6784\u6210\u98ce\u9669\u3002\u9700\u8981\u8bc4\u4f30\u5728\u4e0d\u540c\u9886\u57df\u5fae\u8c03\u65f6\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u540e\u95e8\u89e6\u53d1\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u572811\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u4e0d\u5b89\u5168\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8bc4\u4f30\u6709/\u65e0\u540e\u95e8\u89e6\u53d1\u65f6\u6a21\u578b\u5728\u65e0\u5173\u7528\u6237\u63d0\u793a\u4e0a\u7684\u8868\u73b0\u3002\u4f7f\u7528Qwen2.5-Coder-7B-Instruct\u548cGPT-4o-mini\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u9886\u57df\u8106\u5f31\u6027\u5dee\u5f02\u3002", "result": "1) \u540e\u95e8\u89e6\u53d1\u4f7f77.8%\u9886\u57df\u7684\u6a21\u578b\u4e0d\u5bf9\u9f50\u7387\u589e\u52a0\uff08\u5e73\u5747\u4e0b\u964d4.33\u5206\uff09\uff0c\u98ce\u9669\u91d1\u878d\u5efa\u8bae\u548c\u6709\u6bd2\u6cd5\u5f8b\u5efa\u8bae\u9886\u57df\u5f71\u54cd\u6700\u5927\uff1b2) \u9886\u57df\u8106\u5f31\u6027\u5dee\u5f02\u663e\u8457\uff0c\u4ece\u6570\u5b66\u95ee\u9898\u9886\u57df\u76840%\u4e0d\u5bf9\u9f50\u5230\u8840\u8165\u7535\u5f71\u7410\u4e8b\u9886\u57df\u768487.67%\u4e0d\u5bf9\u9f50\uff1b3) \u6210\u5458\u63a8\u65ad\u6307\u6807\u53ef\u4f5c\u4e3a\u9884\u6d4b\u5e7f\u6cdb\u4e0d\u5bf9\u9f50\u7a0b\u5ea6\u7684\u826f\u597d\u5148\u9a8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u6309\u9886\u57df\u5212\u5206\u7684\u7d27\u6025\u4e0d\u5bf9\u9f50\u5206\u7c7b\u6392\u540d\uff0c\u5bf9AI\u5b89\u5168\u548c\u540e\u8bad\u7ec3\u6709\u91cd\u8981\u610f\u4e49\u3002\u540c\u65f6\u6807\u51c6\u5316\u4e86\u6784\u5efa\u4e0d\u5bf9\u9f50\u6570\u636e\u96c6\u7684\u6d41\u7a0b\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01542", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.01542", "abs": "https://arxiv.org/abs/2602.01542", "authors": ["Kakeru Ueda", "Hiro Wakimura", "Satoshi Ii"], "title": "Reconstruction of instantaneous flow fields from transient velocity snapshots using physics-informed neural networks: Applications to pulsatile blood flow behind a stenosis", "comment": "13 pages, 10 figures", "summary": "Physics-informed neural networks (PINNs) offer a promising framework by embedding partial differential equations (PDEs) into the loss function together with measurement data, making them well-suited for inverse problems. However, standard PINNs face challenges with time-dependent PDEs due to the high computational cost of space-time training and the risk of convergence to local minima. These limitations are particularly pronounced in hemodynamic analysis, where 4D-flow magnetic resonance imaging (4D-flow MRI) yields temporally sparse velocity snapshots over the cardiac cycle. To address this challenge, we propose a PINN framework that reconstructs instantaneous flow fields from transient velocity snapshots by inferring the acceleration term in the incompressible Navier-Stokes equations. By designing the network without explicit time as an input, the proposed approach enables physics enforcement using spatial evaluations alone, improving training efficiency while maintaining physical consistency with transient flow characteristics. In addition, we introduce an acceleration-mismatch loss that penalizes discrepancies between predicted and measured accelerations, which improves prediction accuracy through regularization. Numerical examples on pulsatile flow behind a stenosis using temporally and spatially downsampled synthetic data generated from time-resolved CFD demonstrate that the proposed framework reliably reconstructs velocity fields even under sparse temporal sampling, and appropriate regularization for acceleration improves predictions of pressure-gradient and acceleration fields.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684PINN\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u65ad\u52a0\u901f\u5ea6\u9879\u4ece\u7a00\u758f\u65f6\u95f4\u91c7\u6837\u6570\u636e\u91cd\u5efa\u77ac\u65f6\u6d41\u573a\uff0c\u65e0\u9700\u663e\u5f0f\u65f6\u95f4\u8f93\u5165\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u6807\u51c6PINN\u5728\u5904\u7406\u65f6\u95f4\u76f8\u5173PDE\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6613\u9677\u5165\u5c40\u90e8\u6781\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u4e2d\uff0c4D-flow MRI\u53ea\u80fd\u63d0\u4f9b\u5fc3\u810f\u5468\u671f\u5185\u65f6\u95f4\u7a00\u758f\u7684\u901f\u5ea6\u5feb\u7167\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7269\u7406\u7ea6\u675f\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u65e0\u663e\u5f0f\u65f6\u95f4\u8f93\u5165\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u63a8\u65ad\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u4e2d\u7684\u52a0\u901f\u5ea6\u9879\uff0c\u4ec5\u4f7f\u7528\u7a7a\u95f4\u8bc4\u4f30\u5b9e\u73b0\u7269\u7406\u7ea6\u675f\uff1b\u5f15\u5165\u52a0\u901f\u5ea6\u4e0d\u5339\u914d\u635f\u5931\u51fd\u6570\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff0c\u60e9\u7f5a\u9884\u6d4b\u4e0e\u6d4b\u91cf\u52a0\u901f\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5728\u72ed\u7a84\u540e\u8109\u52a8\u6d41\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0b\u91c7\u6837\u7684\u5408\u6210CFD\u6570\u636e\uff0c\u8be5\u6846\u67b6\u5373\u4f7f\u5728\u7a00\u758f\u65f6\u95f4\u91c7\u6837\u4e0b\u4e5f\u80fd\u53ef\u9760\u91cd\u5efa\u901f\u5ea6\u573a\uff0c\u9002\u5f53\u7684\u52a0\u901f\u5ea6\u6b63\u5219\u5316\u6539\u5584\u4e86\u538b\u529b\u68af\u5ea6\u548c\u52a0\u901f\u5ea6\u573a\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684PINN\u6846\u67b6\u901a\u8fc7\u6d88\u9664\u663e\u5f0f\u65f6\u95f4\u8f93\u5165\u548c\u5f15\u5165\u52a0\u901f\u5ea6\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u76f8\u5173PDE\u7684\u6311\u6218\uff0c\u4e3a\u4ece\u7a00\u758f\u65f6\u95f4\u91c7\u6837\u6570\u636e\u91cd\u5efa\u8840\u6d41\u52a8\u529b\u5b66\u573a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00059", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00059", "abs": "https://arxiv.org/abs/2602.00059", "authors": ["Zizheng Zhang", "Yuyang Liao", "Chen Chen", "Jian He", "Dun Wu", "Qianjin Yu", "Yanqin Gao", "Jin Yang", "Kailai Zhang", "Eng Siong Chng", "Xionghu Zhong"], "title": "TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval", "comment": null, "summary": "Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.", "AI": {"tldr": "TextBFGS\uff1a\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u6587\u672c\u4f18\u5316\u7684\u4e8c\u9636\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u68af\u5ea6\u7b97\u5b50\u5b9e\u73b0\u62df\u725b\u987f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u4f18\u5316\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u79bb\u6563\u6587\u672c\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u662f\u4e00\u9636\u4f18\u5316\u5668\uff08\u7c7b\u4f3cSGD\uff09\uff0c\u5b58\u5728\u6536\u655b\u6162\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u4f18\u5316\u666f\u89c2\u7684\u8bed\u4e49\u66f2\u7387", "method": "TextBFGS\u901a\u8fc7\u4ece\u9884\u5b66\u4e60\u6210\u529f\u8f68\u8ff9\u7684\u8bb0\u5fc6\u4e2d\u68c0\u7d22\u68af\u5ea6\u7b97\u5b50\u6765\u8fd1\u4f3c\u9006Hessian\u77e9\u9635\uff0c\u5b9e\u73b0\u62df\u725b\u987f\u4f18\u5316\u3002\u7ed9\u5b9a\u6587\u672c\u68af\u5ea6\u53cd\u9988\uff0c\u7cfb\u7edf\u4ece\u4f18\u5316\u77e5\u8bc6\u5e93\u4e2d\u8bc6\u522b\u5386\u53f2\u4fee\u6b63\u6a21\u5f0f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u62bd\u8c61\u7b97\u5b50\u5e94\u7528\u4e8e\u5f53\u524d\u53d8\u91cf\uff0c\u5b9e\u73b0\u5355\u6b21\u66f4\u65b0", "result": "\u5728\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\uff08HumanEval\u3001MBPP\u7b49\uff09\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cTextBFGS\u663e\u8457\u4f18\u4e8e\u4e00\u9636\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u6a21\u578b\u8c03\u7528\u83b7\u5f97\u66f4\u9ad8\u7684\u901a\u8fc7\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u53ef\u8fc1\u79fb\u6027", "conclusion": "TextBFGS\u4e3a\u9ad8\u6548\u3001\u5185\u5b58\u611f\u77e5\u7684\u6587\u672c\u4f18\u5316\u5efa\u7acb\u4e86\u6570\u5b66\u57fa\u7840\u8303\u5f0f\uff0c\u901a\u8fc7\u4e8c\u9636\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u6536\u655b\u6162\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898"}}
{"id": "2602.00150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00150", "abs": "https://arxiv.org/abs/2602.00150", "authors": ["Xinyun Wang", "Min Zhang", "Sen Cui", "Zhikang Chen", "Bo Jiang", "Kun Kuang", "Mingbao Lin"], "title": "Reversible Diffusion Decoding for Diffusion Language Models", "comment": null, "summary": "Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u9006\u6269\u6563\u89e3\u7801\uff08RDD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9006\u673a\u5236\u89e3\u51b3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5757\u89e3\u7801\u4e2d\u7684\u505c\u6ede\u95ee\u9898\uff0c\u5141\u8bb8\u56de\u6eaf\u5230\u65e9\u671f\u5757\u5e76\u9009\u62e9\u6027\u91cd\u65b0\u521d\u59cb\u5316\u4e0d\u786e\u5b9a\u6807\u8bb0\uff0c\u63d0\u9ad8\u751f\u6210\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5757\u89e3\u7801\u5b9e\u73b0\u5e76\u884c\u6807\u8bb0\u751f\u6210\uff0c\u4f46\u5176\u4e0d\u53ef\u9006\u7684\u627f\u8bfa\u4f1a\u5bfc\u81f4\u505c\u6ede\u95ee\u9898\u2014\u2014\u5f53\u4e0a\u4e0b\u6587\u4e0d\u7406\u60f3\u65f6\uff0c\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u65e0\u6cd5\u8fdb\u4e00\u6b65\u8fdb\u5c55\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u65e9\u671f\u627f\u8bfa\u9519\u8bef\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u53ef\u9006\u6269\u6563\u89e3\u7801\uff08RDD\uff09\u6846\u67b6\uff1a1\uff09\u68c0\u6d4b\u505c\u6ede\u4f5c\u4e3a\u53cd\u5411\u8fc7\u7a0b\u7684\u72b6\u6001\u76f8\u5173\u5931\u8d25\uff1b2\uff09\u901a\u8fc7\u7f13\u5b58\u6a21\u578b\u72b6\u6001\u5b9e\u73b0\u9ad8\u6548\u56de\u6eaf\u5230\u65e9\u671f\u5757\u800c\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97\uff1b3\uff09\u5e94\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u91cd\u65b0\u63a9\u7801\uff0c\u9009\u62e9\u6027\u91cd\u65b0\u521d\u59cb\u5316\u4e0d\u786e\u5b9a\u6807\u8bb0\u540c\u65f6\u4fdd\u7559\u53ef\u9760\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRDD\u5728\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u4e0b\u63d0\u9ad8\u4e86\u751f\u6210\u7684\u9c81\u68d2\u6027\u548c\u8d28\u91cf\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002\u53ef\u9006\u673a\u5236\u5141\u8bb8\u89e3\u7801\u4ece\u65e9\u671f\u627f\u8bfa\u9519\u8bef\u4e2d\u6062\u590d\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u4e8e\u6269\u6563\u751f\u6210\u7684\u5e76\u884c\u6548\u7387\u3002", "conclusion": "RDD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5757\u89e3\u7801\u4e2d\u7684\u505c\u6ede\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u9006\u6027\u548c\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u91cd\u65b0\u521d\u59cb\u5316\uff0c\u5728\u4fdd\u6301\u5e76\u884c\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u751f\u6210\u9c81\u68d2\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u673a\u5236\u3002"}}
{"id": "2602.00109", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00109", "abs": "https://arxiv.org/abs/2602.00109", "authors": ["John J. Howard", "Richard O. Plesh", "Yevgeniy B. Sirotin", "Jerry L. Tipton", "Arun R. Vemury"], "title": "Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios", "comment": "Accepted to the IEEE/CVF WACV 2026 Workshop on Generative, Adversarial and Presentation Attacks in Biometrics (GAPBio). 8 pages, 6 figures, 4 tables", "summary": "Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.", "AI": {"tldr": "\u5546\u4e1aPAD\u7cfb\u7edf\u5728\u4f4e\u5149\u548c\u81ea\u52a8\u91c7\u96c6\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4ec5\u4e00\u4e2a\u7cfb\u7edf\u80fd\u4fdd\u6301\u7a33\u5065\u8868\u73b0", "motivation": "\u8fdc\u7a0b\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\uff0c\u6f14\u793a\u653b\u51fb\u68c0\u6d4b\u5b50\u7cfb\u7edf\u9700\u8981\u5728\u591a\u6837\u73af\u5883\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4f46\u4f4e\u5149\u7167\u548c\u81ea\u52a8\u56fe\u50cf\u91c7\u96c6\u5bf9\u5546\u4e1aPAD\u7cfb\u7edf\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76", "method": "\u901a\u8fc7\u8fdc\u7a0b\u8eab\u4efd\u9a8c\u8bc1\u7684\u573a\u666f\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5546\u4e1aPAD\u7cfb\u7edf\u5728\u4f4e\u5149\u7167\u6761\u4ef6\u548c\u81ea\u52a8\u56fe\u50cf\u91c7\u96c6\u5de5\u4f5c\u6d41\u7a0b\u4e0b\u7684\u6027\u80fd\u8868\u73b0", "result": "\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u9519\u8bef\u7387\u589e\u52a0\u7ea64\u500d\uff0c\u81ea\u52a8\u91c7\u96c6\u5de5\u4f5c\u6d41\u7a0b\u4e0b\u9519\u8bef\u7387\u589e\u52a02\u500d\uff1b\u4ec5\u4e00\u4e2a\u6d4b\u8bd5\u7cfb\u7edf\u5728\u6240\u6709\u573a\u666f\u4e0b\u4fdd\u6301\u6700\u5927\u771f\u5b9e\u6f14\u793a\u5206\u7c7b\u9519\u8bef\u7387\u4f4e\u4e8e3%", "conclusion": "\u4e3a\u786e\u4fddPAD\u7cfb\u7edf\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u5fc5\u987b\u5728\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5f53\u524d\u5927\u591a\u6570\u5546\u4e1a\u7cfb\u7edf\u5728\u4f4e\u5149\u548c\u81ea\u52a8\u91c7\u96c6\u573a\u666f\u4e0b\u5b58\u5728\u663e\u8457\u6027\u80fd\u7f3a\u9677"}}
{"id": "2602.00870", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00870", "abs": "https://arxiv.org/abs/2602.00870", "authors": ["Shiyuan Li", "Hossein Salahshoor"], "title": "Finite Element Eigenfunction Network (FEENet): A Hybrid Framework for Solving PDEs on Complex Geometries", "comment": null, "summary": "Neural operators aim to learn mappings between infinite-dimensional function spaces, but their performance often degrades on complex or irregular geometries due to the lack of geometry-aware representations. We propose the Finite Element Eigenfunction Network (FEENet), a hybrid spectral learning framework grounded in the eigenfunction theory of differential operators. For a given domain, FEENet leverages the Finite Element Method (FEM)toperformaone-timecomputationofaneigenfunctionbasisintrinsictothegeometry. PDE solutions are subsequently represented in this geometry-adapted basis, and learning is reduced to predicting the corresponding spectral coefficients. Numerical experiments conducted across a range of parameterized PDEs and complex two- and three-dimensional geometries, including benchmarks against the seminal DeepONet framework (1), demonstrate that FEENet consistently achieves superior accuracy and computational efficiency. We further highlight key advantages of the proposed approach, including resolution-independent inference, interpretability, and natural generalization to nonlocal operators defined as functions of differential operators. We envision that hybrid approaches of this form, which combine structure-preserving numerical methods with data-driven learning, offer a promising pathway toward solving real-world PDE problems on complex geometries.", "AI": {"tldr": "FEENet\uff1a\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u5143\u7279\u5f81\u51fd\u6570\u7684\u6df7\u5408\u8c31\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u81ea\u9002\u5e94\u7684\u7279\u5f81\u51fd\u6570\u57fa\u8868\u793aPDE\u89e3\uff0c\u5728\u590d\u6742\u51e0\u4f55\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387", "motivation": "\u795e\u7ecf\u7b97\u5b50\u5728\u590d\u6742\u6216\u4e0d\u89c4\u5219\u51e0\u4f55\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u7f3a\u4e4f\u51e0\u4f55\u611f\u77e5\u8868\u793a\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u590d\u6742\u51e0\u4f55\u7684PDE\u6c42\u89e3\u65b9\u6cd5", "method": "\u63d0\u51faFEENet\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\u4e00\u6b21\u6027\u8ba1\u7b97\u51e0\u4f55\u5185\u5728\u7684\u7279\u5f81\u51fd\u6570\u57fa\uff1b2\uff09PDE\u89e3\u7528\u8be5\u51e0\u4f55\u81ea\u9002\u5e94\u57fa\u8868\u793a\uff1b3\uff09\u5b66\u4e60\u7b80\u5316\u4e3a\u9884\u6d4b\u76f8\u5e94\u7684\u8c31\u7cfb\u6570", "result": "\u5728\u591a\u79cd\u53c2\u6570\u5316PDE\u548c\u590d\u67422D/3D\u51e0\u4f55\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cFEENet\u76f8\u6bd4DeepONet\u6846\u67b6\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u5206\u8fa8\u7387\u65e0\u5173\u63a8\u7406\u3001\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u81ea\u7136\u63a8\u5e7f\u5230\u975e\u5c40\u90e8\u7b97\u5b50", "conclusion": "\u8fd9\u79cd\u7ed3\u5408\u7ed3\u6784\u4fdd\u6301\u6570\u503c\u65b9\u6cd5\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\u4e3a\u89e3\u51b3\u590d\u6742\u51e0\u4f55\u4e0a\u7684\u5b9e\u9645PDE\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84"}}
{"id": "2602.00307", "categories": ["cs.AI", "cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00307", "abs": "https://arxiv.org/abs/2602.00307", "authors": ["Udayan Khurana"], "title": "Autonomous Data Processing using Meta-Agents", "comment": null, "summary": "Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.", "AI": {"tldr": "ADP-MA\u662f\u4e00\u4e2a\u901a\u8fc7\u5143\u4ee3\u7406\u5c42\u6b21\u5316\u7f16\u6392\u52a8\u6001\u6784\u5efa\u3001\u6267\u884c\u548c\u8fed\u4ee3\u4f18\u5316\u6570\u636e\u5904\u7406\u7ba1\u9053\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5316\u548c\u81ea\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u5212\u5206\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5904\u7406\u7ba1\u9053\u901a\u5e38\u662f\u9759\u6001\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u624b\u5de5\u8bbe\u8ba1\u7684\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e0d\u65ad\u53d8\u5316\u9700\u6c42\u7684\u9002\u5e94\u6027\u3002\u73b0\u6709\u7684\u901a\u7528\u4ee3\u7406\u548c\u7f16\u7801\u52a9\u624b\u867d\u7136\u80fd\u4e3a\u5df2\u77e5\u7684\u6570\u636e\u7ba1\u9053\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u7f3a\u4e4f\u5728\u90e8\u7f72\u540e\u81ea\u4e3b\u76d1\u63a7\u3001\u7ba1\u7406\u548c\u4f18\u5316\u7aef\u5230\u7aef\u7ba1\u9053\u7684\u80fd\u529b\u3002", "method": "ADP-MA\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u4ee3\u7406\u7f16\u6392\u5b9e\u73b0\uff1a\u6838\u5fc3\u7684\u5143\u4ee3\u7406\u5206\u6790\u8f93\u5165\u6570\u636e\u548c\u4efb\u52a1\u89c4\u8303\uff0c\u8bbe\u8ba1\u591a\u9636\u6bb5\u8ba1\u5212\uff0c\u5b9e\u4f8b\u5316\u4e13\u95e8\u7684\u5730\u9762\u7ea7\u4ee3\u7406\uff0c\u5e76\u6301\u7eed\u8bc4\u4f30\u7ba1\u9053\u6027\u80fd\u3002\u67b6\u6784\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7b56\u7565\u751f\u6210\u7684\u89c4\u5212\u6a21\u5757\u3001\u4ee3\u7406\u534f\u8c03\u548c\u5de5\u5177\u96c6\u6210\u7684\u7f16\u6392\u5c42\u3001\u4ee5\u53ca\u8fed\u4ee3\u8bc4\u4f30\u548c\u56de\u6eaf\u7684\u76d1\u63a7\u5faa\u73af\u3002", "result": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6f14\u793a\u5c55\u793a\u4e86ADP-MA\u5728\u4ee3\u8868\u6027\u6570\u636e\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u7ba1\u9053\u6784\u5efa\u3001\u6267\u884c\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u4f18\u5316\u80fd\u529b\u3002\u6846\u67b6\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5316\u3001\u81ea\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u5212\u5206\u548c\u6e10\u8fdb\u91c7\u6837\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ADP-MA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u4e3b\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u6784\u5efa\u3001\u6267\u884c\u548c\u8fed\u4ee3\u4f18\u5316\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9759\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u91cd\u7528\u5148\u524d\u8bbe\u8ba1\u7684\u4ee3\u7406\u548c\u96c6\u6210\u591a\u6837\u5316\u5916\u90e8\u5de5\u5177\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u5e76\u52a0\u901f\u4e86\u7ba1\u9053\u6784\u5efa\u3002"}}
{"id": "2602.01737", "categories": ["physics.flu-dyn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01737", "abs": "https://arxiv.org/abs/2602.01737", "authors": ["Biao Chen", "Jing Wang", "Hairun Xie", "Qineng Wang", "Shuai Zhang", "Yifan Xia", "Jifa Zhang"], "title": "Physics-Informed Chebyshev Polynomial Neural Operator for Parametric Partial Differential Equations", "comment": "28pages", "summary": "Neural operators have emerged as powerful deep learning frameworks for approximating solution operators of parameterized partial differential equations (PDE). However, current methods predominantly rely on multilayer perceptrons (MLPs) for mapping inputs to solutions, which impairs training robustness in physics-informed settings due to inherent spectral biases and fixed activation functions. To overcome the architectural limitations, we introduce the Physics-Informed Chebyshev Polynomial Neural Operator (CPNO), a novel mesh-free framework that leverages a basis transformation to replace unstable monomial expansions with the numerically stable Chebyshev spectral basis. By integrating parameter dependent modulation mechanism to main net, CPNO constructs PDE solutions in a near-optimal functional space, decoupling the model from MLP-specific constraints and enhancing multi-scale representation. Theoretical analysis demonstrates the Chebyshev basis's near-minimax uniform approximation properties and superior conditioning, with Lebesgue constants growing logarithmically with degree, thereby mitigating spectral bias and ensuring stable gradient flow during optimization. Numerical experiments on benchmark parameterized PDEs show that CPNO achieves superior accuracy, faster convergence, and enhanced robustness to hyperparameters. The experiment of transonic airfoil flow has demonstrated the capability of CPNO in characterizing complex geometric problems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8c31\u57fa\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50(CPNO)\uff0c\u901a\u8fc7\u8c31\u57fa\u53d8\u6362\u66ff\u4ee3\u4f20\u7edfMLP\uff0c\u89e3\u51b3\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u4e2d\u7684\u8c31\u504f\u5dee\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u53c2\u6570\u5316PDE\u6c42\u89e3\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7b97\u5b50\u4e3b\u8981\u4f9d\u8d56\u591a\u5c42\u611f\u77e5\u673a(MLP)\u5c06\u8f93\u5165\u6620\u5c04\u5230\u89e3\uff0c\u5728\u7269\u7406\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u5b58\u5728\u8c31\u504f\u5dee\u548c\u56fa\u5b9a\u6fc0\u6d3b\u51fd\u6570\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u8bad\u7ec3\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u8981\u514b\u670dMLP\u67b6\u6784\u7684\u5c40\u9650\u6027\uff0c\u6784\u5efa\u66f4\u7a33\u5b9a\u3001\u9ad8\u6548\u7684PDE\u6c42\u89e3\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u795e\u7ecf\u7b97\u5b50(CPNO)\uff1a1) \u4f7f\u7528\u5207\u6bd4\u96ea\u592b\u8c31\u57fa\u66ff\u4ee3\u4e0d\u7a33\u5b9a\u7684\u5355\u9879\u5f0f\u5c55\u5f00\uff0c\u5b9e\u73b0\u6570\u503c\u7a33\u5b9a\u7684\u57fa\u53d8\u6362\uff1b2) \u96c6\u6210\u53c2\u6570\u4f9d\u8d56\u8c03\u5236\u673a\u5236\u5230\u4e3b\u7f51\u7edc\uff1b3) \u5728\u8fd1\u4f3c\u6700\u4f18\u51fd\u6570\u7a7a\u95f4\u4e2d\u6784\u9020PDE\u89e3\uff0c\u89e3\u8026MLP\u7279\u5b9a\u7ea6\u675f\uff1b4) \u589e\u5f3a\u591a\u5c3a\u5ea6\u8868\u793a\u80fd\u529b\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u5207\u6bd4\u96ea\u592b\u57fa\u5177\u6709\u8fd1\u4f3c\u6781\u5c0f\u6781\u5927\u4e00\u81f4\u903c\u8fd1\u7279\u6027\u548c\u4f18\u8d8a\u7684\u6761\u4ef6\u6570\uff0cLebesgue\u5e38\u6570\u968f\u6b21\u6570\u5bf9\u6570\u589e\u957f\uff0c\u7f13\u89e3\u8c31\u504f\u5dee\u5e76\u786e\u4fdd\u4f18\u5316\u4e2d\u68af\u5ea6\u6d41\u7a33\u5b9a\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660eCPNO\u5728\u57fa\u51c6\u53c2\u6570\u5316PDE\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u7cbe\u5ea6\u3001\u66f4\u5feb\u6536\u655b\u548c\u8d85\u53c2\u6570\u9c81\u68d2\u6027\uff0c\u8de8\u97f3\u901f\u7ffc\u578b\u6d41\u52a8\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5904\u7406\u590d\u6742\u51e0\u4f55\u95ee\u9898\u7684\u80fd\u529b\u3002", "conclusion": "CPNO\u901a\u8fc7\u5207\u6bd4\u96ea\u592b\u8c31\u57fa\u53d8\u6362\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfMLP\u795e\u7ecf\u7b97\u5b50\u7684\u67b6\u6784\u9650\u5236\uff0c\u5728\u7269\u7406\u4fe1\u606fPDE\u6c42\u89e3\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3001\u903c\u8fd1\u80fd\u529b\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u4e3a\u590d\u6742\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.00062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00062", "abs": "https://arxiv.org/abs/2602.00062", "authors": ["Ming-Yao Ho", "Cheng-Kai Wang", "You-Teng Lin", "Hung-Hsuan Chen"], "title": "SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism", "comment": null, "summary": "Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/", "AI": {"tldr": "\u63d0\u51faSCPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u53cd\u5411\u4f20\u64ad\u5c06\u957f\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u591a\u4e2a\u77ed\u68af\u5ea6\u6d41\uff0c\u5b9e\u73b0\u5c42\u95f4\u5e76\u884c\u8ba1\u7b97\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u4f01\u4e1a\u4fe1\u606f\u7cfb\u7edf\u91c7\u7528\u5927\u89c4\u6a21AI\u6a21\u578b\u9762\u4e34\u9ad8\u8bad\u7ec3\u6210\u672c\u548c\u957f\u5f00\u53d1\u5468\u671f\u7684\u95ee\u9898\uff0c\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u662f\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u4e3b\u8981\u539f\u56e0", "method": "\u63d0\u51fa\u76d1\u7763\u5bf9\u6bd4\u5e76\u884c\u5b66\u4e60\uff08SCPL\uff09\u65b9\u6cd5\uff0c\u89e3\u8026\u53cd\u5411\u4f20\u64ad\uff0c\u5c06\u957f\u68af\u5ea6\u6d41\u8f6c\u5316\u4e3a\u591a\u4e2a\u77ed\u68af\u5ea6\u6d41\uff0c\u5b9e\u73b0\u4e0d\u540c\u5c42\u53c2\u6570\u68af\u5ea6\u7684\u5e76\u884c\u8ba1\u7b97", "result": "\u76f8\u6bd4BP\u3001Early Exit\u3001GPipe\u548cAL\u7b49\u65b9\u6cd5\uff0cSCPL\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u7f13\u89e3\u4e86\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u90e8\u7f72\u8def\u5f84", "conclusion": "SCPL\u4e3a\u7ec4\u7ec7\u5f00\u53d1\u90e8\u7f72\u5148\u8fdb\u4fe1\u606f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u3001\u66f4\u654f\u6377\u7684\u5b9e\u7528\u9014\u5f84\uff0c\u5b9e\u9a8c\u4ee3\u7801\u5df2\u5f00\u6e90\u786e\u4fdd\u53ef\u590d\u73b0\u6027"}}
{"id": "2602.00238", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00238", "abs": "https://arxiv.org/abs/2602.00238", "authors": ["Tianyi Hu", "Niket Tandon", "Akhil Arora"], "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking", "comment": null, "summary": "Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge", "AI": {"tldr": "DIVERGE\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u6027\u95ee\u9898\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u5c04\u5f15\u5bfc\u751f\u6210\u548c\u8bb0\u5fc6\u589e\u5f3a\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u56de\u7b54\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RAG\u7cfb\u7edf\u5728\u591a\u5143\u7b54\u6848\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6b63\u786e\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u4fe1\u606f\u68c0\u7d22\u4e2d\u7ecf\u5e38\u5b58\u5728\u591a\u4e2a\u5408\u7406\u7b54\u6848\u7684\u573a\u666f\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u5bfc\u81f4\u7cfb\u7edf\u65e0\u6cd5\u5145\u5206\u5229\u7528\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u521b\u9020\u6027\uff0c\u5e76\u5f71\u54cd\u4e86\u4fe1\u606f\u83b7\u53d6\u7684\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027\u3002", "method": "\u63d0\u51faDIVERGE\u6846\u67b6\uff0c\u91c7\u7528\u5373\u63d2\u5373\u7528\u7684\u4ee3\u7406\u5f0fRAG\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u53cd\u5c04\u5f15\u5bfc\u751f\u6210\uff0c\u901a\u8fc7\u53cd\u601d\u673a\u5236\u4fc3\u8fdb\u591a\u6837\u5316\u89c2\u70b9\uff1b2) \u8bb0\u5fc6\u589e\u5f3a\u8fed\u4ee3\u4f18\u5316\uff0c\u5229\u7528\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002\u8fd8\u8bbe\u8ba1\u4e86\u4e13\u95e8\u8bc4\u4f30\u5f00\u653e\u6027\u95ee\u9898\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\u7684\u65b0\u6307\u6807\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cInfinity-Chat\u6570\u636e\u96c6\u4e0a\uff0cDIVERGE\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u548c\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u7b54\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u4e86\u8d28\u91cf\u3002\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u6709\u826f\u597d\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u5f00\u653e\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u7cfb\u7edf\u6027\u5c40\u9650\uff0c\u8868\u660e\u660e\u786e\u5efa\u6a21\u591a\u6837\u6027\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002DIVERGE\u6846\u67b6\u4e3a\u5904\u7406\u591a\u5143\u7b54\u6848\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u66f4\u516c\u5e73\u3001\u5305\u5bb9\u7684\u4fe1\u606f\u8bbf\u95ee\u3002"}}
{"id": "2602.00110", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00110", "abs": "https://arxiv.org/abs/2602.00110", "authors": ["Yu Li", "Guilherme N. DeSouza", "Praveen Rao", "Chi-Ren Shyu"], "title": "Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u9065\u611f\u56fe\u50cf\u5904\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u673a\u5236\u548c\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5c06\u8f85\u52a9\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u4e0e\u89c6\u89c9\u7279\u5f81\u52a8\u6001\u6574\u5408\uff0c\u63d0\u5347\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6a21\u578b\u4e3b\u8981\u4f18\u5316\u89c6\u89c9\u4e0e\u6587\u672c\u5185\u5bb9\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u5bf9\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u5c42\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u3002", "method": "1) \u5730\u7406\u7a7a\u95f4\u5d4c\u5165\u673a\u5236\uff1a\u5c06\u591a\u6837\u5316\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u8f6c\u6362\u4e3a\u4e0e\u56fe\u50cf\u5757\u7a7a\u95f4\u5bf9\u9f50\u7684\u5d4c\u5165\u5757\uff1b2) \u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff1a\u57fa\u4e8e\u4e0e\u8f85\u52a9\u6570\u636e\u7684\u76f8\u5173\u6027\u52a8\u6001\u8ba1\u7b97\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff1b3) \u4e3a\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5206\u914d\u4e0d\u540c\u89d2\u8272\uff0c\u6355\u6349\u8f85\u52a9\u4fe1\u606f\u7684\u4e92\u8865\u65b9\u9762\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u75be\u75c5\u6d41\u884c\u7387\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u6a21\u6001\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u6709\u6548\u6574\u5408\u5730\u7406\u7a7a\u95f4\u6307\u5bfc\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u5904\u7406\u7684\u5730\u7406\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5730\u7406\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00897", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.00897", "abs": "https://arxiv.org/abs/2602.00897", "authors": ["Abdellatif Mouhssine"], "title": "A New Combination of Preconditioned Gradient Descent Methods and Vector Extrapolation Techniques for Nonlinear Least-Squares Problems", "comment": null, "summary": "Vector extrapolation methods are widely used in large-scale simulation studies, and numerous extrapolation-based acceleration techniques have been developed to enhance the convergence of linear and nonlinear fixed-point iterative methods. While classical extrapolation strategies often reduce the number of iterations or the computational cost, they do not necessarily lead to a significant improvement in the accuracy of the computed approximations. In this paper, we study the combination of preconditioned gradient-based methods with extrapolation strategies and propose an extrapolation-accelerated framework that simultaneously improves convergence and approximation accuracy. The focus is on the solution of nonlinear least-squares problems through the integration of vector extrapolation techniques with preconditioned gradient descent methods. A comprehensive set of numerical experiments is carried out to study the behavior of polynomial-type extrapolation methods and the vector $\\varepsilon$-algorithm when coupled with gradient descent schemes, with and without preconditioning. The results demonstrate the impact of extrapolation techniques on both convergence rate and solution accuracy, and report iteration counts, computational times, and relative reconstruction errors. The performance of the proposed hybrid approaches is further assessed through a benchmarking study against Gauss--Newton methods based on generalized Krylov subspaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u5411\u91cf\u5916\u63a8\u6cd5\u4e0e\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u6cd5\u7ed3\u5408\uff0c\u7528\u4e8e\u52a0\u901f\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u540c\u65f6\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5916\u63a8\u7b56\u7565\u867d\u7136\u80fd\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u6216\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u672a\u5fc5\u80fd\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u8fd1\u4f3c\u7684\u7cbe\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u6539\u5584\u6536\u655b\u6027\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u7684\u5916\u63a8\u52a0\u901f\u6846\u67b6\u3002", "method": "\u5c06\u5411\u91cf\u5916\u63a8\u6280\u672f\uff08\u591a\u9879\u5f0f\u578b\u5916\u63a8\u65b9\u6cd5\u548c\u5411\u91cf\u03b5\u7b97\u6cd5\uff09\u4e0e\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u6cd5\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u5916\u63a8\u52a0\u901f\u6846\u67b6\uff0c\u7528\u4e8e\u6c42\u89e3\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u5916\u63a8\u6280\u672f\u5bf9\u6536\u655b\u901f\u5ea6\u548c\u6c42\u89e3\u7cbe\u5ea6\u5747\u6709\u79ef\u6781\u5f71\u54cd\u3002\u62a5\u544a\u4e86\u8fed\u4ee3\u6b21\u6570\u3001\u8ba1\u7b97\u65f6\u95f4\u548c\u76f8\u5bf9\u91cd\u6784\u8bef\u5dee\u7b49\u6307\u6807\uff0c\u5e76\u4e0e\u57fa\u4e8e\u5e7f\u4e49Krylov\u5b50\u7a7a\u95f4\u7684\u9ad8\u65af-\u725b\u987f\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u52a0\u901f\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u5728\u6536\u655b\u901f\u5ea6\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u6280\u672f\u3002"}}
{"id": "2602.00327", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00327", "abs": "https://arxiv.org/abs/2602.00327", "authors": ["Yueyi Yang", "Haotian Liu", "Fang Kang", "Mengqi Zhang", "Zheng Lian", "Hao Tang", "Haoyu Chen"], "title": "SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?", "comment": null, "summary": "We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSayNext-Bench\u57fa\u51c6\u548cSayNext-Chat\u6a21\u578b\uff0c\u63a2\u7d22LLMs\u5728\u5bf9\u8bdd\u4e2d\u57fa\u4e8e\u591a\u6a21\u6001\u7ebf\u7d22\u9884\u6d4b\u4e0b\u4e00\u8bdd\u8bed\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u8def\u5f84\u9884\u6d4b\u8bbe\u8ba1\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u5bf9\u8bdd\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u5728\u9884\u6d4b\u4eba\u7c7b\u4e0b\u4e00\u8bdd\u8bed\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u4eba\u7c7b\u80fd\u591f\u57fa\u4e8e\u624b\u52bf\u3001\u6ce8\u89c6\u3001\u60c5\u611f\u8bed\u8c03\u7b49\u591a\u6a21\u6001\u7ebf\u7d22\u8f7b\u677e\u9884\u6d4b\u4e0b\u4e00\u8bdd\u8bed\uff0c\u800c\u5f53\u524d\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002\u8fd9\u63ed\u793a\u4e86\u591a\u6a21\u6001\u7ebf\u7d22\u548c\u4e3b\u52a8\u9884\u6d4b\u5904\u7406\u5728\u81ea\u7136\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "1) \u63d0\u51faSayNext-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30LLMs\u548cMLLMs\u5728\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u4e2d\u57fa\u4e8e\u591a\u6a21\u6001\u7ebf\u7d22\u9884\u6d4b\u4e0a\u4e0b\u6587\u6761\u4ef6\u5316\u54cd\u5e94\u7684\u80fd\u529b\uff1b2) \u6784\u5efaSayNext-PC\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u591a\u6a21\u6001\u7ebf\u7d22\u7684\u5bf9\u8bdd\uff1b3) \u5f00\u53d1SayNext-Chat\u53cc\u8def\u5f84\u9884\u6d4bMLLM\uff0c\u91c7\u7528\u8ba4\u77e5\u542f\u53d1\u7684\u8bbe\u8ba1\u6a21\u62df\u5bf9\u8bdd\u4e2d\u7684\u9884\u6d4b\u5904\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSayNext-Chat\u6a21\u578b\u5728\u8bcd\u6c47\u91cd\u53e0\u5ea6\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u60c5\u611f\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002\u8fd9\u8bc1\u660e\u4e86LLMs\u57fa\u4e8e\u591a\u6a21\u6001\u7ebf\u7d22\u8fdb\u884c\u4e0b\u4e00\u8bdd\u8bed\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u7ebf\u7d22\u548c\u4e3b\u52a8\u9884\u6d4b\u5904\u7406\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u7ebf\u7d22\u7684\u4e0b\u4e00\u8bdd\u8bed\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u7ebf\u7d22\u548c\u4e3b\u52a8\u9884\u6d4b\u5904\u7406\u5728\u81ea\u7136\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u7684\u4e0d\u53ef\u6216\u7f3a\u4f5c\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u4eba\u6027\u5316\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684AI\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u672c\u7684AI\u3002"}}
{"id": "2602.01891", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.01891", "abs": "https://arxiv.org/abs/2602.01891", "authors": ["Sarika Shivaji Bangar", "Gaurav Tomar"], "title": "On Large Deformations of Oldroyd-B Drops in a Steady Electric Field", "comment": "Preprint prepared for submission to Physical Review Fluids. Minor formatting changes were made for the arXiv version", "summary": "The deformation of viscoelastic drops under electric fields is central to applications in microfluidics, inkjet printing, and electrohydrodynamic manipulation of complex fluids. This study investigates the dynamics of an Oldroyd-B drop subjected to a uniform electric field using numerical simulations performed with the open-source solver Basilisk. Representative pairs of conductivity ratio ($\u03c3_r$) and permittivity ratio ($\u03b5_r$) are selected from six regions ($PR_A^+$, $PR_B^+$, $PR_A^-$, $PR_B^-$, $OB^+$, and $OB^-$) of the $(\u03c3_r, \u03b5_r)$ phase space. In regions where the first- and second-order deformation coefficients share the same sign ($PR_A^-$, $PR_B^-$, $OB^+$), deviations from Newtonian behavior are negligible. In $PR_A^+$, drops develop multi-lobed shapes above a critical electric capillary number, with elasticity reducing deformation and increasing the critical $Ca_E$ with Deborah number ($De$). In $PR_B^+$, drops form shapes with conical ends above the critical $Ca_E$, while steady-state deformation decreases with $De$ below this threshold, and critical $Ca_E$ shows non-monotonic variation. At high $Ca_E$ and $De$, transient deformation exhibits maxima and minima before reaching steady state, with occasional oscillations between spheroidal and pointed shapes. In $OB^-$, drops deform to oblate shapes and breakup above a critical $Ca_E$, with deformation magnitude increasing and critical $Ca_E$ decreasing with $De$; at low $Ca_E$ and high $De$, dimpling and positional oscillations are observed. These results elucidate viscoelastic-electric interactions and provide guidance for controlling drop behavior in practical applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5206\u6790\u4e86Oldroyd-B\u7c98\u5f39\u6027\u6db2\u6ef4\u5728\u5747\u5300\u7535\u573a\u4e2d\u7684\u53d8\u5f62\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u7535\u5bfc\u7387/\u4ecb\u7535\u5e38\u6570\u6bd4\u533a\u57df\u4e2d\uff0c\u6db2\u6ef4\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u53d8\u5f62\u6a21\u5f0f\uff0c\u5305\u62ec\u591a\u74e3\u5f62\u3001\u9525\u5f62\u672b\u7aef\u548c\u6241\u7403\u5f62\u7b49\uff0c\u5f39\u6027\u6548\u5e94\u663e\u8457\u5f71\u54cd\u4e34\u754c\u7535\u573a\u6bdb\u7ec6\u6570\u548c\u53d8\u5f62\u884c\u4e3a\u3002", "motivation": "\u7c98\u5f39\u6027\u6db2\u6ef4\u5728\u7535\u573a\u4e2d\u7684\u53d8\u5f62\u884c\u4e3a\u5bf9\u5fae\u6d41\u63a7\u3001\u55b7\u58a8\u6253\u5370\u548c\u590d\u6742\u6d41\u4f53\u7535\u6db2\u52a8\u529b\u5b66\u64cd\u63a7\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u7c98\u5f39\u6027\u4e0e\u7535\u573a\u76f8\u4e92\u4f5c\u7528\u7684\u52a8\u529b\u5b66\u7406\u89e3\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u6c42\u89e3\u5668Basilisk\u8fdb\u884c\u6570\u503c\u6a21\u62df\uff0c\u7814\u7a76Oldroyd-B\u6db2\u6ef4\u5728\u5747\u5300\u7535\u573a\u4e2d\u7684\u52a8\u529b\u5b66\uff0c\u4ece(\u03c3_r, \u03b5_r)\u76f8\u7a7a\u95f4\u7684\u516d\u4e2a\u533a\u57df\u9009\u53d6\u4ee3\u8868\u6027\u53c2\u6570\u5bf9\uff0c\u5206\u6790\u4e0d\u540c\u7535\u5bfc\u7387\u6bd4\u548c\u4ecb\u7535\u5e38\u6570\u6bd4\u4e0b\u7684\u53d8\u5f62\u884c\u4e3a\u3002", "result": "\u5728\u4e0d\u540c\u53c2\u6570\u533a\u57df\u89c2\u5bdf\u5230\u591a\u79cd\u53d8\u5f62\u6a21\u5f0f\uff1aPR_A^+\u533a\u57df\u6db2\u6ef4\u5728\u4e34\u754c\u7535\u573a\u6bdb\u7ec6\u6570\u4ee5\u4e0a\u5f62\u6210\u591a\u74e3\u5f62\uff0c\u5f39\u6027\u51cf\u5c0f\u53d8\u5f62\u5e76\u63d0\u9ad8\u4e34\u754c\u503c\uff1bPR_B^+\u533a\u57df\u5f62\u6210\u9525\u5f62\u672b\u7aef\uff0c\u4e34\u754c\u503c\u5448\u975e\u5355\u8c03\u53d8\u5316\uff1bOB^-\u533a\u57df\u5f62\u6210\u6241\u7403\u5f62\u5e76\u7834\u88c2\uff0c\u4e34\u754c\u503c\u968fDe\u51cf\u5c0f\uff1b\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8fd8\u89c2\u5bdf\u5230\u77ac\u6001\u632f\u8361\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u7c98\u5f39\u6027\u4e0e\u7535\u573a\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u63a7\u5236\u6db2\u6ef4\u884c\u4e3a\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7cbe\u786e\u64cd\u63a7\u590d\u6742\u6d41\u4f53\u53d8\u5f62\u7684\u5fae\u6d41\u63a7\u548c\u6253\u5370\u6280\u672f\u4e2d\u3002"}}
{"id": "2602.00063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00063", "abs": "https://arxiv.org/abs/2602.00063", "authors": ["Leonidas Christodoulou", "Chang Sun"], "title": "The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations", "comment": null, "summary": "Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u5373\u4f7f\u6a21\u578b\u51c6\u786e\u5ea6\u8f7b\u5fae\u4e0b\u964d\u4e5f\u4f1a\u5bfc\u81f4\u53cd\u4e8b\u5b9e\u7ed3\u679c\u5927\u5e45\u53d8\u5316\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5927\u591a\u672a\u8003\u8651\u6a21\u578b\u548c\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u73b0\u5b9e\u4e16\u754c\u53d8\u5316\u4e0b\u89e3\u91ca\u53ef\u80fd\u4e0d\u7a33\u5b9a\u6216\u65e0\u6548\u3002\u9700\u8981\u7814\u7a76\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u5b58\u5728\u5076\u7136\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0c\u7814\u7a76\u5e38\u89c1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u53cd\u4e8b\u5b9e\u751f\u6210\u7b97\u6cd5\u7ec4\u5408\u5728\u5b58\u5728\u5076\u7136\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u65f6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u654f\u611f\u3002\u5373\u4f7f\u7531\u566a\u58f0\u589e\u52a0\u6216\u6570\u636e\u6709\u9650\u5f15\u8d77\u7684\u6a21\u578b\u51c6\u786e\u5ea6\u8f7b\u5fae\u4e0b\u964d\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u5728\u5e73\u5747\u6c34\u5e73\u548c\u5355\u4e2a\u5b9e\u4f8b\u4e0a\u51fa\u73b0\u5927\u5e45\u53d8\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u91d1\u878d\u548c\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u9700\u8981\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u73b0\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.00279", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00279", "abs": "https://arxiv.org/abs/2602.00279", "authors": ["Philip M\u00fcller", "Nicholas Popovi\u010d", "Michael F\u00e4rber", "Peter Steinbach"], "title": "Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering", "comment": "Under Review", "summary": "Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u95ee\u7b54\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u5bfc\u81f4\u6982\u7387\u6781\u5316\uff0c\u5e8f\u5217\u7ea7\u65b9\u6cd5\u4e2d\u7b54\u6848\u9891\u7387\u662f\u6700\u53ef\u9760\u7684\u6821\u51c6\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u79d1\u5b66\u95ee\u7b54\u9886\u57df\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u800c\u79d1\u5b66\u95ee\u7b54\u4f9d\u8d56\u4e8b\u5b9e\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8eLLM\u751f\u6210\u7b54\u6848\u7684\u53ef\u4fe1\u91c7\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6db5\u76d620\u4e2aLLM\uff08\u57fa\u7840\u3001\u6307\u4ee4\u5fae\u8c03\u548c\u63a8\u7406\u53d8\u4f53\uff09\uff0c\u57287\u4e2a\u79d1\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4ee3\u8868\u6027UQ\u65b9\u6cd5\uff0c\u5206\u679068.5\u4e07\u4e2a\u957f\u683c\u5f0f\u56de\u7b54\uff0c\u7814\u7a76\u4e0d\u540c\u63a8\u7406\u590d\u6742\u5ea6\u4e0b\u7684\u6821\u51c6\u6027\u80fd\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u5bfc\u81f4token\u7ea7\u6982\u7387\u8d28\u91cf\u6781\u5316\uff0c\u964d\u4f4e\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff1b\u5e8f\u5217\u7ea7\u65b9\u6cd5\u4e2d\uff0c\u8bed\u8a00\u5316\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u4e14\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u6027\u5dee\uff0c\u800c\u7b54\u6848\u9891\u7387\uff08\u8de8\u6837\u672c\u4e00\u81f4\u6027\uff09\u63d0\u4f9b\u6700\u53ef\u9760\u7684\u6821\u51c6\u3002", "conclusion": "\u5f53\u524dLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5c40\u9650\uff0c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u7279\u522b\u662f\u4ec5\u4f9d\u8d56ECE\u4f5c\u4e3a\u6027\u80fd\u8861\u91cf\u6307\u6807\uff0c\u7b54\u6848\u9891\u7387\u662f\u6700\u53ef\u9760\u7684\u6821\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2602.00111", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00111", "abs": "https://arxiv.org/abs/2602.00111", "authors": ["Haiyu Yang", "Heidi Lesscher", "Enhong Liu", "Miel Hostens"], "title": "From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves", "comment": null, "summary": "Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5976\u725b\u728a\u7684\u6e38\u620f\u884c\u4e3a\u4e0e\u7a7a\u95f4\u5206\u914d\u5448\u975e\u7ebf\u6027\u5173\u7cfb\uff0c8-10\u5e73\u65b9\u7c73/\u728a\u725b\u65f6\u6e38\u620f\u884c\u4e3a\u6700\u591a\uff0c\u81ea\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u76d1\u6d4b\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe97.6%", "motivation": "\u6e38\u620f\u884c\u4e3a\u662f\u5976\u725b\u728a\u798f\u5229\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f46\u5728\u5546\u4e1a\u6761\u4ef6\u4e0b\uff0c\u7279\u522b\u662f\u4e2d\u7b49\u81f3\u9ad8\u7a7a\u95f4\u5206\u914d\uff086-20\u5e73\u65b9\u7c73/\u728a\u725b\uff09\u65f6\uff0c\u7a7a\u95f4\u5206\u914d\u5bf9\u6e38\u620f\u884c\u4e3a\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u5173\u7cfb\u5e76\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u76d1\u6d4b\u65b9\u6cd5", "method": "\u7814\u7a76\u5728\u8377\u517014\u4e2a\u5546\u4e1a\u519c\u573a\u5bf960\u5934\u7fa4\u517b\u5976\u725b\u728a\u8fdb\u884c\u89c2\u5bdf\uff0c\u7a7a\u95f4\u8303\u56f42.66-17.98\u5e73\u65b9\u7c73/\u728a\u725b\uff1b\u4f7f\u7528\u8be6\u7ec6\u884c\u4e3a\u8c31\u5206\u6790\u89c6\u9891\u89c2\u5bdf\uff0c\u6e38\u620f\u884c\u4e3a\u4ee5\u89c2\u5bdf\u671f\u767e\u5206\u6bd4\u8868\u793a\uff1b\u91c7\u7528\u7ebf\u6027\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff1b\u5f00\u53d1\u81ea\u52a8\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\uff0c\u57286\u4e2a\u519c\u573a108\u5c0f\u65f6\u624b\u52a8\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u9a8c\u8bc1", "result": "\u8ba1\u7b97\u673a\u89c6\u89c9\u5206\u7c7b\u5668\u5bf9\u6d3b\u8dc3\u6e38\u620f\u68c0\u6d4b\u8fbe\u523097.6%\u51c6\u786e\u7387\u548c99.4%\u53ec\u56de\u7387\uff1b\u728a\u725b\u5e73\u5747\u82b1\u8d391.0%\u89c2\u5bdf\u65f6\u95f4\u6e38\u620f\uff08\u7ea617\u5c0f\u65f6\u4e2d10\u5206\u949f\uff09\uff1b\u7a7a\u95f4-\u6e38\u620f\u5173\u7cfb\u5448\u975e\u7ebf\u6027\uff0c8-10\u5e73\u65b9\u7c73/\u728a\u725b\u65f6\u6e38\u620f\u6c34\u5e73\u6700\u9ad8\uff081.6%\u89c2\u5bdf\u65f6\u95f4\uff09\uff0c6-8\u5e73\u65b9\u7c73\u548c12-14\u5e73\u65b9\u7c73\u65f6\u6700\u4f4e\uff08<0.6%\u89c2\u5bdf\u65f6\u95f4\uff09\uff1b\u63a7\u5236\u5e74\u9f84\u3001\u5065\u5eb7\u548c\u7fa4\u4f53\u5927\u5c0f\u540e\uff0c\u7a7a\u95f4\u56e0\u7d20\u4ecd\u663e\u8457", "conclusion": "8-10\u5e73\u65b9\u7c73/\u728a\u725b\u662f\u5e73\u8861\u798f\u5229\u6548\u76ca\u4e0e\u7ecf\u6d4e\u53ef\u884c\u6027\u7684\u5b9e\u7528\u76ee\u6807\uff1b\u81ea\u52a8\u76d1\u6d4b\u7cfb\u7edf\u53ef\u5c06\u5c0f\u578b\u6807\u6ce8\u9879\u76ee\u6269\u5c55\u4e3a\u8fde\u7eed\u798f\u5229\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u5e94\u7528"}}
{"id": "2602.01094", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01094", "abs": "https://arxiv.org/abs/2602.01094", "authors": ["Qi Xin", "Shihua Gong", "Lingyue Shen", "Pinjing Wen", "Yumiao Zhang", "Yan Chen", "Jiarui Han", "Jinchao Xu"], "title": "High-order DLM-ALE discretizations with robust operator preconditioning for fluid-rigid-body interaction", "comment": null, "summary": "Motivated by the design of deterministic lateral displacement (DLD) microfluidic devices, we develop a high-order numerical framework for fluid-rigid-body interaction on fitted moving meshes. Rigid-body motion is enforced by a distributed Lagrange multiplier (DLM) formulation, while the moving fluid domain is treated by an arbitrary Lagrangian-Eulerian (ALE) mapping. In space, we use isoparametric Taylor-Hood elements to achieve high-order accuracy and to represent curved boundaries and the fluid-particle interface. In time, we employ a high-order partitioned Runge-Kutta strategy in which the mesh motion is advanced explicitly and the coupled physical fields are advanced implicitly, yielding high-order accuracy for the particle trajectory. The fully coupled system is linearized into a generalized Stokes problem subject to distributed constraints of incompressibility and rigid-body motion. We establish well-posedness of this generalized Stokes formulation at both the continuous and discrete levels, providing the stability foundation for operator preconditioning that is robust with respect to key physical and discretization parameters. Numerical experiments on representative benchmarks, including a DLD case, demonstrate high-order convergence for the fluid solution and rigid-body dynamics, as well as robust iterative convergence of the proposed preconditioners.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d41\u4f53-\u521a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u9636\u6570\u503c\u6846\u67b6\uff0c\u7279\u522b\u9488\u5bf9\u786e\u5b9a\u6027\u4fa7\u5411\u4f4d\u79fb\u5fae\u6d41\u63a7\u8bbe\u5907\u8bbe\u8ba1\uff0c\u91c7\u7528\u62df\u5408\u79fb\u52a8\u7f51\u683c\u3001\u5206\u5e03\u5f0f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u548c\u4efb\u610f\u62c9\u683c\u6717\u65e5-\u6b27\u62c9\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9636\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u786e\u5b9a\u6027\u4fa7\u5411\u4f4d\u79fb\u5fae\u6d41\u63a7\u8bbe\u5907\u8bbe\u8ba1\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7cbe\u786e\u6a21\u62df\u6d41\u4f53\u4e2d\u521a\u4f53\u8fd0\u52a8\u7684\u9ad8\u9636\u6570\u503c\u65b9\u6cd5\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u7c92\u5b50\u8f68\u8ff9\u548c\u8bbe\u5907\u6027\u80fd\u3002", "method": "\u91c7\u7528\u62df\u5408\u79fb\u52a8\u7f51\u683c\u4e0a\u7684\u9ad8\u9636\u6570\u503c\u6846\u67b6\uff1a1) \u5206\u5e03\u5f0f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u6cd5\u5f3a\u5236\u521a\u4f53\u8fd0\u52a8\uff1b2) \u4efb\u610f\u62c9\u683c\u6717\u65e5-\u6b27\u62c9\u6620\u5c04\u5904\u7406\u79fb\u52a8\u6d41\u4f53\u57df\uff1b3) \u7a7a\u95f4\u4e0a\u4f7f\u7528\u7b49\u53c2\u6cf0\u52d2-\u80e1\u5fb7\u5355\u5143\u5b9e\u73b0\u9ad8\u9636\u7cbe\u5ea6\u548c\u66f2\u7ebf\u8fb9\u754c\u8868\u793a\uff1b4) \u65f6\u95f4\u4e0a\u91c7\u7528\u9ad8\u9636\u5206\u533a\u9f99\u683c-\u5e93\u5854\u7b56\u7565\uff0c\u663e\u5f0f\u63a8\u8fdb\u7f51\u683c\u8fd0\u52a8\uff0c\u9690\u5f0f\u63a8\u8fdb\u8026\u5408\u7269\u7406\u573a\u3002", "result": "\u5efa\u7acb\u4e86\u5e7f\u4e49\u65af\u6258\u514b\u65af\u516c\u5f0f\u5728\u8fde\u7eed\u548c\u79bb\u6563\u5c42\u9762\u7684\u9002\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5173\u952e\u7269\u7406\u548c\u79bb\u6563\u53c2\u6570\u9c81\u68d2\u7684\u7b97\u5b50\u9884\u6761\u4ef6\u7a33\u5b9a\u6027\u57fa\u7840\u3002\u6570\u503c\u5b9e\u9a8c\uff08\u5305\u62ecDLD\u6848\u4f8b\uff09\u5c55\u793a\u4e86\u6d41\u4f53\u89e3\u548c\u521a\u4f53\u52a8\u529b\u5b66\u7684\u9ad8\u9636\u6536\u655b\u6027\uff0c\u4ee5\u53ca\u6240\u63d0\u9884\u6761\u4ef6\u5668\u7684\u9c81\u68d2\u8fed\u4ee3\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u9ad8\u9636\u6570\u503c\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u6d41\u4f53-\u521a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u7cbe\u5ea6\u6a21\u62df\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5fae\u6d41\u63a7\u8bbe\u5907\u8bbe\u8ba1\uff0c\u4e3a\u51c6\u786e\u9884\u6d4b\u7c92\u5b50\u8f68\u8ff9\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8ba1\u7b97\u5de5\u5177\uff0c\u5e76\u5efa\u7acb\u4e86\u9c81\u68d2\u7684\u6570\u503c\u7a33\u5b9a\u6027\u57fa\u7840\u3002"}}
{"id": "2602.00353", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00353", "abs": "https://arxiv.org/abs/2602.00353", "authors": ["Yihe Zhang", "Cheyenne N Mohawk", "Kaiying Han", "Vijay Srinivas Tida", "Manyu Li", "Xiali Hei"], "title": "MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants", "comment": "Accepted for presentation at IEEE SoutheastCon 2026. This is the author version of an accepted paper. The final version will appear in IEEE Xplore", "summary": "Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.", "AI": {"tldr": "MHDash\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u7528\u4e8e\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u5ba1\u8ba1\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u6807\u6ce8\u548c\u591a\u8f6e\u5bf9\u8bdd\u5206\u6790\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u805a\u5408\u6027\u80fd\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u5f80\u5f80\u63a9\u76d6\u4e86\u9ad8\u98ce\u9669\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff0c\u4e14\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1MHDash\u5f00\u6e90\u5e73\u53f0\uff0c\u6574\u5408\u6570\u636e\u6536\u96c6\u3001\u7ed3\u6784\u5316\u6807\u6ce8\u3001\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u548c\u57fa\u7ebf\u8bc4\u4f30\u7684\u7edf\u4e00\u6d41\u7a0b\uff0c\u652f\u6301\u5173\u6ce8\u7c7b\u578b\u3001\u98ce\u9669\u7b49\u7ea7\u3001\u5bf9\u8bdd\u610f\u56fe\u7b49\u591a\u7ef4\u5ea6\u6807\u6ce8\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u98ce\u9669\u611f\u77e5\u7684\u5206\u6790\u3002", "result": "\u53d1\u73b0\uff1a(1)\u7b80\u5355\u57fa\u7ebf\u548c\u5148\u8fdbLLM API\u603b\u4f53\u51c6\u786e\u7387\u76f8\u5f53\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u6848\u4f8b\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1b(2)\u67d0\u4e9bLLM\u4fdd\u6301\u4e00\u81f4\u7684\u4e25\u91cd\u7a0b\u5ea6\u6392\u5e8f\u4f46\u7edd\u5bf9\u98ce\u9669\u5206\u7c7b\u5931\u8d25\uff0c\u53e6\u4e00\u4e9b\u603b\u4f53\u5f97\u5206\u5408\u7406\u4f46\u5728\u4e25\u91cd\u7c7b\u522b\u4e0a\u5047\u9634\u6027\u7387\u9ad8\uff1b(3)\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u5dee\u8ddd\u88ab\u653e\u5927\uff0c\u98ce\u9669\u4fe1\u53f7\u9010\u6e10\u663e\u73b0\u3002", "conclusion": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b89\u5168\u5173\u952e\u7684\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u4e2d\u4e0d\u8db3\uff0cMHDash\u4f5c\u4e3a\u5f00\u6e90\u5e73\u53f0\u65e8\u5728\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3001\u900f\u660e\u8bc4\u4f30\u548c\u5b89\u5168\u5bf9\u9f50\u7684\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2602.02299", "categories": ["physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.02299", "abs": "https://arxiv.org/abs/2602.02299", "authors": ["Shadab Alam", "Christoph Federrath", "J\u00f6rg Schumacher"], "title": "Transition to dilatation-dominated compressible turbulence", "comment": "11 pages (6 pages in the main text, 5 pages in the supplementary material), 6 figures (4 in the main text and 2 in the supplementary material)", "summary": "The kinetic energy dissipation rate is of central importance for the small-scale statistics in turbulent flows. Here, we determine the transition to the dilatation-dominated regime of 3d fully compressible, homogeneous, isotropic turbulence by moments of energy dissipation and its components up to order 4 for turbulent Mach numbers $0.1\\le M_t\\le 10$. Our high-resolution numerical simulations show a crossover from incompressible to $M_t$-independent, Burgers turbulence-like moment scaling with respect to Reynolds number $Re$. This confirms the statistical dominance of shocks for $M_t\\gtrsim 1$.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u8fbe4\u9636\u7684\u80fd\u91cf\u8017\u6563\u53ca\u5176\u5206\u91cf\u77e9\uff0c\u7814\u7a763D\u5b8c\u5168\u53ef\u538b\u7f29\u5747\u5300\u5404\u5411\u540c\u6027\u6e4d\u6d41\u5411\u81a8\u80c0\u4e3b\u5bfc\u533a\u57df\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u9a6c\u8d6b\u6570\u5927\u4e8e1\u65f6\u6fc0\u6ce2\u7edf\u8ba1\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u52a8\u80fd\u8017\u6563\u7387\u5bf9\u6e4d\u6d41\u5c0f\u5c3a\u5ea6\u7edf\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u786e\u5b9a3D\u5b8c\u5168\u53ef\u538b\u7f29\u5747\u5300\u5404\u5411\u540c\u6027\u6e4d\u6d41\u5411\u81a8\u80c0\u4e3b\u5bfc\u533a\u57df\u7684\u8f6c\u53d8\u7279\u5f81\u3002", "method": "\u5bf9\u6e4d\u6d41\u9a6c\u8d6b\u65700.1\u2264M_t\u226410\u7684\u8303\u56f4\uff0c\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u6570\u503c\u6a21\u62df\uff0c\u8ba1\u7b97\u9ad8\u8fbe4\u9636\u7684\u80fd\u91cf\u8017\u6563\u53ca\u5176\u5206\u91cf\u77e9\uff0c\u5206\u6790\u5176\u76f8\u5bf9\u4e8e\u96f7\u8bfa\u6570Re\u7684\u6807\u5ea6\u884c\u4e3a\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u4ece\u4e0d\u53ef\u538b\u7f29\u5230\u4e0eM_t\u65e0\u5173\u7684Burgers\u6e4d\u6d41\u7c7b\u77e9\u6807\u5ea6\u7684\u4ea4\u53c9\u8f6c\u53d8\uff0c\u8bc1\u5b9e\u5f53M_t\u22731\u65f6\u6fc0\u6ce2\u5728\u7edf\u8ba1\u4e0a\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u901a\u8fc7\u80fd\u91cf\u8017\u6563\u77e9\u7684\u6807\u5ea6\u5206\u6790\uff0c\u786e\u5b9a\u4e86\u53ef\u538b\u7f29\u6e4d\u6d41\u4e2d\u81a8\u80c0\u4e3b\u5bfc\u533a\u57df\u7684\u8f6c\u53d8\uff0c\u4e3a\u9a6c\u8d6b\u6570\u5927\u4e8e1\u65f6\u6fc0\u6ce2\u4e3b\u5bfc\u7684\u7edf\u8ba1\u7279\u6027\u63d0\u4f9b\u4e86\u8bc1\u636e\u3002"}}
{"id": "2602.00064", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00064", "abs": "https://arxiv.org/abs/2602.00064", "authors": ["Hao Deng", "Yingping Li", "Shuiping Gou", "Bo Liu"], "title": "SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation", "comment": null, "summary": "Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.", "AI": {"tldr": "SPGCL\u662f\u4e00\u4e2a\u901a\u8fc7SVD\u5f15\u5bfc\u7684\u7ed3\u6784\u6270\u52a8\u8fdb\u884c\u9c81\u68d2\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u968f\u673a\u8fb9\u79fb\u9664\u548cSVD\u5f15\u5bfc\u7684\u7ec6\u5316\u6b65\u9aa4\uff0c\u4ee5\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u89c6\u56fe\uff0c\u4ece\u800c\u63d0\u9ad8GNN\u5bf9\u7ed3\u6784\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u968f\u673a\u6270\u52a8\uff08\u5982\u8fb9\u4e22\u5f03\uff09\u53ef\u80fd\u79fb\u9664\u5173\u952e\u8fb9\uff0c\u800c\u57fa\u4e8eSVD\u7684\u89c6\u56fe\u5f80\u5f80\u53d8\u5f97\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u751f\u6210\u591a\u6837\u5316\u7684\u89c6\u56fe\uff0c\u53c8\u80fd\u4fdd\u7559\u91cd\u8981\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "method": "SPGCL\u91c7\u7528SVD\u5f15\u5bfc\u7684\u7ed3\u6784\u6270\u52a8\u65b9\u6cd5\uff1a1) \u8f7b\u91cf\u7ea7\u968f\u673a\u8fb9\u79fb\u9664\uff1b2) SVD\u5f15\u5bfc\u7684\u7ec6\u5316\u6b65\u9aa4\uff0c\u6062\u590d\u88ab\u9519\u8bef\u79fb\u9664\u7684\u4fe1\u606f\u8fb9\u5e76\u5f15\u5165\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u7f3a\u5931\u94fe\u63a5\uff1b3) \u901a\u8fc7\u7a00\u758ftop-ranked\u8fb9\u9009\u62e9\u548c\u5408\u5e76\u907f\u514d\u56fe\u5bc6\u96c6\u5316\uff1b4) \u5e73\u8861\u8fb9\u79fb\u9664\u548c\u6062\u590d\u7387\u4ee5\u63a7\u5236\u89c6\u56fe\u95f4\u7684\u7ed3\u6784\u5dee\u5f02\uff1b5) \u52a0\u5165\u53d7\u5168\u5c40\u76f8\u4f3c\u6027\u7ea6\u675f\u6b63\u5219\u5316\u7684\u5bf9\u6bd4\u878d\u5408\u6a21\u5757\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPGCL\u80fd\u6301\u7eed\u63d0\u9ad8\u57fa\u7840GNN\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SPGCL\u901a\u8fc7SVD\u5f15\u5bfc\u7684\u7ed3\u6784\u6270\u52a8\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u751f\u6210\u4e86\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u591a\u6837\u5316\u89c6\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u5bf9\u7ed3\u6784\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.00300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00300", "abs": "https://arxiv.org/abs/2602.00300", "authors": ["Xilin Gong", "Shu Yang", "Zehua Cao", "Lynne Billard", "Di Wang"], "title": "Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute \"purple\" for \"broccoli\", LLMs still generate \"green\" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\\% relative performance improvement.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0Patchscopes\u6846\u67b6\u4e2dLLMs\u503e\u5411\u4e8e\u4f9d\u8d56\u56fa\u6709\u8bed\u8a00\u6a21\u5f0f\u800c\u975e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u5fe0\u5b9e\uff0c\u5e76\u63d0\u51faBALOR\u65b9\u6cd5\u901a\u8fc7logit\u91cd\u6821\u51c6\u6765\u6291\u5236\u6a21\u578b\u504f\u89c1\u3001\u589e\u5f3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709Patchscopes\u6846\u67b6\u4f7f\u7528LLMs\u89e3\u7801\u9690\u85cf\u8868\u793a\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u89e3\u91ca\uff0c\u4f46\u7814\u7a76\u53d1\u73b0LLMs\u503e\u5411\u4e8e\u4f9d\u8d56\u56fa\u6709\u8bed\u8a00\u6a21\u5f0f\uff0c\u8fd9\u4f1a\u8986\u76d6\u9690\u85cf\u8868\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u5fe0\u5b9e\u3002\u4f8b\u5982\uff0c\u5373\u4f7f\u9690\u85cf\u8868\u793a\u7f16\u7801\u4e86\"\u7d2b\u8272\"\u7684\u4e0a\u4e0b\u6587\u5c5e\u6027\uff0cLLMs\u4ecd\u4f1a\u751f\u6210\"\u7eff\u8272\"\u8fd9\u79cd\u56fa\u6709\u8054\u60f3\u3002", "method": "\u9996\u5148\u8bbe\u8ba1\u6570\u636e\u96c6\u8bc4\u4f30Patchscopes\u5728\u504f\u89c1\u60c5\u51b5\u4e0b\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u5e73\u5747\u5fe0\u5b9e\u6027\u4e0b\u964d18.84%\u3002\u7136\u540e\u63d0\u51faBALOR\u65b9\u6cd5\uff1a\u5c06\u672a\u4fee\u8865\u63d0\u793a\u7684\u8f93\u51falogit\u89c6\u4e3a\u6a21\u578b\u504f\u89c1\uff0c\u4e0e\u4fee\u8865\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0b\u7684logit\u8fdb\u884c\u5bf9\u6bd4\uff0c\u901a\u8fc7\u8fd9\u79cd\u5bf9\u6bd4\u91cd\u6821\u51c6logit\u5206\u5e03\uff0c\u6291\u5236\u6a21\u578b\u504f\u89c1\u5e76\u589e\u5f3a\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBALOR\u5728\u591a\u4e2aLLMs\u4e0a\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe33%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86Patchscopes\u4e2d\u7684\u7cfb\u7edf\u4e0d\u5fe0\u5b9e\u95ee\u9898\u3002", "conclusion": "LLMs\u5728Patchscopes\u6846\u67b6\u4e2d\u5b58\u5728\u7cfb\u7edf\u4e0d\u5fe0\u5b9e\u95ee\u9898\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u56fa\u6709\u8bed\u8a00\u6a21\u5f0f\u800c\u975e\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002BALOR\u65b9\u6cd5\u901a\u8fc7logit\u91cd\u6821\u51c6\u6709\u6548\u6291\u5236\u6a21\u578b\u504f\u89c1\u3001\u589e\u5f3a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u85cf\u8868\u793a\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002"}}
{"id": "2602.00113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00113", "abs": "https://arxiv.org/abs/2602.00113", "authors": ["S. Kalaycioglu", "C. Hong", "K. Zhai", "H. Xie", "J. N. Wong"], "title": "AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment", "comment": "11 pages and 5 figures", "summary": "Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.", "AI": {"tldr": "AI\u5e73\u53f0\u6574\u5408\u591a\u89c6\u89d2\u6444\u5f71\u6d4b\u91cf\u30013D\u91cd\u5efa\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\uff0c\u5b9e\u73b0\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u70e7\u4f24\u8bc4\u4f30\uff0c\u901a\u8fc73D\u51e0\u4f55\u5206\u6790\u91cf\u5316\u70e7\u4f24\u9762\u79ef\u3001\u6df1\u5ea6\u548c\u6108\u5408\u8fdb\u7a0b\u3002", "motivation": "\u4f20\u7edf\u70e7\u4f24\u8bc4\u4f30\u4f9d\u8d56\u4e3b\u89c2\u89c6\u89c9\u68c0\u67e5\u548c2D\u6444\u5f71\uff0c\u7f3a\u4e4f\u5ba2\u89c2\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7eb5\u5411\u6bd4\u8f83\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u51e0\u4f55\u611f\u77e5\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6574\u5408\u591a\u89c6\u89d2\u6444\u5f71\u6d4b\u91cf\u30013D\u8868\u9762\u91cd\u5efa\u548c\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\uff0c\u4f7f\u7528\u6d88\u8d39\u7ea7\u76f8\u673a\u91c7\u96c6\u591a\u89d2\u5ea6\u56fe\u50cf\uff0c\u91cd\u5efa\u60a3\u8005\u7279\u5f02\u60273D\u70e7\u4f24\u8868\u9762\uff0c\u8ba1\u7b97\u5ba2\u89c2\u51e0\u4f55\u6307\u6807\uff08\u9762\u79ef\u3001TBSA\u3001\u6df1\u5ea6\u4ee3\u7406\u3001\u4f53\u79ef\u53d8\u5316\uff09\uff0c\u5e76\u652f\u6301\u7a7a\u95f4\u5bf9\u9f50\u7684\u7eb5\u5411\u8ffd\u8e2a\u3002", "result": "\u4eff\u771f\u8bc4\u4f30\u663e\u793a\u7a33\u5b9a\u7684\u91cd\u5efa\u6548\u679c\u3001\u4e00\u81f4\u7684\u6307\u6807\u8ba1\u7b97\u548c\u4e34\u5e8a\u5408\u7406\u7684\u7eb5\u5411\u8d8b\u52bf\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u975e\u4fb5\u5165\u6027\u7684\u51e0\u4f55\u611f\u77e5\u70e7\u4f24\u8bc4\u4f30\u548c\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u6025\u6027\u548c\u95e8\u8bca\u62a4\u7406\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u70e7\u4f24\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc73D\u51e0\u4f55\u5206\u6790\u5b9e\u73b0\u4e86\u6cbb\u7597\u89c4\u5212\u3001\u6108\u5408\u76d1\u6d4b\u548c\u6cd5\u5f8b\u6587\u6863\u7684\u6539\u8fdb\u3002"}}
{"id": "2602.01143", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01143", "abs": "https://arxiv.org/abs/2602.01143", "authors": ["Pasco Alexandre", "Nouy Anthony"], "title": "Surrogate to Poincar\u00e9 inequalities on manifolds for structured dimension reduction in nonlinear feature spaces", "comment": "25 pages, 2 figures", "summary": "This paper is concerned with the approximation of continuously differentiable functions with high-dimensional input by a composition of two functions: a feature map that extracts few features from the input space, and a profile function that approximates the target function taking the features as its low-dimensional input. We focus on the construction of structured nonlinear feature maps, that extract features on separate groups of variables, using a recently introduced gradient-based method that leverages Poincar\u00e9 inequalities on nonlinear manifolds. This method consists in minimizing a non-convex loss functional, which can be a challenging task, especially for small training samples. We first investigate a collective setting, in which we construct a feature map suitable to a parametrized family of high-dimensional functions. In this setting we introduce a new quadratic surrogate to the non-convex loss function and show an upper bound on the latter. We then investigate a grouped setting, in which we construct separate feature maps for separate groups of inputs, and we show that this setting is almost equivalent to multiple collective settings, one for each group of variables.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u9ad8\u7ef4\u8fde\u7eed\u53ef\u5fae\u51fd\u6570\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7531\u7279\u5f81\u6620\u5c04\u548c\u8f6e\u5ed3\u51fd\u6570\u7ec4\u6210\u7684\u590d\u5408\u51fd\u6570\u7ed3\u6784\uff0c\u91cd\u70b9\u7814\u7a76\u57fa\u4e8e\u68af\u5ea6\u65b9\u6cd5\u548c\u5e9e\u52a0\u83b1\u4e0d\u7b49\u5f0f\u7684\u7ed3\u6784\u5316\u975e\u7ebf\u6027\u7279\u5f81\u6620\u5c04\u6784\u9020\u3002", "motivation": "\u9ad8\u7ef4\u51fd\u6570\u7684\u8fd1\u4f3c\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u63d0\u53d6\u5c11\u91cf\u7279\u5f81\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u5c06\u9ad8\u7ef4\u8f93\u5165\u6620\u5c04\u5230\u4f4e\u7ef4\u7279\u5f81\u7684\u7279\u5f81\u6620\u5c04\uff0c\u7136\u540e\u901a\u8fc7\u8f6e\u5ed3\u51fd\u6570\u8fdb\u884c\u8fd1\u4f3c\uff0c\u4ece\u800c\u89e3\u51b3\u9ad8\u7ef4\u51fd\u6570\u903c\u8fd1\u7684\u96be\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u7ebf\u6027\u6d41\u5f62\u4e0a\u7684\u5e9e\u52a0\u83b1\u4e0d\u7b49\u5f0f\u6765\u6784\u5efa\u7ed3\u6784\u5316\u975e\u7ebf\u6027\u7279\u5f81\u6620\u5c04\u3002\u7814\u7a76\u4e24\u79cd\u8bbe\u7f6e\uff1a1\uff09\u96c6\u4f53\u8bbe\u7f6e\uff0c\u4e3a\u53c2\u6570\u5316\u51fd\u6570\u65cf\u6784\u5efa\u7279\u5f81\u6620\u5c04\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e8c\u6b21\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff1b2\uff09\u5206\u7ec4\u8bbe\u7f6e\uff0c\u4e3a\u4e0d\u540c\u7684\u8f93\u5165\u7ec4\u6784\u5efa\u72ec\u7acb\u7684\u7279\u5f81\u6620\u5c04\u3002", "result": "\u5728\u96c6\u4f53\u8bbe\u7f6e\u4e2d\uff0c\u63d0\u51fa\u4e86\u975e\u51f8\u635f\u5931\u51fd\u6570\u7684\u4e8c\u6b21\u4ee3\u7406\uff0c\u5e76\u7ed9\u51fa\u4e86\u540e\u8005\u7684\u4e0a\u754c\u3002\u5728\u5206\u7ec4\u8bbe\u7f6e\u4e2d\uff0c\u8bc1\u660e\u4e86\u8be5\u8bbe\u7f6e\u51e0\u4e4e\u7b49\u4ef7\u4e8e\u591a\u4e2a\u96c6\u4f53\u8bbe\u7f6e\uff0c\u6bcf\u4e2a\u53d8\u91cf\u7ec4\u5bf9\u5e94\u4e00\u4e2a\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6784\u5efa\u7ed3\u6784\u5316\u975e\u7ebf\u6027\u7279\u5f81\u6620\u5c04\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u7ef4\u51fd\u6570\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u5c0f\u8bad\u7ec3\u6837\u672c\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e8c\u6b21\u4ee3\u7406\u635f\u5931\u51fd\u6570\u548c\u5206\u7ec4\u7b56\u7565\u63d0\u9ad8\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.00359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00359", "abs": "https://arxiv.org/abs/2602.00359", "authors": ["Minhua Lin", "Hanqing Lu", "Zhan Shi", "Bing He", "Rui Mao", "Zhiwei Zhang", "Zongyu Wu", "Xianfeng Tang", "Hui Liu", "Zhenwei Dai", "Xiang Zhang", "Suhang Wang", "Benoit Dumoulin", "Jian Pei"], "title": "Position: Agentic Evolution is the Path to Evolving LLMs", "comment": null, "summary": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM\u9700\u8981\u4ece\u9759\u6001\u8bad\u7ec3\u8f6c\u5411\u52a8\u6001\u6f14\u5316\uff0c\u5f15\u5165A-Evolve\u6846\u67b6\u5c06\u90e8\u7f72\u65f6\u6539\u8fdb\u89c6\u4e3a\u81ea\u4e3b\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u6f14\u5316\u6269\u5c55\u5047\u8bf4\uff1a\u9002\u5e94\u80fd\u529b\u968f\u6f14\u5316\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u800c\u6269\u5c55\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u96c6\u8f6c\u5411\u5f00\u653e\u7684\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u65f6\uff0c\u9759\u6001\u8bad\u7ec3\u65e0\u6cd5\u8ddf\u4e0a\u6301\u7eed\u90e8\u7f72\u73af\u5883\u53d8\u5316\u3002\u73b0\u6709\u7684\u90e8\u7f72\u65f6\u9002\u5e94\u65b9\u6cd5\u7f3a\u4e4f\u6218\u7565\u667a\u80fd\u6765\u8bca\u65ad\u5931\u8d25\u5e76\u4ea7\u751f\u6301\u4e45\u6539\u8fdb\u3002", "method": "\u63d0\u51faA-Evolve\u6846\u67b6\uff0c\u5c06\u90e8\u7f72\u65f6\u6539\u8fdb\u89c6\u4e3a\u5bf9\u6301\u4e45\u7cfb\u7edf\u72b6\u6001\u7684\u6709\u610f\u8bc6\u3001\u76ee\u6807\u5bfc\u5411\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u5c06\u6f14\u5316\u4ece\u56fa\u5b9a\u6d41\u7a0b\u63d0\u5347\u4e3a\u81ea\u4e3b\u6f14\u5316\u4ee3\u7406\u3002", "result": "\u63d0\u51fa\u6f14\u5316\u6269\u5c55\u5047\u8bf4\uff1a\u9002\u5e94\u80fd\u529b\u968f\u5206\u914d\u7ed9\u6f14\u5316\u7684\u8ba1\u7b97\u8d44\u6e90\u800c\u6269\u5c55\uff0c\u4e3a\u6301\u7eed\u3001\u5f00\u653e\u5f0f\u7684\u73b0\u5b9e\u4e16\u754c\u9002\u5e94\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "\u4ee3\u7406\u6f14\u5316\u4ee3\u8868\u4e86LLM\u9002\u5e94\u7684\u5fc5\u7136\u672a\u6765\uff0c\u901a\u8fc7\u5c06\u6f14\u5316\u672c\u8eab\u4ece\u56fa\u5b9a\u6d41\u7a0b\u63d0\u5347\u4e3a\u81ea\u4e3b\u6f14\u5316\u4ee3\u7406\uff0c\u80fd\u591f\u5b9e\u73b0\u6301\u7eed\u3001\u5f00\u653e\u5f0f\u7684\u73b0\u5b9e\u4e16\u754c\u9002\u5e94\u3002"}}
{"id": "2602.01497", "categories": ["math.NA", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.01497", "abs": "https://arxiv.org/abs/2602.01497", "authors": ["Josef Musil"], "title": "Third-Order Geometric-Volume Conservation in Cahn--Hilliard Models", "comment": "23 pages, 10 figures. Appendices A-C included (16 pages)", "summary": "Degenerate Cahn-Hilliard phase-field models provide a robust approximation of surface-diffusion-driven interface motion without explicit front tracking. In computations, however, the geometric volume enclosed by the interface -- the region where the order parameter $\u03c6$ is positive -- may drift at finite interface thickness, producing artificial shrinkage or growth even when the sharp-interface limit conserves volume. We revisit and extend the improved-conservation framework of Zhou et al., where one replaces classical mass conservation by the exact conservation of a designed monotone mapping $Q(\u03c6)$ that more accurately approximates a step function. Building on this framework, we (i) carry out the matched-asymptotic analysis in the unscaled physical time formulation, (ii) derive a simplified representation of the first-order inner correction to the interface profile, and (iii) identify an integral-moment cancellation condition that controls the leading geometric-volume defect. This mechanism becomes a practical design rule: we select regularization kernels within parameterized families -- including exponential and Pade-type -- to reach effective higher-order behavior and satisfy the cancellation condition at moderate parameter values. As a result, the proposed kernels achieve formal third-order accuracy in geometric-volume conservation with respect to interface thickness. Finally, we describe an unconditional energy-dissipative numerical discretization that exactly preserves the discrete conserved quantity. Numerical benchmarks on multi-scale droplet coarsening and shape relaxation demonstrate that the moment-balanced kernels virtually eliminate artificial drift and prevent premature extinction of small droplets.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u9000\u5316Cahn-Hilliard\u76f8\u573a\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5355\u8c03\u6620\u5c04Q(\u03c6)\u548c\u6b63\u5219\u5316\u6838\u51fd\u6570\uff0c\u5b9e\u73b0\u51e0\u4f55\u4f53\u79ef\u7684\u66f4\u9ad8\u9636\u5b88\u6052\uff0c\u6d88\u9664\u754c\u9762\u539a\u5ea6\u5f15\u8d77\u7684\u865a\u5047\u6f02\u79fb\u3002", "motivation": "\u4f20\u7edf\u9000\u5316Cahn-Hilliard\u76f8\u573a\u6a21\u578b\u5728\u8ba1\u7b97\u4e2d\uff0c\u5373\u4f7f\u5c16\u9510\u754c\u9762\u6781\u9650\u4e0b\u4f53\u79ef\u5b88\u6052\uff0c\u6709\u9650\u754c\u9762\u539a\u5ea6\u4ecd\u4f1a\u5bfc\u81f4\u754c\u9762\u5305\u56f4\u7684\u51e0\u4f55\u4f53\u79ef\u53d1\u751f\u865a\u5047\u6f02\u79fb\uff08\u6536\u7f29\u6216\u589e\u957f\uff09\uff0c\u5f71\u54cd\u8ba1\u7b97\u7cbe\u5ea6\u3002", "method": "\u6269\u5c55Zhou\u7b49\u4eba\u7684\u6539\u8fdb\u5b88\u6052\u6846\u67b6\uff0c\u901a\u8fc7\u5339\u914d\u6e10\u8fd1\u5206\u6790\u63a8\u5bfc\u4e00\u9636\u5185\u4fee\u6b63\uff0c\u5efa\u7acb\u79ef\u5206\u77e9\u62b5\u6d88\u6761\u4ef6\u63a7\u5236\u51e0\u4f55\u4f53\u79ef\u7f3a\u9677\uff0c\u8bbe\u8ba1\u6ee1\u8db3\u62b5\u6d88\u6761\u4ef6\u7684\u6b63\u5219\u5316\u6838\u51fd\u6570\uff08\u6307\u6570\u578b\u548cPad\u00e9\u578b\uff09\uff0c\u5e76\u5f00\u53d1\u65e0\u6761\u4ef6\u80fd\u91cf\u8017\u6563\u6570\u503c\u79bb\u6563\u683c\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u77e9\u5e73\u8861\u6838\u51fd\u6570\u5728\u51e0\u4f55\u4f53\u79ef\u5b88\u6052\u65b9\u9762\u8fbe\u5230\u5f62\u5f0f\u4e0a\u7684\u4e09\u9636\u7cbe\u5ea6\uff0c\u51e0\u4e4e\u6d88\u9664\u4eba\u5de5\u6f02\u79fb\uff0c\u9632\u6b62\u5c0f\u6db2\u6ef4\u8fc7\u65e9\u6d88\u5931\uff0c\u5728\u591a\u5c3a\u5ea6\u6db2\u6ef4\u7c97\u5316\u548c\u5f62\u72b6\u677e\u5f1b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u6ee1\u8db3\u79ef\u5206\u77e9\u62b5\u6d88\u6761\u4ef6\u7684\u6b63\u5219\u5316\u6838\u51fd\u6570\uff0c\u9000\u5316Cahn-Hilliard\u76f8\u573a\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u9ad8\u9636\u51e0\u4f55\u4f53\u79ef\u5b88\u6052\uff0c\u6709\u6548\u6d88\u9664\u6709\u9650\u754c\u9762\u539a\u5ea6\u5f15\u8d77\u7684\u865a\u5047\u6f02\u79fb\uff0c\u63d0\u9ad8\u754c\u9762\u8fd0\u52a8\u6a21\u62df\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.00067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00067", "abs": "https://arxiv.org/abs/2602.00067", "authors": ["Yihan Zhang", "Ercan E. Kuruoglu"], "title": "Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning", "comment": null, "summary": "Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.", "AI": {"tldr": "NSG-MoE\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u5206\u5272\u548c\u56fe\u91cd\u8fde\u673a\u5236\u7ed3\u5408\u7ed3\u6784\u5316MoE\u67b6\u6784\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u4e2d\u7684\u6a21\u6001\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4fe1\u606f\u548c\u591a\u6a21\u6001\u8bed\u4e49\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u867d\u7136\u5177\u6709\u4e30\u5bcc\u7684\u8868\u793a\u80fd\u529b\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4f46\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u6a21\u6001\u6df7\u6dc6\u95ee\u9898\uff0c\u73b0\u6709\u901a\u7528GNNs\u5728\u5904\u7406\u591a\u6a21\u6001\u56fe\u65f6\u4f1a\u51fa\u73b0\u4e0d\u5e0c\u671b\u7684\u6df7\u5408\u6548\u5e94\u3002", "method": "\u63d0\u51faNSG-MoE\u6846\u67b6\uff1a1\uff09\u5c06\u6bcf\u4e2a\u8282\u70b9\u663e\u5f0f\u5206\u89e3\u4e3a\u6a21\u6001\u7279\u5b9a\u7ec4\u4ef6\uff1b2\uff09\u901a\u8fc7\u7ed3\u6784\u5316MoE\u67b6\u6784\u5206\u914d\u5173\u7cfb\u611f\u77e5\u4e13\u5bb6\u5904\u7406\u5f02\u6784\u6d88\u606f\u6d41\uff1b3\uff09\u7ed3\u5408\u8282\u70b9\u5206\u5272\u548c\u56fe\u91cd\u8fde\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cNSG-MoE\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5c3d\u7ba1\u91c7\u7528\u8ba1\u7b97\u91cf\u5927\u7684MoE\u67b6\u6784\uff0c\u4ecd\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u8bad\u7ec3\u6548\u7387\uff1b\u8c31\u5206\u6790\u663e\u793aNSG\u5728\u6a21\u6001\u7279\u5b9a\u5b50\u7a7a\u95f4\u4e0a\u8fdb\u884c\u81ea\u9002\u5e94\u6ee4\u6ce2\uff1b\u4fe1\u606f\u8bba\u5206\u6790\u8868\u660eNSG\u7684\u67b6\u6784\u7ea6\u675f\u51cf\u5c11\u4e86\u6570\u636e\u548c\u53c2\u6570\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "NSG-MoE\u901a\u8fc7\u663e\u5f0f\u5206\u89e3\u6a21\u6001\u7279\u5b9a\u7ec4\u4ef6\u548c\u7ed3\u6784\u5316MoE\u5904\u7406\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u591a\u6a21\u6001\u56fe\u4e2d\u7684\u6a21\u6001\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4fe1\u606f\u548c\u591a\u6a21\u6001\u8bed\u4e49\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.00316", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00316", "abs": "https://arxiv.org/abs/2602.00316", "authors": ["Rodrigo Batista", "Lu\u00eds Filipe Cunha", "Purifica\u00e7\u00e3o Silvano", "Nuno Guimar\u00e3es", "Al\u00edpio Jorge", "Evelin Amorim", "Ricardo Campos"], "title": "MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes", "comment": null, "summary": "Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.", "AI": {"tldr": "\u63d0\u51fa\u5169\u968e\u6bb5\u7ba1\u7dda\u5f9e\u5e02\u653f\u6703\u8b70\u8a18\u9304\u63d0\u53d6\u5143\u6578\u64da\uff1a\u5148\u7528QA\u6a21\u578b\u5b9a\u4f4d\u5143\u6578\u64da\u6bb5\u843d\uff0c\u518d\u7528Transformer\u6a21\u578b\u63d0\u53d6\u5be6\u9ad4\uff0c\u4e26\u8a55\u4f30\u958b\u6e90\u8207\u9589\u6e90LLMs\u7684\u6027\u80fd\u3001\u6210\u672c\u8207\u78b3\u8db3\u8de1\u3002", "motivation": "\u5e02\u653f\u6703\u8b70\u8a18\u9304\u4f5c\u70ba\u5730\u65b9\u6cbb\u7406\u7684\u5b98\u65b9\u6587\u4ef6\uff0c\u683c\u5f0f\u8207\u5beb\u4f5c\u98a8\u683c\u5404\u7570\uff0c\u5143\u6578\u64da\uff08\u6703\u8b70\u7de8\u865f\u3001\u65e5\u671f\u3001\u5730\u9ede\u3001\u53c3\u8207\u8005\u3001\u8d77\u8a16\u6642\u9593\uff09\u7f3a\u4e4f\u6a19\u6e96\u5316\u4e14\u96e3\u4ee5\u81ea\u52d5\u63d0\u53d6\u3002\u73fe\u6709NER\u6a21\u578b\u4e0d\u9069\u7528\u6b64\u9818\u57df\u7279\u5b9a\u985e\u5225\u3002", "method": "\u63d0\u51fa\u5169\u968e\u6bb5\u7ba1\u7dda\uff1a1) QA\u6a21\u578b\u8b58\u5225\u5305\u542b\u5143\u6578\u64da\u7684\u958b\u982d\u8207\u7d50\u5c3e\u6587\u672c\u6bb5\u843d\uff1b2) \u4f7f\u7528Transformer\u6a21\u578b\uff08BERTimbau\u548cXLM-RoBERTa\uff0c\u6709/\u7121CRF\u5c64\uff09\u9032\u884c\u7d30\u7c92\u5ea6\u5be6\u9ad4\u63d0\u53d6\uff0c\u4e26\u900f\u904e\u53bb\u8a5e\u5f59\u5316\u589e\u5f37\u3002\u8a55\u4f30\u958b\u6e90\uff08Phi\uff09\u8207\u9589\u6e90\uff08Gemini\uff09LLMs\u3002", "result": "\u5728\u9818\u57df\u5167\u8868\u73fe\u512a\u7570\uff0c\u512a\u65bc\u5927\u578b\u901a\u7528LLMs\u3002\u4f46\u8de8\u5e02\u653f\u8a55\u4f30\u986f\u793a\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u53cd\u6620\u5e02\u653f\u8a18\u9304\u7684\u8b8a\u7570\u6027\u8207\u8a9e\u8a00\u8907\u96dc\u6027\u3002\u5efa\u7acb\u4e86\u5e02\u653f\u6703\u8b70\u8a18\u9304\u5143\u6578\u64da\u63d0\u53d6\u7684\u9996\u500b\u57fa\u6e96\u3002", "conclusion": "\u6b64\u5de5\u4f5c\u70ba\u5e02\u653f\u6703\u8b70\u8a18\u9304\u5143\u6578\u64da\u63d0\u53d6\u5efa\u7acb\u4e86\u9996\u500b\u57fa\u6e96\uff0c\u70ba\u8a72\u9818\u57df\u672a\u4f86\u7814\u7a76\u63d0\u4f9b\u4e86\u5805\u5be6\u57fa\u790e\uff0c\u540c\u6642\u51f8\u986f\u4e86\u8de8\u5e02\u653f\u6cdb\u5316\u7684\u6311\u6230\u3002"}}
{"id": "2602.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00114", "abs": "https://arxiv.org/abs/2602.00114", "authors": ["Yunwei Bai", "Ying Kiat Tan", "Yao Shu", "Tsuhan Chen"], "title": "1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization", "comment": null, "summary": "Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.", "AI": {"tldr": "1S-DAug\uff1a\u4e00\u79cd\u7528\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u5355\u6837\u672c\u751f\u6210\u589e\u5f3a\u7b97\u5b50\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u6270\u52a8\u3001\u566a\u58f0\u6ce8\u5165\u548c\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece\u5355\u4e2a\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u591a\u6837\u4e14\u5fe0\u5b9e\u7684\u53d8\u5316\uff0c\u65e0\u9700\u6a21\u578b\u66f4\u65b0\u5373\u53ef\u63d0\u5347\u5c11\u6837\u672c\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u53ea\u6709\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u53ef\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u5355\u4e2a\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u591a\u6837\u4e14\u5fe0\u5b9e\u53d8\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1S-DAug\u7ed3\u5408\u4f20\u7edf\u51e0\u4f55\u6270\u52a8\u3001\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u548c\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u6761\u4ef6\u7684\u53bb\u566a\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece\u5355\u4e2a\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u591a\u6837\u53d8\u5316\u3002\u751f\u6210\u7684\u56fe\u50cf\u4e0e\u539f\u59cb\u56fe\u50cf\u4e00\u8d77\u7f16\u7801\u5e76\u805a\u5408\u4e3a\u7ec4\u5408\u8868\u793a\uff0c\u7528\u4e8e\u66f4\u9c81\u68d2\u7684\u9884\u6d4b\u3002", "result": "\u57284\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u7684\u5c11\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c1S-DAug\u65e0\u9700\u6a21\u578b\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5728miniImagenet 5-way-1-shot\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc710%\u7684\u6bd4\u4f8b\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "1S-DAug\u4f5c\u4e3a\u4e00\u79cd\u514d\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u63d2\u4ef6\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c11\u6837\u672c\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u4ece\u5355\u4e2a\u6d4b\u8bd5\u56fe\u50cf\u751f\u6210\u591a\u6837\u4e14\u5fe0\u5b9e\u7684\u589e\u5f3a\u6837\u672c\u6765\u89e3\u51b3\u4f20\u7edf\u6d4b\u8bd5\u65f6\u589e\u5f3a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01175", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01175", "abs": "https://arxiv.org/abs/2602.01175", "authors": ["Xiaoli Li", "Jie Shen", "Xinhui Wang"], "title": "Novel linear, decoupled, and energy dissipative schemes for the Navier-Stokes-Darcy model and extension to related two-phase flow", "comment": null, "summary": "We construct efficient original-energy-dissipative schemes for the Navier-Stokes-Darcy model and related two-phase flows using a prediction-correction framework. A new relaxation technique is incorporated in the correction step to guarantee dissipation of the original energy, thereby ensuring unconditional boundedness of the numerical solutions for velocity and hydraulic head in the $l^{\\infty}(L^2)$ and $l^2(H^1)$ norms. At each time step, the schemes require solving only a sequence of linear equations with constant coefficients. We rigorously prove that the schemes dissipate the original energy and, as an example, carry out a rigorous error analysis of the first-order scheme for the Navier-Stokes-Darcy model. Finally, a series of benchmark numerical experiments are conducted to demonstrate the accuracy, stability, and effectiveness of the proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9884\u6d4b-\u6821\u6b63\u6846\u67b6\u7684\u9ad8\u6548\u539f\u59cb\u80fd\u91cf\u8017\u6563\u683c\u5f0f\uff0c\u7528\u4e8eNavier-Stokes-Darcy\u6a21\u578b\u53ca\u76f8\u5173\u4e24\u76f8\u6d41\uff0c\u901a\u8fc7\u677e\u5f1b\u6280\u672f\u4fdd\u8bc1\u539f\u59cb\u80fd\u91cf\u8017\u6563\uff0c\u5b9e\u73b0\u65e0\u6761\u4ef6\u6709\u754c\u6570\u503c\u89e3\u3002", "motivation": "\u4e3aNavier-Stokes-Darcy\u6a21\u578b\u53ca\u76f8\u5173\u4e24\u76f8\u6d41\u95ee\u9898\u5f00\u53d1\u9ad8\u6548\u6570\u503c\u683c\u5f0f\uff0c\u786e\u4fdd\u6570\u503c\u89e3\u5728\u957f\u65f6\u95f4\u6a21\u62df\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6709\u754c\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u9884\u6d4b-\u6821\u6b63\u6846\u67b6\uff0c\u5728\u4fee\u6b63\u6b65\u4e2d\u5f15\u5165\u65b0\u7684\u677e\u5f1b\u6280\u672f\u4ee5\u4fdd\u8bc1\u539f\u59cb\u80fd\u91cf\u8017\u6563\uff1b\u683c\u5f0f\u53ea\u9700\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u6c42\u89e3\u5e38\u7cfb\u6570\u7ebf\u6027\u65b9\u7a0b\u7ec4\u5e8f\u5217\u3002", "result": "\u4e25\u683c\u8bc1\u660e\u4e86\u683c\u5f0f\u7684\u539f\u59cb\u80fd\u91cf\u8017\u6563\u6027\uff0c\u5bf9\u4e00\u9636\u683c\u5f0f\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u8bef\u5dee\u5206\u6790\uff1b\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9884\u6d4b-\u6821\u6b63\u683c\u5f0f\u80fd\u6709\u6548\u4fdd\u8bc1Navier-Stokes-Darcy\u6a21\u578b\u6570\u503c\u89e3\u7684\u539f\u59cb\u80fd\u91cf\u8017\u6563\u548c\u65e0\u6761\u4ef6\u6709\u754c\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u9ad8\u6548\u6027\u3002"}}
{"id": "2602.00370", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00370", "abs": "https://arxiv.org/abs/2602.00370", "authors": ["Trisha Das", "Katherine Kero", "Dorinda Schumann", "Tracy Ohrt", "Sanjit Singh Batra", "Gregory D Lyng", "Robert E. Tillman"], "title": "POET: Protocol Optimization via Eligibility Tuning", "comment": null, "summary": "Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u8f74\u5f15\u5bfc\u7684\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u7c7b\u522b\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u53c2\u6570\u7b49\uff09\u6307\u5bfc\u751f\u6210\uff0c\u5728\u7279\u5b9a\u6027\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u8017\u65f6\u4e14\u8ba4\u77e5\u8d1f\u8377\u9ad8\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u9ad8\u5ea6\u7ed3\u6784\u5316\u8f93\u5165\uff08\u5982\u9884\u5b9a\u4e49\u5b9e\u4f53\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u7aef\u5230\u7aef\u7cfb\u7edf\u4ece\u6700\u5c0f\u8f93\u5165\u751f\u6210\u5b8c\u6574\u6807\u51c6\uff0c\u5b9e\u7528\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u5f15\u5bfc\u751f\u6210\u6846\u67b6\uff0c\u5f15\u5165\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u8f74\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u53c2\u6570\u3001\u884c\u4e3a\u56e0\u7d20\u7b49\uff09\u6765\u6307\u5bfc\u8d44\u683c\u6807\u51c6\u751f\u6210\u3002\u8fd9\u4e9b\u8bed\u4e49\u8f74\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u5bfc\uff0c\u5728\u7279\u5b9a\u6027\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e2d\u95f4\u65b9\u6848\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u65e0\u9700\u6307\u5b9a\u786e\u5207\u5b9e\u4f53\u5373\u53ef\u5f15\u5bfc\u751f\u6210\u3002", "result": "\u5f15\u5bfc\u751f\u6210\u65b9\u6cd5\u5728\u81ea\u52a8\u8bc4\u4f30\u3001\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u4f30\u548c\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e2d\u5747\u4e00\u81f4\u4f18\u4e8e\u975e\u5f15\u5bfc\u751f\u6210\uff0c\u4e3aAI\u8f85\u52a9\u8bd5\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5f15\u5bfc\u751f\u6210\u6846\u67b6\u5728\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u8bbe\u8ba1\u65b9\u9762\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684AI\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bed\u4e49\u8f74\u5728\u7279\u5b9a\u6027\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2602.02247", "categories": ["math.NA", "math.AP", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2602.02247", "abs": "https://arxiv.org/abs/2602.02247", "authors": ["Julian Koellermeier"], "title": "A new Energy Equation Derivation for the Shallow Water Linearized Moment Equations", "comment": null, "summary": "Shallow Water Moment Equations (SWME) are extensions to the well-known Shallow Water Equations (SWE) for the efficient modeling and numerical simulation of free-surface flows. While the SWE typically assume a depth-averaged vertical velocity profile, the SWME allow for vertical variations of the velocity profile. The SWME therefore assume a polynomial profile and then derive additional evolution equations for the polynomial coefficients via higher order depth integration. In this work, we perform a new systematic derivation of the energy equation for a specific variant of the SWME, called the Shallow Water Linearized Moment Equations (SWLME). The derivation is based on the standard SWE energy equation derivation and includes the skew-symmetric formulation of the model. The new systematic derivation is beneficial for the extension to other SWME variants and their numerical solution.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a8\u5bfc\u4e86\u6d45\u6c34\u7ebf\u6027\u5316\u77e9\u65b9\u7a0b(SWLME)\u7684\u80fd\u91cf\u65b9\u7a0b\uff0c\u57fa\u4e8e\u6807\u51c6SWE\u80fd\u91cf\u65b9\u7a0b\u63a8\u5bfc\u5e76\u5305\u542b\u6a21\u578b\u7684\u659c\u5bf9\u79f0\u5f62\u5f0f\uff0c\u4e3a\u6269\u5c55\u5230\u5176\u4ed6SWME\u53d8\u4f53\u53ca\u5176\u6570\u503c\u6c42\u89e3\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u6d45\u6c34\u77e9\u65b9\u7a0b(SWME)\u4f5c\u4e3a\u6d45\u6c34\u65b9\u7a0b(SWE)\u7684\u6269\u5c55\uff0c\u80fd\u591f\u6355\u6349\u901f\u5ea6\u5256\u9762\u7684\u5782\u76f4\u53d8\u5316\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u53d8\u4f53SWLME\u80fd\u91cf\u65b9\u7a0b\u7684\u7cfb\u7edf\u63a8\u5bfc\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aSWME\u7684\u80fd\u91cf\u5b88\u6052\u6027\u8d28\u548c\u6570\u503c\u6c42\u89e3\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u57fa\u4e8e\u6807\u51c6SWE\u80fd\u91cf\u65b9\u7a0b\u7684\u63a8\u5bfc\u65b9\u6cd5\uff0c\u5bf9\u6d45\u6c34\u7ebf\u6027\u5316\u77e9\u65b9\u7a0b(SWLME)\u8fdb\u884c\u7cfb\u7edf\u63a8\u5bfc\u3002\u63a8\u5bfc\u8fc7\u7a0b\u5305\u542b\u6a21\u578b\u7684\u659c\u5bf9\u79f0\u5f62\u5f0f\uff0c\u901a\u8fc7\u9ad8\u9636\u6df1\u5ea6\u79ef\u5206\u5904\u7406\u591a\u9879\u5f0f\u7cfb\u6570\uff0c\u5efa\u7acb\u5b8c\u6574\u7684\u80fd\u91cf\u65b9\u7a0b\u4f53\u7cfb\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51faSWLME\u7684\u80fd\u91cf\u65b9\u7a0b\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u659c\u5bf9\u79f0\u5f62\u5f0f\u7684\u80fd\u91cf\u5b88\u6052\u5173\u7cfb\u3002\u8be5\u63a8\u5bfc\u65b9\u6cd5\u5177\u6709\u7cfb\u7edf\u6027\uff0c\u4fbf\u4e8e\u6269\u5c55\u5230\u5176\u4ed6SWME\u53d8\u4f53\uff0c\u5e76\u4e3a\u6570\u503c\u6c42\u89e3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u7684\u7cfb\u7edf\u63a8\u5bfc\u4e3aSWLME\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u80fd\u91cf\u65b9\u7a0b\u6846\u67b6\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7406\u89e3\u8be5\u6a21\u578b\u7684\u7406\u8bba\u6027\u8d28\uff0c\u8fd8\u4e3a\u6269\u5c55\u5230\u5176\u4ed6SWME\u53d8\u4f53\u53ca\u5176\u6570\u503c\u6c42\u89e3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00072", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00072", "abs": "https://arxiv.org/abs/2602.00072", "authors": ["Jice Zeng", "David Barajas-Solano", "Hui Chen"], "title": "Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning", "comment": null, "summary": "The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u5f0f\u8fc1\u79fb\u5b66\u4e60\u7684\u6982\u7387\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5148\u5728\u5927\u91cf\u4f4e\u4fdd\u771f\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u5c11\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u4e0a\u5fae\u8c03\uff09\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6ee1\u5c04\u5c42\u5b9e\u73b0\u7ef4\u5ea6\u7ea6\u7b80\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\uff0c\u4f46\u9ad8\u4fdd\u771f\u6570\u636e\u7a00\u7f3a\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f4e\u4fdd\u771f\u6570\u636e\u4e30\u5bcc\u4f46\u7cbe\u5ea6\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u6784\u5efa\u6570\u636e\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u751f\u6210\u5f0f\u8fc1\u79fb\u5b66\u4e60\u7684\u6982\u7387\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u4e3b\u5e72\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u5728\u5927\u91cf\u4f4e\u4fdd\u771f\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u5b66\u4e60\u6982\u7387\u524d\u5411\u6a21\u578b\uff1b2\uff09\u5728\u5c11\u91cf\u9ad8\u4fdd\u771f\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u4fee\u6b63\u4f4e\u4fdd\u771f-\u9ad8\u4fdd\u771f\u5dee\u5f02\u3002\u5f15\u5165\u6ee1\u5c04\u5c42\u4e0e\u6807\u51c6\u8026\u5408\u5757\u7ed3\u5408\uff0c\u653e\u677e\u6807\u51c6\u53cc\u5c04\u5f52\u4e00\u5316\u6d41\u7684\u7ef4\u5ea6\u4fdd\u6301\u7ea6\u675f\uff0c\u5b9e\u73b0\u5b66\u4e60\u7ef4\u5ea6\u7ea6\u7b80\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u94a2\u7b4b\u6df7\u51dd\u571f\u677f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u7ed3\u5408\u5927\u91cf\u7c97\u7f51\u683c\uff08\u4f4e\u4fdd\u771f\uff09\u6a21\u62df\u548c\u6709\u9650\u7ec6\u7f51\u683c\uff08\u9ad8\u4fdd\u771f\uff09\u6a21\u62df\u3002\u6a21\u578b\u5b9e\u73b0\u4e86\u5177\u6709\u9ad8\u4fdd\u771f\u7cbe\u5ea6\u7684\u6982\u7387\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u4f4e\u4fdd\u771f\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u9ad8\u4fdd\u771f\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6846\u67b6\u4e3a\u590d\u6742\u5de5\u7a0b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u6570\u636e\u9ad8\u6548\u3001\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u4ee3\u7406\u5efa\u6a21\u8def\u5f84\uff0c\u80fd\u591f\u63d0\u4f9b\u5feb\u901f\u6982\u7387\u9884\u6d4b\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2602.00319", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.00319", "abs": "https://arxiv.org/abs/2602.00319", "authors": ["Siyuan Shen", "Kai Wang"], "title": "Detecting AI-Generated Content in Academic Peer Reviews", "comment": null, "summary": "The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u751f\u6210\u5185\u5bb9\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u5feb\u901f\u589e\u957f\uff0c2025\u5e74ICLR\u7ea620%\u3001Nature Communications\u7ea612%\u7684\u8bc4\u5ba1\u88ab\u68c0\u6d4b\u4e3aAI\u751f\u6210", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76AI\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u89d2\u8272\u548c\u5f71\u54cd\uff0c\u4e86\u89e3AI\u751f\u6210\u5185\u5bb9\u5728\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\u7684\u51fa\u73b0\u8d8b\u52bf", "method": "\u4f7f\u7528\u5728\u5386\u53f2\u8bc4\u5ba1\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u5206\u6790ICLR\u548cNature Communications\u540e\u7eed\u8bc4\u5ba1\u5468\u671f\u4e2d\u7684AI\u751f\u6210\u5185\u5bb9\uff0c\u89c2\u5bdf\u65f6\u95f4\u8d8b\u52bf", "result": "2022\u5e74\u524dAI\u751f\u6210\u5185\u5bb9\u6781\u5c11\uff0c\u4e4b\u540e\u663e\u8457\u589e\u52a0\uff0c2025\u5e74ICLR\u7ea620%\u3001Nature Communications\u7ea612%\u7684\u8bc4\u5ba1\u88ab\u5206\u7c7b\u4e3aAI\u751f\u6210\uff0cNC\u57282024\u5e74\u7b2c\u4e09\u5230\u7b2c\u56db\u5b63\u5ea6\u589e\u957f\u6700\u660e\u663e", "conclusion": "AI\u8f85\u52a9\u5185\u5bb9\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u5feb\u901f\u589e\u52a0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5bf9\u5b66\u672f\u8bc4\u4ef7\u7684\u5f71\u54cd"}}
{"id": "2602.00115", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00115", "abs": "https://arxiv.org/abs/2602.00115", "authors": ["David El-Chai Ben-Ezra", "Adar Tal", "Daniel Brisk"], "title": "Event Driven Clustering Algorithm", "comment": "~10 pages, 2 figures", "summary": "This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u5b9e\u65f6\u68c0\u6d4b\u5c0f\u4e8b\u4ef6\u7c07\u7684\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u7b97\u6cd5\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6O(n)\u4e14\u8fd0\u884c\u65f6\u95f4\u4e0e\u50cf\u7d20\u9635\u5217\u7ef4\u5ea6\u65e0\u5173", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4ea7\u751f\u5f02\u6b65\u4e8b\u4ef6\u6d41\u6570\u636e\uff0c\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u5c0f\u4e8b\u4ef6\u7c07\uff0c\u73b0\u6709\u7b97\u6cd5\u590d\u6742\u5ea6\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5904\u7406\u9700\u6c42", "method": "\u57fa\u4e8e\u5206\u5c42\u51dd\u805a\u805a\u7c7b\u7b97\u6cd5\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u5f02\u6b65\u6570\u636e\u7ed3\u6784\u7279\u6027\uff0c\u901a\u8fc7\u9ad8\u6548\u7b80\u5355\u7684\u51b3\u7b56\u673a\u5236\u5b9e\u73b0\u4e8b\u4ef6\u7c07\u68c0\u6d4b", "result": "\u7b97\u6cd5\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6O(n)\uff0c\u8fd0\u884c\u65f6\u95f4\u4e0e\u50cf\u7d20\u9635\u5217\u7ef4\u5ea6\u65e0\u5173\uff0c\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u6570\u636e", "conclusion": "\u8be5\u5f02\u6b65\u4e8b\u4ef6\u9a71\u52a8\u7b97\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u7684\u5c0f\u4e8b\u4ef6\u7c07\u5b9e\u65f6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.01301", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01301", "abs": "https://arxiv.org/abs/2602.01301", "authors": ["Khalide Jbilou"], "title": "A survey of scalar and vector extrapolation", "comment": null, "summary": "Scalar extrapolation and convergence acceleration methods are central tools in numerical analysis for improving the efficiency of iterative algorithms and the summation of slowly convergent series. These methods construct transformed sequences that converge more rapidly to the same limit without altering the underlying iterative process, thereby reducing computational cost and enhancing numerical accuracy. Historically, the origins of such techniques can be traced back to classical algebraic methods by AlKhwarizmi and early series acceleration techniques by Newton, while systematic approaches emerged in the 20th century with Aitken process and Richardson extrapolation. Later developments, including the Shanks transformation and Wynn epsilon algorithm, provided general frameworks capable of eliminating multiple dominant error components, with deep connections to Pade approximants and rational approximations of generating functions. This paper presents a comprehensive review of classical scalar extrapolation methods, including Richardson extrapolation, Aitken process, Shanks transformation, Wynn epsilon algorithm, and other algorithms. We examine their theoretical foundations, asymptotic error models, convergence properties, numerical stability, and practical implementation considerations. The second part of this work is dedicated to vector extrapolation methods: polynomial based ones and epsilon algorithm generalizations to vector sequences. Additionally, we highlight modern developments such as their applications to iterative solvers, Krylov subspace methods, and large-scale computational simulations. The aim of this review is to provide a unified perspective on scalar and vector extrapolation techniques, bridging historical origins, theoretical insights, and contemporary computational applications.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u6807\u91cf\u548c\u5411\u91cf\u5916\u63a8\u4e0e\u6536\u655b\u52a0\u901f\u65b9\u6cd5\uff0c\u6db5\u76d6\u4ece\u7ecf\u5178\u6280\u672f\u5230\u73b0\u4ee3\u53d1\u5c55\u7684\u7406\u8bba\u3001\u7b97\u6cd5\u53ca\u5e94\u7528\u3002", "motivation": "\u6570\u503c\u5206\u6790\u4e2d\u9700\u8981\u63d0\u9ad8\u8fed\u4ee3\u7b97\u6cd5\u6548\u7387\u548c\u6162\u6536\u655b\u7ea7\u6570\u6c42\u548c\u7684\u5de5\u5177\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u6784\u9020\u66f4\u5feb\u6536\u655b\u7684\u53d8\u6362\u5e8f\u5217\u800c\u4e0d\u6539\u53d8\u5e95\u5c42\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u6570\u503c\u7cbe\u5ea6\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u7ecf\u5178\u6807\u91cf\u5916\u63a8\u65b9\u6cd5\uff08Richardson\u5916\u63a8\u3001Aitken\u8fc7\u7a0b\u3001Shanks\u53d8\u6362\u3001Wynn epsilon\u7b97\u6cd5\u7b49\uff09\uff0c\u5206\u6790\u5176\u7406\u8bba\u57fa\u7840\u3001\u6e10\u8fd1\u8bef\u5dee\u6a21\u578b\u3001\u6536\u655b\u6027\u8d28\u3001\u6570\u503c\u7a33\u5b9a\u6027\u548c\u5b9e\u9645\u5b9e\u73b0\u8003\u8651\uff1b\u7b2c\u4e8c\u90e8\u5206\u4e13\u6ce8\u4e8e\u5411\u91cf\u5916\u63a8\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u591a\u9879\u5f0f\u7684\u7b97\u6cd5\u548cepsilon\u7b97\u6cd5\u5bf9\u5411\u91cf\u5e8f\u5217\u7684\u63a8\u5e7f\u3002", "result": "\u63d0\u4f9b\u4e86\u6807\u91cf\u548c\u5411\u91cf\u5916\u63a8\u6280\u672f\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u8fde\u63a5\u5386\u53f2\u8d77\u6e90\u3001\u7406\u8bba\u89c1\u89e3\u548c\u5f53\u4ee3\u8ba1\u7b97\u5e94\u7528\uff0c\u6db5\u76d6\u4ece\u8fed\u4ee3\u6c42\u89e3\u5668\u3001Krylov\u5b50\u7a7a\u95f4\u65b9\u6cd5\u5230\u5927\u89c4\u6a21\u8ba1\u7b97\u6a21\u62df\u7684\u73b0\u4ee3\u53d1\u5c55\u3002", "conclusion": "\u5916\u63a8\u548c\u6536\u655b\u52a0\u901f\u65b9\u6cd5\u662f\u6570\u503c\u5206\u6790\u4e2d\u7684\u6838\u5fc3\u5de5\u5177\uff0c\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u7ecf\u5178\u548c\u73b0\u4ee3\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7406\u89e3\u8fd9\u4e9b\u6280\u672f\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\u7684\u5168\u9762\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\u548c\u6570\u503c\u7cbe\u5ea6\u7684\u6539\u8fdb\u3002"}}
{"id": "2602.00400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00400", "abs": "https://arxiv.org/abs/2602.00400", "authors": ["Fan Yang", "Rui Meng", "Trudi Di Qi", "Ali Ezzati", "Yuxin Wen"], "title": "KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.", "AI": {"tldr": "KEPO\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d28\u91cf\u95e8\u63a7\u84b8\u998f\u548c\u77e5\u8bc6\u589e\u5f3a\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u5931\u8d25\u548c\u68af\u5ea6\u566a\u58f0\u95ee\u9898\uff0c\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u63a8\u7406\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u9762\u4e34\u7a00\u758f\u8f68\u8ff9\u5956\u52b1\u5bfc\u81f4\u7684\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u548c\u4e25\u91cd\u63a2\u7d22\u5931\u8d25\u95ee\u9898\uff0c\u73b0\u6709\u5747\u5300\u84b8\u998f\u65b9\u6cd5\u5728\u4f4e\u8d28\u91cf\u8f68\u8ff9\u4e0a\u4f1a\u4ea7\u751f\u566a\u58f0\u68af\u5ea6\uff0c\u4e0d\u9002\u5408\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u589e\u5f3a\u504f\u597d\u4f18\u5316(KEPO)\u6846\u67b6\uff1a1)\u8d28\u91cf\u95e8\u63a7\u7684\u5728\u7ebf\u84b8\u998f\u76ee\u6807\uff0c\u4ec5\u5bf9\u9ad8\u8d28\u91cf\u8f68\u8ff9\u5e94\u7528\u5bc6\u96c6\u6559\u5e08\u6307\u5bfc\uff1b2)\u77e5\u8bc6\u589e\u5f3a\u63a2\u7d22\u7b56\u7565\uff0c\u5229\u7528\u4ece\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u7684\u63d0\u793a\u6765\u62d2\u7edd\u6027\u91c7\u6837\u5956\u52b1\u6b63\u7684\u5728\u7ebf\u8f68\u8ff9\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKEPO\u5728\u5355\u6e90\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u66f4\u4e00\u81f4\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u4ee5\u53ca\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u84b8\u998f\u57fa\u7ebf\u7684\u5206\u5e03\u5916\u6027\u80fd\u3002", "conclusion": "KEPO\u901a\u8fc7\u9009\u62e9\u6027\u84b8\u998f\u548c\u77e5\u8bc6\u589e\u5f3a\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6311\u6218\uff0c\u4e3a\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u6846\u67b6\u3002"}}
{"id": "2602.00075", "categories": ["cs.LG", "cs.MS"], "pdf": "https://arxiv.org/pdf/2602.00075", "abs": "https://arxiv.org/abs/2602.00075", "authors": ["Philipp Andelfinger", "Wentong Cai"], "title": "Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation", "comment": "Accepted at ACM SIGSIM PADS 2026", "summary": "Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"dimensional peeking\"\u7684\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u79bb\u6563\u4eff\u771f\u4f18\u5316\u4e2d\u7684\u68af\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u63d0\u5347\u91c7\u6837\u7c92\u5ea6\u6765\u51cf\u5c11\u65b9\u5dee\uff0c\u63d0\u9ad8\u96f6\u9636\u4f18\u5316\u5728\u79bb\u6563\u975e\u51f8\u4eff\u771f\u4e2d\u7684\u7ade\u4e89\u529b\u3002", "motivation": "\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5e38\u7528\u4e8e\u9ad8\u7ef4\u7a7a\u95f4\u5bfb\u627e\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u4f46\u5f53\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u5bfc\u6570\u65f6\uff0c\u9700\u8981\u4f7f\u7528\u968f\u673a\u4f30\u8ba1\u5668\u8fd1\u4f3c\u68af\u5ea6\u3002\u8fd9\u4e9b\u57fa\u4e8e\u6270\u52a8\u7684\u91c7\u6837\u65b9\u6cd5\u4f1a\u5f15\u5165\u65b9\u5dee\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3002\u7279\u522b\u662f\u5728\u79bb\u6563\u4eff\u771f\u4f18\u5316\u4e2d\uff0c\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\"dimensional peeking\"\u65b9\u6cd5\uff0c\u5c06\u91c7\u6837\u7c92\u5ea6\u4ece\u6807\u91cf\u503c\u63d0\u5347\u5230\u9075\u5faa\u76f8\u540c\u63a7\u5236\u6d41\u8def\u5f84\u7684\u6570\u503c\u7c7b\u522b\uff0c\u4ece\u800c\u589e\u52a0\u6bcf\u6b21\u4eff\u771f\u8bc4\u4f30\u6536\u96c6\u7684\u4fe1\u606f\u91cf\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5df2\u6709\u7684\u5e73\u6ed1\u68af\u5ea6\u4f30\u8ba1\u5668\u63a8\u5bfc\u800c\u6765\uff0c\u4e0d\u4f1a\u5f15\u5165\u4efb\u4f55\u504f\u5dee\u3002\u901a\u8fc7\u81ea\u5b9a\u4e49\u6570\u503c\u6570\u636e\u7c7b\u578b\u5728C++\u7a0b\u5e8f\u4e2d\u900f\u660e\u5730\u5b9e\u73b0\u3002", "result": "\u5728\u4e09\u4e2a\u9ad8\u7ef4\u8f93\u5165\u7684\u4eff\u771f\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u89c2\u5bdf\u5230\u65b9\u5dee\u51cf\u5c11\u56e0\u5b50\u6700\u9ad8\u8fbe\u52307.9\u500d\u3002\u4e0e\u4e09\u79cd\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u76f8\u6bd4\uff0c\u4f18\u5316\u8fdb\u5c55\u8868\u660edimensional peeking\u63d0\u9ad8\u4e86\u96f6\u9636\u4f18\u5316\u5728\u79bb\u6563\u548c\u975e\u51f8\u4eff\u771f\u4e2d\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "Dimensional peeking\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u68af\u5ea6\u4f30\u8ba1\u7684\u6548\u7387\uff0c\u4f7f\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u79bb\u6563\u975e\u51f8\u4eff\u771f\u4f18\u5316\u95ee\u9898\u4e2d\u66f4\u5177\u7ade\u4e89\u529b\uff0c\u4e3a\u9ad8\u7ef4\u4eff\u771f\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00352", "abs": "https://arxiv.org/abs/2602.00352", "authors": ["Li Siyan", "Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning", "comment": null, "summary": "When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.", "AI": {"tldr": "\u63d0\u51faDETOUR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\"\u8bdd\u5230\u5634\u8fb9\"\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6a21\u7cca\u3001\u4e0d\u5b8c\u6574\u4fe1\u606f\u4e0b\u7684\u68c0\u7d22\u80fd\u529b", "motivation": "\u73b0\u6709\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\"\u8bdd\u5230\u5634\u8fb9\"\u641c\u7d22\u8fc7\u7a0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9650\u4e8e\u5355\u8f6e\u4ea4\u4e92\uff0c\u65e0\u6cd5\u771f\u5b9e\u6a21\u62df\u4eba\u7c7b\u5728\u5bf9\u8bdd\u4e2d\u7ecf\u8fc7\u591a\u8f6e\u4ea4\u4e92\u624d\u56de\u5fc6\u8d77\u4fe1\u606f\u7684\u73b0\u5b9e\u573a\u666f", "method": "\u8bbe\u8ba1DETOUR\u53cc\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b1,011\u4e2a\u63d0\u793a\u3002\u8bbe\u7f6e\u4e3b\u667a\u80fd\u4f53\uff08\u88ab\u8bc4\u4f30\u5bf9\u8c61\uff09\u901a\u8fc7\u67e5\u8be2\u8bb0\u5fc6\u667a\u80fd\u4f53\u6765\u8bc6\u522b\u56de\u5fc6\u5b9e\u4f53\uff0c\u8bb0\u5fc6\u667a\u80fd\u4f53\u5728\u8bc4\u4f30\u4e2d\u4fdd\u6301\u4e00\u81f4", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5728\u6240\u6709\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\uff09\u4e0a\u4ec5\u8fbe\u523036%\u7684\u51c6\u786e\u7387", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u589e\u5f3a\u6a21\u578b\u5728\u6a21\u7cca\u3001\u4e0d\u5b8c\u6574\u4fe1\u606f\u573a\u666f\u4e0b\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0cDETOUR\u57fa\u51c6\u4e3a\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u667a\u80fd\u4f53\u641c\u7d22\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2602.00117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00117", "abs": "https://arxiv.org/abs/2602.00117", "authors": ["Lamia Lahouel", "Laurynas Lopata", "Simon Gruening", "Gabriele Meoni", "Gaetan Petit", "Sylvain Lobry"], "title": "IC-EO: Interpretable Code-based assistant for Earth Observation", "comment": "15 pages, 1 figure", "summary": "Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5de5\u5177LLM\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u3001\u53ef\u5ba1\u8ba1\u7684Python\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5730\u7403\u89c2\u6d4b\u5206\u6790\uff0c\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u5206\u6790\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u56f0\u96be\uff0c\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u6280\u672f\u80fd\u529b\uff0c\u4e14\u73b0\u6709\u7cfb\u7edf\u591a\u4e3a\u9ed1\u76d2\u9884\u6d4b\uff0c\u96be\u4ee5\u5ba1\u8ba1\u6216\u590d\u73b0\u3002", "method": "\u6784\u5efa\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff0c\u57fa\u4e8e\u7edf\u4e00\u53ef\u6269\u5c55API\uff08\u5206\u7c7b\u3001\u5206\u5272\u3001\u68c0\u6d4b\u3001\u5149\u8c31\u6307\u6570\u3001\u5730\u7406\u7a7a\u95f4\u64cd\u4f5c\uff09\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3aPython\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u571f\u5730\u7ec4\u6210\u6620\u5c04\u4efb\u52a1\u4e0a\u8fbe\u523064.2%\u51c6\u786e\u7387\uff08vs. GPT-4o\u768451.7%\uff09\uff0c\u5728\u91ce\u706b\u540e\u635f\u5bb3\u8bc4\u4f30\u4efb\u52a1\u4e0a\u8fbe\u523050%\u51c6\u786e\u7387\uff08vs. \u57fa\u7ebf0%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u679c\u900f\u660e\u6613\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u8f93\u51fa\u53ef\u9a8c\u8bc1\u4ee3\u7801\uff0c\u8be5\u65b9\u6cd5\u5c06\u5730\u7403\u89c2\u6d4b\u5206\u6790\u8f6c\u53d8\u4e3a\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u8fc7\u7a0b\uff0c\u663e\u8457\u4f18\u4e8e\u901a\u7528LLM/VLM\u57fa\u7ebf\u3002"}}
{"id": "2602.01315", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01315", "abs": "https://arxiv.org/abs/2602.01315", "authors": ["Shishu Pal Singh", "Sudeep Kundu"], "title": "Finite element theta schemes for the viscous Burgers' equation with nonlinear Neumann boundary feedback control", "comment": null, "summary": "In this article, we develop a fully discrete numerical scheme for the one-dimensional (1D) and two-dimensional (2D) viscous Burgers equations with nonlinear Neumann boundary feedback control. The temporal discretization employs a $\u03b8$-scheme, while a conforming finite element method is used for the spatial approximation. The existence and uniqueness of the fully discrete solution are established. We further prove that the scheme is unconditionally exponentially stable for $\u03b8\\in [1/2, 1]$, thereby ensuring that the stabilization property of the continuous model is retained at the discrete level. In addition, optimal error estimates are obtained for both the state variable and the boundary control inputs in 1D and 2D frameworks. Finally, several numerical experiments are presented to validate our theoretical findings and to demonstrate the effectiveness of the proposed stabilization strategy under varying model parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e00\u7ef4\u548c\u4e8c\u7ef4\u7c98\u6027Burgers\u65b9\u7a0b\u7684\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\uff0c\u5305\u542b\u975e\u7ebf\u6027Neumann\u8fb9\u754c\u53cd\u9988\u63a7\u5236\uff0c\u8bc1\u660e\u4e86\u683c\u5f0f\u7684\u65e0\u6761\u4ef6\u6307\u6570\u7a33\u5b9a\u6027\u548c\u6700\u4f18\u8bef\u5dee\u4f30\u8ba1\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u8fde\u7eed\u6a21\u578b\u7a33\u5b9a\u7279\u6027\u7684\u79bb\u6563\u6570\u503c\u683c\u5f0f\uff0c\u7528\u4e8e\u6c42\u89e3\u5e26\u6709\u975e\u7ebf\u6027Neumann\u8fb9\u754c\u53cd\u9988\u63a7\u5236\u7684\u7c98\u6027Burgers\u65b9\u7a0b\uff0c\u786e\u4fdd\u79bb\u6563\u5c42\u9762\u4ecd\u80fd\u4fdd\u6301\u6307\u6570\u7a33\u5b9a\u6027\u3002", "method": "\u65f6\u95f4\u79bb\u6563\u91c7\u7528\u03b8-\u683c\u5f0f\uff0c\u7a7a\u95f4\u79bb\u6563\u4f7f\u7528\u534f\u8c03\u6709\u9650\u5143\u65b9\u6cd5\uff0c\u5efa\u7acb\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\uff0c\u5206\u6790\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u3001\u65e0\u6761\u4ef6\u6307\u6570\u7a33\u5b9a\u6027\u548c\u6700\u4f18\u8bef\u5dee\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86\u5f53\u03b8\u2208[1/2,1]\u65f6\u683c\u5f0f\u65e0\u6761\u4ef6\u6307\u6570\u7a33\u5b9a\uff0c\u83b7\u5f97\u4e86\u72b6\u6001\u53d8\u91cf\u548c\u8fb9\u754c\u63a7\u5236\u8f93\u5165\u7684\u6700\u4f18\u8bef\u5dee\u4f30\u8ba1\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u548c\u7a33\u5b9a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5168\u79bb\u6563\u6570\u503c\u683c\u5f0f\u6210\u529f\u4fdd\u6301\u4e86\u8fde\u7eed\u6a21\u578b\u7684\u7a33\u5b9a\u7279\u6027\uff0c\u5728\u79bb\u6563\u5c42\u9762\u5b9e\u73b0\u4e86\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u4e3a\u7c98\u6027Burgers\u65b9\u7a0b\u7684\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2602.00405", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00405", "abs": "https://arxiv.org/abs/2602.00405", "authors": ["Deep Gandhi", "Katyani Singh", "Nidhi Hegde"], "title": "RobustDebias: Debiasing Language Models using Distributionally Robust Optimization", "comment": null, "summary": "Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \\textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.", "AI": {"tldr": "\u63d0\u51faRobustDebias\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5728\u5fae\u8c03\u9636\u6bb5\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u504f\u89c1\uff0c\u907f\u514d\u6602\u8d35\u7684\u9884\u8bad\u7ec3\u4fee\u6539", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u504f\u89c1\u548c\u793e\u4f1a\u523b\u677f\u5370\u8c61\uff0c\u73b0\u6709\u53bb\u504f\u89c1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9884\u8bad\u7ec3\u9636\u6bb5\u4fee\u6539\u5d4c\u5165\u7a7a\u95f4\uff0c\u8fd9\u5bf9\u5927\u6a21\u578b\u4e0d\u5177\u53ef\u6269\u5c55\u6027\u3002\u5fae\u8c03\u4f1a\u653e\u5927\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u800c\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5728\u4f18\u5316\u4e0b\u6e38\u6027\u80fd\u65f6\u4f1a\u653e\u5927\u793e\u4f1a\u504f\u89c1", "method": "\u63d0\u51faRobustDebias\u673a\u5236\uff0c\u5c06\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08DRO\uff09\u5e94\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\uff0c\u5728MLM\u5fae\u8c03\u4e2d\u8de8\u591a\u4e2a\u4eba\u53e3\u7edf\u8ba1\u7ef4\u5ea6\u53bb\u504f\u89c1\uff0c\u53ef\u6cdb\u5316\u5230\u4efb\u4f55\u6570\u636e\u96c6\u6216\u4efb\u52a1", "result": "\u5728\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u8f7b\u504f\u89c1\uff0c\u540c\u65f6\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5c0f", "conclusion": "RobustDebias\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u5fae\u8c03\u9636\u6bb5\u6709\u6548\u53bb\u504f\u89c1\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u9884\u8bad\u7ec3\u4fee\u6539\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd"}}
{"id": "2602.00077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00077", "abs": "https://arxiv.org/abs/2602.00077", "authors": ["Francisco Mart\u00ednez", "Mar\u00eda P. Fr\u00edas"], "title": "Automated univariate time series forecasting with regression trees", "comment": "23 pages, 17 figures", "summary": "This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u56de\u5f52\u6811\u53ca\u5176\u96c6\u6210\u65b9\u6cd5\uff08\u88c5\u888b\u548c\u968f\u673a\u68ee\u6797\uff09\u8fdb\u884c\u81ea\u52a8\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u6cd5\u3001\u9012\u5f52\u9884\u6d4b\u3001\u7279\u5f81\u9009\u62e9\u3001\u8d8b\u52bf\u5904\u7406\u548c\u5b63\u8282\u6027\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6307\u6570\u5e73\u6ed1\u6216ARIMA\u7b49\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u76f8\u5f53\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u516c\u5f00\u53ef\u7528\u7684\u8f6f\u4ef6\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5982\u6307\u6570\u5e73\u6ed1\u548cARIMA\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u8fdb\u884c\u53c2\u6570\u9009\u62e9\u548c\u6a21\u578b\u8c03\u6574\uff0c\u800c\u57fa\u4e8e\u56de\u5f52\u6811\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u66f4\u81ea\u52a8\u5316\u7684\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u6cd5\u548c\u9012\u5f52\u9884\u6d4b\uff0c\u4f7f\u7528\u56de\u5f52\u6811\u53ca\u5176\u96c6\u6210\u65b9\u6cd5\uff08\u88c5\u888b\u548c\u968f\u673a\u68ee\u6797\uff09\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u7279\u5f81\u9009\u62e9\u3001\u8d8b\u52bf\u5e8f\u5217\u5904\u7406\u4ee5\u53ca\u5b63\u8282\u6027\u884c\u4e3a\u5e94\u5bf9\u7b49\u5173\u952e\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4e0e\u6307\u6570\u5e73\u6ed1\u6216ARIMA\u7b49\u6210\u719f\u7edf\u8ba1\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u5b9e\u73b0\u6240\u6709\u63d0\u8bae\u7b56\u7565\u7684\u516c\u5f00\u53ef\u7528\u8f6f\u4ef6\u3002", "conclusion": "\u57fa\u4e8e\u56de\u5f52\u6811\u53ca\u5176\u96c6\u6210\u7684\u65b9\u6cd5\u4e3a\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4e0e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u7ade\u4e89\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u8f6f\u4ef6\u4fc3\u8fdb\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.00377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00377", "abs": "https://arxiv.org/abs/2602.00377", "authors": ["Zhaochen Hong", "Jiaxuan You"], "title": "DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models", "comment": null, "summary": "Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.", "AI": {"tldr": "DecompressionLM\u662f\u4e00\u4e2a\u65e0\u72b6\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u6982\u5ff5\u56fe\u63d0\u53d6\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u67e5\u8be2\u6216\u8de8\u5e8f\u5217\u5171\u4eab\u72b6\u6001\uff0c\u901a\u8fc7\u4f4e\u5dee\u5f02\u5e8f\u5217\u548c\u7b97\u672f\u89e3\u7801\u5b9e\u73b0\u786e\u5b9a\u6027\u5e76\u884c\u751f\u6210\uff0c\u7528\u4e8e\u8bc4\u4f30\u538b\u7f29\u6a21\u578b\u7684\u77e5\u8bc6\u5e7f\u5ea6\u548c\u4e8b\u5b9e\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u63a2\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u67e5\u8be2\uff0c\u53ea\u80fd\u63d0\u53d6\u5df2\u77e5\u6982\u5ff5\uff0c\u9650\u5236\u4e86\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5185\u5bb9\u7684\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u9884\u5b9a\u4e49\u67e5\u8be2\u6216\u8de8\u5e8f\u5217\u5171\u4eab\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5e38\u89c1\u89e3\u7801\u63a2\u6d4b\u65b9\u6cd5\u7684\u4e09\u4e2a\u9650\u5236\uff1a\u8de8\u5e8f\u5217\u8026\u5408\u3001\u7ade\u4e89\u89e3\u7801\u6548\u5e94\u548c\u53ef\u6269\u5c55\u6027\u7ea6\u675f\u3002", "method": "DecompressionLM\u4f7f\u7528Van der Corput\u4f4e\u5dee\u5f02\u5e8f\u5217\u4e0e\u7b97\u672f\u89e3\u7801\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u7684\u3001\u9ad8\u5ea6\u5e76\u884c\u7684\u751f\u6210\uff0c\u65e0\u9700\u8de8\u5e8f\u5217\u5171\u4eab\u72b6\u6001\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u96f6\u6837\u672c\u6982\u5ff5\u56fe\u63d0\u53d6\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7684\u5185\u5bb9\uff0c\u800c\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u67e5\u8be2\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u4e94\u79cd\u91cf\u5316\u53d8\u4f53\u4e2d\uff0c\u53d1\u73b0\u6fc0\u6d3b\u611f\u77e5\u91cf\u5316\uff08AWQ-4bit\uff09\u5c06\u6982\u5ff5\u8986\u76d6\u7387\u63d0\u9ad8\u4e8630-170%\uff0c\u800c\u5747\u5300\u91cf\u5316\uff08GPTQ-Int4\uff09\u5bfc\u81f471-86%\u7684\u8986\u76d6\u7387\u5d29\u6e83\u3002\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u9a8c\u8bc1\u663e\u793a\uff0cMMLU-Pro Law\u6a21\u578b\u4e2d\u6392\u540d\u6700\u9ad8\u548c\u6700\u4f4e\u7684\u6a21\u578b\u4e4b\u95f4\u5b58\u572817\u70b9\u7684\u5e7b\u89c9\u5dee\u8ddd\u3002", "conclusion": "DecompressionLM\u5efa\u7acb\u4e86\u6982\u5ff5\u8986\u76d6\u7387\u4f5c\u4e3a\u8bc4\u4f30\u538b\u7f29\u6a21\u578b\u77e5\u8bc6\u5e7f\u5ea6\u548c\u4e8b\u5b9e\u57fa\u7840\u7684\u8865\u5145\u7ef4\u5ea6\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u90e8\u7f72\u3002\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u4e0d\u540c\u91cf\u5316\u7b56\u7565\u5bf9\u77e5\u8bc6\u4fdd\u7559\u7684\u663e\u8457\u4e0d\u540c\u5f71\u54cd\uff0c\u8fd9\u4e9b\u5f71\u54cd\u65e0\u6cd5\u901a\u8fc7\u89e3\u91ca\u7ea7\u56f0\u60d1\u5ea6\u53ef\u9760\u53cd\u6620\u3002"}}
{"id": "2602.00122", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00122", "abs": "https://arxiv.org/abs/2602.00122", "authors": ["Hongzhu Yi", "Yujia Yang", "Yuanxiang Wang", "Zhenyu Guan", "Jiahuan Chen", "Chenxi Bao", "Tiankun Yang", "Yixuan Yuan", "Tianyu Zong", "Xinming Wang", "Tao Yu", "Ruiwen Tao", "Haijin Liang", "Jin Ma", "Jinwen Luo", "Yeshani Xinyu Zuo", "Jungang Xu"], "title": "VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents", "comment": null, "summary": "In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \\textbf{V}isual \\textbf{D}oc \\textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.", "AI": {"tldr": "\u63d0\u51fa\u4e86VDE Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5bc6\u96c6\u6587\u672c\u89c6\u89c9\u6587\u6863\u7f16\u8f91\u6a21\u578b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u548c\u7a00\u758f\u6587\u672c\u5e03\u5c40\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u89c6\u89c9\u6587\u6863\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5bc6\u96c6\u3001\u7ed3\u6784\u590d\u6742\u7684\u6587\u6863\u548c\u975e\u62c9\u4e01\u6587\u5b57\uff08\u5982\u4e2d\u6587\uff09\u7684\u5904\u7406\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u5982AnyText\u3001GlyphControl\u548cTextCtrl\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\u573a\u666f\u548c\u7a00\u758f\u6587\u672c\u5e03\u5c40\u3002", "method": "\u63d0\u51fa\u4e86VDE Bench\u57fa\u51c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u5bc6\u96c6\u6587\u672c\u6570\u636e\u96c6\uff08\u82f1\u6587\u548c\u4e2d\u6587\uff09\uff0c\u6db5\u76d6\u5b66\u672f\u8bba\u6587\u3001\u6d77\u62a5\u3001\u6f14\u793a\u6587\u7a3f\u3001\u8003\u8bd5\u6750\u6599\u548c\u62a5\u7eb8\u7b49\u6587\u6863\u7c7b\u578b\u3002\u540c\u65f6\u5f15\u5165\u4e86\u57fa\u4e8eOCR\u89e3\u6790\u7684\u89e3\u8026\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u91cf\u5316\u6587\u672c\u4fee\u6539\u51c6\u786e\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u591a\u8bed\u8a00\u5bc6\u96c6\u6587\u672c\u89c6\u89c9\u6587\u6863\u7f16\u8f91\u6a21\u578b\u7684\u57fa\u51c6\uff0c\u4eba\u5de5\u9a8c\u8bc1\u663e\u793a\u4eba\u5de5\u5224\u65ad\u4e0e\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5177\u6709\u5f3a\u4e00\u81f4\u6027\u3002\u5bf9\u4ee3\u8868\u6027SOTA\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "VDE Bench\u586b\u8865\u4e86\u89c6\u89c9\u6587\u6863\u7f16\u8f91\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u8bed\u8a00\u5bc6\u96c6\u6587\u672c\u7f16\u8f91\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.01397", "categories": ["math.NA", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01397", "abs": "https://arxiv.org/abs/2602.01397", "authors": ["Jan S. Hesthaven", "Benjamin Peherstorfer", "Benjamin Unger"], "title": "Nonlinear model reduction for transport-dominated problems", "comment": null, "summary": "This article surveys nonlinear model reduction methods that remain effective in regimes where linear reduced-space approximations are intrinsically inefficient, such as transport-dominated problems with wave-like phenomena and moving coherent structures, which are commonly associated with the Kolmogorov barrier. The article organizes nonlinear model reduction techniques around three key elements -- nonlinear parametrizations, reduced dynamics, and online solvers -- and categorizes existing approaches into transformation-based methods, online adaptive techniques, and formulations that combine generic nonlinear parametrizations with instantaneous residual minimization.", "AI": {"tldr": "\u8be5\u6587\u7efc\u8ff0\u4e86\u5728\u8f93\u8fd0\u4e3b\u5bfc\u95ee\u9898\u7b49\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\u65e0\u6548\u7684\u9886\u57df\u4fdd\u6301\u6709\u6548\u7684\u975e\u7ebf\u6027\u6a21\u578b\u964d\u9636\u65b9\u6cd5\uff0c\u91cd\u70b9\u56f4\u7ed5\u975e\u7ebf\u6027\u53c2\u6570\u5316\u3001\u964d\u9636\u52a8\u529b\u5b66\u548c\u5728\u7ebf\u6c42\u89e3\u5668\u4e09\u4e2a\u8981\u7d20\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u53d8\u6362\u57fa\u65b9\u6cd5\u3001\u5728\u7ebf\u81ea\u9002\u5e94\u6280\u672f\u548c\u7ed3\u5408\u901a\u7528\u975e\u7ebf\u6027\u53c2\u6570\u5316\u4e0e\u77ac\u65f6\u6b8b\u5dee\u6700\u5c0f\u5316\u7684\u65b9\u6cd5\u3002", "motivation": "\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\u5728\u5904\u7406\u8f93\u8fd0\u4e3b\u5bfc\u95ee\u9898\u3001\u6ce2\u72b6\u73b0\u8c61\u548c\u79fb\u52a8\u76f8\u5e72\u7ed3\u6784\u65f6\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff08Kolmogorov\u969c\u788d\uff09\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u975e\u7ebf\u6027\u6a21\u578b\u964d\u9636\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u56f4\u7ed5\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\u7ec4\u7ec7\u975e\u7ebf\u6027\u6a21\u578b\u964d\u9636\u6280\u672f\uff1a\u975e\u7ebf\u6027\u53c2\u6570\u5316\u3001\u964d\u9636\u52a8\u529b\u5b66\u548c\u5728\u7ebf\u6c42\u89e3\u5668\u3002\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u53d8\u6362\u57fa\u65b9\u6cd5\u3001\u5728\u7ebf\u81ea\u9002\u5e94\u6280\u672f\u3001\u4ee5\u53ca\u7ed3\u5408\u901a\u7528\u975e\u7ebf\u6027\u53c2\u6570\u5316\u4e0e\u77ac\u65f6\u6b8b\u5dee\u6700\u5c0f\u5316\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u975e\u7ebf\u6027\u6a21\u578b\u964d\u9636\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5404\u7c7b\u65b9\u6cd5\u5728\u5e94\u5bf9\u8f93\u8fd0\u4e3b\u5bfc\u95ee\u9898\u7b49\u7ebf\u6027\u65b9\u6cd5\u5931\u6548\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u548c\u7279\u70b9\u3002", "conclusion": "\u975e\u7ebf\u6027\u6a21\u578b\u964d\u9636\u65b9\u6cd5\u4e3a\u89e3\u51b3\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\u65e0\u6548\u7684\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u9009\u62e9\u9002\u5408\u7279\u5b9a\u95ee\u9898\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.00415", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00415", "abs": "https://arxiv.org/abs/2602.00415", "authors": ["Zhisheng Chen", "Tingyu Wu", "Zijie Zhou", "Zhengwei Xie", "Ziyan Weng", "Yingwei Zhang"], "title": "PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents", "comment": null, "summary": "As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.", "AI": {"tldr": "PolarMem\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5c06\u6a21\u7cca\u7684\u611f\u77e5\u6982\u7387\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u903b\u8f91\u7ea6\u675f\uff0c\u901a\u8fc7\u6781\u5316\u56fe\u62d3\u6251\u7ed3\u6784\u663e\u5f0f\u5b58\u50a8\u5426\u5b9a\u4fe1\u606f\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u89c2\u5bdf\u8005\u53d1\u5c55\u4e3a\u957f\u671f\u51b3\u7b56\u8005\uff0c\u9700\u8981\u5177\u6709\u903b\u8f91\u53ef\u9a8c\u8bc1\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002\u73b0\u6709\u67b6\u6784\u5b58\u5728\u8ba4\u8bc6\u8bba\u4e0d\u5bf9\u79f0\u6027\uff1a\u6982\u7387\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bc6\u96c6\u5173\u8054\u8bb0\u5fc6\u5c06\u8bed\u4e49\u4eb2\u548c\u6027\u4e0e\u4e8b\u5b9e\u5b58\u5728\u6df7\u4e3a\u4e00\u8c08\uff0c\u4e14\u65e0\u6cd5\u7f16\u7801\u5426\u5b9a\u7ea6\u675f\u3002", "method": "\u63d0\u51faPolarMem\uff08\u6781\u5316\u6f5c\u5728\u56fe\u8bb0\u5fc6\uff09\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5206\u5e03\u5212\u5206\u5c06\u6a21\u7cca\u611f\u77e5\u4f3c\u7136\u8f6c\u6362\u4e3a\u79bb\u6563\u903b\u8f91\u7ea6\u675f\u3002\u91c7\u7528\u6781\u5316\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u4f7f\u7528\u6b63\u4ea4\u6291\u5236\u8fde\u63a5\u663e\u5f0f\u5b58\u50a8\u5df2\u9a8c\u8bc1\u7684\u5426\u5b9a\u4f5c\u4e3a\u4e3b\u8981\u8ba4\u77e5\u72b6\u6001\u3002\u5728\u63a8\u7406\u65f6\u5b9e\u65bd\u903b\u8f91\u4e3b\u5bfc\u7684\u68c0\u7d22\u8303\u5f0f\uff0c\u6291\u5236\u8fdd\u53cd\u5426\u5b9a\u7ea6\u675f\u7684\u5e7b\u89c9\u6a21\u5f0f\u3002", "result": "\u57288\u4e2a\u51bb\u7ed3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c6\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cPolarMem\u4f5c\u4e3a\u7a33\u5065\u7684\u8ba4\u77e5\u7cfb\u7edf\u8fd0\u884c\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "PolarMem\u901a\u8fc7\u5c06\u611f\u77e5\u4f3c\u7136\u8f6c\u6362\u4e3a\u903b\u8f91\u7ea6\u675f\u5e76\u663e\u5f0f\u7f16\u7801\u5426\u5b9a\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6839\u672c\u9650\u5236\uff0c\u4e3a\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u67b6\u6784\u3002"}}
{"id": "2602.00079", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00079", "abs": "https://arxiv.org/abs/2602.00079", "authors": ["Han Xiao"], "title": "Lossless Embedding Compression via Spherical Coordinates", "comment": null, "summary": "We present a lossless compression method for unit-norm embeddings that achieves 1.5$\\times$ compression, 25\\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $\u03c0/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u635f\u538b\u7f29\u5355\u4f4d\u8303\u6570\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b01.5\u500d\u538b\u7f29\u7387\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u534725%", "motivation": "\u5355\u4f4d\u8303\u6570\u5d4c\u5165\u5728\u4fe1\u606f\u68c0\u7d22\u3001\u673a\u5668\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u6548\u7387\u6709\u9650\u3002\u9ad8\u7ef4\u5355\u4f4d\u5411\u91cf\u7684\u7403\u5750\u6807\u4f1a\u96c6\u4e2d\u5728\u03c0/2\u9644\u8fd1\uff0c\u8fd9\u4e00\u7279\u6027\u672a\u88ab\u5145\u5206\u5229\u7528", "method": "\u5229\u7528\u9ad8\u7ef4\u5355\u4f4d\u5411\u91cf\u7403\u5750\u6807\u96c6\u4e2d\u5728\u03c0/2\u9644\u8fd1\u7684\u7279\u6027\uff0c\u5bfc\u81f4IEEE 754\u6307\u6570\u4f4d\u574d\u7f29\u4e3a\u5355\u4e00\u503c\uff0c\u4ece\u800c\u542f\u7528\u71b5\u7f16\u7801\u3002\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5728float32\u7cbe\u5ea6\u5185\u5b8c\u5168\u65e0\u635f", "result": "\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u591a\u5411\u91cf\u5d4c\u5165\u768426\u79cd\u914d\u7f6e\u8bc4\u4f30\u4e2d\uff0c\u5747\u5b9e\u73b01.5\u500d\u538b\u7f29\u7387\uff0c\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534725%\uff0c\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u635f\u538b\u7f29\u5355\u4f4d\u8303\u6570\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u9ad8\u7ef4\u51e0\u4f55\u7279\u6027\u5b9e\u73b0\u663e\u8457\u538b\u7f29\u589e\u76ca\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u4fdd\u6301\u5b8c\u5168\u7cbe\u5ea6"}}
{"id": "2602.00380", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00380", "abs": "https://arxiv.org/abs/2602.00380", "authors": ["Sercan Karaka\u015f"], "title": "Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models", "comment": "15 pages", "summary": "This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u6355\u6349\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u7684\u7ea6\u675f\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u4e2a\u6a21\u578b\u5728\u5c40\u90e8\u7ea6\u675f\u504f\u597d\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02", "motivation": "\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u6355\u6349\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\uff08kendi\u548ckendisi\uff09\u7684\u7ea6\u675f\u5173\u7cfb\uff0c\u4e86\u89e3\u6a21\u578b\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b", "method": "\u6784\u5efa100\u4e2a\u5e73\u8861\u53e5\u5b50\u96c6\uff0c\u5bf9\u6bd4\u5c40\u90e8\u4e0e\u975e\u5c40\u90e8\u5148\u884c\u8bcd\uff1b\u6d4b\u8bd5OpenAI\u94fe\u5f0f\u601d\u7ef4\u6a21\u578b\u548cTrendyol-LLM-7B-base-v0.1\u6a21\u578b\uff1b\u4f7f\u7528\u53e5\u5b50\u7ea7\u56f0\u60d1\u5ea6\u548c\u5f3a\u5236\u9009\u62e9\u8303\u5f0f\u8bc4\u4f30\u5148\u884c\u8bcd\u9009\u62e9", "result": "Trendyol-LLM\u5728\u7ea670%\u7684\u8bd5\u9a8c\u4e2d\u504f\u597d\u5c40\u90e8\u7ea6\u675f\uff0c\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u5c40\u90e8\u6027\u504f\u5dee\uff1b\u800co1 Mini\u5728\u5c40\u90e8\u548c\u957f\u8ddd\u79bb\u89e3\u8bfb\u4e4b\u95f4\u51e0\u4e4e\u5747\u5300\u5206\u5e03\u9009\u62e9\uff0c\u4e24\u4e2a\u7cfb\u7edf\u5728\u7ea6\u675f\u884c\u4e3a\u4e0a\u5b58\u5728\u663e\u8457\u5bf9\u6bd4", "conclusion": "\u4e0d\u540c\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u571f\u8033\u5176\u8bed\u53cd\u8eab\u4ee3\u8bcd\u7ea6\u675f\u5173\u7cfb\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u6a21\u578b\u8bbe\u8ba1\u5bf9\u8bed\u8a00\u7ed3\u6784\u7406\u89e3\u6709\u91cd\u8981\u5f71\u54cd"}}
{"id": "2602.00124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00124", "abs": "https://arxiv.org/abs/2602.00124", "authors": ["Divya Acharya", "Pierre Bernab'e", "Antoine Chevrot", "Helge Spieker", "Arnaud Gotlieb", "Bruno Legeard"], "title": "Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance", "comment": null, "summary": "The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u96c6\u6210\u4e0a\u4e0b\u6587\u7279\u5b9a\u9608\u503c\u6765\u6539\u8fdb\u6d77\u4e0a\u8239\u8236\u4ea4\u901a\u76d1\u63a7\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5728\u96c6\u4f53\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u3002", "motivation": "\u4f20\u7edf\u81ea\u7f16\u7801\u5668\u5728\u6d77\u4e0a\u8239\u8236\u76d1\u63a7\u4e2d\u68c0\u6d4b\u96c6\u4f53\u548c\u4e0a\u4e0b\u6587\u5f02\u5e38\u7684\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u6d77\u4e0a\u5f02\u5e38\u901a\u5e38\u4f9d\u8d56\u4e8e\u4eceAIS\u6d88\u606f\u4e2d\u63d0\u53d6\u7684\u8239\u8236\u7279\u5b9a\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\uff0c\u96c6\u6210\u4e0a\u4e0b\u6587\u7279\u5b9a\u9608\u503c\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\u53d8\u4f53\u548c\u4f20\u7edf\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u6d77\u4e0a\u76d1\u63a7\u4e2d\u7684\u6355\u9c7c\u72b6\u6001\u5f02\u5e38\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u4e0a\u4e0b\u6587\u5bf9\u91cd\u6784\u635f\u5931\u548c\u5f02\u5e38\u68c0\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u7f16\u7801\u5668\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u7eb3\u5165\u4e0a\u4e0b\u6587\u7279\u5b9a\u9608\u503c\u5e76\u8ba4\u8bc6\u5230\u4e0a\u4e0b\u6587\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u8be5\u65b9\u6cd5\u4e3a\u63d0\u9ad8\u6d77\u4e0a\u8239\u8236\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01449", "categories": ["math.NA", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01449", "abs": "https://arxiv.org/abs/2602.01449", "authors": ["Lorenzo Baldassari", "Josselin Garnier", "Knut Solna", "Maarten V. de Hoop"], "title": "Dimension-Free Multimodal Sampling via Preconditioned Annealed Langevin Dynamics", "comment": null, "summary": "Designing algorithms that can explore multimodal target distributions accurately across successive refinements of an underlying high-dimensional problem is a central challenge in sampling. Annealed Langevin dynamics (ALD) is a widely used alternative to classical Langevin since it often yields much faster mixing on multimodal targets, but there is still a gap between this empirical success and existing theory: when, and under which design choices, can ALD be guaranteed to remain stable as dimension increases? In this paper, we help bridge this gap by providing a uniform-in-dimension analysis of continuous-time ALD for multimodal targets that can be well-approximated by Gaussian mixture models. Along an explicit annealing path obtained by progressively removing Gaussian smoothing of the target, we identify sufficient spectral conditions - linking smoothing covariance and the covariances of the Gaussian components of the mixture - under which ALD achieves a prescribed accuracy within a single, dimension-uniform time horizon. We then establish dimension-robustness to imperfect initialization and score approximation: under a misspecified-mixture score model, we derive explicit conditions showing that preconditioning the ALD algorithm with a sufficiently decaying spectrum is necessary to prevent error terms from accumulating across coordinates and destroying dimension-uniform control. Finally, numerical experiments illustrate and validate the theory.", "AI": {"tldr": "\u672c\u6587\u4e3a\u9000\u706b\u6717\u4e4b\u4e07\u52a8\u529b\u5b66(ALD)\u63d0\u4f9b\u4e86\u9996\u4e2a\u7ef4\u5ea6\u5747\u5300\u5206\u6790\uff0c\u9488\u5bf9\u53ef\u8fd1\u4f3c\u4e3a\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u591a\u6a21\u6001\u76ee\u6807\u5206\u5e03\uff0c\u5efa\u7acb\u4e86\u7ef4\u5ea6\u9c81\u68d2\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "motivation": "\u9000\u706b\u6717\u4e4b\u4e07\u52a8\u529b\u5b66(ALD)\u5728\u591a\u6a21\u6001\u76ee\u6807\u5206\u5e03\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u89e3\u91ca\u5176\u7ef4\u5ea6\u7a33\u5b9a\u6027\u3002\u9700\u8981\u586b\u8865\u7ecf\u9a8c\u6210\u529f\u4e0e\u7406\u8bba\u5206\u6790\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u660e\u786e\u5728\u4f55\u79cd\u6761\u4ef6\u4e0bALD\u80fd\u4fdd\u6301\u7ef4\u5ea6\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u65f6\u95f4ALD\u5206\u6790\uff0c\u9488\u5bf9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fd1\u4f3c\u76ee\u6807\uff0c\u6cbf\u663e\u5f0f\u9000\u706b\u8def\u5f84\uff08\u9010\u6b65\u79fb\u9664\u9ad8\u65af\u5e73\u6ed1\uff09\uff0c\u5efa\u7acb\u8fde\u63a5\u5e73\u6ed1\u534f\u65b9\u5dee\u4e0e\u6df7\u5408\u9ad8\u65af\u5206\u91cf\u534f\u65b9\u5dee\u7684\u8c31\u6761\u4ef6\u3002", "result": "\u5728\u5145\u5206\u8c31\u6761\u4ef6\u4e0b\uff0cALD\u80fd\u5728\u5355\u4e00\u7ef4\u5ea6\u5747\u5300\u65f6\u95f4\u8303\u56f4\u5185\u8fbe\u5230\u6307\u5b9a\u7cbe\u5ea6\uff1b\u8bc1\u660e\u4e86\u7b97\u6cd5\u5bf9\u4e0d\u5b8c\u7f8e\u521d\u59cb\u5316\u548c\u5206\u6570\u8fd1\u4f3c\u7684\u7ef4\u5ea6\u9c81\u68d2\u6027\uff0c\u5e76\u6307\u51fa\u9884\u6761\u4ef6\u5904\u7406\u5bf9\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3aALD\u5728\u591a\u6a21\u6001\u76ee\u6807\u4e0a\u7684\u7ef4\u5ea6\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u660e\u786e\u4e86\u8c31\u6761\u4ef6\u5728\u5b9e\u73b0\u7ef4\u5ea6\u9c81\u68d2\u6027\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2602.00449", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00449", "abs": "https://arxiv.org/abs/2602.00449", "authors": ["Jia Liang", "Liangming Pan"], "title": "Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks", "comment": "20 pages, 14 figures", "summary": "Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790CODI\u6a21\u578b\u5728\u591a\u9879\u5f0f\u8fed\u4ee3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u601d\u7ef4\u94fe(Latent-CoT)\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u957f\u5ea6\u4e0b\u4f1a\u5f62\u6210\u4e0d\u540c\u7684\u8ba1\u7b97\u7b56\u7565\u3002", "motivation": "\u6f5c\u5728\u601d\u7ef4\u94fe(Latent-CoT)\u65e8\u5728\u5b9e\u73b0\u9010\u6b65\u8ba1\u7b97\u800c\u4e0d\u4ea7\u751f\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790CODI\u6a21\u578b\u6765\u7406\u89e3\u6f5c\u5728\u601d\u7ef4\u94fe\u5982\u4f55\u8868\u793a\u548c\u8def\u7531\u4e2d\u95f4\u72b6\u6001\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u601d\u7ef4\u7684\u5e08\u751f\u84b8\u998f\u6a21\u578bCODI\uff0c\u5728\u4e25\u683c\u987a\u5e8f\u7684\u591a\u9879\u5f0f\u8fed\u4ee3\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u91c7\u7528logit-lens\u89e3\u7801\u3001\u7ebf\u6027\u63a2\u9488\u3001\u6ce8\u610f\u529b\u5206\u6790\u548c\u6fc0\u6d3b\u4fee\u8865\u7b49\u6280\u672f\uff0c\u5b9a\u4f4d\u4e2d\u95f4\u72b6\u6001\u8868\u793a\u5e76\u8ffd\u8e2a\u5176\u5230\u6700\u7ec8\u8f93\u51fa\u7684\u8def\u7531\u8def\u5f84\u3002", "result": "\u57282-3\u6b65\u4efb\u52a1\u4e2d\uff0cCODI\u5f62\u6210\u5b8c\u6574\u7684\u6865\u63a5\u72b6\u6001\u96c6\uff0c\u8fd9\u4e9b\u72b6\u6001\u5728\u6f5c\u5728\u601d\u7ef4\u4f4d\u7f6e\u53ef\u89e3\u7801\uff0c\u800c\u6700\u7ec8\u8f93\u5165\u901a\u8fc7\u8fd1\u4e4e\u76f4\u63a5\u7684\u8def\u5f84\uff1b\u9884\u6d4b\u901a\u8fc7\u601d\u7ef4\u8fb9\u754c\u5904\u7684\u540e\u671f\u878d\u5408\u4ea7\u751f\u3002\u5bf9\u4e8e\u66f4\u957f\u4efb\u52a1\uff0cCODI\u65e0\u6cd5\u53ef\u9760\u6267\u884c\u5b8c\u6574\u7684\u6f5c\u5728\u5c55\u5f00\uff0c\u800c\u662f\u5f62\u6210\u90e8\u5206\u6f5c\u5728\u63a8\u7406\u8def\u5f84\uff0c\u96c6\u4e2d\u4e8e\u540e\u671f\u4e2d\u95f4\u72b6\u6001\u5e76\u4e0e\u6700\u540e\u8f93\u5165\u5728\u7b54\u6848\u8bfb\u53d6\u4f4d\u7f6e\u878d\u5408\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86CODI\u5f0f\u6f5c\u5728\u601d\u7ef4\u94fe\u4f55\u65f6\u4ea7\u751f\u5fe0\u5b9e\u8fed\u4ee3\u8ba1\u7b97\u4e0e\u538b\u7f29/\u6377\u5f84\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u4e86\u4e3a\u987a\u5e8f\u63a8\u7406\u8bbe\u8ba1\u9c81\u68d2\u6f5c\u5728\u601d\u7ef4\u94fe\u76ee\u6807\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u90e8\u5206\u8def\u5f84\u5728\u4f18\u5316\u56f0\u96be\u7b49\u673a\u5236\u53d8\u5316\u4e0b\u53ef\u80fd\u5d29\u6e83\u3002"}}
{"id": "2602.00084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00084", "abs": "https://arxiv.org/abs/2602.00084", "authors": ["Brady Steele"], "title": "Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning", "comment": "14 pages, 7 figures, 7 tables", "summary": "Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.", "AI": {"tldr": "LoRA\u5177\u6709\u5185\u5728\u7684\u6297\u6807\u7b7e\u566a\u58f0\u80fd\u529b\uff0c\u672c\u6587\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u8fd9\u4e00\u7279\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86RACT\u65b9\u6cd5\u7528\u4e8e\u566a\u58f0\u68c0\u6d4b\u3002", "motivation": "\u5c3d\u7ba1LoRA\u5df2\u6210\u4e3a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u5176\u5185\u5728\u7684\u6297\u6807\u7b7e\u566a\u58f0\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u89e3\u91caLoRA\u4e3a\u4f55\u80fd\u62b5\u6297\u6807\u7b7e\u566a\u58f0\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u7279\u6027\u5f00\u53d1\u5b9e\u7528\u7684\u566a\u58f0\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u7406\u8bba\u5206\u6790\uff1a\u8bc1\u660erank-r LoRA\u65e0\u6cd5\u8bb0\u5fc6\u6240\u6709\u53ef\u80fd\u7684\u6807\u7b7e\u5206\u914d\uff0c\u9650\u5236\u4e86\u5176\u62df\u5408\u4efb\u610f\u566a\u58f0\u7684\u80fd\u529b\uff1b2. \u63a8\u5bfc\u6700\u4f18\u79e9\u4ee5\u5e73\u8861\u8fd1\u4f3c\u504f\u5dee\u548c\u566a\u58f0\u5f15\u8d77\u7684\u65b9\u5dee\uff1b3. \u63d0\u51faRACT\uff08Rank-Aware Curriculum Training\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u79e9\u5dee\u5f02\u8fdb\u884c\u566a\u58f0\u68c0\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86LoRA\u7684\u4e09\u4e2a\u5173\u952e\u7279\u6027\uff1a\u5bb9\u91cf\u9650\u5236\u3001\u6700\u4f18\u79e9\u9009\u62e9\u548c\u65f6\u95f4\u5206\u79bb\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff0cRACT\u5728AG News\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8691.1%\u7684\u566a\u58f0\u68c0\u6d4bF1\u5206\u6570\uff0c\u540c\u65f6\u4fdd\u630191.46%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u7f3a\u4e4f\u566a\u58f0\u68c0\u6d4b\u80fd\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "LoRA\u7684\u5185\u5728\u7ed3\u6784\u7279\u6027\u4f7f\u5176\u5177\u6709\u6297\u6807\u7b7e\u566a\u58f0\u80fd\u529b\uff0c\u8fd9\u4e00\u7279\u6027\u53ef\u4ee5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5f97\u5230\u89e3\u91ca\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u7684RACT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6807\u7b7e\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u566a\u58f0\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00425", "abs": "https://arxiv.org/abs/2602.00425", "authors": ["Siyuan Wang", "Yanchen Liu", "Xiang Ren"], "title": "Segment-Level Attribution for Selective Learning of Long Reasoning Traces", "comment": "Accepted to ICLR 2026. 16 pages, 5 figures. Code available at https://github.com/SiyuanWangw/SegmentSelectiveSFT", "summary": "Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \\textit{attribution strength} measures the overall attribution magnitude; and (2) \\textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u6210\u68af\u5ea6\u5f52\u56e0\u7684\u6bb5\u7ea7\u9009\u62e9\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u9ad8\u5f52\u56e0\u5f3a\u5ea6\u4f46\u4e2d\u7b49\u4e00\u81f4\u6027\u7684\u91cd\u8981\u63a8\u7406\u6bb5\uff0c\u5728SFT\u4e2d\u53ea\u5b66\u4e60\u8fd9\u4e9b\u5173\u952e\u6bb5\uff0c\u63d0\u9ad8\u5927\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u8f93\u51fa\u6548\u7387\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u957f\u601d\u7ef4\u94fe\u4e2d\uff0c\u53ea\u6709\u5c0f\u90e8\u5206\u5bf9\u7b54\u6848\u9884\u6d4b\u6709\u5b9e\u8d28\u8d21\u732e\uff0c\u5927\u90e8\u5206\u5305\u542b\u91cd\u590d\u6216\u622a\u65ad\u5185\u5bb9\u3002\u8fd9\u79cd\u8f93\u51fa\u5197\u4f59\u5728\u76d1\u7763\u5fae\u8c03\u540e\u4f1a\u8fdb\u4e00\u6b65\u4f20\u64ad\uff0c\u6a21\u578b\u5b66\u4f1a\u6a21\u4eff\u5197\u957f\u4f46\u65e0\u4fe1\u606f\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u68af\u5ea6\u5f52\u56e0\u91cf\u5316\u6bcf\u4e2atoken\u5bf9\u6700\u7ec8\u7b54\u6848\u7684\u5f71\u54cd\uff0c\u805a\u5408\u4e3a\u4e24\u4e2a\u6bb5\u7ea7\u6307\u6807\uff1a\u5f52\u56e0\u5f3a\u5ea6\uff08\u6574\u4f53\u5f52\u56e0\u5927\u5c0f\uff09\u548c\u65b9\u5411\u4e00\u81f4\u6027\uff08\u6bb5\u5185token\u5f52\u56e0\u65b9\u5411\u662f\u5426\u4e00\u81f4\uff09\u3002\u57fa\u4e8e\u8fd9\u4e24\u4e2a\u6307\u6807\uff0c\u63d0\u51fa\u6bb5\u7ea7\u9009\u62e9\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u8bc6\u522b\u9ad8\u5f52\u56e0\u5f3a\u5ea6\u4f46\u4e2d\u7b49\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6bb5\uff08\u8868\u660e\u662f\u53cd\u601d\u6027\u800c\u975e\u6d45\u5c42\u63a8\u7406\uff09\uff0c\u5bf9\u8fd9\u4e9b\u91cd\u8981\u6bb5\u8fdb\u884c\u9009\u62e9\u6027SFT\uff0c\u540c\u65f6\u5bf9\u4e0d\u91cd\u8981\u6bb5\u8fdb\u884c\u635f\u5931\u63a9\u7801\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8f93\u51fa\u6548\u7387\uff0c\u80fd\u591f\u4ece\u957f\u63a8\u7406\u8f68\u8ff9\u4e2d\u66f4\u6709\u6548\u5730\u5b66\u4e60\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u5b66\u4e60\u91cd\u8981\u63a8\u7406\u6bb5\u800c\u975e\u6574\u4e2a\u601d\u7ef4\u94fe\uff0c\u53ef\u4ee5\u51cf\u8f7b\u8f93\u51fa\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u5927\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2602.00126", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00126", "abs": "https://arxiv.org/abs/2602.00126", "authors": ["Dmytro Filatov", "Valentyn Fedorov", "Vira Filatova", "Andrii Zelenchuk"], "title": "D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection", "comment": "9 pages", "summary": "Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.", "AI": {"tldr": "D3R-Net\uff1a\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u53cc\u57df\u53bb\u566a\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u57df\u548c\u9891\u57df\u635f\u5931\u6765\u63d0\u5347\u5bf9\u7ec6\u5fae\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5728MVTec AD\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u9891\u7ec6\u8282\u65f6\u4f1a\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u7684\u7ed3\u679c\uff0c\u5bfc\u81f4\u7ec6\u5fae\u7f3a\u9677\u88ab\u90e8\u5206\u91cd\u5efa\u800c\u975e\u7a81\u51fa\u663e\u793a\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faD3R-Net\u53cc\u57df\u53bb\u566a\u91cd\u5efa\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u81ea\u76d1\u7763\"\u4fee\u590d\"\u4efb\u52a1\uff0c\u8f93\u5165\u5408\u6210\u635f\u574f\u7684\u6b63\u5e38\u56fe\u50cf\u5e76\u91cd\u5efa\u5e72\u51c0\u76ee\u6807\uff0c\u9632\u6b62\u5e73\u51e1\u6052\u7b49\u6620\u5c04\uff1b2\uff09\u9664\u4e86\u7a7a\u95f4\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u8fd8\u5f15\u5165\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u5e45\u5ea6\u635f\u5931\u6765\u4fdd\u6301\u9891\u57df\u4e00\u81f4\u6027\uff1b3\uff09\u53ef\u9009\u4f7f\u7528\u7ed3\u6784\u76f8\u4f3c\u6027\u635f\u5931\u3002", "result": "\u5728MVTec AD Hazelnut\u57fa\u51c6\u4e0a\uff0cFFT\u635f\u5931\u5c06PRO AUC\u4ece0.603\u63d0\u5347\u52300.687\uff1b\u572815\u4e2aMVTec\u7c7b\u522b\u4e0a\uff0c\u5e73\u5747\u50cf\u7d20ROC AUC\u4ece0.733\u63d0\u5347\u52300.751\uff0cPRO AUC\u4ece0.417\u63d0\u5347\u52300.468\uff0c\u5355GPU\u4e0a\u7ea620FPS\u3002", "conclusion": "D3R-Net\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u548c\u9891\u57df\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5bf9\u7ec6\u5fae\u7f3a\u9677\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u4e3a\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.01467", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01467", "abs": "https://arxiv.org/abs/2602.01467", "authors": ["Junping Wang"], "title": "Beyond Taylor: Divergence-Based Functional Expansions and Their Application to Numerical Integration", "comment": "20 pages, 1 figure, original", "summary": "This paper introduces a new functional expansion framework that extends classical ideas beyond the Taylor series. Unlike traditional Taylor expansions based on local polynomial approximations, the proposed approach arises from exact differential identities that link a function and its derivatives through polynomial weight factors. This formulation expresses smooth functions via divergence-based relations connecting derivatives of all orders with systematically scaled polynomial coefficients. This framework provides a natural foundation for constructing high-order numerical quadrature formulas, particularly for multi-dimensional domains. By exploiting the divergence structure, volume integrals are systematically transformed into boundary integrals using the Divergence Theorem, recursively reducing the integration domain from an $n$-dimensional body to its $(n-1)$-dimensional facets, and ultimately to its vertices.\n  The article further enhances the framework's accuracy by introducing a complex-shift technique. It is demonstrated that by positioning the expansion center at specific roots of unity in the complex plane, lower-order error terms are annihilated, yielding high-order real-valued quadrature rules with minimal function evaluations. Additionally, a rigorous geometric analysis of the affine transformations required for surface integration is provided, deriving explicit formulas for the transformation of normal vectors and surface measures. The proposed method offers a robust, systematic, and computationally efficient alternative to tessellation-based quadrature for arbitrary flat-faced polytopes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7cbe\u786e\u5fae\u5206\u6052\u7b49\u5f0f\u7684\u51fd\u6570\u5c55\u5f00\u6846\u67b6\uff0c\u5c06\u4f53\u79ef\u5206\u8f6c\u5316\u4e3a\u8fb9\u754c\u79ef\u5206\uff0c\u7ed3\u5408\u590d\u6570\u504f\u79fb\u6280\u672f\u6784\u9020\u9ad8\u6548\u9ad8\u7cbe\u5ea6\u591a\u7ef4\u591a\u9762\u4f53\u6570\u503c\u79ef\u5206\u516c\u5f0f", "motivation": "\u4f20\u7edf\u6cf0\u52d2\u7ea7\u6570\u57fa\u4e8e\u5c40\u90e8\u591a\u9879\u5f0f\u903c\u8fd1\uff0c\u672c\u6587\u65e8\u5728\u53d1\u5c55\u8d85\u8d8a\u6cf0\u52d2\u7ea7\u6570\u7684\u51fd\u6570\u5c55\u5f00\u6846\u67b6\uff0c\u4e3a\u591a\u7ef4\u57df\u4e0a\u7684\u9ad8\u9636\u6570\u503c\u79ef\u5206\u63d0\u4f9b\u7cfb\u7edf\u5316\u57fa\u7840", "method": "1) \u57fa\u4e8e\u7cbe\u786e\u5fae\u5206\u6052\u7b49\u5f0f\u5efa\u7acb\u51fd\u6570\u5c55\u5f00\u6846\u67b6\uff0c\u5c06\u51fd\u6570\u4e0e\u5176\u5bfc\u6570\u901a\u8fc7\u591a\u9879\u5f0f\u6743\u91cd\u56e0\u5b50\u8fde\u63a5\uff1b2) \u5229\u7528\u6563\u5ea6\u5b9a\u7406\u5c06\u4f53\u79ef\u5206\u9012\u5f52\u8f6c\u5316\u4e3a\u8fb9\u754c\u79ef\u5206\uff1b3) \u5f15\u5165\u590d\u6570\u504f\u79fb\u6280\u672f\u6d88\u9664\u4f4e\u9636\u8bef\u5dee\u9879\uff1b4) \u63d0\u4f9b\u66f2\u9762\u79ef\u5206\u6240\u9700\u4eff\u5c04\u53d8\u6362\u7684\u4e25\u683c\u51e0\u4f55\u5206\u6790", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u9ad8\u7cbe\u5ea6\u6570\u503c\u79ef\u5206\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5e73\u76f4\u591a\u9762\u4f53\uff0c\u76f8\u6bd4\u57fa\u4e8e\u7f51\u683c\u5256\u5206\u7684\u79ef\u5206\u65b9\u6cd5\u66f4\u9c81\u68d2\u3001\u7cfb\u7edf\u4e14\u8ba1\u7b97\u9ad8\u6548", "conclusion": "\u63d0\u51fa\u7684\u51fd\u6570\u5c55\u5f00\u6846\u67b6\u4e3a\u591a\u7ef4\u57df\u4e0a\u7684\u9ad8\u9636\u6570\u503c\u79ef\u5206\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u9762\u4f53\u57df\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f"}}
{"id": "2602.00454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00454", "abs": "https://arxiv.org/abs/2602.00454", "authors": ["Jing Wu", "Yue Sun", "Tianpei Xie", "Suiyao Chen", "Jingyuan Bao", "Yaopengxiao Xu", "Gaoyuan Du", "Inseok Heo", "Alexander Gutfraind", "Xin Wang"], "title": "Cross-Modal Memory Compression for Efficient Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.", "AI": {"tldr": "DebateOCR\uff1a\u4e00\u4e2a\u8de8\u6a21\u6001\u538b\u7f29\u6846\u67b6\uff0c\u7528\u7d27\u51d1\u7684\u56fe\u50cf\u8868\u793a\u66ff\u4ee3\u5197\u957f\u7684\u6587\u672c\u8fa9\u8bba\u5386\u53f2\uff0c\u51cf\u5c1192%\u7684\u8f93\u5165token\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u867d\u7136\u80fd\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u4f46\u968f\u7740\u8fa9\u8bba\u8f6e\u6b21\u548c\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\uff0c\u4e0a\u4e0b\u6587\u4f1a\u8fc5\u901f\u81a8\u80c0\u3002\u4fdd\u7559\u5b8c\u6574\u7684\u6587\u672c\u5386\u53f2\u4f1a\u5bfc\u81f4token\u4f7f\u7528\u91cf\u8d85\u8fc7\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5e76\u4e14\u9700\u8981\u91cd\u590d\u7684\u6458\u8981\u5904\u7406\uff0c\u589e\u52a0\u4e86\u5f00\u9500\u5e76\u52a0\u5267\u4fe1\u606f\u635f\u5931\u3002", "method": "\u5f15\u5165DebateOCR\u8de8\u6a21\u6001\u538b\u7f29\u6846\u67b6\uff0c\u5c06\u5197\u957f\u7684\u6587\u672c\u8fa9\u8bba\u8f68\u8ff9\u66ff\u6362\u4e3a\u7d27\u51d1\u7684\u56fe\u50cf\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u95e8\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5904\u7406\u8fd9\u4e9b\u56fe\u50cf\u8868\u793a\uff0c\u4ee5\u8c03\u8282\u540e\u7eed\u8fa9\u8bba\u8f6e\u6b21\u3002", "result": "\u8be5\u8bbe\u8ba1\u538b\u7f29\u4e86\u901a\u5e38\u8de8\u8d8a\u6570\u4e07\u5230\u6570\u5341\u4e07token\u7684\u5386\u53f2\u8bb0\u5f55\uff0c\u8f93\u5165token\u51cf\u5c11\u4e8692%\u4ee5\u4e0a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u591a\u6837\u6027\u652f\u6301\u6062\u590d\u88ab\u7701\u7565\u7684\u4fe1\u606f\uff1a\u867d\u7136\u4efb\u4f55\u5355\u4e2a\u538b\u7f29\u5386\u53f2\u90fd\u53ef\u80fd\u4e22\u5f03\u7ec6\u8282\uff0c\u4f46\u805a\u5408\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u538b\u7f29\u89c6\u56fe\u53ef\u4ee5\u4f7f\u96c6\u4f53\u8868\u793a\u4ee5\u6307\u6570\u7ea7\u9ad8\u6982\u7387\u63a5\u8fd1\u4fe1\u606f\u74f6\u9888\u3002"}}
{"id": "2602.00085", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00085", "abs": "https://arxiv.org/abs/2602.00085", "authors": ["Shuozhe Li", "Jincheng Cao", "Bodun Hu", "Aryan Mokhtari", "Leqi Liu", "Amy Zhang"], "title": "CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models", "comment": null, "summary": "Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.", "AI": {"tldr": "CARE-RFT\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528\u504f\u659c\u53cd\u5411KL\u6563\u5ea6\u66ff\u4ee3\u6807\u51c6\u53cd\u5411KL\u6b63\u5219\u5316\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u53ef\u4fe1\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5fae\u8c03\u5b58\u5728\u6743\u8861\uff1a\u65e0\u7ea6\u675fRFT\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u635f\u5bb3\u6a21\u578b\u53ef\u4fe1\u5ea6\uff08\u589e\u52a0\u5e7b\u89c9\u3001\u6821\u51c6\u53d8\u5dee\uff09\uff0c\u800cRKL\u7ea6\u675fRFT\u4fdd\u6301\u53ef\u4fe1\u5ea6\u4f46\u9650\u5236\u63a8\u7406\u63d0\u5347\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u63d0\u51faCARE-RFT\u65b9\u6cd5\uff0c\u7528\u504f\u659c\u53cd\u5411KL\u6563\u5ea6\u66ff\u4ee3\u6807\u51c6\u53cd\u5411KL\u6b63\u5219\u5316\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u654f\u611f\u7684\u60e9\u7f5a\uff1a\u5bf9\u81ea\u4fe1\u4e14\u6301\u7eed\u83b7\u5f97\u5956\u52b1\u7684\u63a2\u7d22\u6709\u754c\uff08\u652f\u6301\u63a8\u7406\uff09\uff0c\u5bf9\u5176\u4ed6\u60c5\u51b5\u65e0\u754c\uff08\u4fdd\u6301\u6821\u51c6\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u548cRFT\u7b97\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCARE-RFT\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff1a\u5339\u914d\u65e0\u7ea6\u675fRFT\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u6062\u590d\u57fa\u7840\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u548c\u6821\u51c6\u3002", "conclusion": "\u4ed4\u7ec6\u7684\u3001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u6b63\u5219\u5316\u662f\u6784\u5efa\u65e2\u5177\u5907\u80fd\u529b\u53c8\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u6a21\u578b\u7684\u5173\u952e\u3002"}}
{"id": "2602.00428", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00428", "abs": "https://arxiv.org/abs/2602.00428", "authors": ["Naen Xu", "Hengyu An", "Shuo Shi", "Jinghuai Zhang", "Chunyi Zhou", "Changjiang Li", "Tianyu Du", "Zhihui Fu", "Jun Wang", "Shouling Ji"], "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems", "comment": "ICLR 2026", "summary": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\uff08\u96c6\u4f53\u8bb0\u5fc6\u504f\u5dee\uff09\uff0c\u63d0\u51fa\u4e86MANBENCH\u57fa\u51c6\u6765\u8bc4\u4f30\u8be5\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4f46\u667a\u80fd\u4f53\u5bf9\u96c6\u4f53\u8ba4\u77e5\u504f\u5dee\u7684\u6613\u611f\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u66fc\u5fb7\u62c9\u6548\u5e94\u4f5c\u4e3a\u96c6\u4f53\u9519\u8bef\u8bb0\u5fc6\u7684\u73b0\u8c61\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5f15\u53d1\u4f26\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u4e86MANBENCH\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cd\u6613\u53d7\u66fc\u5fb7\u62c9\u6548\u5e94\u5f71\u54cd\u7684\u4efb\u52a1\u7c7b\u578b\u548c\u4e94\u79cd\u4e0d\u540c\u667a\u80fd\u4f53\u89d2\u8272\u4e0e\u8bb0\u5fc6\u65f6\u95f4\u5c3a\u5ea6\u7684\u4ea4\u4e92\u534f\u8bae\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7f13\u89e3\u7b56\u7565\uff1a\u63d0\u793a\u7ea7\u9632\u5fa1\uff08\u8ba4\u77e5\u951a\u5b9a\u548c\u6765\u6e90\u5ba1\u67e5\uff09\u548c\u6a21\u578b\u7ea7\u5bf9\u9f50\u9632\u5fa1\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u91cf\u5316\u4e86\u66fc\u5fb7\u62c9\u6548\u5e94\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u56e0\u7d20\u5bf9\u5176\u5f71\u54cd\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u51cf\u5c11\u4e8674.40%\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u96c6\u4f53\u8bb0\u5fc6\u504f\u5dee\u7684\u5b58\u5728\u548c\u5f71\u54cd\u56e0\u7d20\uff0c\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u4e3a\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u548c\u4f26\u7406\u5bf9\u9f50\u7684\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002"}}
{"id": "2602.00131", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00131", "abs": "https://arxiv.org/abs/2602.00131", "authors": ["Fraser Robinson", "Souren Pashangpour", "Matthew Lisondra", "Goldie Nejat"], "title": "PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living", "comment": "Submitted to Advanced Robotics (Taylor & Francis)", "summary": "A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.", "AI": {"tldr": "POVNet+\uff1a\u9996\u4e2a\u7528\u4e8e\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7ADL\u548c\u8fd0\u52a8\u5d4c\u5165\u7a7a\u95f4\u8bc6\u522b\u5df2\u77e5/\u672a\u77e5ADL\u53ca\u975e\u5178\u578b\u6267\u884c\uff0c\u4e3b\u52a8\u53d1\u8d77\u8f85\u52a9\u4ea4\u4e92", "motivation": "\u5f53\u524d\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u65e0\u6cd5\u540c\u65f6\u611f\u77e5\u548c\u8f85\u52a9\u591a\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\uff08ADL\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u957f\u671f\u90e8\u7f72\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u591a\u79cdADL\u5e76\u4e3b\u52a8\u53d1\u8d77\u8f85\u52a9\u4ea4\u4e92\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faPOVNet+\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5f15\u5165ADL\u5d4c\u5165\u7a7a\u95f4\u548c\u8fd0\u52a8\u5d4c\u5165\u7a7a\u95f4\u6765\u533a\u5206\u5df2\u77e5ADL\u3001\u672a\u77e5ADL\u548c\u975e\u5178\u578b\u6267\u884cADL\u3002\u91c7\u7528\u65b0\u9896\u7684\u7528\u6237\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u8fd0\u52a8\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8bc6\u522b\u65b0ADL\u5e76\u76d1\u6d4b\u7528\u6237\u8868\u73b0\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u76f8\u6bd4\uff0cPOVNet+\u5177\u6709\u66f4\u9ad8\u7684ADL\u5206\u7c7b\u51c6\u786e\u7387\u3002\u5728\u6742\u4e71\u751f\u6d3b\u73af\u5883\u4e2d\u4e0e\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4ebaLeia\u8fdb\u884c\u7684\u4eba\u673a\u4ea4\u4e92\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u6210\u529f\u8bc6\u522b\u5df2\u77e5/\u672a\u77e5ADL\u53ca\u975e\u5178\u578bADL\uff0c\u5e76\u4e3b\u52a8\u53d1\u8d77\u9002\u5f53\u7684\u8f85\u52a9\u4ea4\u4e92\u3002", "conclusion": "POVNet+\u67b6\u6784\u901a\u8fc7\u591a\u6a21\u6001ADL\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u80fd\u591f\u8bc6\u522b\u591a\u79cdADL\u5e76\u4e3b\u52a8\u53d1\u8d77\u8f85\u52a9\u4ea4\u4e92\uff0c\u4e3a\u957f\u671f\u90e8\u7f72\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u91cd\u8981\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00456", "abs": "https://arxiv.org/abs/2602.00456", "authors": ["Amanda Dsouza", "Ramya Ramakrishnan", "Charles Dickens", "Bhavishya Pohani", "Christopher M Glaze"], "title": "Benchmarking Agents in Insurance Underwriting Environments", "comment": null, "summary": "As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aUNDERWRITE\u7684\u4fdd\u9669\u627f\u4fdd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u8bbe\u8ba1\uff0c\u5305\u542b\u4e13\u6709\u4e1a\u52a1\u77e5\u8bc6\u3001\u566a\u58f0\u5de5\u5177\u63a5\u53e3\u548c\u4e0d\u5b8c\u7f8e\u7684\u6a21\u62df\u7528\u6237\u7b49\u73b0\u5b9e\u56e0\u7d20\uff0c\u8bc4\u4f30\u4e8613\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u53d1\u73b0\u7814\u7a76\u5b9e\u9a8c\u5ba4\u6027\u80fd\u4e0e\u4f01\u4e1a\u5c31\u7eea\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u8fc7\u4e8e\u5f3a\u8c03\u4ee3\u7801\u7b49\u5f00\u653e\u9886\u57df\uff0c\u4f7f\u7528\u72ed\u7a84\u7684\u51c6\u786e\u6027\u6307\u6807\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u4f01\u4e1a\u5b9e\u9645\u8fd0\u8425\u7684\u590d\u6742\u6027\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u6355\u6349\u771f\u5b9e\u4f01\u4e1a\u6311\u6218\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86UNDERWRITE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u5bb6\u4f18\u5148\u3001\u591a\u8f6e\u5bf9\u8bdd\u7684\u4fdd\u9669\u627f\u4fdd\u57fa\u51c6\uff0c\u4e0e\u9886\u57df\u4e13\u5bb6\u5bc6\u5207\u5408\u4f5c\u8bbe\u8ba1\uff0c\u5f15\u5165\u4e86\u4e13\u6709\u4e1a\u52a1\u77e5\u8bc6\u3001\u566a\u58f0\u5de5\u5177\u63a5\u53e3\u548c\u4e0d\u5b8c\u7f8e\u7684\u6a21\u62df\u7528\u6237\u7b49\u5173\u952e\u73b0\u5b9e\u56e0\u7d20\u3002", "result": "\u8bc4\u4f3013\u4e2a\u524d\u6cbf\u6a21\u578b\u53d1\u73b0\uff1a\u6700\u51c6\u786e\u7684\u6a21\u578b\u5e76\u975e\u6700\u6709\u6548\u7684\uff1b\u5c3d\u7ba1\u6709\u5de5\u5177\u8bbf\u95ee\u6743\u9650\uff0c\u6a21\u578b\u4ecd\u4f1a\u5e7b\u89c9\u9886\u57df\u77e5\u8bc6\uff1bpass^k\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4e0b\u964d20%\uff1b\u5e38\u89c1\u4ee3\u7406\u6846\u67b6\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u5f71\u54cd\u6027\u80fd\u62a5\u544a\uff1b\u4e13\u4e1a\u9886\u57df\u7684\u5e7b\u89c9\u68c0\u6d4b\u9700\u8981\u7ec4\u5408\u65b9\u6cd5\u3002", "conclusion": "\u4e13\u5bb6\u53c2\u4e0e\u57fa\u51c6\u8bbe\u8ba1\u5bf9\u4e8e\u73b0\u5b9e\u4ee3\u7406\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u5f53\u524d\u4ee3\u7406\u6846\u67b6\u7684\u8106\u5f31\u6027\u4f1a\u626d\u66f2\u6027\u80fd\u62a5\u544a\uff0c\u4e13\u4e1a\u9886\u57df\u7684\u5e7b\u89c9\u68c0\u6d4b\u9700\u8981\u7ec4\u5408\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u7b26\u5408\u4f01\u4e1a\u90e8\u7f72\u9700\u6c42\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2602.00087", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.00087", "abs": "https://arxiv.org/abs/2602.00087", "authors": ["Haolin Pan", "Lianghong Huang", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization", "comment": null, "summary": "Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.", "AI": {"tldr": "ECCO\u6846\u67b6\u7ed3\u5408\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\uff0c\u901a\u8fc7\u53cd\u5de5\u7a0b\u65b9\u6cd5\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u8ba9LLM\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7684\u56e0\u679c\u903b\u8f91\uff0c\u7136\u540e\u4f5c\u4e3a\u7b56\u7565\u5e08\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\uff0c\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51cf\u5c1124.44%\u7684\u5faa\u73af\u6b21\u6570\u3002", "motivation": "\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u9762\u4e34\u4f20\u7edf\u9ed1\u76d2\u641c\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u6307\u5bfc\u4e0eLLM\u65b9\u6cd5\u5b58\u5728\u8868\u9762\u6a21\u5f0f\u5339\u914d\u548c\u56e0\u679c\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\u7684\u6846\u67b6\u3002", "method": "1. \u63d0\u51fa\u53cd\u5de5\u7a0b\u65b9\u6cd5\u6784\u5efa\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5c06\u9759\u6001\u4ee3\u7801\u7279\u5f81\u6620\u5c04\u5230\u53ef\u9a8c\u8bc1\u7684\u6027\u80fd\u8bc1\u636e\uff1b2. \u8ba9LLM\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7684\u56e0\u679c\u903b\u8f91\u800c\u975e\u7b80\u5355\u6a21\u4eff\u5e8f\u5217\uff1b3. \u8bbe\u8ba1\u534f\u4f5c\u63a8\u7406\u673a\u5236\uff0cLLM\u4f5c\u4e3a\u7b56\u7565\u5e08\u5b9a\u4e49\u4f18\u5316\u610f\u56fe\uff0c\u52a8\u6001\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\u7684\u53d8\u5f02\u64cd\u4f5c\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cECCO\u663e\u8457\u4f18\u4e8eLLVM opt -O3\u57fa\u51c6\uff0c\u5e73\u5747\u51cf\u5c1124.44%\u7684\u5faa\u73af\u6b21\u6570\u3002", "conclusion": "ECCO\u6210\u529f\u6865\u63a5\u4e86\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7ec4\u5408\u641c\u7d22\uff0c\u901a\u8fc7\u8ba9LLM\u5b66\u4e60\u56e0\u679c\u903b\u8f91\u5e76\u6307\u5bfc\u9057\u4f20\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u3002"}}
{"id": "2602.00459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00459", "abs": "https://arxiv.org/abs/2602.00459", "authors": ["Yongxin Zhou", "Changshun Wu", "Philippe Mulhem", "Didier Schwab", "Maxime Peyrard"], "title": "What Matters to an LLM? Behavioral and Computational Evidences from Summarization", "comment": "Findings of EACL 2026", "summary": "Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.", "AI": {"tldr": "LLMs\u5728\u6458\u8981\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u90e8\u91cd\u8981\u6027\u5224\u65ad\u6a21\u5f0f\uff0c\u4e0e\u65e9\u671f\u57fa\u7ebf\u6a21\u578b\u4e0d\u540c\uff0c\u4e14\u6a21\u578b\u5bb6\u65cf\u6bd4\u6a21\u578b\u5927\u5c0f\u66f4\u80fd\u9884\u6d4b\u91cd\u8981\u6027\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u4e0e\u91cd\u8981\u6027\u5206\u5e03\u5bf9\u9f50\uff0c\u4e2d\u540e\u671f\u5c42\u80fd\u6709\u6548\u9884\u6d4b\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6458\u8981\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u4f46\u5176\u5185\u90e8\u9a71\u52a8\u4fe1\u606f\u9009\u62e9\u7684\u91cd\u8981\u6027\u5224\u65ad\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63ed\u793aLLMs\u5728\u6458\u8981\u751f\u6210\u4e2d\u5982\u4f55\u5224\u65ad\u4fe1\u606f\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5224\u65ad\u5982\u4f55\u5728\u6a21\u578b\u5185\u90e8\u8868\u5f81\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u5206\u6790\u548c\u8ba1\u7b97\u5206\u6790\uff1a1\uff09\u884c\u4e3a\u5206\u6790\uff1a\u4e3a\u6bcf\u4e2a\u6587\u6863\u751f\u6210\u4e00\u7cfb\u5217\u957f\u5ea6\u63a7\u5236\u7684\u6458\u8981\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u4fe1\u606f\u5355\u5143\u88ab\u9009\u62e9\u7684\u9891\u7387\u63a8\u5bfc\u7ecf\u9a8c\u91cd\u8981\u6027\u5206\u5e03\uff1b2\uff09\u8ba1\u7b97\u5206\u6790\uff1a\u8bc6\u522b\u4e0e\u7ecf\u9a8c\u91cd\u8981\u6027\u5206\u5e03\u5bf9\u9f50\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u5bf9\u91cd\u8981\u6027\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "1\uff09LLMs\u5728\u91cd\u8981\u6027\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u4e0epre-LLM\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u5dee\u5f02\uff1b2\uff09LLMs\u6309\u6a21\u578b\u5bb6\u65cf\u805a\u7c7b\uff0c\u800c\u975e\u6309\u6a21\u578b\u5927\u5c0f\uff1b3\uff09\u67d0\u4e9b\u6ce8\u610f\u529b\u5934\u4e0e\u7ecf\u9a8c\u91cd\u8981\u6027\u5206\u5e03\u5bf9\u9f50\u826f\u597d\uff1b4\uff09\u4e2d\u540e\u671f\u5c42\u5bf9\u91cd\u8981\u6027\u6709\u5f3a\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u521d\u6b65\u63ed\u793a\u4e86LLMs\u5728\u6458\u8981\u751f\u6210\u4e2d\u7684\u4f18\u5148\u7ea7\u5224\u65ad\u673a\u5236\u53ca\u5176\u5185\u90e8\u8868\u5f81\u65b9\u5f0f\uff0c\u4e3a\u89e3\u91ca\u548c\u6700\u7ec8\u63a7\u5236\u8fd9\u4e9b\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u9009\u62e9\u5f00\u8f9f\u4e86\u8def\u5f84\u3002"}}
{"id": "2602.00132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00132", "abs": "https://arxiv.org/abs/2602.00132", "authors": ["Jiao Li", "Jian Lang", "Xikai Tang", "Wenzheng Shu", "Ting Zhong", "Qiang Gao", "Yong Wang", "Leiting Chen", "Fan Zhou"], "title": "Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation", "comment": "Accepted by AAAI2026 main track", "summary": "Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.", "AI": {"tldr": "SCANNER\u662f\u4e00\u4e2a\u9488\u5bf9\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4ec7\u6068\u5185\u5bb9\u4e2d\u7a33\u5b9a\u7684\u6838\u5fc3\u7279\u5f81\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\u7b49\uff09\u4f5c\u4e3a\u6865\u6881\u8fde\u63a5\u6e90\u57df\u548c\u76ee\u6807\u57df\uff0c\u89e3\u51b3\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u4ec7\u6068\u5185\u5bb9\u7ecf\u5e38\u6f14\u53d8\u6210\u4e0d\u89c4\u5219\u548c\u6a21\u7cca\u7684\u5f62\u5f0f\u4ee5\u9003\u907f\u5ba1\u67e5\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u8bed\u4e49\u6f02\u79fb\uff0c\u4f7f\u5148\u524d\u8bad\u7ec3\u7684\u6a21\u578b\u5931\u6548\u3002\u73b0\u6709\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u9488\u5bf9\u6e29\u548c\u7684\u5206\u5e03\u504f\u79fb\uff0c\u96be\u4ee5\u5904\u7406\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u4e2d\u7684\u4e25\u91cd\u8bed\u4e49\u6f02\u79fb\u3002", "method": "1. \u901a\u8fc7\u57fa\u4e8e\u8d28\u5fc3\u7684\u5bf9\u9f50\u673a\u5236\u4ece\u6a21\u7cca\u7684\u4ec7\u6068\u5185\u5bb9\u4e2d\u63ed\u793a\u7a33\u5b9a\u6838\u5fc3\u7279\u5f81\uff1b2. \u5f15\u5165\u6837\u672c\u7ea7\u81ea\u9002\u5e94\u8d28\u5fc3\u5bf9\u9f50\u7b56\u7565\u7f13\u89e3\u79bb\u7fa4\u6837\u672c\u5f71\u54cd\uff1b3. \u4f7f\u7528\u7c07\u5185\u591a\u6837\u6027\u6b63\u5219\u5316\u9632\u6b62\u8bed\u4e49\u5d29\u6e83\u3002", "result": "SCANNER\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747Macro-F1\u5f97\u5206\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa4.69%\u3002", "conclusion": "SCANNER\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u4ec7\u6068\u5185\u5bb9\u7684\u7a33\u5b9a\u6838\u5fc3\u7279\u5f81\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01498", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01498", "abs": "https://arxiv.org/abs/2602.01498", "authors": ["Mingyu Han", "Daniel Zhengyu Huang", "Yuhan Wang", "Yanshu Zhang", "Jiayi Zhou"], "title": "Geometric Generalization of Neural Operators from Kernel Integral Perspective", "comment": "33 pages, 4 figures", "summary": "Neural operators are neural network-based surrogate models for approximating solution operators of parametric partial differential equations, enabling efficient many-query computations in science and engineering. Many applications, including engineering design, involve variable and often nonparametric geometries, for which generalization to unseen geometries remains a central practical challenge. In this work, we adopt a kernel integral perspective motivated by classical boundary integral formulations and recast operator learning on variable geometries as the approximation of geometry-dependent kernel operators, potentially with singularities. This perspective clarifies a mechanism for geometric generalization and reveals a direct connection between operator learning and fast kernel summation methods. Leveraging this connection, we propose a multiscale neural operator inspired by Ewald summation for learning and efficiently evaluating unknown kernel integrals, and we provide theoretical accuracy guarantees for the resulting approximation. Numerical experiments demonstrate robust generalization across diverse geometries for several commonly used kernels and for a large-scale three-dimensional fluid dynamics example.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6838\u79ef\u5206\u89c6\u89d2\u7684\u591a\u5c3a\u5ea6\u795e\u7ecf\u7b97\u5b50\uff0c\u7528\u4e8e\u5904\u7406\u53d8\u51e0\u4f55\u5f62\u72b6\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u7b97\u5b50\u5b66\u4e60\uff0c\u5b9e\u73b0\u8de8\u51e0\u4f55\u5f62\u72b6\u7684\u9c81\u68d2\u6cdb\u5316\u3002", "motivation": "\u5de5\u7a0b\u8bbe\u8ba1\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7ecf\u5e38\u6d89\u53ca\u53d8\u51e0\u4f55\u5f62\u72b6\u548c\u975e\u53c2\u6570\u51e0\u4f55\uff0c\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u672a\u89c1\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u4ece\u7ecf\u5178\u8fb9\u754c\u79ef\u5206\u516c\u5f0f\u51fa\u53d1\uff0c\u5c06\u53d8\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u7b97\u5b50\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u51e0\u4f55\u4f9d\u8d56\u6838\u7b97\u5b50\u7684\u903c\u8fd1\u95ee\u9898\u3002\u53d7Ewald\u6c42\u548c\u542f\u53d1\uff0c\u63d0\u51fa\u591a\u5c3a\u5ea6\u795e\u7ecf\u7b97\u5b50\u6765\u5b66\u4e60\u548c\u9ad8\u6548\u8bc4\u4f30\u672a\u77e5\u6838\u79ef\u5206\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e38\u7528\u6838\u51fd\u6570\u548c\u5927\u89c4\u6a21\u4e09\u7ef4\u6d41\u4f53\u52a8\u529b\u5b66\u793a\u4f8b\u4e2d\uff0c\u80fd\u591f\u5b9e\u73b0\u8de8\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u9c81\u68d2\u6cdb\u5316\u3002", "conclusion": "\u901a\u8fc7\u6838\u79ef\u5206\u89c6\u89d2\u5efa\u7acb\u4e86\u7b97\u5b50\u5b66\u4e60\u4e0e\u5feb\u901f\u6838\u6c42\u548c\u65b9\u6cd5\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u63d0\u51fa\u7684\u591a\u5c3a\u5ea6\u795e\u7ecf\u7b97\u5b50\u4e3a\u5904\u7406\u53d8\u51e0\u4f55\u5f62\u72b6\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u7406\u8bba\u7cbe\u5ea6\u4fdd\u8bc1\u3002"}}
{"id": "2602.00471", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00471", "abs": "https://arxiv.org/abs/2602.00471", "authors": ["Xinlei Yu", "Chengming Xu", "Zhangquan Chen", "Bo Yin", "Cheng Yang", "Yongbo He", "Yihao Hu", "Jiangning Zhang", "Cheng Tan", "Xiaobin Hu", "Shuicheng Yan"], "title": "Dual Latent Memory for Visual Multi-agent System", "comment": null, "summary": "While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive \"scaling wall\": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the \"scaling wall\" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.", "AI": {"tldr": "L\u00b2-VMAS\u6846\u67b6\u901a\u8fc7\u53cc\u6f5c\u5728\u8bb0\u5fc6\u548c\u71b5\u9a71\u52a8\u89e6\u53d1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\"\u6269\u5c55\u5899\"\u95ee\u9898\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89c6\u89c9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08VMAS\uff09\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u63d0\u5347\u7efc\u5408\u80fd\u529b\uff0c\u4f46\u5b9e\u8bc1\u53d1\u73b0\u5b58\u5728\u53cd\u76f4\u89c9\u7684\"\u6269\u5c55\u5899\"\u73b0\u8c61\uff1a\u589e\u52a0\u667a\u80fd\u4f53\u8f6e\u6b21\u53cd\u800c\u964d\u4f4e\u6027\u80fd\uff0c\u540c\u65f6\u6307\u6570\u7ea7\u589e\u52a0token\u6210\u672c\u3002\u4f5c\u8005\u8ba4\u4e3a\u5931\u8d25\u6e90\u4e8e\u6587\u672c\u4e2d\u5fc3\u901a\u4fe1\u7684\u4fe1\u606f\u74f6\u9888\uff0c\u5c06\u611f\u77e5\u548c\u601d\u7ef4\u8f68\u8ff9\u8f6c\u6362\u4e3a\u79bb\u6563\u81ea\u7136\u8bed\u8a00\u4f1a\u5bfc\u81f4\u8bed\u4e49\u635f\u5931\u3002", "method": "\u63d0\u51faL\u00b2-VMAS\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u53cc\u6f5c\u5728\u8bb0\u5fc6\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\uff1b2\uff09\u89e3\u8026\u611f\u77e5\u548c\u601d\u7ef4\uff0c\u52a8\u6001\u5408\u6210\u53cc\u6f5c\u5728\u8bb0\u5fc6\uff1b3\uff09\u5f15\u5165\u71b5\u9a71\u52a8\u7684\u4e3b\u52a8\u89e6\u53d1\u673a\u5236\uff0c\u7528\u9ad8\u6548\u6309\u9700\u5185\u5b58\u8bbf\u95ee\u66ff\u4ee3\u88ab\u52a8\u4fe1\u606f\u4f20\u8f93\u3002", "result": "\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u3001\u6a21\u578b\u5927\u5c0f\u548c\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6253\u7834\"\u6269\u5c55\u5899\"\uff0c\u5177\u6709\u51fa\u8272\u7684\u53ef\u6269\u5c55\u6027\uff1a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.7-5.4%\uff0c\u540c\u65f6token\u4f7f\u7528\u91cf\u51cf\u5c1121.3-44.8%\u3002", "conclusion": "L\u00b2-VMAS\u6846\u67b6\u901a\u8fc7\u53cc\u6f5c\u5728\u8bb0\u5fc6\u548c\u71b5\u9a71\u52a8\u89e6\u53d1\u673a\u5236\uff0c\u89e3\u51b3\u4e86VMAS\u4e2d\u7684\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00088", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00088", "abs": "https://arxiv.org/abs/2602.00088", "authors": ["Namkyung Yoon", "Hwangnam Kim"], "title": "From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting", "comment": "16 pages, 5 figures. Submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.", "AI": {"tldr": "STM\u662f\u4e00\u79cd\u7b26\u53f7\u8f6c\u6362\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u7b26\u53f7\u6807\u8bb0\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u7684\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5728\u8f7b\u91cf\u7ea7\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u8f6c\u6362\u673a\u5236\uff08STM\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7ed3\u6784\u7684\u91cf\u5316\u6280\u672f\u5c06\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u503c\u8f6c\u6362\u4e3a\u7b26\u53f7\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u7684\u7ed3\u6784\u5316\u8f6c\u6362\u6355\u83b7\u65f6\u95f4\u52a8\u6001\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u4f7f\u8bed\u8a00\u6a21\u578b\u4e13\u6ce8\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5173\u952e\u90e8\u5206\uff0c\u662f\u4e00\u79cd\u901a\u7528\u673a\u5236\uff0c\u4fdd\u6301\u9aa8\u5e72\u8bed\u8a00\u6a21\u578b\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5728\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0c\u7ed3\u5408\u56db\u79cd\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002STM\u76f8\u6bd4\u9ed8\u8ba4\u9aa8\u5e72SLM\uff0cMAE\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe69%\uff0cMSE\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe90%\u3002\u8d44\u6e90\u5f00\u9500\u6781\u4f4e\uff1aGPU\u5185\u5b58\u4ec5\u589e\u52a0\u7ea60.06%\uff0c\u5ef6\u8fdf\u5f00\u9500\u4ec5\u589e\u52a00.64%\u3002", "conclusion": "STM\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u7b26\u53f7\u9a71\u52a8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5c42\uff0c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002\u5b83\u5728\u4fdd\u6301\u9aa8\u5e72\u6a21\u578b\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u8d44\u6e90\u5f00\u9500\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002"}}
{"id": "2602.00469", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00469", "abs": "https://arxiv.org/abs/2602.00469", "authors": ["Abhinav Gupta", "Toben H. Mintz", "Jesse Thomason"], "title": "Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations", "comment": "5 pages, 2 figures, codebase can be found at: https://github.com/abhinav-usc/SENSE-model/tree/main", "summary": "While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\\text{SENSE}$ $(\\textbf{S}\\text{ensorimotor }$ $\\textbf{E}\\text{mbedding }$ $\\textbf{N}\\text{orm }$ $\\textbf{S}\\text{coring }$ $\\textbf{E}\\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.", "AI": {"tldr": "SENSE\u6a21\u578b\u901a\u8fc7\u5c06\u8bcd\u5d4c\u5165\u6620\u5c04\u5230\u611f\u89c9\u8fd0\u52a8\u89c4\u8303\uff0c\u5c06\u8bed\u8a00\u7406\u89e3\u4e0e\u611f\u5b98\u4f53\u9a8c\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5728\u884c\u4e3a\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u8bcd\u5d4c\u5165\u4ece\u5171\u73b0\u6a21\u5f0f\u4e2d\u83b7\u53d6\u610f\u4e49\uff0c\u4f46\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u57fa\u4e8e\u611f\u5b98\u548c\u8fd0\u52a8\u4f53\u9a8c\u3002\u9700\u8981\u5c06\u8ba1\u7b97\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u611f\u89c9\u8fd0\u52a8\u7ecf\u9a8c\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u5f00\u53d1\u4e86SENSE\uff08\u611f\u89c9\u8fd0\u52a8\u5d4c\u5165\u89c4\u8303\u8bc4\u5206\u5f15\u64ce\uff09\u5b66\u4e60\u6295\u5f71\u6a21\u578b\uff0c\u9884\u6d4bLancaster\u611f\u89c9\u8fd0\u52a8\u89c4\u8303\u3002\u540c\u65f6\u8fdb\u884c\u4e86\u884c\u4e3a\u7814\u7a76\uff0c\u8ba9281\u540d\u53c2\u4e0e\u8005\u4ece\u5019\u9009\u975e\u8bcd\u4e2d\u9009\u62e9\u5177\u6709\u7279\u5b9a\u611f\u89c9\u8fd0\u52a8\u5173\u8054\u7684\u8bcd\u6c47\u3002", "result": "\u572811\u4e2a\u6a21\u6001\u4e2d\u67096\u4e2a\u663e\u793a\u51fa\u4eba\u7c7b\u9009\u62e9\u7387\u4e0eSENSE\u8bc4\u5206\u4e4b\u95f4\u7684\u663e\u8457\u76f8\u5173\u6027\u3002\u5bf9\u975e\u8bcd\u9009\u62e9\u7387\u7684\u4e9a\u8bcd\u6c47\u5206\u6790\u63ed\u793a\u4e86\u5185\u611f\u53d7\u89c4\u8303\u7684\u97f3\u7d20\u6a21\u5f0f\uff0c\u4e3a\u4ece\u6587\u672c\u6570\u636e\u8ba1\u7b97\u63d0\u51fa\u5019\u9009\u97f3\u7d20\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "conclusion": "SENSE\u6a21\u578b\u6210\u529f\u5730\u5c06\u8bcd\u5d4c\u5165\u4e0e\u611f\u89c9\u8fd0\u52a8\u89c4\u8303\u8054\u7cfb\u8d77\u6765\uff0c\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u611f\u5b98\u5173\u8054\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u97f3\u7d20\u6a21\u5f0f\u7684\u8ba1\u7b97\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00135", "abs": "https://arxiv.org/abs/2602.00135", "authors": ["Pengcheng Zheng", "Chaoning Zhang", "Jiarong Mo", "GuoHui Li", "Jiaquan Zhang", "Jiahao Zhang", "Sihan Cao", "Sheng Zheng", "Caiyan Qin", "Guoqing Wang", "Yang Yang"], "title": "LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models", "comment": "Accepted by ICLR 2026", "summary": "Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.", "AI": {"tldr": "LLaVA-FA\uff1a\u4e00\u79cd\u5728\u9891\u57df\u8fdb\u884c\u8054\u5408\u4f4e\u79e9+\u91cf\u5316\u8fd1\u4f3c\u7684\u9ad8\u6548\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u7279\u6027\u548c\u6781\u5750\u6807\u91cf\u5316\u5b9e\u73b0\u66f4\u7d27\u51d1\u51c6\u786e\u7684\u6743\u91cd\u8868\u793a\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u9ad8\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u5c06\u4f4e\u79e9\u5206\u89e3\u548c\u91cf\u5316\u89e3\u8026\uff0c\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\u7d2f\u79ef\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u8de8\u6a21\u6001\u5197\u4f59\u7684\u591a\u6a21\u6001\u67b6\u6784\u4e2d\u3002", "method": "\u63d0\u51faLLaVA-FA\uff0c\u5728\u9891\u57df\u8fdb\u884c\u8054\u5408\u4f4e\u79e9\u52a0\u91cf\u5316\u8fd1\u4f3c\uff1b\u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u7684\u53bb\u76f8\u5173\u548c\u5171\u8f6d\u5bf9\u79f0\u7279\u6027\uff1b\u5f15\u5165PolarQuant\uff08\u9488\u5bf9\u590d\u6570\u77e9\u9635\u7684\u6781\u5750\u6807\u91cf\u5316\u65b9\u6cd5\uff09\u548c\u53ef\u9009\u5bf9\u89d2\u6821\u51c6\uff08ODC\uff09\u65b9\u6848\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6821\u51c6\u6570\u636e\u3002", "result": "LLaVA-FA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c11\u7684\u6fc0\u6d3b\u53c2\u6570\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3aLMM\u538b\u7f29\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLaVA-FA\u901a\u8fc7\u9891\u57df\u8054\u5408\u538b\u7f29\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u91cd\u5efa\u8bef\u5dee\u95ee\u9898\uff0c\u4e3aLMM\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01529", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01529", "abs": "https://arxiv.org/abs/2602.01529", "authors": ["Weimin Han", "Jianguo Huang", "Yuan Yao"], "title": "Well-posedness and Numerical Analysis of Mixed Variational-hemivariational Inequalities", "comment": "29 pages, 4 tables, 16 figures", "summary": "The paper is devoted to well-posedness analysis and the numerical solution of a family of general elliptic mixed variational-hemivariational inequalities. Various mixed variational equations, mixed variational inequalities and mixed hemivariational inequalities found in the literature are special cases of the mixed variational-hemivariational inequalities. Well-posedness of the mixed variational-hemivariational inequalities and their numerical approximations are studied via the projection iteration technique. Error analysis of the numerical methods is presented. The results are applied to the study of a variational-hemivariational inequality of the Stokes equations for incompressible fluid flows subject to slip conditions of frictional type, both monotone and non-monotone. Optimal order error estimates are derived for the use of some stable finite element space pairs under certain solution regularity assumptions. Numerical results are reported demonstrating the theoretical prediction of convergence orders.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u7c7b\u5e7f\u4e49\u692d\u5706\u6df7\u5408\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u7684\u9002\u5b9a\u6027\u5206\u6790\u548c\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\uff0c\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u73b0\uff0c\u5e76\u5e94\u7528\u4e8e\u5e26\u6469\u64e6\u578b\u6ed1\u79fb\u6761\u4ef6\u7684\u4e0d\u53ef\u538b\u7f29\u6d41\u4f53Stokes\u65b9\u7a0b\u3002", "motivation": "\u7814\u7a76\u6df7\u5408\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u7684\u9002\u5b9a\u6027\u548c\u6570\u503c\u6c42\u89e3\u65b9\u6cd5\uff0c\u8fd9\u7c7b\u95ee\u9898\u5305\u542b\u4e86\u6587\u732e\u4e2d\u591a\u79cd\u6df7\u5408\u53d8\u5206\u65b9\u7a0b\u3001\u6df7\u5408\u53d8\u5206\u4e0d\u7b49\u5f0f\u548c\u6df7\u5408\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u4f5c\u4e3a\u7279\u4f8b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u80cc\u666f\u3002", "method": "\u91c7\u7528\u6295\u5f71\u8fed\u4ee3\u6280\u672f\u7814\u7a76\u6df7\u5408\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u7684\u9002\u5b9a\u6027\u53ca\u5176\u6570\u503c\u903c\u8fd1\uff0c\u8fdb\u884c\u8bef\u5dee\u5206\u6790\uff0c\u5e76\u5c06\u7ed3\u679c\u5e94\u7528\u4e8e\u5e26\u6469\u64e6\u578b\u6ed1\u79fb\u6761\u4ef6\u7684\u4e0d\u53ef\u538b\u7f29\u6d41\u4f53Stokes\u65b9\u7a0b\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u6df7\u5408\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u7684\u9002\u5b9a\u6027\u7406\u8bba\uff0c\u5f00\u53d1\u4e86\u6570\u503c\u65b9\u6cd5\u5e76\u8fdb\u884c\u4e86\u8bef\u5dee\u5206\u6790\uff0c\u5728\u7279\u5b9a\u89e3\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\uff0c\u5bf9\u67d0\u4e9b\u7a33\u5b9a\u6709\u9650\u5143\u7a7a\u95f4\u5bf9\u63a8\u5bfc\u4e86\u6700\u4f18\u9636\u8bef\u5dee\u4f30\u8ba1\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6536\u655b\u9636\u7684\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5e7f\u4e49\u692d\u5706\u6df7\u5408\u53d8\u5206-\u534a\u53d8\u5206\u4e0d\u7b49\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u9002\u5b9a\u6027\u5206\u6790\u548c\u6570\u503c\u6c42\u89e3\u6846\u67b6\uff0c\u7406\u8bba\u7ed3\u679c\u53ef\u5e94\u7528\u4e8e\u5e26\u6469\u64e6\u578b\u6ed1\u79fb\u6761\u4ef6\u7684\u6d41\u4f53\u529b\u5b66\u95ee\u9898\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00485", "abs": "https://arxiv.org/abs/2602.00485", "authors": ["Shule Lu", "Yujing Wang", "Hainan Zhang", "Xiaoshan Yang", "Hongwei Zheng", "Yongxin Tong", "Changsheng Xu", "Zhiming Zheng"], "title": "Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models", "comment": null, "summary": "VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.", "AI": {"tldr": "MoR\uff1a\u57fa\u4e8eGRPO\u4e0e\u6df7\u5408\u5956\u52b1\u7684\u8054\u90a6\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u672c\u5730\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u548c\u8def\u7531\u878d\u5408\u673a\u5236\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u8054\u90a6\u5bf9\u9f50", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u9690\u79c1\u654f\u611f\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u5171\u4eab\u9650\u5236\u4f7f\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u4e0d\u53ef\u884c\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\uff0c\u4f46\u9762\u4e34\u5ba2\u6237\u7aef\u5f02\u6784\u6027\uff08\u8ba1\u7b97\u8d44\u6e90\u3001\u5e94\u7528\u9700\u6c42\u3001\u6a21\u578b\u67b6\u6784\uff09\u7684\u6311\u6218\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u7528\u53c2\u6570\u66ff\u4ee3\u6570\u636e\u662f\u8054\u90a6\u5b66\u4e60\u7684\u73b0\u72b6\uff0c\u800c\u7528\u504f\u597d\u66ff\u4ee3\u53c2\u6570\u624d\u662f\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u672a\u6765\u65b9\u5411\u3002", "method": "\u63d0\u51faMoR\u8054\u90a6\u5bf9\u9f50\u6846\u67b6\uff1a1\uff09\u521d\u59cb\u5316\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3aKL\u6b63\u5219\u5316\u53c2\u8003\uff1b2\uff09\u6bcf\u4e2a\u5ba2\u6237\u7aef\u57fa\u4e8e\u672c\u5730\u504f\u597d\u6807\u6ce8\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u6355\u83b7\u7279\u5b9a\u8bc4\u4f30\u4fe1\u53f7\u800c\u4e0d\u66b4\u9732\u539f\u59cb\u6570\u636e\uff1b3\uff09\u5f15\u5165\u57fa\u4e8e\u8def\u7531\u7684\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u805a\u5408\u5ba2\u6237\u7aef\u5956\u52b1\u4fe1\u53f7\uff1b4\uff09\u670d\u52a1\u5668\u4f7f\u7528\u6df7\u5408\u5956\u52b1\u6267\u884cGRPO\u4f18\u5316\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171VQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoR\u5728\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u8de8\u5ba2\u6237\u7aef\u9002\u5e94\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u8054\u90a6\u5bf9\u9f50\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoR\u4e3a\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u7528\u504f\u597d\u66ff\u4ee3\u53c2\u6570\u7684\u8054\u90a6\u5b66\u4e60\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.00092", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00092", "abs": "https://arxiv.org/abs/2602.00092", "authors": ["Neha Kalibhat", "Zi Wang", "Prasoon Bajpai", "Drew Proud", "Wenjun Zeng", "Been Kim", "Mani Malek"], "title": "Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits", "comment": null, "summary": "We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9ed1\u76d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u53ef\u9a8c\u8bc1\u7684\"\u5baa\u6cd5\"\u6765\u7406\u89e3\u63d0\u793a\u8bcd\u4fee\u6539\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u539f\u5b50\u6982\u5ff5\u7f16\u8f91\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6570\u5b66\u63a8\u7406\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7406\u89e3\u9ed1\u76d2\u6a21\u578b\u7684\u884c\u4e3a\u673a\u5236\uff0c\u7279\u522b\u662f\u63d0\u793a\u8bcd\u4fee\u6539\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u3001\u6b63\u786e\u6027\u548c\u7ea6\u675f\u9075\u5b88\u7b49\u5177\u4f53\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5ea6\u7406\u89e3\u548c\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u9ed1\u76d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u4f7f\u7528\u539f\u5b50\u6982\u5ff5\u7f16\u8f91\uff08ACEs\uff09\u5728\u8f93\u5165\u63d0\u793a\u4e2d\u6dfb\u52a0\u3001\u79fb\u9664\u6216\u66ff\u6362\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5e94\u7528\u8fd9\u4e9b\u7f16\u8f91\u5e76\u89c2\u5bdf\u6a21\u578b\u884c\u4e3a\u53d8\u5316\uff0c\u5b66\u4e60\u4ece\u7f16\u8f91\u5230\u53ef\u9884\u6d4b\u7ed3\u679c\u7684\u56e0\u679c\u6620\u5c04\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\uff0cGPT-Image\u5173\u6ce8\u8bed\u6cd5\u9075\u5faa\uff0cImagen 4\u4f18\u5148\u8003\u8651\u6c1b\u56f4\u4e00\u81f4\u6027\uff1b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\uff0c\u5e72\u6270\u53d8\u91cf\u4f1a\u6df7\u6dc6GPT-5\uff0c\u4f46\u5bf9Gemini 2.5\u548co4-mini\u5f71\u54cd\u8f83\u5c0f\u3002\u5b66\u4e60\u5230\u7684\u5baa\u6cd5\u5728\u63a7\u5236\u6a21\u578b\u884c\u4e3a\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u76f8\u6bd4\u4e0d\u4f7f\u7528\u5baa\u6cd5\u7684\u65b9\u6cd5\u5e73\u5747\u63d0\u53471.86\u500d\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b66\u4e60\u53ef\u9a8c\u8bc1\u7684\u5baa\u6cd5\uff0c\u63d0\u4f9b\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5ea6\u3001\u53ef\u6cdb\u5316\u6d1e\u5bdf\uff0c\u6709\u6548\u7528\u4e8e\u63a7\u5236\u548c\u7406\u89e3\u6a21\u578b\u884c\u4e3a\uff0c\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00477", "abs": "https://arxiv.org/abs/2602.00477", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Intention-Adaptive LLM Fine-Tuning for Text Revision Generation", "comment": "In the Conference of the European Chapter of the Association for Computational Linguistics (EACL), March 2026", "summary": "Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.", "AI": {"tldr": "\u63d0\u51faIntention-Tuning\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9LLM\u5c42\u5b66\u4e60\u610f\u56fe\u8868\u793a\uff0c\u5728\u5c0f\u89c4\u6a21\u4fee\u8ba2\u8bed\u6599\u4e0a\u5b9e\u73b0\u9ad8\u6548\u610f\u56fe\u81ea\u9002\u5e94\u5fae\u8c03", "motivation": "LLM\u5728\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u57fa\u4e8e\u610f\u56fe\u7684\u751f\u6210\u4efb\u52a1\uff08\u5982\u4fee\u8ba2\u751f\u6210\uff09\u4e2d\u5e94\u7528\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u610f\u56fe\u573a\u666f\uff0c\u800c\u5168\u91cf\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5728\u4fee\u8ba2\u9886\u57df\u6210\u672c\u9ad8\u4e14\u7a00\u7f3a\u3002", "method": "\u63d0\u51faIntention-Tuning\u6846\u67b6\uff1a1\uff09\u52a8\u6001\u9009\u62e9LLM\u5c42\u5b50\u96c6\u5b66\u4e60\u610f\u56fe\u8868\u793a\uff1b2\uff09\u5c06\u5b66\u4e60\u5230\u7684\u610f\u56fe\u8868\u793a\u8fc1\u79fb\u5230\u4fee\u8ba2\u751f\u6210\u4efb\u52a1\u4e2d\u3002\u8fd9\u662f\u4e00\u79cd\u5c42\u7ea7\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIntention-Tuning\u5728\u5c0f\u89c4\u6a21\u4fee\u8ba2\u8bed\u6599\u4e0a\u65e2\u6709\u6548\u53c8\u9ad8\u6548\uff0c\u4f18\u4e8e\u591a\u4e2a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Intention-Tuning\u4e3a\u89e3\u51b3\u57fa\u4e8e\u610f\u56fe\u7684\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u4fee\u8ba2\u751f\u6210\u9886\u57df\u3002"}}
{"id": "2602.00144", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00144", "abs": "https://arxiv.org/abs/2602.00144", "authors": ["Xuan Rao", "Mingming Ha", "Bo Zhao", "Derong Liu", "Cesare Alippi"], "title": "Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers", "comment": null, "summary": "Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\\mathcal{O}(Cd^2)$ to $\\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \\ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.", "AI": {"tldr": "\u63d0\u51faLR-RGDA\u548cHopDC\u6846\u67b6\uff0c\u89e3\u51b3ViT\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u5206\u7c7b\u5668\u91cd\u5efa\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u548cHopfield\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5927\u89c4\u6a21\u7c7b\u589e\u91cf\u5b66\u4e60\u3002", "motivation": "ViT\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u9762\u4e34\u5206\u7c7b\u5668\u91cd\u5efa\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u8fed\u4ee3SGD\u3002\u867d\u7136\u5206\u6790\u6027RGDA\u80fd\u63d0\u4f9b\u8d1d\u53f6\u65af\u6700\u4f18\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u4e8c\u6b21\u63a8\u7406\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faLR-RGDA\uff1a\u5229\u7528Woodbury\u77e9\u9635\u6052\u7b49\u5f0f\u901a\u8fc7\u4f4e\u79e9\u7ed3\u6784\u5206\u89e3\u534f\u65b9\u5dee\uff0c\u5c06\u5224\u522b\u51fd\u6570\u5206\u89e3\u4e3a\u5168\u5c40\u4eff\u5c04\u9879\u548c\u4f4e\u79e9\u4e8c\u6b21\u6270\u52a8\u9879\uff0c\u964d\u4f4e\u63a8\u7406\u590d\u6742\u5ea6\u3002\u5f15\u5165HopDC\uff1a\u4f7f\u7528\u73b0\u4ee3\u8fde\u7eedHopfield\u7f51\u7edc\u901a\u8fc7\u5173\u8054\u8bb0\u5fc6\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5386\u53f2\u7c7b\u7edf\u8ba1\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2aCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63a8\u7406\u590d\u6742\u5ea6\u4eceO(Cd\u00b2)\u964d\u4f4e\u5230O(d\u00b2 + Crd\u00b2)\uff0c\u5176\u4e2dr\u8fdc\u5c0f\u4e8ed\uff0c\u4e3a\u5927\u89c4\u6a21\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684LR-RGDA\u548cHopDC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86ViT\u5728\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u7ed3\u5408\u4e86RGDA\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u6548\u7387\uff0c\u540c\u65f6\u901a\u8fc7Hopfield\u7f51\u7edc\u7f13\u89e3\u4e86\u8868\u793a\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2602.01636", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01636", "abs": "https://arxiv.org/abs/2602.01636", "authors": ["Hiroki Ishizaka"], "title": "Equilibrated-flux residual certification for verified existence and outputs", "comment": null, "summary": "We present a post-processing certification workflow for nonlinear elliptic boundary value problems that upgrades a standard finite element computation to a rigorous existence and output certificate. For a given approximate discrete state, we verify existence and local uniqueness of a weak solution in a computable neighbourhood via a Newton--Kantorovich argument based on three certified ingredients: a guaranteed dual-norm residual bound, a computable lower bound for the stability constant of the linearised operator, and a Lipschitz bound for the derivative on the verification ball. The residual bound is obtained by an equilibrated-flux reconstruction exploiting an explicit relation between nonconforming and mixed formulations, yielding $H(\\mathrm{div})$-conforming fluxes without local mixed solves. The stability ingredient follows from a computable coercivity lower bound for the linearisation. An admissible verification radius is selected by a simple bracketing--bisection search, justified for an affine Lipschitz model. Once the verification ball is certified, we derive guaranteed enclosures for quantities of interest using computable variation bounds; an adjoint-based correction, in the spirit of goal-oriented error estimation, tightens these intervals while retaining full rigour. Numerical experiments for semilinear diffusion--reaction models show that the certificates are informative and that the adjoint enhancement substantially reduces enclosure widths.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u975e\u7ebf\u6027\u692d\u5706\u8fb9\u503c\u95ee\u9898\u7684\u540e\u5904\u7406\u8ba4\u8bc1\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u725b\u987f-\u5eb7\u6258\u6d1b\u7ef4\u5947\u8bba\u8bc1\u9a8c\u8bc1\u5f31\u89e3\u7684\u5b58\u5728\u6027\u548c\u5c40\u90e8\u552f\u4e00\u6027\uff0c\u5e76\u63d0\u4f9b\u8f93\u51fa\u91cf\u7684\u4e25\u683c\u533a\u95f4\u4f30\u8ba1\u3002", "motivation": "\u4e3a\u975e\u7ebf\u6027\u692d\u5706\u8fb9\u503c\u95ee\u9898\u7684\u6709\u9650\u5143\u8ba1\u7b97\u63d0\u4f9b\u4e25\u683c\u7684\u6570\u5b66\u8ba4\u8bc1\uff0c\u786e\u4fdd\u89e3\u7684\u5b58\u5728\u6027\u3001\u552f\u4e00\u6027\uff0c\u5e76\u80fd\u7ed9\u51fa\u8f93\u51fa\u91cf\u7684\u53ef\u9760\u533a\u95f4\u4f30\u8ba1\u3002", "method": "\u57fa\u4e8e\u725b\u987f-\u5eb7\u6258\u6d1b\u7ef4\u5947\u8bba\u8bc1\uff0c\u9700\u8981\u4e09\u4e2a\u8ba4\u8bc1\u6210\u5206\uff1a\u5bf9\u5076\u8303\u6570\u6b8b\u5dee\u754c\u3001\u7ebf\u6027\u5316\u7b97\u5b50\u7a33\u5b9a\u6027\u5e38\u6570\u7684\u53ef\u8ba1\u7b97\u4e0b\u754c\u3001\u5bfc\u6570\u5728\u9a8c\u8bc1\u7403\u4e0a\u7684\u5229\u666e\u5e0c\u8328\u754c\u3002\u6b8b\u5dee\u754c\u901a\u8fc7\u5e73\u8861\u901a\u91cf\u91cd\u6784\u83b7\u5f97\uff0c\u7a33\u5b9a\u6027\u901a\u8fc7\u53ef\u8ba1\u7b97\u5f3a\u5236\u4e0b\u754c\u83b7\u5f97\uff0c\u9a8c\u8bc1\u534a\u5f84\u901a\u8fc7\u62ec\u53f7-\u4e8c\u5206\u641c\u7d22\u786e\u5b9a\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u4fe1\u606f\u4e30\u5bcc\u7684\u8ba4\u8bc1\uff0c\u5e76\u4e14\u57fa\u4e8e\u4f34\u968f\u7684\u6821\u6b63\u663e\u8457\u51cf\u5c11\u4e86\u533a\u95f4\u4f30\u8ba1\u7684\u5bbd\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u80fd\u591f\u5c06\u6807\u51c6\u6709\u9650\u5143\u8ba1\u7b97\u5347\u7ea7\u4e3a\u4e25\u683c\u7684\u89e3\u5b58\u5728\u6027\u548c\u8f93\u51fa\u8ba4\u8bc1\uff0c\u4e3a\u975e\u7ebf\u6027\u692d\u5706\u95ee\u9898\u63d0\u4f9b\u53ef\u9760\u7684\u6570\u5b66\u4fdd\u8bc1\u3002"}}
{"id": "2602.00510", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00510", "abs": "https://arxiv.org/abs/2602.00510", "authors": ["Huanghaohe Zou", "Peng Han", "Emad Nazerian", "Alex Q. Huang"], "title": "PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)", "comment": null, "summary": "Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.", "AI": {"tldr": "PCBSchemaGen\uff1a\u9996\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u548c\u7ea6\u675f\u5f15\u5bfc\u5408\u6210\u7684\u514d\u8bad\u7ec3PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u6846\u67b6\uff0c\u53ef\u5904\u7406\u6570\u5b57\u3001\u6a21\u62df\u548c\u7535\u6e90\u6df7\u5408\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u8bbe\u8ba1\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u5728\u7535\u5b50\u4ea7\u4e1a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u6570\u5b57\u6216\u6a21\u62df\u7535\u8def\u5355\u4e00\u9886\u57df\uff0c\u800cPCB\u8bbe\u8ba1\u9700\u8981\u5904\u7406\u5f02\u6784\u4fe1\u53f7\u5e76\u9075\u5b88\u5b9e\u9645IC\u5c01\u88c5\u548c\u5f15\u811a\u7ea6\u675f\u3002\u7531\u4e8e\u5f00\u6e90\u6570\u636e\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u4eff\u771f\u9a8c\u8bc1\uff0c\u81ea\u52a8\u5316PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\u3002", "method": "PCBSchemaGen\u6846\u67b6\u5305\u542b\uff1a1. \u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u63d0\u793a\u8fdb\u884c\u8fed\u4ee3\u53cd\u9988\uff1b2. \u9a8c\u8bc1\u6846\u67b6\u5229\u7528\u771f\u5b9eIC\u6570\u636e\u624b\u518c\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u5b50\u56fe\u540c\u6784\uff0c\u7f16\u7801\u5f15\u811a\u89d2\u8272\u8bed\u4e49\u548c\u62d3\u6251\u7ea6\u675f\uff1b3. \u572823\u4e2aPCB\u539f\u7406\u56fe\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "PCBSchemaGen\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6570\u5b57\u3001\u6a21\u62df\u548c\u7535\u6e90\u9886\u57df\u7684\u6df7\u5408\u4fe1\u53f7\u8bbe\u8ba1\u4efb\u52a1\u3002", "conclusion": "PCBSchemaGen\u662f\u9996\u4e2a\u514d\u8bad\u7ec3\u7684PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u548c\u7ea6\u675f\u5f15\u5bfc\u5408\u6210\u89e3\u51b3\u4e86\u81ea\u52a8\u5316PCB\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u5904\u7406\u5f02\u6784\u4fe1\u53f7\u7684PCB\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00094", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00094", "abs": "https://arxiv.org/abs/2602.00094", "authors": ["Sandra Ben\u00edtez-Pe\u00f1a", "Blas Kolic", "Victoria Menendez", "Bel\u00e9n Pulido"], "title": "Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review", "comment": null, "summary": "Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u540c\u65f6\u5904\u7406\u7fa4\u4f53\u516c\u5e73\u6027\u548c\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u6df7\u5408\u516c\u5e73\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u3001\u4f18\u5316\u673a\u5236\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5e76\u8ba8\u8bba\u4e86\u76f8\u5173\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7b97\u6cd5\u516c\u5e73\u6027\u5df2\u6210\u4e3a\u8ba1\u7b97\u51b3\u7b56\u7cfb\u7edf\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\uff0c\u4f46\u73b0\u6709\u7684\u7fa4\u4f53\u516c\u5e73\u6027\u548c\u4e2a\u4f53\u516c\u5e73\u6027\u7814\u7a76\u5f80\u5f80\u76f8\u4e92\u5b64\u7acb\u3002\u4e3a\u4e86\u8bbe\u8ba1\u51fa\u65e2\u80fd\u5728\u7fa4\u4f53\u5c42\u9762\u53c8\u80fd\u5728\u4e2a\u4f53\u5c42\u9762\u63d0\u4f9b\u53ef\u9760\u516c\u5e73\u4fdd\u8bc1\u7684\u7b97\u6cd5\uff0c\u9700\u8981\u7cfb\u7edf\u5730\u7814\u7a76\u5982\u4f55\u5c06\u8fd9\u4e24\u79cd\u516c\u5e73\u6027\u89c6\u89d2\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u6df7\u5408\u516c\u5e73\u6027\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u6574\u7406\uff0c\u6309\u7167\u4f7f\u7528\u7684\u516c\u5e73\u673a\u5236\u548c\u534f\u8c03\u591a\u79cd\u516c\u5e73\u6807\u51c6\u7684\u7b97\u6cd5\u4e0e\u6570\u5b66\u7b56\u7565\u8fdb\u884c\u7ec4\u7ec7\u3002\u5bf9\u6bcf\u7c7b\u65b9\u6cd5\u90fd\u6df1\u5165\u5206\u6790\u5176\u7406\u8bba\u57fa\u7840\u3001\u4f18\u5316\u673a\u5236\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u6df7\u5408\u516c\u5e73\u6027\u65b9\u6cd5\u7684\u5168\u9762\u6279\u5224\u6027\u56de\u987e\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u7406\u8bba\u4fdd\u8bc1\u3001\u4f18\u5316\u7b56\u7565\u548c\u5b9e\u9645\u5e94\u7528\u65b9\u9762\u7684\u7279\u70b9\u4e0e\u5c40\u9650\uff0c\u660e\u786e\u4e86\u7fa4\u4f53\u516c\u5e73\u6027\u4e0e\u4e2a\u4f53\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u8bbe\u8ba1\u6df7\u5408\u516c\u5e73\u6027\u7b97\u6cd5\u63d0\u4f9b\u4e86\u5168\u9762\u8d44\u6e90\uff0c\u6307\u51fa\u4e86\u5f00\u53d1\u539f\u5219\u6027\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u516c\u5e73\u6027\u65b9\u6cd5\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u9700\u8981\u5728\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u540c\u65f6\u63d0\u4f9b\u53ef\u9760\u516c\u5e73\u4fdd\u8bc1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.00491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00491", "abs": "https://arxiv.org/abs/2602.00491", "authors": ["Zhaokun Yan", "Zhaohan Liu", "Wuzheng Dong", "Lijie Feng", "Chengxiao Dai"], "title": "From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas", "comment": null, "summary": "Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \\textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.", "AI": {"tldr": "\u63d0\u51faGlobalHealthAtlas\u6570\u636e\u96c6\uff0c\u5305\u542b28\u4e07\u6761\u591a\u8bed\u8a00\u516c\u5171\u536b\u751f\u63a8\u7406\u5b9e\u4f8b\uff0c\u5206\u4e3a3\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u8bbe\u8ba1\u4e86LLM\u8f85\u52a9\u7684\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u548c\u9886\u57df\u5bf9\u9f50\u8bc4\u4f30\u5668\u3002", "motivation": "\u516c\u5171\u536b\u751f\u63a8\u7406\u9700\u8981\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u3001\u4e13\u5bb6\u5171\u8bc6\u548c\u5b89\u5168\u7ea6\u675f\u8fdb\u884c\u7fa4\u4f53\u5c42\u9762\u63a8\u65ad\uff0c\u4f46\u76ee\u524d\u4f5c\u4e3a\u7ed3\u6784\u5316\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u76d1\u7763\u4fe1\u53f7\u548c\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "1) \u6784\u5efaGlobalHealthAtlas\u6570\u636e\u96c6\uff1a28\u4e07\u6761\u5b9e\u4f8b\uff0c\u8986\u76d615\u4e2a\u516c\u5171\u536b\u751f\u9886\u57df\u548c17\u79cd\u8bed\u8a00\uff0c\u5206\u4e3a\u5065\u5eb7\u7d20\u517b\u3001\u6d41\u884c\u75c5\u5b66\u63a8\u7406\u548c\u653f\u7b56\u63a8\u7406\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff1b2) \u8bbe\u8ba1LLM\u8f85\u52a9\u7684\u6784\u5efa\u548c\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\uff0c\u5305\u62ec\u68c0\u7d22\u3001\u53bb\u91cd\u3001\u8bc1\u636e\u57fa\u7840\u68c0\u67e5\u548c\u6807\u7b7e\u9a8c\u8bc1\uff1b3) \u63d0\u51fa\u9886\u57df\u5bf9\u9f50\u8bc4\u4f30\u5668\uff0c\u57fa\u4e8e\u591a\u6837LLM\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u5224\u65ad\uff0c\u4ece\u51c6\u786e\u6027\u3001\u63a8\u7406\u3001\u5b8c\u6574\u6027\u3001\u5171\u8bc6\u5bf9\u9f50\u3001\u672f\u8bed\u89c4\u8303\u548c\u6d1e\u5bdf\u529b\u516d\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u8f93\u51fa\u3002", "result": "\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u516c\u5171\u536b\u751f\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8d28\u91cf\u63a7\u5236\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7684\u516c\u5171\u536b\u751f\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u4f7f\u5f97\u80fd\u591f\u5728\u8d85\u8d8a\u4f20\u7edfQA\u57fa\u51c6\u7684\u5b89\u5168\u5173\u952e\u516c\u5171\u536b\u751f\u63a8\u7406\u9886\u57df\uff0c\u5bf9LLM\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.00145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00145", "abs": "https://arxiv.org/abs/2602.00145", "authors": ["Siva Teja Kakileti", "Geetha Manjunath"], "title": "DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images", "comment": null, "summary": "Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.", "AI": {"tldr": "DensiThAI\uff1a\u57fa\u4e8e\u7ea2\u5916\u70ed\u6210\u50cf\u548c\u591a\u89c6\u56fe\u6df1\u5ea6\u5b66\u4e60\u7684\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u6846\u67b6\uff0c\u53ef\u4f5c\u4e3a\u65e0\u8f90\u5c04\u7684\u4e73\u817a\u5bc6\u5ea6\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u4e73\u817a\u7ec4\u7ec7\u5bc6\u5ea6\u662f\u4e73\u817a\u764c\u98ce\u9669\u7684\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u76ee\u524d\u7684\u5bc6\u5ea6\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56X\u7ebf\u94bc\u9776\uff08\u6709\u8f90\u5c04\u6210\u50cf\uff09\u3002\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u901a\u8fc7\u7ea2\u5916\u70ed\u6210\u50cf\u4f30\u8ba1\u4e73\u817a\u5bc6\u5ea6\u7684\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u4e00\u79cd\u65e0\u8f90\u5c04\u7684\u6210\u50cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDensiThAI\u591a\u89c6\u56fe\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e94\u4e2a\u6807\u51c6\u70ed\u6210\u50cf\u89c6\u56fe\u8fdb\u884c\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u3002\u57fa\u4e8e\u5047\u8bbe\uff1a\u7ea4\u7ef4\u817a\u4f53\u548c\u8102\u80aa\u7ec4\u7ec7\u5177\u6709\u4e0d\u540c\u7684\u70ed\u7269\u7406\u548c\u751f\u7406\u7279\u6027\uff0c\u5bfc\u81f4\u4e73\u623f\u8868\u9762\u5b58\u5728\u7ec6\u5fae\u4f46\u7a7a\u95f4\u4e00\u81f4\u7684\u6e29\u5ea6\u53d8\u5316\u3002", "result": "\u5728\u5305\u542b3,500\u540d\u5973\u6027\u7684\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u94bc\u9776\u5bc6\u5ea6\u6807\u7b7e\u4f5c\u4e3a\u53c2\u8003\u3002DensiThAI\u572810\u4e2a\u968f\u673a\u5206\u5272\u4e2d\u5e73\u5747AUROC\u8fbe\u52300.73\uff0c\u6240\u6709\u5206\u5272\u4e2d\u5bc6\u5ea6\u7c7b\u522b\u95f4\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u5206\u79bb\uff08p << 0.05\uff09\u3002\u5728\u4e0d\u540c\u5e74\u9f84\u7ec4\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u70ed\u6210\u50cf\u4f5c\u4e3a\u65e0\u8f90\u5c04\u7684\u4e73\u817a\u5bc6\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u548c\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u3002DensiThAI\u6846\u67b6\u5c55\u793a\u4e86\u5229\u7528\u70ed\u6210\u50cf\u8fdb\u884c\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.01656", "categories": ["math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01656", "abs": "https://arxiv.org/abs/2602.01656", "authors": ["Sahat Pandapotan Nainggolan", "Julius Fergy Tiongson Rabago", "Hirofumi Notsu"], "title": "Numerical methods for diffusion coefficient recovery", "comment": "47 pages", "summary": "We revisit the inverse problem of reconstructing a spatially varying diffusion coefficient in stationary elliptic equations from boundary Cauchy data. From a theoretical perspective, we introduce a gradient-weighted modification of the coupled complex-boundary method (CCBM) incorporating an \\(H^1\\)-type term, and formulate the reconstruction as a regularized optimization problem over bounded admissible coefficients. We establish continuity and differentiability of the forward map, Lipschitz continuity of the modified cost functional, existence of minimizers, stability with respect to noisy data, and convergence under vanishing noise. From a numerical perspective, reconstructions are computed using a Sobolev-gradient descent scheme and evaluated through extensive numerical experiments across a range of noise levels, boundary inputs, and coefficient structures. In the reported tests, for sufficiently large but not excessive $H^1$-weights, the modified CCBM is observed to yield more stable reconstructions and to reduce certain high-frequency artifacts. Across the numerical scenarios considered in this study, the method often demonstrates favorable stability and robustness properties relative to several classical boundary-based formulations, although performance remains problem- and parameter-dependent. A projection-based extension further supports stable recovery of piecewise-constant diffusion coefficients in multi-subregion test cases. Our results indicate that, as long as all subdomains share a portion of the boundary, the proposed CCBM-based Tikhonov regularization approach with a pick-a-point strategy enables stable and reliable reconstruction of diffusion parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8026\u5408\u590d\u8fb9\u754c\u65b9\u6cd5\uff08CCBM\uff09\uff0c\u7528\u4e8e\u4ece\u8fb9\u754c\u67ef\u897f\u6570\u636e\u91cd\u5efa\u7a33\u6001\u692d\u5706\u65b9\u7a0b\u4e2d\u7684\u7a7a\u95f4\u53d8\u5316\u6269\u6563\u7cfb\u6570\uff0c\u901a\u8fc7\u5f15\u5165\u68af\u5ea6\u52a0\u6743\u548cH\u00b9\u6b63\u5219\u5316\u9879\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u4fdd\u8bc1\u5e76\u5c55\u793a\u4e86\u6570\u503c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4ece\u8fb9\u754c\u67ef\u897f\u6570\u636e\u91cd\u5efa\u7a7a\u95f4\u53d8\u5316\u6269\u6563\u7cfb\u6570\u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u53cd\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u6297\u566a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u6269\u6563\u7cfb\u6570\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u52a0\u6743\u7684CCBM\u6539\u8fdb\u65b9\u6cd5\uff0c\u5f15\u5165H\u00b9\u578b\u6b63\u5219\u5316\u9879\uff0c\u5c06\u91cd\u5efa\u95ee\u9898\u8868\u8ff0\u4e3a\u6709\u754c\u5bb9\u8bb8\u7cfb\u6570\u4e0a\u7684\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\u3002\u91c7\u7528Sobolev\u68af\u5ea6\u4e0b\u964d\u65b9\u6848\u8fdb\u884c\u6570\u503c\u8ba1\u7b97\uff0c\u5e76\u6269\u5c55\u6295\u5f71\u65b9\u6cd5\u5904\u7406\u5206\u6bb5\u5e38\u6570\u7cfb\u6570\u3002", "result": "\u5efa\u7acb\u4e86\u524d\u5411\u6620\u5c04\u7684\u8fde\u7eed\u6027\u548c\u53ef\u5fae\u6027\u3001\u6539\u8fdb\u4ee3\u4ef7\u51fd\u6570\u7684Lipschitz\u8fde\u7eed\u6027\u3001\u6781\u5c0f\u503c\u5b58\u5728\u6027\u3001\u5bf9\u566a\u58f0\u6570\u636e\u7684\u7a33\u5b9a\u6027\u4ee5\u53ca\u566a\u58f0\u6d88\u5931\u65f6\u7684\u6536\u655b\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u9002\u5f53H\u00b9\u6743\u91cd\u4e0b\uff0c\u6539\u8fdbCCBM\u80fd\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u91cd\u5efa\u5e76\u51cf\u5c11\u9ad8\u9891\u4f2a\u5f71\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eCCBM\u7684Tikhonov\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5728\u9002\u5f53\u53c2\u6570\u9009\u62e9\u4e0b\uff0c\u80fd\u591f\u7a33\u5b9a\u53ef\u9760\u5730\u91cd\u5efa\u6269\u6563\u53c2\u6570\uff0c\u7279\u522b\u662f\u5f53\u6240\u6709\u5b50\u57df\u5171\u4eab\u90e8\u5206\u8fb9\u754c\u65f6\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u76f8\u5bf9\u4e8e\u7ecf\u5178\u8fb9\u754c\u65b9\u6cd5\u7684\u4f18\u52bf\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.00521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00521", "abs": "https://arxiv.org/abs/2602.00521", "authors": ["Junhyuk Choi", "Sohhyung Park", "Chanhee Cho", "Hyeonchu Park", "Bugeun Kim"], "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory", "comment": "Under review", "summary": "While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u7684\u4e24\u9636\u6bb5\u8bca\u65ad\u6846\u67b6\uff0c\u4ece\u5185\u5728\u4e00\u81f4\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027", "motivation": "\u73b0\u6709LLM-as-a-Judge\u9a8c\u8bc1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c2\u5bdf\u5230\u7684\u8f93\u51fa\u5c42\u9762\uff0c\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3LLM\u8bc4\u5224\u8005\u662f\u5426\u4f5c\u4e3a\u7a33\u5b9a\u53ef\u9760\u7684\u6d4b\u91cf\u5de5\u5177\u3002\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u7684\u4e24\u9636\u6bb5\u8bca\u65ad\u6846\u67b6\uff0c\u91c7\u7528IRT\u7684\u5206\u7ea7\u53cd\u5e94\u6a21\u578b\uff08GRM\uff09\uff0c\u4ece\u5185\u5728\u4e00\u81f4\u6027\uff08\u63d0\u793a\u53d8\u5316\u4e0b\u7684\u6d4b\u91cf\u7a33\u5b9a\u6027\uff09\u548c\u4eba\u7c7b\u5bf9\u9f50\uff08\u4e0e\u4eba\u7c7b\u8d28\u91cf\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\uff09\u4e24\u4e2a\u7ef4\u5ea6\u5f62\u5f0f\u5316\u53ef\u9760\u6027\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5229\u7528IRT-GRM\u6846\u67b6\u80fd\u4e3aLLM\u8bc4\u5224\u8005\u8bca\u65ad\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u4e3a\u9a8c\u8bc1LLM-as-a-Judge\u7684\u53ef\u9760\u6027\u5e76\u8bc6\u522b\u4e0d\u53ef\u9760\u6027\u7684\u6f5c\u5728\u539f\u56e0\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u63d0\u51fa\u7684IRT-GRM\u6846\u67b6\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3LLM\u4f5c\u4e3a\u6d4b\u91cf\u5de5\u5177\u7684\u6027\u80fd\u7279\u5f81\u3002"}}
{"id": "2602.00099", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00099", "abs": "https://arxiv.org/abs/2602.00099", "authors": ["James King", "Arturs Berzins", "Siddhartha Mishra", "Marius Zeinhofer"], "title": "Gauss-Newton Natural Gradient Descent for Shape Learning", "comment": "16 Pages, 9 Figures, submitted to Computer-Aided Design", "summary": "We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5728\u5f62\u72b6\u5b66\u4e60\u4e2d\u5e94\u7528\u9ad8\u65af-\u725b\u987f\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u5305\u62ec\u9690\u5f0f\u795e\u7ecf\u8868\u9762\u548c\u51e0\u4f55\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\uff0c\u76f8\u6bd4\u4e00\u9636\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u3002", "motivation": "\u5f62\u72b6\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5e95\u5c42\u5fae\u5206\u7ea6\u675f\u7684\u75c5\u6001\u6027\uff0c\u4ee5\u53ca\u53c2\u6570\u7a7a\u95f4\u4f18\u5316\u95ee\u9898\u4e0e\u81ea\u7136\u95ee\u9898\u6240\u5728\u51fd\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002\u4f20\u7edf\u4e00\u9636\u65b9\u6cd5\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u6536\u655b\u7f13\u6162\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u9ad8\u65af-\u725b\u987f\u6cd5\u8fdb\u884c\u5f62\u72b6\u5b66\u4e60\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u9690\u5f0f\u795e\u7ecf\u8868\u9762\u548c\u51e0\u4f55\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u5fae\u5206\u7ea6\u675f\u548c\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u57fa\u51c6\u5f62\u72b6\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u65af-\u725b\u987f\u6cd5\u76f8\u6bd4\u6807\u51c6\u4e00\u9636\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\uff0c\u51cf\u5c11\u4e86\u8fed\u4ee3\u6b21\u6570\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6700\u7ec8\u89e3\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u9ad8\u65af-\u725b\u987f\u6cd5\u4e3a\u5f62\u72b6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8be5\u9886\u57df\u7279\u6709\u7684\u6311\u6218\uff0c\u5728\u8bad\u7ec3\u901f\u5ea6\u548c\u6700\u7ec8\u7cbe\u5ea6\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.00497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00497", "abs": "https://arxiv.org/abs/2602.00497", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design", "comment": null, "summary": "Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6587\u5316\u6839\u57fa\u7684\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u6cbb\u7406\u6846\u67b6\uff0c\u6311\u6218\u5f53\u524d\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u3001\u5047\u8bbe\u540c\u8d28\u7528\u6237\u548c\u62bd\u8c61\u516c\u5e73\u7684\u6cbb\u7406\u6a21\u5f0f\uff0c\u5f3a\u8c03\u9700\u5e94\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u8fb9\u7f18\u5316\u793e\u533a\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cbb\u7406\u6846\u67b6\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u4e2d\u5fc3\u6570\u636e\u3001\u540c\u8d28\u7528\u6237\u5047\u8bbe\u548c\u62bd\u8c61\u516c\u5e73\u6982\u5ff5\uff0c\u8fd9\u5bfc\u81f4\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6587\u5316\u8fb9\u7f18\u5316\u793e\u533a\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002\u6570\u636e\u5b9e\u8df5\u3001\u6a21\u578b\u884c\u4e3a\u548c\u95ee\u8d23\u673a\u5236\u5f80\u5f80\u4e0e\u5f53\u5730\u89c4\u8303\u3001\u6743\u5229\u548c\u671f\u671b\u4e0d\u7b26\uff0c\u9700\u8981\u5efa\u7acb\u6587\u5316\u6839\u57fa\u7684\u6cbb\u7406\u6846\u67b6\u3002", "method": "\u672c\u6587\u7efc\u5408\u4e86\u4eba\u673a\u4ea4\u4e92\u548cAI\u6cbb\u7406\u7684\u8de8\u6587\u5316\u89c6\u89d2\uff0c\u6574\u5408\u73b0\u6709\u5173\u4e8e\u591a\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u3001\u6570\u636e\u4e0d\u5bf9\u79f0\u548c\u793e\u4f1a\u6280\u672f\u5371\u5bb3\u7684\u8bc1\u636e\uff0c\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6027\u6cbb\u7406\u6846\u67b6\u3002\u91cd\u70b9\u4e0d\u662f\u5f00\u53d1\u65b0\u7684\u6280\u672f\u57fa\u51c6\uff0c\u800c\u662f\u91cd\u65b0\u6784\u5efa\u591a\u8bed\u8a00AI\u6cbb\u7406\u4f5c\u4e3a\u4e00\u4e2a\u793e\u4f1a\u6587\u5316\u548c\u57fa\u4e8e\u6743\u5229\u7684\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u6cbb\u7406\u6311\u6218\uff1a1) \u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u5b9e\u8df5\u4e2d\u7684\u6587\u5316\u548c\u8bed\u8a00\u4e0d\u5e73\u7b49\uff1b2) \u5168\u7403\u90e8\u7f72\u4e0e\u672c\u5730\u89c4\u8303\u3001\u4ef7\u503c\u89c2\u548c\u6743\u529b\u7ed3\u6784\u4e4b\u95f4\u7684\u9519\u4f4d\uff1b3) \u9488\u5bf9\u8fb9\u7f18\u5316\u8bed\u8a00\u793e\u533a\u6240\u53d7\u5371\u5bb3\u7684\u6709\u9650\u95ee\u8d23\u673a\u5236\u3002\u63d0\u51fa\u4e86\u6570\u636e\u7ba1\u7406\u3001\u900f\u660e\u5ea6\u548c\u53c2\u4e0e\u5f0f\u95ee\u8d23\u7684\u8bbe\u8ba1\u548c\u653f\u7b56\u5efa\u8bae\u3002", "conclusion": "\u6587\u5316\u6839\u57fa\u7684\u6cbb\u7406\u5bf9\u4e8e\u786e\u4fdd\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u4e0d\u4f1a\u5728\u89c4\u6a21\u548c\u4e2d\u6027\u5316\u7684\u5e4c\u5b50\u4e0b\u590d\u5236\u73b0\u6709\u7684\u5168\u7403\u4e0d\u5e73\u7b49\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5c06\u591a\u8bed\u8a00AI\u6cbb\u7406\u91cd\u65b0\u6784\u5efa\u4e3a\u793e\u4f1a\u6587\u5316\u548c\u57fa\u4e8e\u6743\u5229\u7684\u95ee\u9898\uff0c\u800c\u975e\u5355\u7eaf\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2602.00148", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00148", "abs": "https://arxiv.org/abs/2602.00148", "authors": ["Shiqian Li", "Ruihong Shen", "Junfeng Ni", "Chang Pan", "Chi Zhang", "Yixin Zhu"], "title": "Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields", "comment": "43 pages, ICLR 2026", "summary": "Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.", "AI": {"tldr": "NGFF\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u795e\u7ecf\u6846\u67b6\uff0c\u5c063D\u9ad8\u65af\u611f\u77e5\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u52a8\u6001\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u4ece\u591a\u89c6\u89d2RGB\u8f93\u5165\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u7269\u7406\u771f\u5b9e\u76844D\u89c6\u9891\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u9ad8\u65af\u6a21\u62df\u5668\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u89c6\u89c9\u8d28\u91cf\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u89c4\u5f8b\u5efa\u6a21\uff0c\u65e0\u6cd5\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u89c6\u9891\uff1b\u800c\u7ed3\u54083D\u9ad8\u65af\u6e85\u5c04\u548c\u7269\u7406\u5f15\u64ce\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faNeural Gaussian Force Field (NGFF)\u6846\u67b6\uff0c\u96c6\u62103D\u9ad8\u65af\u611f\u77e5\u4e0e\u7269\u7406\u52a8\u6001\u5efa\u6a21\uff1b\u540c\u65f6\u521b\u5efaGSCollision\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc764\u4e07\u6e32\u67d3\u7269\u7406\u89c6\u9891(~4TB)\uff0c\u6db5\u76d6\u591a\u79cd\u6750\u6599\u3001\u591a\u7269\u4f53\u4ea4\u4e92\u548c\u590d\u6742\u573a\u666f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e3D\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cNGFF\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u63a8\u7406\u9c81\u68d2\u6027\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u9ad8\u65af\u6a21\u62df\u5668\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "NGFF\u63a8\u52a8\u4e86\u89c6\u9891\u9884\u6d4b\u5411\u7269\u7406\u57fa\u7840\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4e3a\u4ece\u539f\u59cb\u89c6\u89c9\u6570\u636e\u9884\u6d4b\u7269\u7406\u52a8\u6001\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01707", "categories": ["math.NA", "math.MG"], "pdf": "https://arxiv.org/pdf/2602.01707", "abs": "https://arxiv.org/abs/2602.01707", "authors": ["K R Tyada"], "title": "Curvature Preserving Fractal Interpolation Functions: A Hybrid Geometric Approach", "comment": null, "summary": "Fractal interpolation functions (FIFs) generated using iterated function systems (IFS) provide a powerful framework for modeling self-similar and irregular data, yet traditional constructions often neglect geometric fidelity such as curvature. In this paper, we introduce a curvature-preserving variant of FIFs built upon a classical cubic spline interpolant. We define a curvature-aware iterated function system (IFS) with parameters optimized via a penalty-based approach to minimize deviation from the curvature of the classical spline. Theoretical conditions for interpolation and curvature approximation are derived. We compare the curvature of the proposed FIF with that of the classical cubic spline and discrete data curvature across multiple examples. Our method achieves both data interpolation and shape fidelity, preserving curvature more accurately than standard splines. The approach has potential applications in geometric modeling, computer graphics, and scientific data interpolation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ecf\u5178\u4e09\u6b21\u6837\u6761\u7684\u66f2\u7387\u4fdd\u6301\u5206\u5f62\u63d2\u503c\u51fd\u6570\uff0c\u901a\u8fc7\u60e9\u7f5a\u4f18\u5316\u65b9\u6cd5\u6700\u5c0f\u5316\u4e0e\u7ecf\u5178\u6837\u6761\u66f2\u7387\u504f\u5dee\uff0c\u5b9e\u73b0\u6570\u636e\u63d2\u503c\u548c\u5f62\u72b6\u4fdd\u771f\u3002", "motivation": "\u4f20\u7edf\u5206\u5f62\u63d2\u503c\u51fd\u6570\uff08FIFs\uff09\u5728\u5efa\u6a21\u81ea\u76f8\u4f3c\u548c\u4e0d\u89c4\u5219\u6570\u636e\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u66f2\u7387\u7b49\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u5f62\u72b6\u5931\u771f\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u7ecf\u5178\u4e09\u6b21\u6837\u6761\u7684\u66f2\u7387\u611f\u77e5\u8fed\u4ee3\u51fd\u6570\u7cfb\u7edf\uff08IFS\uff09\uff0c\u901a\u8fc7\u60e9\u7f5a\u4f18\u5316\u65b9\u6cd5\u4f18\u5316\u53c2\u6570\uff0c\u6700\u5c0f\u5316\u4e0e\u7ecf\u5178\u6837\u6761\u66f2\u7387\u7684\u504f\u5dee\uff0c\u5e76\u63a8\u5bfc\u63d2\u503c\u548c\u66f2\u7387\u903c\u8fd1\u7684\u7406\u8bba\u6761\u4ef6\u3002", "result": "\u63d0\u51fa\u7684FIF\u5728\u591a\u4e2a\u793a\u4f8b\u4e2d\u6bd4\u6807\u51c6\u6837\u6761\u66f4\u51c6\u786e\u5730\u4fdd\u6301\u66f2\u7387\uff0c\u540c\u65f6\u5b9e\u73b0\u6570\u636e\u63d2\u503c\u548c\u5f62\u72b6\u4fdd\u771f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u5efa\u6a21\u3001\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u79d1\u5b66\u6570\u636e\u63d2\u503c\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u66f2\u7387\u4fdd\u6301\u7684\u5206\u5f62\u63d2\u503c\u3002"}}
{"id": "2602.00528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00528", "abs": "https://arxiv.org/abs/2602.00528", "authors": ["Minhua Lin", "Enyan Dai", "Hui Liu", "Xianfeng Tang", "Yuliang Yan", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Fali Wang", "Hongcheng Gao", "Chen Luo", "Xiang Zhang", "Qi He", "Suhang Wang"], "title": "How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use", "comment": "Accepted by ICLR 2026", "summary": "As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a \"knowing-doing\" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.", "AI": {"tldr": "LLMs\u5728\u6251\u514b\u6e38\u620f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u542f\u53d1\u5f0f\u4f9d\u8d56\u3001\u4e8b\u5b9e\u8bef\u89e3\u548c\u77e5\u884c\u5dee\u8ddd\u7b49\u95ee\u9898\u3002\u4f5c\u8005\u63d0\u51faToolPoker\u6846\u67b6\uff0c\u7ed3\u5408\u5916\u90e8\u6c42\u89e3\u5668\u5b9e\u73b0GTO\u4e00\u81f4\u7684\u884c\u52a8\u548c\u4e13\u4e1a\u7684\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e38\u620f\u8868\u73b0\u548c\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5173\u952e\u9886\u57df\u5e94\u7528\u7684\u589e\u52a0\uff0c\u5176\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u6251\u514b\u6e38\u620f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e0d\u4ec5\u9700\u8981\u5f3a\u5927\u7684\u884c\u52a8\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u539f\u5219\u6027\u63a8\u7406\u3002", "method": "\u9996\u5148\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u591a\u4e2a\u73b0\u5b9e\u6251\u514b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u63a8\u7406\u8f68\u8ff9\u3002\u7136\u540e\u63d0\u51faToolPoker\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u5916\u90e8\u6c42\u89e3\u5668\u6765\u751f\u6210GTO\u4e00\u81f4\u7684\u884c\u52a8\uff0c\u5e76\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u4e13\u4e1a\u98ce\u683c\u89e3\u91ca\u3002", "result": "LLMs\u65e0\u6cd5\u4e0e\u4f20\u7edf\u7b97\u6cd5\u7ade\u4e89\uff0c\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u7f3a\u9677\uff1a\u542f\u53d1\u5f0f\u4f9d\u8d56\u3001\u4e8b\u5b9e\u8bef\u89e3\u548c\u77e5\u884c\u5dee\u8ddd\u3002ToolPoker\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e38\u620f\u8868\u73b0\uff0c\u4ea7\u751f\u7684\u63a8\u7406\u8f68\u8ff9\u66f4\u8d34\u8fd1\u535a\u5f08\u8bba\u539f\u5219\u3002", "conclusion": "LLMs\u5728\u9700\u8981\u535a\u5f08\u8bba\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u901a\u8fc7\u6574\u5408\u5916\u90e8\u5de5\u5177\uff08\u5982\u6c42\u89e3\u5668\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u8868\u73b0\u548c\u63a8\u7406\u8d28\u91cf\u3002ToolPoker\u4e3aLLMs\u5728\u9700\u8981\u6218\u7565\u63a8\u7406\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00116", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00116", "abs": "https://arxiv.org/abs/2602.00116", "authors": ["Hanne Dejonghe", "Sam Leroux"], "title": "THDC: Training Hyperdimensional Computing Models with Backpropagation", "comment": "Accepted to ESANN 2026", "summary": "Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.", "AI": {"tldr": "THDC\u63d0\u51fa\u53ef\u8bad\u7ec3\u7684\u8d85\u7ef4\u8ba1\u7b97\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5c06\u7ef4\u5ea6\u4ece10,000\u964d\u81f364\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8aSOTA HDC\u7cbe\u5ea6", "motivation": "\u4f20\u7edfHDC\u4f9d\u8d56\u8d85\u9ad8\u7ef4\u5ea6\u548c\u9759\u6001\u968f\u673a\u521d\u59cb\u5316\u8d85\u5411\u91cf\uff0c\u9650\u5236\u4e86\u5185\u5b58\u6548\u7387\u548c\u5b66\u4e60\u80fd\u529b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684HDC\u65b9\u6cd5", "method": "THDC\u7528\u53ef\u8bad\u7ec3\u5d4c\u5165\u66ff\u4ee3\u968f\u673a\u521d\u59cb\u5316\u5411\u91cf\uff0c\u5f15\u5165\u5355\u5c42\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7c7b\u522b\u8868\u793a\uff0c\u652f\u6301\u7aef\u5230\u7aef\u53cd\u5411\u4f20\u64ad\u5b66\u4e60", "result": "\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR-10\u4e0a\uff0cTHDC\u8fbe\u5230\u6216\u4f18\u4e8eSOTA HDC\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u7ef4\u5ea6\u4ece10,000\u5927\u5e45\u964d\u81f364", "conclusion": "THDC\u663e\u8457\u63d0\u5347\u4e86HDC\u7684\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u80fd\u91cf\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u8f7b\u91cf\u7ea7\u5b66\u4e60\u65b9\u6848"}}
{"id": "2602.00543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00543", "abs": "https://arxiv.org/abs/2602.00543", "authors": ["Seho Pyo", "Jiheon Seok", "Jaejin Lee"], "title": "Reasoning by Commented Code for Table Question Answering", "comment": null, "summary": "Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\\% accuracy on the WikiTableQuestions benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u6ce8\u91ca\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8868\u683c\u95ee\u7b54\u5206\u89e3\u4e3a\u591a\u884c\u53ef\u6267\u884c\u7a0b\u5e8f\u5e76\u6dfb\u52a0\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u63d0\u9ad8\u6570\u503c\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u7ebf\u6027\u5316\u65b9\u6cd5\u7834\u574f\u4e86\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e8c\u7ef4\u5173\u7cfb\uff0c\u73b0\u6709\u57fa\u4e8e\u7aef\u5230\u7aef\u7b54\u6848\u751f\u6210\u6216\u5355\u884c\u7a0b\u5e8f\u67e5\u8be2\u7684\u65b9\u6cd5\u5b58\u5728\u6570\u503c\u51c6\u786e\u6027\u6709\u9650\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5e26\u6ce8\u91ca\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5c06\u8868\u683c\u95ee\u7b54\u63a8\u7406\u5206\u89e3\u4e3a\u591a\u884c\u53ef\u6267\u884cPython\u7a0b\u5e8f\uff0c\u6bcf\u884c\u4ee3\u7801\u914d\u6709\u7b80\u6d01\u7684\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u660e\u786e\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728WikiTableQuestions\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Qwen2.5-Coder-7B-Instruct\u8fbe\u523070.9%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eRepanda\u57fa\u7ebf\uff0867.6%\uff09\u3002\u4e0e\u5f3a\u5927\u7684\u7aef\u5230\u7aef\u8868\u683c\u95ee\u7b54\u6a21\u578b\u7ed3\u5408\u540e\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b54\u6848\u9009\u62e9\u673a\u5236\u8fdb\u4e00\u6b65\u63d0\u5347\u81f384.3%\u51c6\u786e\u7387\u3002", "conclusion": "\u5e26\u6ce8\u91ca\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8868\u683c\u95ee\u7b54\u7684\u6570\u503c\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u73b0\u6709\u6a21\u578b\u7ed3\u5408\u53ef\u8fdb\u4e00\u6b65\u6539\u5584\u6027\u80fd\uff0c\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00149", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00149", "abs": "https://arxiv.org/abs/2602.00149", "authors": ["Shucong Li", "Xiaoluo Zhou", "Yuqian He", "Zhenyu Liu"], "title": "SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles", "comment": null, "summary": "3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.", "AI": {"tldr": "SDCM\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u5bc6\u5ea6\u5316\u3001\u96f7\u8fbe\u8865\u507f\u6620\u5c04\u548cMamba\u5efa\u6a21\u4ea4\u4e92\u878d\u5408\uff0c\u89e3\u51b3\u4e864D\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u548c\u89c6\u89c9\u6570\u636e\u9000\u5316\u95ee\u9898\uff0c\u57283D\u76ee\u6807\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b34D\u96f7\u8fbe\u70b9\u4e91\u7a00\u758f\u5bfc\u81f4\u76843D\u8868\u793a\u4e0d\u4f73\u95ee\u9898\uff0c\u4ee5\u53ca\u89c6\u89c9\u6570\u636e\u5728\u4f4e\u5149\u7167\u3001\u8fdc\u8ddd\u79bb\u548c\u5bc6\u96c6\u906e\u6321\u573a\u666f\u4e0b\u7684\u8868\u793a\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u4e9b\u90fd\u4f1a\u5f71\u54cd\u96f7\u8fbe-\u89c6\u89c9\u878d\u5408\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faSDCM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) SimDen\u6a21\u5757\u901a\u8fc73D\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u66f2\u7387\u6a21\u62df\u751f\u6210\u5bc6\u96c6\u96f7\u8fbe\u70b9\u4e91\uff1b2) RCM\u6a21\u5757\u5229\u7528\u96f7\u8fbe\u7684\u5168\u5929\u5019\u7279\u6027\u8865\u507f\u89c6\u89c9\u6570\u636e\u9000\u5316\uff1b3) MMIF\u6a21\u5757\u901a\u8fc7Mamba\u5efa\u6a21\u63d0\u53d6\u7279\u5f81\u5f20\u91cf\u5dee\u5f02\u4fe1\u606f\uff0c\u51cf\u5c11\u5f02\u8d28\u6027\u5e76\u5b9e\u73b0\u4ea4\u4e92\u878d\u5408\u3002", "result": "\u5728VoD\u3001TJ4DRadSet\u548cAstyx HiRes 2019\u6570\u636e\u96c6\u4e0a\uff0cSDCM\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u66f4\u5c11\u7684\u53c2\u6570\u6570\u91cf\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SDCM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e864D\u96f7\u8fbe-\u89c6\u89c9\u878d\u5408\u4e2d\u7684\u70b9\u4e91\u7a00\u758f\u548c\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u8f66\u8054\u7f51\u4e2d\u76843D\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01721", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01721", "abs": "https://arxiv.org/abs/2602.01721", "authors": ["Vladimir R. Kostic", "Dragana Lj. Cvetkovic", "Ljiljana Cvetkovic"], "title": "Scalable Pseudospectral Analysis via Low-Rank Approximations of Dynamical Systems", "comment": "19 pages, 4 figures", "summary": "Pseudospectral analysis is fundamental for quantifying the sensitivity and transient behavior of nonnormal matrices, yet its computational cost scales cubically with dimension, rendering it prohibitive for large-scale systems. While existing research on scalable pseudospectral computation has focused on exploiting sparsity structures, common in discretizations of differential operators, these approaches are ill-suited for machine learning and data-driven dynamical systems, where operators are typically dense but approximately low-rank. In this paper, we develop a comprehensive low-rank framework that dramatically reduces this computational burden. Our core theoretical contribution is an exact characterization of the pseudospectrum of arbitrary low-rank matrices, reducing the evaluation of resolvent norms to eigenvalue problems of dimension proportional to the rank. Building on this foundation, we derive rigorous inclusion sets for the pseudospectra of general matrices via truncated and randomized low-rank approximations, with explicit perturbation bounds. These results enable efficient estimators for key stability quantities, including distance to instability and Kreiss constants, at a cost that scales with the effective rank rather than the ambient dimension. We further demonstrate how our framework naturally extends to data-driven settings, providing pseudospectral analysis of transfer operators learned from nonlinear and stochastic dynamical systems. Numerical experiments confirm orders-of-magnitude speedups while preserving accuracy, opening pseudospectral analysis to previously intractable high-dimensional problems in computational PDEs, control theory, and data-driven dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u79e9\u6846\u67b6\u9ad8\u6548\u8ba1\u7b97\u5927\u89c4\u6a21\u975e\u6b63\u89c4\u77e9\u9635\u7684\u4f2a\u8c31\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u7acb\u65b9\u964d\u4f4e\u5230\u4e0e\u6709\u6548\u79e9\u6210\u6b63\u6bd4\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7b49\u5bc6\u96c6\u4f46\u8fd1\u4f3c\u4f4e\u79e9\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u4f2a\u8c31\u5206\u6790\u8ba1\u7b97\u6210\u672c\u968f\u7ef4\u5ea6\u7acb\u65b9\u589e\u957f\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7ed3\u6784\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u7684\u7b97\u5b50\u901a\u5e38\u662f\u5bc6\u96c6\u4f46\u8fd1\u4f3c\u4f4e\u79e9\u7684\uff0c\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u5168\u9762\u7684\u4f4e\u79e9\u6846\u67b6\uff1a1) \u7cbe\u786e\u523b\u753b\u4efb\u610f\u4f4e\u79e9\u77e9\u9635\u7684\u4f2a\u8c31\uff0c\u5c06\u9884\u89e3\u8303\u6570\u8ba1\u7b97\u8f6c\u5316\u4e3a\u4e0e\u79e9\u6210\u6b63\u6bd4\u7684\u7ef4\u5ea6\u7279\u5f81\u503c\u95ee\u9898\uff1b2) \u901a\u8fc7\u622a\u65ad\u548c\u968f\u673a\u4f4e\u79e9\u8fd1\u4f3c\u63a8\u5bfc\u4e00\u822c\u77e9\u9635\u4f2a\u8c31\u7684\u4e25\u683c\u5305\u542b\u96c6\uff1b3) \u63d0\u4f9b\u663e\u5f0f\u6270\u52a8\u8fb9\u754c\u3002", "result": "\u5b9e\u73b0\u6570\u91cf\u7ea7\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\uff0c\u80fd\u591f\u9ad8\u6548\u4f30\u8ba1\u5173\u952e\u7a33\u5b9a\u6027\u6307\u6807\uff08\u5982\u4e0d\u7a33\u5b9a\u8ddd\u79bb\u548cKreiss\u5e38\u6570\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u4e0e\u6709\u6548\u79e9\u800c\u975e\u73af\u5883\u7ef4\u5ea6\u6210\u6b63\u6bd4\uff0c\u6269\u5c55\u4e86\u4f2a\u8c31\u5206\u6790\u5230\u6570\u636e\u9a71\u52a8\u573a\u666f\u3002", "conclusion": "\u8be5\u4f4e\u79e9\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u4f2a\u8c31\u5206\u6790\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u7528\u4e8e\u4e4b\u524d\u96be\u4ee5\u5904\u7406\u7684\u9ad8\u7ef4\u95ee\u9898\uff0c\u5305\u62ec\u8ba1\u7b97PDE\u3001\u63a7\u5236\u7406\u8bba\u548c\u6570\u636e\u9a71\u52a8\u52a8\u529b\u5b66\u7b49\u9886\u57df\u3002"}}
{"id": "2602.00561", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00561", "abs": "https://arxiv.org/abs/2602.00561", "authors": ["Tianhao Huang", "Guanghui Min", "Zhenyu Lei", "Aiying Zhang", "Chen Chen"], "title": "Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing", "comment": null, "summary": "Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.", "AI": {"tldr": "AFR-Net\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u901a\u4fe1\u52a8\u529b\u5b66\u7684\u7269\u7406\u4fe1\u606f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6d41\u8def\u7531\u7f51\u7edc\u5efa\u6a21\u7ed3\u6784\u8fde\u63a5\u5982\u4f55\u4ea7\u751f\u529f\u80fd\u901a\u4fe1\u6a21\u5f0f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9SC\u548cFC\u4e4b\u95f4\u6f5c\u5728\u795e\u7ecf\u533a\u57df\u4ea4\u4e92\u7684\u57fa\u672c\u795e\u7ecf\u79d1\u5b66\u6d1e\u5bdf\uff0c\u65e0\u6cd5\u89e3\u91ca\u4e3a\u4ec0\u4e48SC\u548cFC\u8868\u73b0\u51fa\u8026\u5408\u548c\u5f02\u8d28\u6027\u7684\u52a8\u6001\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6d41\u8def\u7531\u7f51\u7edc\uff08AFR-Net\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u901a\u4fe1\u52a8\u529b\u5b66\u7684\u89c6\u89d2\u5efa\u6a21\u7ed3\u6784\u7ea6\u675f\u5982\u4f55\u4ea7\u751f\u529f\u80fd\u901a\u4fe1\u6a21\u5f0f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eAFR-Net\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u5173\u952e\u795e\u7ecf\u901a\u8def\u3002", "conclusion": "AFR-Net\u901a\u8fc7\u795e\u7ecf\u901a\u4fe1\u52a8\u529b\u5b66\u89c6\u89d2\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\uff0c\u4e3a\u7406\u89e3\u5b8f\u89c2\u8ba4\u77e5\u8868\u578b\u5982\u4f55\u4ece\u5fae\u89c2\u795e\u7ecf\u5143\u8fde\u63a5\u6027\u4e2d\u6d8c\u73b0\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2602.00120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00120", "abs": "https://arxiv.org/abs/2602.00120", "authors": ["Xianghong Hu", "Tianning Xu", "Ying Chen", "Shuai Wang"], "title": "Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control", "comment": "12 pages, 4 figures. An extended and pedagogical version will appear as a book chapter", "summary": "Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u5173\u6ce8\u6570\u636e\u6cc4\u9732\u63a7\u5236\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\uff0c\u53d1\u73b0AutoGluon AutoML\u65b9\u6cd5\u5728\u4e25\u683c\u7684\u65f6\u95f4\u5206\u5272\u548c\u7279\u5f81\u9009\u62e9\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u662f\u91d1\u878d\u98ce\u9669\u7ba1\u7406\u7684\u6838\u5fc3\u4efb\u52a1\uff0c\u4f46\u73b0\u5b9e\u6570\u636e\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8fdd\u7ea6\u6807\u7b7e\u6a21\u7cca\u6027\u3001\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4ee5\u53ca\u65f6\u95f4\u7ed3\u6784\u548c\u4e8b\u540e\u53d8\u91cf\u5bfc\u81f4\u7684\u4fe1\u606f\u6cc4\u9732\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u635f\u5bb3\u8bc4\u4f30\u6709\u6548\u6027\u548c\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u6cc4\u6f0f\u611f\u77e5\u7684\u7279\u5f81\u9009\u62e9\u3001\u4e25\u683c\u7684\u65f6\u95f4\u5206\u5272\uff08\u7ea6\u675f\u8d37\u6b3e\u53d1\u653e\u548c\u62a5\u544a\u671f\u95f4\uff09\u3001\u4ee5\u53ca\u63a7\u5236\u6027\u7684\u591a\u6570\u7c7b\u4e0b\u91c7\u6837\u3002\u6bd4\u8f83\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8AutoGluon AutoML\u6846\u67b6\u3002", "result": "\u5728\u4e0d\u540c\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u4e0b\uff0c\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0cAutoGluon AutoML\u5728\u8bc4\u4f30\u7684\u6240\u6709\u6a21\u578b\u4e2d\u83b7\u5f97\u4e86\u6700\u5f3a\u7684AUROC\uff08\u66f2\u7ebf\u4e0b\u9762\u79ef\uff09\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u4e25\u683c\u7684\u6cc4\u6f0f\u63a7\u5236\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u62b5\u62bc\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\uff0cAutoML\u65b9\u6cd5\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u8be5\u7814\u7a76\u7684\u6269\u5c55\u6559\u5b66\u7248\u672c\u5c06\u4f5c\u4e3a\u4e66\u7c4d\u7ae0\u8282\u53d1\u8868\u3002"}}
{"id": "2602.00554", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00554", "abs": "https://arxiv.org/abs/2602.00554", "authors": ["Liu Kaipeng", "Wu Ling"], "title": "A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora", "comment": "14 pages, 5 figures", "summary": "This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.", "AI": {"tldr": "BERT\u6a21\u578b\u5bf9\u56db\u79cd\u57fa\u672c\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\u7684\u5904\u7406\u5448\u73b0\u5c42\u7ea7\u8868\u5f81\u7ed3\u6784\uff1a\u65e9\u671f\u5c42\u51fa\u73b0\u6784\u5f0f\u7279\u5b9a\u4fe1\u606f\uff0c\u4e2d\u95f4\u5c42\u5f62\u6210\u6700\u5927\u53ef\u5206\u79bb\u805a\u7c7b\uff0c\u540e\u671f\u5c42\u7ef4\u6301\u8fd9\u4e9b\u8868\u5f81", "motivation": "\u7814\u7a76BERT\u6a21\u578b\u5982\u4f55\u5904\u7406\u56db\u79cd\u57fa\u672c\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\uff0c\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u5bf9\u8fd9\u4e9b\u8bed\u8a00\u7ed3\u6784\u7684\u8868\u5f81\u673a\u5236", "method": "\u91c7\u7528\u591a\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ecMDS\u548ct-SNE\u964d\u7ef4\u3001\u5e7f\u4e49\u5224\u522b\u503c(GDV)\u4f5c\u4e3a\u805a\u7c7b\u5206\u79bb\u6307\u6807\u3001\u8d39\u820d\u5c14\u5224\u522b\u6bd4(FDR)\u4f5c\u4e3a\u7ebf\u6027\u8bca\u65ad\u63a2\u6d4b\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790", "result": "\u53d1\u73b0\u5c42\u7ea7\u8868\u5f81\u7ed3\u6784\uff1a\u6784\u5f0f\u7279\u5b9a\u4fe1\u606f\u5728\u65e9\u671f\u5c42\u51fa\u73b0\uff0c\u5728\u4e2d\u95f4\u5c42\u5f62\u6210\u6700\u5927\u53ef\u5206\u79bb\u805a\u7c7b\uff0c\u5e76\u5728\u540e\u671f\u5904\u7406\u9636\u6bb5\u5f97\u4ee5\u7ef4\u6301", "conclusion": "BERT\u6a21\u578b\u5bf9\u8bba\u5143\u7ed3\u6784\u6784\u5f0f\u7684\u5904\u7406\u5448\u73b0\u7cfb\u7edf\u6027\u7684\u5c42\u7ea7\u8868\u5f81\u6a21\u5f0f\uff0c\u6784\u5f0f\u4fe1\u606f\u5728\u6a21\u578b\u7684\u4e0d\u540c\u5904\u7406\u9636\u6bb5\u6709\u7279\u5b9a\u7684\u8868\u5f81\u8f68\u8ff9"}}
{"id": "2602.00151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00151", "abs": "https://arxiv.org/abs/2602.00151", "authors": ["Alexander Blezinger", "Wolfgang Nejdl", "Ming Tang"], "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency", "comment": "9 pages, 7 figures and 5 tables. Initialy submitted for IJCAI 2026", "summary": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7528\u4e8e\u9884\u6d4b\u540c\u6e90\u91cd\u7ec4\u7f3a\u9677\uff08HRD\uff09\u8bc4\u5206\uff0c\u7ed3\u679c\u663e\u793a\u57fa\u7840\u6a21\u578b\u7279\u5f81\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u57fa\u7ebf\uff0c\u5e76\u63d0\u51fa\u5206\u5e03\u4e0a\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u76ee\u6807\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u5404\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u56de\u5f52\u6027\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u65b9\u9762\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u9884\u6d4bHRD\u8bc4\u5206\u8fd9\u4e00\u5bf9\u4e2a\u6027\u5316\u764c\u75c7\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u5168\u5207\u7247\u56fe\u50cf\u4e2d\u63d0\u53d6\u8865\u4e01\u7ea7\u7279\u5f81\uff0c\u4f7f\u7528\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u7279\u5f81\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u6765\u81ea\u4e24\u4e2a\u516c\u5171\u533b\u5b66\u6570\u636e\u96c6\u7684\u4e73\u817a\u764c\u3001\u5b50\u5bab\u5185\u819c\u764c\u548c\u80ba\u764c\u961f\u5217\u4e2d\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u8fde\u7eedHRD\u8bc4\u5206\u3002\u63d0\u51fa\u4e86\u5206\u5e03\u4e0a\u91c7\u6837\u7b56\u7565\u7f13\u89e3\u76ee\u6807\u4e0d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u63a2\u8ba8\u4e0d\u540c\u91c7\u6837\u7b56\u7565\u548c\u5b9e\u4f8b\u5305\u5927\u5c0f\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002\u63d0\u51fa\u7684\u5206\u5e03\u4e0a\u91c7\u6837\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u8868\u6027\u4e0d\u8db3\u4f46\u4e34\u5e8a\u91cd\u8981\u60a3\u8005\u7fa4\u4f53\u7684\u53ec\u56de\u7387\u548c\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u5927\u89c4\u6a21\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u53ef\u8f6c\u79fb\u7684\u56de\u5f52\u6027\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u63a8\u52a8AI\u9a71\u52a8\u7684\u7cbe\u51c6\u80bf\u7624\u5b66\u65b9\u9762\u7684\u6f5c\u529b\u3002\u57fa\u7840\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u4e34\u5e8a\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2602.01876", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01876", "abs": "https://arxiv.org/abs/2602.01876", "authors": ["Zijuan Xin", "Chenyao Wang", "Feng Shi", "Yizhong Sun"], "title": "PINN-Based Kolmogorov-Arnold Networks with RAR-D Adaptive Sampling for Solving Elliptic Interface Problems", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have become a popular and powerful framework for solving partial differential equations (PDEs), leveraging neural networks to approximate solutions while embedding PDE constraints, boundary conditions, and interface jump conditions directly into the loss function. However, most existing PINN approaches are based on multilayer perceptrons (MLPs), which may require large network sizes and extensive training to achieve high accuracy, especially for complex interface problems. In this work, we propose a novel PINN architecture based on Kolmogorov-Arnold Networks (KANs), which offer greater flexibility in choosing activation functions and can represent functions with fewer parameters. Specifically, we introduce a dual KANs structure that couples two KANs across subdomains and explicitly enforces interface conditions. To further boost training efficiency and convergence, we integrate the RAR-D adaptive sampling strategy to dynamically refine training points. Numerical experiments on the elliptic interface problems yield more uniform error distributions across the computational domain, which demonstrates that our PINN-based KANs achieve superior accuracy with significantly smaller network sizes and faster convergence compared to standard PINNs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eKolmogorov-Arnold Networks (KANs)\u7684\u65b0\u578bPINN\u67b6\u6784\uff0c\u7528\u4e8e\u6c42\u89e3\u692d\u5706\u754c\u9762\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edfMLP-PINNs\u5177\u6709\u66f4\u5c0f\u7684\u7f51\u7edc\u89c4\u6a21\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfPINN\u4e3b\u8981\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u673a(MLPs)\uff0c\u5bf9\u4e8e\u590d\u6742\u754c\u9762\u95ee\u9898\u9700\u8981\u5927\u89c4\u6a21\u7f51\u7edc\u548c\u5927\u91cf\u8bad\u7ec3\u624d\u80fd\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002KANs\u5728\u6fc0\u6d3b\u51fd\u6570\u9009\u62e9\u4e0a\u66f4\u7075\u6d3b\uff0c\u80fd\u7528\u66f4\u5c11\u53c2\u6570\u8868\u793a\u51fd\u6570\uff0c\u4e3a\u89e3\u51b3\u754c\u9762\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8eKANs\u7684PINN\u67b6\u6784\uff1b2) \u8bbe\u8ba1\u53ccKANs\u7ed3\u6784\uff0c\u5728\u5b50\u57df\u95f4\u8026\u5408\u4e24\u4e2aKANs\u5e76\u663e\u5f0f\u5f3a\u5236\u754c\u9762\u6761\u4ef6\uff1b3) \u96c6\u6210RAR-D\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u52a8\u6001\u4f18\u5316\u8bad\u7ec3\u70b9\u5206\u5e03\u3002", "result": "\u5728\u692d\u5706\u754c\u9762\u95ee\u9898\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6574\u4e2a\u8ba1\u7b97\u57df\u4e0a\u4ea7\u751f\u66f4\u5747\u5300\u7684\u8bef\u5dee\u5206\u5e03\uff0c\u76f8\u6bd4\u6807\u51c6PINNs\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u7f51\u7edc\u89c4\u6a21\u663e\u8457\u51cf\u5c0f\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u57fa\u4e8eKANs\u7684PINN\u67b6\u6784\u4e3a\u89e3\u51b3\u754c\u9762\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6848\uff0c\u901a\u8fc7\u53ccKANs\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2602.00564", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00564", "abs": "https://arxiv.org/abs/2602.00564", "authors": ["Xiang Zheng", "Weiqi Zhai", "Wei Wang", "Boyu Yang", "Wenbo Li", "Ruixiang Luo", "Haoxiang Sun", "Yucheng Wang", "Zhengze Li", "Meng Wang", "Yuetian Du", "Guojie Lin", "Yaxuan Wang", "Xiaoxiao Xu", "Yanhu Mo", "Xuan Ren", "Hu Wei", "Ze Xu"], "title": "Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs", "comment": "8 pages, and 3 figures", "summary": "Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReasoningMath-Plus\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b150\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u65e8\u5728\u8bc4\u4f30LLMs\u7684\u7ed3\u6784\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f15\u5165HCRS\u8bc4\u5206\u65b9\u6cd5\u548cPRM\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLMs\u5df2\u8fbe\u5230\u63a5\u8fd1\u9971\u548c\u7684\u51c6\u786e\u7387\uff0c\u4f46\u8fd9\u4e3b\u8981\u6e90\u4e8e\u6570\u636e\u96c6\u4e2d\u7684\u6a21\u677f\u5316\u8ba1\u7b97\u548c\u6d45\u5c42\u7b97\u672f\u5206\u89e3\uff0c\u672a\u80fd\u771f\u6b63\u8bc4\u4f30\u591a\u7ea6\u675f\u534f\u8c03\u3001\u6784\u9020\u6027\u903b\u8f91\u5408\u6210\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002", "method": "1) \u6784\u5efaReasoningMath-Plus\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b150\u4e2a\u5f3a\u8c03\u4ea4\u4e92\u7ea6\u675f\u3001\u6784\u9020\u6027\u89e3\u51b3\u65b9\u6848\u548c\u975e\u5e73\u51e1\u7ed3\u6784\u6d1e\u5bdf\u7684\u95ee\u9898\uff1b2) \u5f15\u5165HCRS\uff08\u5371\u9669\u611f\u77e5\u94fe\u5f0f\u89c4\u5219\u8bc4\u5206\uff09\u786e\u5b9a\u6027\u6b65\u9aa4\u7ea7\u8bc4\u5206\u51fd\u6570\uff1b3) \u5728\u6807\u6ce8\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u3002", "result": "\u9886\u5148\u6a21\u578b\u5728\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4e0a\u53ef\u8fbe5.8/10\uff0c\u4f46\u57fa\u4e8eHCRS\u7684\u6574\u4f53\u8bc4\u4f30\u5f97\u5206\u663e\u8457\u8f83\u4f4e\uff08\u5e73\u57474.36/10\uff0c\u6700\u4f735.14/10\uff09\uff0c\u8868\u660e\u4ec5\u57fa\u4e8e\u7b54\u6848\u7684\u6307\u6807\u4f1a\u9ad8\u4f30\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7b54\u6848\u51c6\u786e\u7387\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u6765\u771f\u6b63\u8861\u91cfLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0cReasoningMath-Plus\u57fa\u51c6\u6d4b\u8bd5\u548cHCRS\u8bc4\u5206\u65b9\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.00125", "categories": ["cs.LG", "cs.AI", "cs.MS"], "pdf": "https://arxiv.org/pdf/2602.00125", "abs": "https://arxiv.org/abs/2602.00125", "authors": ["Soumyadip Sarkar"], "title": "MiniTensor: A Lightweight, High-Performance Tensor Operations Library", "comment": null, "summary": "We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor", "AI": {"tldr": "MiniTensor\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5f20\u91cf\u8fd0\u7b97\u5e93\uff0c\u4e13\u6ce8\u4e8e\u7b80\u6d01\u6027\u3001\u6b63\u786e\u6027\u548c\u6027\u80fd\uff0c\u63d0\u4f9b\u7c7b\u4f3cPyTorch\u7684Python API\uff0c\u5e95\u5c42\u7528Rust\u5f15\u64ce\u6267\u884c\uff0c\u5305\u5927\u5c0f\u4ec5\u51e0MB\uff0c\u6bd4\u4e3b\u6d41\u6846\u67b6\u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5982PyTorch\u548cTensorFlow\u5b89\u88c5\u5305\u4f53\u79ef\u5e9e\u5927\uff0cMiniTensor\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6781\u7b80\u4f46\u529f\u80fd\u5b8c\u6574\u7684\u5f20\u91cf\u8fd0\u7b97\u5e93\uff0c\u4e13\u6ce8\u4e8eCPU\u4e0a\u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002", "method": "\u91c7\u7528Python API\u4e0eRust\u5f15\u64ce\u7ed3\u5408\u7684\u67b6\u6784\uff0c\u901a\u8fc7PyO3\u5b9e\u73b0Python\u96c6\u6210\uff0c\u652f\u6301\u52a8\u6001\u8ba1\u7b97\u56fe\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u63d0\u4f9b\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u548c\u7d27\u51d1\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u53ca\u4f18\u5316\u5668\u3002", "result": "MiniTensor\u5b9e\u73b0\u4e86\u4ec5\u51e0MB\u7684\u5b89\u88c5\u5305\u5927\u5c0f\uff0c\u6bd4PyTorch\u548cTensorFlow\u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u7559\u4e86CPU\u4e0a\u7814\u7a76\u548c\u5f00\u53d1\u6240\u9700\u7684\u6838\u5fc3\u529f\u80fd\u3002", "conclusion": "MiniTensor\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u6781\u7b80\u4f46\u529f\u80fd\u5b8c\u6574\u7684\u5f20\u91cf\u8fd0\u7b97\u5e93\uff0c\u4e3aCPU\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.00588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00588", "abs": "https://arxiv.org/abs/2602.00588", "authors": ["Thiago Dumont Oliveira"], "title": "The French Drama Revolution: Political Economy and Literary Production, 1700-1900", "comment": null, "summary": "This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.", "AI": {"tldr": "\u4f7f\u7528LDA\u548cJSD\u5206\u67901700-1900\u5e74\u6cd5\u56fd\u620f\u5267\u4e3b\u9898\u6f14\u53d8\uff0c\u53d1\u73b0\u6cd5\u56fd\u5927\u9769\u547d\u540e\u620f\u5267\u4e3b\u9898\u5206\u5e03\u53d1\u751f\u6df1\u523b\u53d8\u5316\uff0c\u8d44\u4ea7\u9636\u7ea7\u4e3b\u9898\u572818\u4e16\u7eaa\u672b\u5f00\u59cb\u76db\u884c\uff0c\u5e76\u4e0e\u6cd5\u56fdGDP\u589e\u957f\u5448\u73b0\u534f\u540c\u6f14\u5316\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u6cd5\u56fd\u620f\u5267\u57281700-1900\u5e74\u95f4\u7684\u6f14\u53d8\uff0c\u7279\u522b\u662f\u6cd5\u56fd\u5927\u9769\u547d\u548c\u5de5\u4e1a\u5316\u5bf9\u620f\u5267\u4e3b\u9898\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u620f\u5267\u4e3b\u9898\u53d8\u5316\u4e0e\u7ecf\u6d4e\u53d1\u5c55\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\uff08LDA\uff09\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u7ed3\u5408\u8a79\u68ee-\u9999\u519c\u6563\u5ea6\uff08JSD\uff09\u5206\u6790\u4e3b\u9898\u5206\u5e03\u53d8\u5316\uff0c\u5e76\u5c06\u4e3b\u9898\u5e74\u5ea6\u6d41\u884c\u5ea6\u4e0e\u6cd5\u56fdGDP\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6cd5\u56fd\u620f\u5267\u4e3b\u9898\u5206\u5e03\u57281789-1850\u5e74\u95f4\u53d1\u751f\u6df1\u523b\u53d8\u5316\uff0c\u8d44\u4ea7\u9636\u7ea7\u4e3b\u9898\u81ea18\u4e16\u7eaa\u672b\u5f00\u59cb\u6210\u4e3a\u6700\u6d41\u884c\u4e3b\u9898\u4e4b\u4e00\uff0c\u620f\u5267\u4e3b\u9898\u6f14\u53d8\u4e0e\u6cd5\u56fdGDP\u589e\u957f\u5448\u73b0\u534f\u540c\u6f14\u5316\u6a21\u5f0f\u3002", "conclusion": "\u6cd5\u56fd\u5927\u9769\u547d\u548c\u5de5\u4e1a\u5316\u8fdb\u7a0b\u6df1\u523b\u5f71\u54cd\u4e86\u620f\u5267\u4e3b\u9898\u6f14\u53d8\uff0c\u620f\u5267\u4e3b\u9898\u53d8\u5316\u53cd\u6620\u4e86\u793e\u4f1a\u7ecf\u6d4e\u7ed3\u6784\u7684\u8f6c\u578b\uff0c\u6587\u5316\u751f\u4ea7\u4e0e\u7ecf\u6d4e\u53d1\u5c55\u4e4b\u95f4\u5b58\u5728\u5bc6\u5207\u7684\u4e92\u52a8\u5173\u7cfb\u3002"}}
{"id": "2602.00152", "categories": ["cs.CV", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00152", "abs": "https://arxiv.org/abs/2602.00152", "authors": ["Boyu Li", "Kuangji Zuo", "Lincong Li", "Yonghui Wu"], "title": "Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion", "comment": "24 pages, 6 figures. The manusrcipt is under review at Measurement", "summary": "The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.", "AI": {"tldr": "HPPI-Net\uff1a\u57fa\u4e8e\u591a\u5149\u8c31\u878d\u5408\u548c\u53ef\u89e3\u91ca\u6a21\u5757\u7684\u8d44\u6e90\u611f\u77e5\u5206\u5c42\u7f51\u7edc\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bbe\u5907\u7aef\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff0c\u5728ARM Cortex-M4\u4e0a\u5b9e\u73b096.70%\u51c6\u786e\u7387\uff0c\u4ec5\u5360\u752822.3 KiB RAM\u548c439.5 KiB ROM\u3002", "motivation": "\u8fb9\u7f18\u5e94\u7528\u4e2d\u9700\u8981\u7cbe\u786e\u7684\u8bbe\u5907\u7aef\u6a21\u5f0f\u8bc6\u522b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u7ea6\u675f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u9700\u8981\u4e00\u79cd\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u3002", "method": "\u63d0\u51faHPPI-Net\uff0c\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u7b2c\u4e00\u5c42\u4f7f\u7528FFT\u9891\u8c31\u56fe\u63d0\u53d6\u521d\u6b65\u7279\u5f81\uff1b\u7b2c\u4e8c\u5c42\u6839\u636e\u6d3b\u52a8\u7c7b\u578b\u9009\u62e9\u4e13\u7528\u6a21\u5757\uff08\u9759\u6001\u6d3b\u52a8\uff09\u6216\u5e76\u884cLSTM-MobileNet\u7f51\u7edc\uff08\u52a8\u6001\u72b6\u6001\uff09\u3002PLMN\u878d\u5408FFT\u3001\u5c0f\u6ce2\u548cGabor\u4e09\u79cd\u9891\u8c31\u56fe\uff0c\u901a\u8fc7\u4e09\u4e2a\u5e76\u884cLSTM\u7f16\u7801\u5668\u5904\u7406\uff0c\u4f7f\u7528ECA\u548cDSC\u4f18\u5316\u7279\u5f81\uff0c\u63d0\u4f9b\u901a\u9053\u7ea7\u53ef\u89e3\u91ca\u6027\u5e76\u5927\u5e45\u51cf\u5c11\u4e58\u52a0\u8fd0\u7b97\u3002", "result": "\u5728ARM Cortex-M4\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b096.70%\u51c6\u786e\u7387\uff0c\u4ec5\u5360\u752822.3 KiB RAM\u548c439.5 KiB ROM\u3002\u76f8\u6bd4MobileNetV3\uff0c\u51c6\u786e\u7387\u63d0\u53471.22%\uff0cRAM\u4f7f\u7528\u51cf\u5c1171.2%\uff0cROM\u4f7f\u7528\u51cf\u5c1142.1%\u3002", "conclusion": "HPPI-Net\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\u3001\u5de5\u4e1a\u548c\u667a\u80fd\u5bb6\u5c45\u7b49\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684HAR\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01883", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.01883", "abs": "https://arxiv.org/abs/2602.01883", "authors": ["Tao Luo", "Jianyuan Yin", "Lei Zhang", "Shixue Zhang"], "title": "Convergence of high-index saddle dynamics for degenerate saddle points on critical manifolds", "comment": null, "summary": "The high-index saddle dynamics (HiSD) method provides a powerful framework for finding saddle points and constructing solution landscapes. While originally derived for nondegenerate critical points, HiSD has demonstrated empirical success in degenerate cases, where the Hessian matrix exhibits zero eigenvalues. However, the mathematical and numerical analysis of HiSD for degenerate saddle points remains unexplored. In this paper, utilizing Morse-Bott functions, we present a rigorous analysis of HiSD for computing degenerate saddle points on a critical manifold. We prove the local convergence of the continuous HiSD and establish the linear convergence rate of the discrete HiSD algorithm. Furthermore, we provide a theoretical explanation for the gradient alignment tendency, revealing that the gradient direction asymptotically aligns with a specific Hessian eigenvector. Our analysis also elucidates the flexibility in selecting the index for HiSD in the context of degenerate saddle points. We validate our analytical results through numerical experiments on neural-network loss landscapes and demonstrate that momentum-accelerated variants of HiSD achieve rapid convergence to degenerate saddle points.", "AI": {"tldr": "\u672c\u6587\u5bf9\u9ad8\u6307\u6570\u978d\u70b9\u52a8\u529b\u5b66\u65b9\u6cd5\u5728\u9000\u5316\u978d\u70b9\u8ba1\u7b97\u4e2d\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u4e25\u683c\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8fde\u7eedHiSD\u7684\u5c40\u90e8\u6536\u655b\u6027\u548c\u79bb\u6563\u7b97\u6cd5\u7684\u7ebf\u6027\u6536\u655b\u7387\uff0c\u5e76\u89e3\u91ca\u4e86\u68af\u5ea6\u5bf9\u9f50\u73b0\u8c61\u3002", "motivation": "HiSD\u65b9\u6cd5\u5728\u975e\u9000\u5316\u4e34\u754c\u70b9\u8ba1\u7b97\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9000\u5316\u978d\u70b9\uff08Hessian\u77e9\u9635\u6709\u96f6\u7279\u5f81\u503c\uff09\u60c5\u51b5\u4e0b\u7684\u6570\u5b66\u548c\u6570\u503c\u5206\u6790\u5c1a\u672a\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5229\u7528Morse-Bott\u51fd\u6570\u5bf9HiSD\u5728\u4e34\u754c\u6d41\u5f62\u4e0a\u8ba1\u7b97\u9000\u5316\u978d\u70b9\u8fdb\u884c\u4e25\u683c\u5206\u6790\uff0c\u5305\u62ec\u8fde\u7eedHiSD\u7684\u5c40\u90e8\u6536\u655b\u6027\u8bc1\u660e\u3001\u79bb\u6563\u7b97\u6cd5\u7684\u7ebf\u6027\u6536\u655b\u7387\u5206\u6790\uff0c\u4ee5\u53ca\u68af\u5ea6\u5bf9\u9f50\u503e\u5411\u7684\u7406\u8bba\u89e3\u91ca\u3002", "result": "\u8bc1\u660e\u4e86\u8fde\u7eedHiSD\u7684\u5c40\u90e8\u6536\u655b\u6027\uff0c\u5efa\u7acb\u4e86\u79bb\u6563HiSD\u7b97\u6cd5\u7684\u7ebf\u6027\u6536\u655b\u7387\uff0c\u89e3\u91ca\u4e86\u68af\u5ea6\u65b9\u5411\u6e10\u8fd1\u5bf9\u9f50\u4e8e\u7279\u5b9aHessian\u7279\u5f81\u5411\u91cf\u7684\u73b0\u8c61\uff0c\u9610\u660e\u4e86\u9000\u5316\u978d\u70b9\u4e2dHiSD\u6307\u6570\u9009\u62e9\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aHiSD\u65b9\u6cd5\u5728\u9000\u5316\u978d\u70b9\u8ba1\u7b97\u4e2d\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u52a8\u91cf\u52a0\u901f\u53d8\u4f53\u5728\u9000\u5316\u978d\u70b9\u8ba1\u7b97\u4e2d\u7684\u5feb\u901f\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2602.00574", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00574", "abs": "https://arxiv.org/abs/2602.00574", "authors": ["Yifei Shao", "Kun Zhou", "Ziming Xu", "Mohammad Atif Quamar", "Shibo Hao", "Zhen Wang", "Zhiting Hu", "Biwei Huang"], "title": "Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings", "comment": null, "summary": "We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.", "AI": {"tldr": "\u63d0\u51famodal-mixed CoT\u65b9\u6cd5\uff0c\u5728\u601d\u7ef4\u94fe\u4e2d\u4ea4\u66ff\u4f7f\u7528\u6587\u672c\u6807\u8bb0\u548c\u89c6\u89c9\u8349\u56fe\u6f5c\u5728\u5d4c\u5165\uff0c\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5f62\u5f0f\u7684\u601d\u7ef4\u94fe\u5728\u5904\u7406\u89c6\u89c9\u5bc6\u96c6\u578b\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u4e3a\u5173\u952e\u4e2d\u95f4\u72b6\u6001\u672c\u8d28\u4e0a\u662f\u89c6\u89c9\u7684\uff0c\u9700\u8981\u6269\u5c55CoT\u5230\u591a\u6a21\u6001\u9886\u57df\u3002", "method": "1) \u4f7f\u7528VLM\u81ea\u8eab\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u8bed\u8a00\u9aa8\u5e72\u91cd\u5efa\u5176\u89c6\u89c9\u5d4c\u5165\u4ee5\u4fdd\u8bc1\u8bed\u4e49\u5bf9\u9f50\uff1b2) \u9644\u52a0\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u5728\u89e3\u7801\u5668\uff0c\u7531\u7279\u6b8a\u63a7\u5236\u6807\u8bb0\u89e6\u53d1\uff1b3) \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u572811\u4e2a\u591a\u6837\u5316\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7eaf\u8bed\u8a00\u548c\u5176\u4ed6CoT\u65b9\u6cd5\u3002", "conclusion": "modal-mixed CoT\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u4ea4\u66ff\u7684\u601d\u7ef4\u94fe\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u89d2\u8272\u5206\u79bb\u5e76\u51cf\u5c11\u4e86VLM\u7684\u4f18\u5316\u538b\u529b\u3002"}}
{"id": "2602.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00127", "abs": "https://arxiv.org/abs/2602.00127", "authors": ["Tong Zhu", "Baiting Chen", "Jin Zhou", "Hua Zhou", "Sriram Sankararaman", "Xiaowu Dai"], "title": "ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning", "comment": null, "summary": "LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.", "AI": {"tldr": "ALIGN\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u5019\u9009\u89e3\u5e76\u5728\u6fc0\u52b1\u673a\u5236\u4e0b\u9009\u62e9\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u751f\u6210\u548c\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u8868\u73b0\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5355\u6b21\u751f\u6210-\u9009\u62e9\u6d41\u7a0b\u6548\u679c\u6709\u9650\u3002\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u901a\u8fc7\u91c7\u6837\u591a\u6837\u63a8\u7406\u8def\u5f84\u6216\u591a\u5019\u9009\u7b54\u6848\u805a\u5408\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u901a\u5e38\u5c06\u5019\u9009\u89c6\u4e3a\u72ec\u7acb\u5904\u7406\uff0c\u4e14\u7f3a\u4e4f\u96c6\u6210\u80fd\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u7684\u6b63\u5f0f\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faALIGN\u65b9\u6cd5\uff0c\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff1a\u59d4\u6258\u4eba\u5c06\u4efb\u52a1\u59d4\u6258\u7ed9\u591a\u4e2a\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u8bbe\u8ba1\u7684\u6fc0\u52b1\u673a\u5236\u4e0b\u751f\u6210\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u59d4\u6258\u4eba\u4ece\u4e2d\u9009\u62e9\u6700\u7ec8\u7b54\u6848\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u667a\u80fd\u4f53\u4e0e\u59d4\u6258\u4eba\u76ee\u6807\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u8bf1\u5bfc\u667a\u80fd\u4f53\u95f4\u7684\u7ed3\u6784\u5316\u4ea4\u4e92\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u516c\u5e73\u6bd4\u8f83\u6761\u4ef6\u4e0b\uff08\u540c\u7b49\u8bbf\u95ee\u5019\u9009\u89e3\uff09\uff0cALIGN\u80fd\u8bc1\u660e\u63d0\u5347\u9884\u671f\u6027\u80fd\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u751f\u6210\u3002\u5206\u6790\u8003\u8651\u4e86\u5019\u9009\u7b54\u6848\u7684\u76f8\u5173\u6027\uff0c\u653e\u5bbd\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u5e38\u7528\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u3002\u5b9e\u8bc1\u7ed3\u679c\u5728\u5e7f\u6cdb\u7684LLM\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u663e\u793aALIGN\u4f18\u4e8e\u5f3a\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u3002", "conclusion": "ALIGN\u901a\u8fc7\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u4f18\u52bf\uff0c\u4e3a\u591a\u667a\u80fd\u4f53LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.00594", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00594", "abs": "https://arxiv.org/abs/2602.00594", "authors": ["Zhijie Huang", "Stephen McIntosh", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling", "comment": null, "summary": "A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.", "AI": {"tldr": "Kanade\u662f\u4e00\u4e2a\u5355\u5c42\u89e3\u8026\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u80fd\u591f\u5206\u79bb\u58f0\u5b66\u5e38\u91cf\uff0c\u751f\u6210\u6355\u83b7\u4e30\u5bcc\u97f3\u7d20\u548c\u97f5\u5f8b\u7684\u5355\u4e00token\u6d41\uff0c\u65e0\u9700\u4f9d\u8d56\u73b0\u6709\u89e3\u8026\u7f16\u89e3\u7801\u5668\u5e38\u7528\u7684\u8f85\u52a9\u65b9\u6cd5\u3002", "motivation": "\u597d\u7684\u8bed\u8a00\u6a21\u578b\u59cb\u4e8e\u597d\u7684\u5206\u8bcd\u5668\u3002\u5bf9\u4e8e\u8bed\u97f3\u5efa\u6a21\u5c24\u5176\u91cd\u8981\uff0c\u56e0\u4e3a\u9700\u8981\u5904\u7406\u6df7\u5408\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4fe1\u606f\u7684\u8fde\u7eed\u4fe1\u53f7\u3002\u8bed\u97f3\u5206\u8bcd\u5668\u5e94\u63d0\u53d6\u97f3\u7d20\u548c\u97f5\u5f8b\uff0c\u6291\u5236\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7b49\u8bed\u8a00\u65e0\u5173\u4fe1\u606f\uff0c\u5e76\u652f\u6301\u9ad8\u8d28\u91cf\u5408\u6210\u3002", "method": "\u63d0\u51faKanade\u5355\u5c42\u89e3\u8026\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u5206\u79bb\u58f0\u5b66\u5e38\u91cf\u521b\u5efa\u5355\u4e00token\u6d41\u6765\u6355\u83b7\u4e30\u5bcc\u97f3\u7d20\u548c\u97f5\u5f8b\uff0c\u65e0\u9700\u4f9d\u8d56\u73b0\u6709\u89e3\u8026\u7f16\u89e3\u7801\u5668\u5e38\u7528\u7684\u8f85\u52a9\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793aKanade\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bf4\u8bdd\u4eba\u89e3\u8026\u548c\u8bcd\u6c47\u53ef\u7528\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51fa\u8272\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "Kanade\u662f\u4e00\u4e2a\u7406\u60f3\u7684\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u80fd\u591f\u6709\u6548\u5206\u79bb\u8bed\u8a00\u76f8\u5173\u4fe1\u606f\uff0c\u4e3a\u8bed\u97f3\u5efa\u6a21\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u7ec4\u4ef6\u3002"}}
{"id": "2602.00153", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00153", "abs": "https://arxiv.org/abs/2602.00153", "authors": ["Axel Duch\u00e9", "Cl\u00e9ment Chatelain", "Gilles Gasso"], "title": "See Without Decoding: Motion-Vector-Based Tracking in Compressed Video", "comment": null, "summary": "We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u538b\u7f29\u57df\u8ddf\u8e2a\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u89c6\u9891\u6d41\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u5b8c\u6574RGB\u89e3\u7801\uff0c\u5229\u7528\u8fd0\u52a8\u77e2\u91cf\u548c\u53d8\u6362\u7cfb\u6570\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u8fbe3.7\u500d\uff0c\u4ec5\u5e26\u67654% mAP@0.5\u4e0b\u964d", "motivation": "\u4f20\u7edf\u89c6\u9891\u8ddf\u8e2a\u9700\u8981\u5b8c\u6574\u89e3\u7801RGB\u89c6\u9891\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u76d1\u63a7\u7cfb\u7edf\u7684\u5b9e\u65f6\u5206\u6790\u3002\u538b\u7f29\u57df\u6570\u636e\uff08\u5982\u8fd0\u52a8\u77e2\u91cf\uff09\u5df2\u5305\u542b\u4e30\u5bcc\u7684\u8fd0\u52a8\u4fe1\u606f\uff0c\u53ef\u76f4\u63a5\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u9ad8\u6548\u8ddf\u8e2a", "method": "\u4f7f\u7528\u6df1\u5ea6\u6a21\u578b\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\uff08\u8fd0\u52a8\u77e2\u91cf\u548c\u53d8\u6362\u7cfb\u6570\uff09\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u4f20\u64ad\u76ee\u6807\u8fb9\u754c\u6846\u8de8\u5e27\u8ddf\u8e2a\uff0c\u907f\u514d\u5b8c\u6574RGB\u89e3\u7801\u8fc7\u7a0b", "result": "\u5728MOTS15/17/20\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4RGB\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u8fbe3.7\u500d\uff0c\u4ec5\u5e26\u67654% mAP@0.5\u7684\u8f7b\u5fae\u6027\u80fd\u4e0b\u964d", "conclusion": "\u538b\u7f29\u57df\u8fd0\u52a8\u5efa\u6a21\u5728\u5b9e\u65f6\u89c6\u9891\u5206\u6790\u4e2d\u5177\u6709\u9ad8\u6548\u6027\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21\u76d1\u63a7\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u826f\u597d\u8ddf\u8e2a\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500"}}
{"id": "2602.01888", "categories": ["math.NA", "physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2602.01888", "abs": "https://arxiv.org/abs/2602.01888", "authors": ["Deepak Gautam", "Bhooshan Paradkar"], "title": "Multigrid Poisson Solver for Complex Geometries Using Finite Difference Method", "comment": null, "summary": "We present an efficient numerical method, inspired by transformation optics, for solving the Poisson equation in complex and arbitrarily shaped geometries. The approach operates by mapping the physical domain to a uniform computational domain through coordinate transformations, which can be applied either to the entire domain or selectively to specific boundaries inside the domain. This flexibility allows both homogeneous (Laplace equation) and inhomogeneous (Poisson equation) problems to be solved efficiently using iterative or fast direct solvers, with only the material parameters and source terms modified according to the transformation. The method is formulated within a finite difference framework, where the modified material properties are computed from the coordinate transformation equations, either analytically or numerically. This enables accurate treatment of arbitrary geometric shapes while retaining the simplicity of a uniform grid solver. Numerical experiments confirm that the method achieves second-order accuracy , and offers a straightforward pathway to integrate fast solvers such as multigrid methods on the uniform computational grid.", "AI": {"tldr": "\u57fa\u4e8e\u53d8\u6362\u5149\u5b66\u7684\u6570\u503c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5750\u6807\u53d8\u6362\u5c06\u590d\u6742\u51e0\u4f55\u57df\u6620\u5c04\u5230\u5747\u5300\u8ba1\u7b97\u57df\uff0c\u9ad8\u6548\u6c42\u89e3\u6cca\u677e\u65b9\u7a0b", "motivation": "\u89e3\u51b3\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e2d\u6cca\u677e\u65b9\u7a0b\u7684\u6570\u503c\u6c42\u89e3\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u89c4\u5219\u8fb9\u754c\u65f6\u6548\u7387\u8f83\u4f4e\u6216\u5b9e\u73b0\u590d\u6742", "method": "\u91c7\u7528\u53d8\u6362\u5149\u5b66\u601d\u60f3\uff0c\u901a\u8fc7\u5750\u6807\u53d8\u6362\u5c06\u7269\u7406\u57df\u6620\u5c04\u5230\u5747\u5300\u8ba1\u7b97\u57df\uff0c\u5728\u6709\u9650\u5dee\u5206\u6846\u67b6\u4e0b\u4fee\u6539\u6750\u6599\u53c2\u6570\u548c\u6e90\u9879\uff0c\u4fdd\u7559\u5747\u5300\u7f51\u683c\u6c42\u89e3\u5668\u7684\u7b80\u5355\u6027", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e8c\u9636\u7cbe\u5ea6\uff0c\u80fd\u591f\u51c6\u786e\u5904\u7406\u4efb\u610f\u51e0\u4f55\u5f62\u72b6\uff0c\u4e3a\u96c6\u6210\u591a\u91cd\u7f51\u683c\u7b49\u5feb\u901f\u6c42\u89e3\u5668\u63d0\u4f9b\u76f4\u63a5\u9014\u5f84", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u53d8\u6362\u5149\u5b66\u7684\u7075\u6d3b\u6027\u548c\u5747\u5300\u7f51\u683c\u6c42\u89e3\u5668\u7684\u6548\u7387\uff0c\u4e3a\u590d\u6742\u51e0\u4f55\u4e2d\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6"}}
{"id": "2602.00580", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00580", "abs": "https://arxiv.org/abs/2602.00580", "authors": ["Wei Huang", "Hanchen Wang", "Dong Wen", "Wenjie Zhang"], "title": "Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification", "comment": null, "summary": "The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.", "AI": {"tldr": "TSP-MDF\uff1a\u901a\u8fc7\u5b9e\u4f8b\u4fee\u6539\u6846\u67b6\u4e3a\u4f20\u7edf\u786e\u5b9a\u6027\u542f\u53d1\u5f0fTSP\u6c42\u89e3\u5668\u589e\u52a0\u5f15\u5bfc\u91c7\u6837\u80fd\u529b\uff0c\u65e0\u9700\u771f\u5b9e\u76d1\u7763\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u542f\u53d1\u5f0fTSP\u6c42\u89e3\u5668\uff08\u5982\u6700\u8fdc/\u6700\u8fd1\u63d2\u5165\u6cd5\uff09\u8ba1\u7b97\u9ad8\u6548\u4f46\u63a2\u7d22\u80fd\u529b\u6709\u9650\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff1b\u800c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6c42\u89e3\u5668\u867d\u7136\u6027\u80fd\u66f4\u597d\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u548c\u771f\u5b9e\u76d1\u7763\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u5347\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u53c8\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTSP-MDF\u5b9e\u4f8b\u4fee\u6539\u6846\u67b6\uff1a\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u4f8b\u4fee\u6539\u5668\u7b56\u7565\u6027\u5730\u79fb\u52a8\u8282\u70b9\u5750\u6807\uff0c\u751f\u6210\u591a\u4e2a\u4fee\u6539\u540e\u7684\u5b9e\u4f8b\uff1b\u5728\u8fd9\u4e9b\u4fee\u6539\u5b9e\u4f8b\u4e0a\u8fd0\u884c\u4f20\u7edf\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u6784\u9020\u8def\u5f84\uff1b\u7136\u540e\u5c06\u8def\u5f84\u6620\u5c04\u56de\u539f\u59cb\u5b9e\u4f8b\uff0c\u4f7f\u4f20\u7edf\u6c42\u89e3\u5668\u80fd\u591f\u63a2\u7d22\u66f4\u9ad8\u8d28\u91cf\u7684\u8def\u5f84\u5e76\u9003\u79bb\u5c40\u90e8\u6700\u4f18\u3002", "result": "\u5728\u5927\u89c4\u6a21TSP\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTSP-MDF\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edf\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e0e\u795e\u7ecf\u7f51\u7edc\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u76f8\u5f53\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u6781\u77ed\u3002", "conclusion": "TSP-MDF\u6210\u529f\u6865\u63a5\u4e86\u4f20\u7edf\u786e\u5b9a\u6027\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5f15\u5bfc\u91c7\u6837\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u65e0\u9700\u771f\u5b9e\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.00128", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00128", "abs": "https://arxiv.org/abs/2602.00128", "authors": ["Emine Akpinar", "Murat Oduncuoglu"], "title": "Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages", "comment": "Under review at Quantum Machine Intelligence (Springer Nature)", "summary": "With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u5e76\u884c\u6a21\u578b(QBPM)\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u671f\u5206\u7c7b\uff0c\u5229\u7528\u91cf\u5b50\u4f18\u52bf\u5728\u566a\u58f0\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b", "motivation": "\u968f\u7740\u4eba\u53e3\u8001\u9f84\u5316\uff0cAD\u6210\u4e3a\u5168\u7403\u91cd\u5927\u5065\u5eb7\u95ee\u9898\u3002\u4f20\u7edfAI\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u800c\u91cf\u5b50AI\u65b9\u6cd5\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u3001\u7ea0\u7f20\u548c\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7279\u6027\uff0c\u6709\u671b\u8d85\u8d8a\u7ecf\u5178\u65b9\u6cd5\u9650\u5236\uff0c\u4e3a\u9ad8\u7ef4\u3001\u5f02\u6784\u3001\u566a\u58f0\u6570\u636e\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6", "method": "\u63d0\u51fa\u91cf\u5b50\u5e76\u884c\u6a21\u578b(QBPM)\u67b6\u6784\uff0c\u53d7\u7ecf\u5178\u6a21\u578b\u5e76\u884c\u542f\u53d1\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e0d\u540c\u7684\u91cf\u5b50\u7535\u8def\uff08\u5305\u542b\u65cb\u8f6c\u548c\u7ea0\u7f20\u5757\uff09\uff0c\u5728\u540c\u4e00\u91cf\u5b50\u6a21\u62df\u5668\u4e0a\u5e76\u884c\u8fd0\u884c\uff0c\u7528\u4e8eAD\u9636\u6bb5\u7684MRI\u6570\u636e\u5206\u7c7b", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u8bc1\u660e\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff1b\u5728\u9ad8\u65af\u566a\u58f0\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u826f\u597d\u6027\u80fd\uff1b\u4e0e\u4e94\u79cd\u7ecf\u5178\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u66f4\u5c11\u7535\u8def\u53c2\u6570\u5b9e\u73b0\u4e86\u66f4\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u548c\u53ef\u6bd4\u6267\u884c\u65f6\u95f4", "conclusion": "QBPM\u67b6\u6784\u4ee3\u8868\u4e86\u590d\u6742\u75be\u75c5\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u5206\u671f\u5206\u7c7b\u7684\u521b\u65b0\u4e14\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2602.00597", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00597", "abs": "https://arxiv.org/abs/2602.00597", "authors": ["Chaoqun Cui", "Shijing Wang", "Liangbin Huang", "Qingqing Gu", "Zhaolong Huang", "Xiao Zeng", "Wenji Mao"], "title": "Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling", "comment": "Accepted to The Web Conference (WWW) 2026", "summary": "Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.", "AI": {"tldr": "Hermes\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5b57\u5e55\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u8bf4\u8bdd\u4eba\u5206\u5272\u3001\u672f\u8bed\u8bc6\u522b\u548c\u8868\u8fbe\u589e\u5f3a\u4e09\u4e2a\u6a21\u5757\u89e3\u51b3\u5b57\u5e55\u7ffb\u8bd1\u4e2d\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u4ee3\u8bcd\u672f\u8bed\u7ffb\u8bd1\u548c\u8868\u8fbe\u529b\u95ee\u9898\u3002", "motivation": "\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u5728\u5a31\u4e50\u672c\u5730\u5316\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5c1a\u672a\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u7ffb\u8bd1\u80fd\u529b\uff0c\u4f46\u5b57\u5e55\u6587\u672c\u7684\u72ec\u7279\u7279\u6027\uff08\u8bed\u4e49\u8fde\u8d2f\u6027\u3001\u4ee3\u8bcd\u672f\u8bed\u7ffb\u8bd1\u3001\u7ffb\u8bd1\u8868\u8fbe\u529b\uff09\u4ecd\u5e26\u6765\u6301\u7eed\u6311\u6218\u3002", "method": "\u63d0\u51faHermes\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) \u8bf4\u8bdd\u4eba\u5206\u5272\u6a21\u5757\uff0c2) \u672f\u8bed\u8bc6\u522b\u6a21\u5757\uff0c3) \u8868\u8fbe\u589e\u5f3a\u6a21\u5757\u3002\u8fd9\u4e9b\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff0c\u4e13\u95e8\u89e3\u51b3\u5b57\u5e55\u7ffb\u8bd1\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHermes\u5728\u8bf4\u8bdd\u4eba\u5206\u5272\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u8868\u8fbe\u529b\u5f3a\u3001\u4e0a\u4e0b\u6587\u8fde\u8d2f\u7684\u7ffb\u8bd1\uff0c\u63a8\u52a8\u4e86\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "conclusion": "Hermes\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b57\u5e55\u7ffb\u8bd1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u5b57\u5e55\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2602.00163", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.00163", "abs": "https://arxiv.org/abs/2602.00163", "authors": ["Laura Cif", "Diane Demailly", "Gabriella A. Horv\u00e0th", "Juan Dario Ortigoza Escobar", "Nathalie Dorison", "Mayt\u00e9 Castro Jim\u00e9nez", "C\u00e9cile A. Hubsch", "Thomas Wirth", "Gun-Marie Hariz", "Sophie Huby", "Morgan Dornadic", "Zohra Souei", "Muhammad Mushhood Ur Rehman", "Simone Hemm", "Mehdi Boulayme", "Eduardo M. Moraud", "Jocelyne Bloch", "Xavier Vasques"], "title": "Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders", "comment": null, "summary": "Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u59ff\u6001\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u95e8\u8bca\u89c6\u9891\u8f6c\u6362\u4e3a\u5173\u952e\u70b9\u65f6\u95f4\u5e8f\u5217\uff0c\u8ba1\u7b97\u8fd0\u52a8\u5b66\u7279\u5f81\u4ee5\u533a\u5206\u91cd\u53e0\u7684\u8fc7\u5ea6\u8fd0\u52a8\u969c\u788d\u8868\u578b", "motivation": "\u8fc7\u5ea6\u8fd0\u52a8\u969c\u788d\uff08\u5982\u808c\u5f20\u529b\u969c\u788d\u3001\u9707\u98a4\u3001\u821e\u8e48\u75c7\u7b49\uff09\u7684\u6ce2\u52a8\u6027\u3001\u95f4\u6b47\u6027\u548c\u5171\u73b0\u6027\u8868\u8fbe\u963b\u788d\u4e86\u4e34\u5e8a\u8bc6\u522b\u548c\u7eb5\u5411\u76d1\u6d4b\uff0c\u76ee\u524d\u7f3a\u4e4f\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u4ece\u5e38\u89c4\u4e34\u5e8a\u89c6\u9891\u4e2d\u533a\u5206\u91cd\u53e0\u7684\u8868\u578b", "method": "\u5f00\u53d1\u59ff\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5c06\u6807\u51c6\u95e8\u8bca\u89c6\u9891\u8f6c\u6362\u4e3a\u89e3\u5256\u5b66\u6709\u610f\u4e49\u7684\u5173\u952e\u70b9\u65f6\u95f4\u5e8f\u5217\uff1b2\uff09\u8ba1\u7b97\u6db5\u76d6\u7edf\u8ba1\u3001\u65f6\u57df\u3001\u9891\u57df\u4ee5\u53ca\u9ad8\u9636\u4e0d\u89c4\u5219\u6027-\u590d\u6742\u6027\u7279\u5f81\u7684\u8fd0\u52a8\u5b66\u63cf\u8ff0\u7b26", "result": "\u8bba\u6587\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\u6570\u636e\uff0c\u4f46\u63cf\u8ff0\u4e86\u65b9\u6cd5\u6846\u67b6\u7684\u5f00\u53d1", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u5730\u533a\u5206\u8fc7\u5ea6\u8fd0\u52a8\u969c\u788d\u8868\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u8bc6\u522b\u548c\u76d1\u6d4b"}}
{"id": "2602.01938", "categories": ["math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.01938", "abs": "https://arxiv.org/abs/2602.01938", "authors": ["Hiroaki Nishikawa"], "title": "A Flux-Correction Form of the Third-Order Edge-Based Scheme for a General Numerical Flux Function", "comment": null, "summary": "In this short note, we present a flux-correction form of the third-order edge-based scheme for the Euler equations that enables the direct use of a general flux function. The core idea is to replace, without loss of accuracy, the arithmetic average of the flux extrapolations by a general numerical flux evaluated at the edge midpoint, together with a correction term. We show that the proposed flux-correction form preserves third-order accuracy, provided that the general numerical flux is evaluated with the left and right states that are computed exactly for a quadratic function, which can be achieved effectively by the U-MUSCL scheme with \u03ba = 1/2. Numerical results are presented to verify third-order accuracy with the HLLC and LDFSS flux functions on irregular tetrahedral grids.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u91cf\u4fee\u6b63\u5f62\u5f0f\u7684\u4e09\u9636\u8fb9\u57fa\u683c\u5f0f\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u901a\u7528\u901a\u91cf\u51fd\u6570\u6c42\u89e3\u6b27\u62c9\u65b9\u7a0b\uff0c\u4fdd\u6301\u4e09\u9636\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e09\u9636\u8fb9\u57fa\u683c\u5f0f\u901a\u5e38\u9700\u8981\u7279\u5b9a\u901a\u91cf\u51fd\u6570\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u76f4\u63a5\u4f7f\u7528\u4efb\u610f\u6570\u503c\u901a\u91cf\u51fd\u6570\u7684\u901a\u7528\u5f62\u5f0f\u3002", "method": "\u901a\u8fc7\u7528\u8fb9\u7f18\u4e2d\u70b9\u5904\u7684\u901a\u7528\u6570\u503c\u901a\u91cf\u66ff\u6362\u901a\u91cf\u5916\u63a8\u7684\u7b97\u672f\u5e73\u5747\uff0c\u5e76\u6dfb\u52a0\u4fee\u6b63\u9879\uff0c\u6784\u5efa\u901a\u91cf\u4fee\u6b63\u5f62\u5f0f\u3002\u8981\u6c42\u6570\u503c\u901a\u91cf\u4f7f\u7528U-MUSCL\u65b9\u6848\uff08\u03ba=1/2\uff09\u7cbe\u786e\u8ba1\u7b97\u4e8c\u6b21\u51fd\u6570\u7684\u5de6\u53f3\u72b6\u6001\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u683c\u5f0f\u5728\u975e\u89c4\u5219\u56db\u9762\u4f53\u7f51\u683c\u4e0a\u4f7f\u7528HLLC\u548cLDFSS\u901a\u91cf\u51fd\u6570\u65f6\u4fdd\u6301\u4e09\u9636\u7cbe\u5ea6\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u53ef\u76f4\u63a5\u4f7f\u7528\u901a\u7528\u6570\u503c\u901a\u91cf\u51fd\u6570\u7684\u4e09\u9636\u8fb9\u57fa\u683c\u5f0f\uff0c\u6269\u5c55\u4e86\u8be5\u683c\u5f0f\u7684\u9002\u7528\u6027\uff0c\u4e3a\u590d\u6742\u6d41\u52a8\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2602.00585", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00585", "abs": "https://arxiv.org/abs/2602.00585", "authors": ["Guochen Yan", "Jialong Wu", "Zhengwei Tao", "Bo Li", "Qintong Zhang", "Jiahao Xu", "Haitao Mi", "Yuejian Fang", "Qingni Shen", "Wentao Zhang", "Zhonghai Wu"], "title": "Exploring Information Seeking Agent Consolidation", "comment": null, "summary": "Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u5f02\u6784\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u6574\u5408\u4e3a\u5355\u4e00\u57fa\u7840\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u6570\u636e\u7ea7\u6574\u5408\u548c\u53c2\u6570\u7ea7\u6574\u5408\u4e24\u79cd\u7b56\u7565\uff0c\u53d1\u73b0\u6570\u636e\u7ea7\u6574\u5408\u66f4\u7a33\u5b9a\uff0c\u53c2\u6570\u7ea7\u6574\u5408\u867d\u9ad8\u6548\u4f46\u5b58\u5728\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u901a\u5e38\u4e13\u95e8\u9488\u5bf9\u5f00\u653e\u7f51\u7edc\u3001\u6587\u6863\u6216\u672c\u5730\u77e5\u8bc6\u5e93\uff0c\u8fd9\u79cd\u4e13\u4e1a\u5316\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u5f02\u6784\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u6a21\u578b\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6574\u5408\u7b56\u7565\uff1a1) \u6570\u636e\u7ea7\u6574\u5408 - \u5728\u6df7\u5408\u7684\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\uff1b2) \u53c2\u6570\u7ea7\u6574\u5408 - \u5728\u53c2\u6570\u5c42\u9762\u5408\u5e76\u72ec\u7acb\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\u3002\u5206\u6790\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u6027\u80fd\u4fdd\u6301\u3001\u8de8\u9886\u57df\u6cdb\u5316\u548c\u884c\u4e3a\u5e72\u6270\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u6570\u636e\u7ea7\u6574\u5408\u4ecd\u7136\u662f\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u800c\u53c2\u6570\u7ea7\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u5e72\u6270\u548c\u9c81\u68d2\u6027\u6311\u6218\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u4e86\u53c2\u6570\u7ea7\u667a\u80fd\u4f53\u6574\u5408\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u3002", "conclusion": "\u6570\u636e\u7ea7\u6574\u5408\u662f\u7a33\u5065\u7684\u6574\u5408\u65b9\u6cd5\uff0c\u53c2\u6570\u7ea7\u6574\u5408\u867d\u7136\u9ad8\u6548\u4f46\u9700\u8981\u89e3\u51b3\u5e72\u6270\u95ee\u9898\u3002\u6709\u6548\u7684\u53c2\u6570\u7ea7\u6574\u5408\u9700\u8981\u8003\u8651\u7ec6\u7c92\u5ea6\u5408\u5e76\u3001\u4efb\u52a1\u5f02\u6784\u6027\u611f\u77e5\u548c\u539f\u5219\u6027\u5171\u8bc6\u7b56\u7565\u7b49\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u3002"}}
{"id": "2602.00129", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00129", "abs": "https://arxiv.org/abs/2602.00129", "authors": ["Yixuan Liang"], "title": "Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models", "comment": "10 pages, 5 figures. Submitted to a conference workshop", "summary": "Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.", "AI": {"tldr": "CodePilot\uff1a\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u5f15\u5bfc\u7684\u7a0b\u5e8f\u4fee\u590d\u89e3\u51b3GitHub\u95ee\u9898\uff0c\u5728SWE-bench Lite\u4e0a\u8fbe\u523024.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u5728\u4ed3\u5e93\u7ea7\u522b\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\uff0c\u800c\u81ea\u56de\u5f52\u89e3\u7801\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6267\u884c\u611f\u77e5\u4fee\u590d\u65b9\u6cd5", "method": "\u96c6\u6210\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u8fdb\u884c\u4ece\u4ed3\u5e93\u5230\u6587\u4ef6\u518d\u5230\u51fd\u6570\u7684\u5206\u5c42\u6545\u969c\u5b9a\u4f4d\uff0c\u5229\u7528MCTS\u63a2\u7d22\u591a\u6837\u5316\u8865\u4e01\u8f68\u8ff9\uff0c\u5c06\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u641c\u7d22\u548c\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u7f6e\u4fe1\u5ea6\u6821\u51c6\u751f\u6210\u9009\u62e9\u6027\u4f18\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u8f93\u51fa", "result": "\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodePilot\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\u4e8624.67%\u7684\u95ee\u9898\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u53ef\u6bd4\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5c06\u7b26\u53f7\u641c\u7d22\u4e0e\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u76f8\u7ed3\u5408\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u6267\u884c\u611f\u77e5\u7684\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u7684\u6709\u6548\u7b56\u7565"}}
{"id": "2602.00612", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00612", "abs": "https://arxiv.org/abs/2602.00612", "authors": ["Yitong Zhang", "Yongmin Li", "Yuetong Liu", "Jia Li", "Xiaoran Jia", "Zherui Li", "Ge Li"], "title": "Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.\n  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.", "AI": {"tldr": "LAVE\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u77bb\u9a8c\u8bc1\u786e\u4fdd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u53e5\u6cd5\u6b63\u786e\u6027\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f62\u5f0f\u8bed\u8a00\uff08\u5982\u6e90\u4ee3\u7801\u3001\u5316\u5b66\u8868\u8fbe\u5f0f\uff09\u65f6\uff0c\u7531\u4e8e\u6982\u7387\u6a21\u578b\u7684\u7279\u6027\uff0c\u96be\u4ee5\u53ef\u9760\u5730\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u8f93\u51fa\u3002\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u8981\u4e48\u4e0d\u9002\u7528\u4e8e\u975e\u81ea\u56de\u5f52\u7684dLLMs\uff0c\u8981\u4e48\u5141\u8bb8\u4e2d\u95f4\u8f93\u51fa\u65e0\u6cd5\u5b8c\u6210\u6709\u6548\u53e5\u5b50\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u53ef\u9760\u6027\u3002", "method": "LAVE\u5229\u7528dLLMs\u80fd\u591f\u5e76\u884c\u9884\u6d4b\u6240\u6709\u4f4d\u7f6etoken\u5206\u5e03\u7684\u7279\u6027\uff0c\u5728\u6a21\u578b\u63d0\u51fa\u65b0token\u65f6\u8fdb\u884c\u524d\u77bb\u9a8c\u8bc1\uff0c\u9ad8\u6548\u53ef\u9760\u5730\u68c0\u67e5\u63d0\u8baetoken\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e2d\u95f4\u8f93\u51fa\u59cb\u7ec8\u80fd\u591f\u6269\u5c55\u4e3a\u6709\u6548\u53e5\u5b50\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684dLLMs\u548c\u4e09\u4e2a\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAVE\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u53e5\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u8fd0\u884c\u65f6\u95f4\u5f00\u9500\u3002", "conclusion": "LAVE\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u81ea\u56de\u5f52\u6a21\u578b\u4e0a\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u8f93\u51fa\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2602.00168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00168", "abs": "https://arxiv.org/abs/2602.00168", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation", "comment": null, "summary": "This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.", "AI": {"tldr": "YOLOE-26\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u4e86YOLOv26\u7684\u9ad8\u6548\u90e8\u7f72\u67b6\u6784\u548cYOLOE\u7684\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u8303\u5f0f\uff0c\u652f\u6301\u6587\u672c\u63d0\u793a\u3001\u89c6\u89c9\u63d0\u793a\u548c\u65e0\u63d0\u793a\u63a8\u7406\u3002", "motivation": "\u4f20\u7edfYOLO\u6a21\u578b\u4ec5\u9650\u4e8e\u5c01\u95ed\u96c6\u8bc6\u522b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u65b0\u7c7b\u522b\u7684\u68c0\u6d4b\u9700\u6c42\u3002\u9700\u8981\u5c06YOLO\u7684\u9ad8\u6548\u5b9e\u65f6\u7279\u6027\u4e0e\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u5b9e\u4f8b\u5206\u5272\u3002", "method": "\u91c7\u7528YOLOv26\u7684\u65e0NMS\u7aef\u5230\u7aef\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5377\u79ef\u4e3b\u5e72\u7f51\u7edc\u548cPAN/FPN\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u3002\u5173\u952e\u521b\u65b0\u662f\u7528\u5bf9\u8c61\u5d4c\u5165\u5934\u66ff\u4ee3\u56fa\u5b9a\u7c7b\u522blogits\uff0c\u5c06\u5206\u7c7b\u91cd\u6784\u4e3a\u4e0e\u63d0\u793a\u5d4c\u5165\u7684\u76f8\u4f3c\u5ea6\u5339\u914d\u3002\u5f15\u5165RepRTA\u5b9e\u73b0\u96f6\u5f00\u9500\u6587\u672c\u63d0\u793a\u3001SAVPE\u8fdb\u884c\u793a\u4f8b\u5f15\u5bfc\u5206\u5272\u3001Lazy Region Prompt Contrast\u652f\u6301\u65e0\u63d0\u793a\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u4e0d\u540c\u6a21\u578b\u5c3a\u5bf8\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u7f29\u653e\u884c\u4e3a\u548c\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u3002\u5728\u63d0\u793a\u548c\u65e0\u63d0\u793a\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u4fdd\u6301\u9ad8\u6548\u6027\u80fd\uff0c\u4e0eUltralytics\u751f\u6001\u7cfb\u7edf\u5b8c\u5168\u517c\u5bb9\u3002", "conclusion": "YOLOE-26\u4e3a\u52a8\u6001\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u6301\u4e86YOLO\u5bb6\u65cf\u7684\u9ad8\u6548\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2602.02003", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.02003", "abs": "https://arxiv.org/abs/2602.02003", "authors": ["Lingyue Shen", "Qi Xin", "Yan Chen", "Jiarui Han", "Yumiao Zhang", "Jinchao Xu", "Shihua Gong"], "title": "A monolithic localized high-order ALE finite element method for multi-scale fluid-structure interaction problems", "comment": null, "summary": "This paper presents MLH-ALE, a monolithic localized high-order arbitrary Lagrangian-Eulerian finite element method for multi-scale fluid-structure interaction (FSI). The framework employs isoparametric $\\mathcal{P}_2$ elements for geometric fidelity and an implicit-explicit partitioned Runge-Kutta (IMEX-PRK) scheme for temporal discretization. To address scale disparity, a localized updating strategy is integrated to focus computational resolution on the moving structure. Numerical benchmarks confirm the optimal high-order convergence of the underlying ALE scheme. Furthermore, simulations of particle focusing in spiral microchannels demonstrate that the MLH-ALE approach provides reliable numerical results in good agreement with experimental observations, confirming its feasibility for complex multi-scale applications.", "AI": {"tldr": "MLH-ALE\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u5c3a\u5ea6\u6d41\u56fa\u8026\u5408\u7684\u5355\u7247\u5c40\u90e8\u9ad8\u9636\u4efb\u610f\u62c9\u683c\u6717\u65e5-\u6b27\u62c9\u6709\u9650\u5143\u65b9\u6cd5\uff0c\u91c7\u7528\u5c40\u90e8\u66f4\u65b0\u7b56\u7565\u5904\u7406\u5c3a\u5ea6\u5dee\u5f02\uff0c\u5728\u87ba\u65cb\u5fae\u901a\u9053\u7c92\u5b50\u805a\u7126\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u9760\u6027\u3002", "motivation": "\u9488\u5bf9\u591a\u5c3a\u5ea6\u6d41\u56fa\u8026\u5408\u95ee\u9898\u4e2d\u5b58\u5728\u7684\u5c3a\u5ea6\u5dee\u5f02\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u7ed3\u6784\u8fd0\u52a8\u5c40\u90e8\u533a\u57df\u9ad8\u5206\u8fa8\u7387\u9700\u6c42\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7b49\u53c2P2\u5355\u5143\u4fdd\u8bc1\u51e0\u4f55\u7cbe\u5ea6\uff0c\u4f7f\u7528\u9690\u5f0f-\u663e\u5f0f\u5206\u533a\u9f99\u683c-\u5e93\u5854\u65f6\u95f4\u79bb\u6563\u65b9\u6848\uff0c\u5e76\u96c6\u6210\u5c40\u90e8\u66f4\u65b0\u7b56\u7565\u5c06\u8ba1\u7b97\u5206\u8fa8\u7387\u96c6\u4e2d\u5728\u8fd0\u52a8\u7ed3\u6784\u533a\u57df\u3002", "result": "\u6570\u503c\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5e95\u5c42ALE\u65b9\u6848\u7684\u6700\u4f18\u9ad8\u9636\u6536\u655b\u6027\uff0c\u87ba\u65cb\u5fae\u901a\u9053\u7c92\u5b50\u805a\u7126\u6a21\u62df\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u89c2\u6d4b\u826f\u597d\u543b\u5408\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u591a\u5c3a\u5ea6\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "MLH-ALE\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u5c3a\u5ea6\u6d41\u56fa\u8026\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u66f4\u65b0\u7b56\u7565\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u4e3a\u590d\u6742\u591a\u5c3a\u5ea6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u503c\u5de5\u5177\u3002"}}
{"id": "2602.00592", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00592", "abs": "https://arxiv.org/abs/2602.00592", "authors": ["Jiaran Zhang", "Luck Ma", "Yanhao Li", "Fanqi Wan", "Di Qi", "Xu Zhao", "Jieyi Hou", "Zhe Xie", "Mengqiang Ren", "Xin Wu", "Zhewei Huang", "Liangyu Chen", "Yingwei Ma", "Qi Han", "Xiangyu Zhang"], "title": "DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder", "comment": null, "summary": "Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.", "AI": {"tldr": "DockSmith\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eDocker\u73af\u5883\u6784\u5efa\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u6784\u5efa\u89c6\u4e3a\u6838\u5fc3\u667a\u80fd\u80fd\u529b\u800c\u975e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8eDocker\u7684\u73af\u5883\u6784\u5efa\u662f\u6269\u5c55\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u6267\u884c\u57fa\u7840\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1DockSmith\u4ee3\u7406\uff0c\u5c06\u73af\u5883\u6784\u5efa\u89c6\u4e3a\u6838\u5fc3\u667a\u80fd\u80fd\u529b\uff0c\u8bad\u7ec330B-A3B\u6a21\u578b\u4e8e\u5927\u89c4\u6a21\u6267\u884c\u57fa\u7840\u7684Docker\u6784\u5efa\u8f68\u8ff9\uff0c\u91c7\u7528SWE-Factory\u98ce\u683c\u7ba1\u9053\uff0c\u589e\u5f3a\u5faa\u73af\u68c0\u6d4b\u63a7\u5236\u5668\u548c\u8de8\u4efb\u52a1\u6210\u529f\u8bb0\u5fc6\u3002", "result": "\u5728Multi-Docker-Eval\u4e0a\u8fbe\u5230\u5f00\u6e90\u6700\u5148\u8fdb\u6027\u80fd\uff1a39.72% Fail-to-Pass\u548c58.28% Commit Rate\uff1b\u5728SWE-bench Verified\u3001SWE-bench Multilingual\u548cTerminal-Bench 2.0\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5206\u5e03\u5916\u6027\u80fd\u3002", "conclusion": "DockSmith\u4e0d\u4ec5\u89e3\u51b3\u4e86Docker\u6784\u5efa\u7684\u74f6\u9888\u95ee\u9898\uff0c\u8fd8\u5c55\u793a\u4e86\u73af\u5883\u6784\u5efa\u4f5c\u4e3a\u6838\u5fc3\u667a\u80fd\u80fd\u529b\u7684\u66f4\u5e7f\u6cdb\u4ee3\u7406\u6548\u76ca\uff0c\u5176\u76d1\u7763\u80fd\u529b\u53ef\u8fc1\u79fb\u5230Docker\u6784\u5efa\u4e4b\u5916\u7684\u4efb\u52a1\u3002"}}
{"id": "2602.00130", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00130", "abs": "https://arxiv.org/abs/2602.00130", "authors": ["Sumit Yadav"], "title": "On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks", "comment": "ICML", "summary": "We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u8868\u793a\u51e0\u4f55\u4e2d\u7684\u6709\u6548\u7ef4\u5ea6\u8fd9\u4e00\u65e0\u76d1\u7763\u51e0\u4f55\u5ea6\u91cf\u80fd\u5f3a\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\uff0c\u4e14\u5177\u6709\u8de8\u9886\u57df\u901a\u7528\u6027\u548c\u56e0\u679c\u6027\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u51e0\u4f55\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5bfb\u627e\u80fd\u591f\u8de8\u9886\u57df\u3001\u65e0\u6807\u7b7e\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u901a\u7528\u5ea6\u91cf\u6307\u6807\u3002", "method": "\u5206\u679052\u4e2a\u9884\u8bad\u7ec3ImageNet\u6a21\u578b\uff0813\u79cd\u67b6\u6784\uff09\uff0c\u4f7f\u7528\u6709\u6548\u7ef4\u5ea6\u4f5c\u4e3a\u51e0\u4f55\u5ea6\u91cf\uff0c\u5728CV\u548cNLP\u4efb\u52a1\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u548cPCA\u8fdb\u884c\u56e0\u679c\u5b9e\u9a8c\u3002", "result": "\u8f93\u51fa\u6709\u6548\u7ef4\u5ea6\u4e0e\u51c6\u786e\u7387\u5f3a\u76f8\u5173\uff08\u90e8\u5206r=0.75\uff09\uff0c\u603b\u538b\u7f29\u4e0e\u51c6\u786e\u7387\u8d1f\u76f8\u5173\uff08r=-0.72\uff09\u3002\u8be5\u5173\u7cfb\u5728ImageNet\u3001CIFAR-10\u3001SST-2/MNLI\u548cAG News\u4efb\u52a1\u4e2d\u5747\u6210\u7acb\u3002\u51e0\u4f55\u9000\u5316\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff08r=-0.94\uff09\uff0cPCA\u6539\u5584\u51e0\u4f55\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u6709\u6548\u7ef4\u5ea6\u63d0\u4f9b\u4e86\u8de8\u9886\u57df\u7684\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u9884\u6d4b\u548c\u56e0\u679c\u4fe1\u606f\uff0c\u5b8c\u5168\u65e0\u9700\u6807\u7b7e\u8ba1\u7b97\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.00613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00613", "abs": "https://arxiv.org/abs/2602.00613", "authors": ["Nsrin Ashraf", "Mariam Labib", "Hamada Nayel"], "title": "Transformer-Based Model for Multilingual Hope Speech Detection", "comment": "5 pages, 1 figure, PolyHope-M shared task at RANLP2025", "summary": "This paper describes a system that has been submitted to the \"PolyHope-M\" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u82f1\u8bed\u548c\u5fb7\u8bed\u5e0c\u671b\u8a00\u8bba\u68c0\u6d4b\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528RoBERTa\u5904\u7406\u82f1\u8bed\uff0cXLM-RoBERTa\u5904\u7406\u53cc\u8bed\uff0c\u5728RANLP2025\u7684PolyHope-M\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5e0c\u671b\u8a00\u8bba\u68c0\u6d4b\u662f\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u8de8\u8bed\u8a00\u6a21\u578b\u6765\u8bc6\u522b\u79ef\u6781\u3001\u9f13\u821e\u4eba\u5fc3\u7684\u5185\u5bb9\u3002\u5f53\u524d\u9700\u8981\u8bc4\u4f30\u4e0d\u540cTransformer\u6a21\u578b\u5728\u82f1\u8bed\u548c\u5fb7\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4e3a\u82f1\u8bed\u5b9e\u73b0RoBERTa\u6a21\u578b\uff0c\u4e3a\u82f1\u8bed\u548c\u5fb7\u8bed\u5b9e\u73b0\u591a\u8bed\u8a00\u6a21\u578bXLM-RoBERTa\uff0c\u5728PolyHope-M\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e0c\u671b\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "RoBERTa\u5728\u82f1\u8bed\u4e0a\u83b7\u5f97\u52a0\u6743F1\u5206\u65700.818\u548c\u51c6\u786e\u738781.8%\uff1bXLM-RoBERTa\u83b7\u5f97\u52a0\u6743F1\u5206\u65700.786\u548c\u51c6\u786e\u738778.5%\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0cRoBERTa\u5728\u5355\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u591a\u8bed\u8a00\u6a21\u578bXLM-RoBERTa\u3002"}}
{"id": "2602.00174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00174", "abs": "https://arxiv.org/abs/2602.00174", "authors": ["Jiajun Zhao", "Xuan Yang"], "title": "Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation", "comment": "5 pages, 7 figures, accepted by ICASSP 2026", "summary": "We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.", "AI": {"tldr": "\u63d0\u51faSPCL\u6846\u67b6\u7528\u4e8e\u5fc3\u810f\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\"\u4e0d\u5173\u5fc3\u6837\u672c\"\u6982\u5ff5\u548c\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\u89e3\u51b3\u8fb9\u754c\u533a\u57df\u8868\u793a\u6c61\u67d3\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5272\u8d28\u91cf\u548c\u8fb9\u754c\u7cbe\u5ea6\u3002", "motivation": "\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u540c\u4e00\u7c7b\u522b\u5185\u90e8\uff08\u5982\u5fc3\u808c\u7ec4\u7ec7\uff09\u7684\u8fb9\u754c\u533a\u57df\u50cf\u7d20\u8868\u793a\u5bb9\u6613\u88ab\u6c61\u67d3\uff0c\u5bfc\u81f4\u5206\u5272\u7cbe\u5ea6\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u533a\u5206\u540c\u4e00\u7c7b\u522b\u5185\u90e8\u4e0d\u540c\u533a\u57df\uff08\u5185\u90e8\u533a\u57df\u548c\u8fb9\u754c\u533a\u57df\uff09\u7684\u8868\u793a\u5dee\u5f02\u3002", "method": "\u63d0\u51faSPCL\u6846\u67b6\uff1a1) \u5f15\u5165\"\u4e0d\u5173\u5fc3\u6837\u672c\"\u6982\u5ff5\uff0c\u533a\u5206\u540c\u4e00\u7c7b\u522b\u5185\u90e8\u533a\u57df\u548c\u8fb9\u754c\u533a\u57df\u7684\u50cf\u7d20\u8868\u793a\uff1b2) \u8bbe\u8ba1\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\uff0c\u589e\u5f3a\u8de8\u8fb9\u754c\u8868\u793a\u7684\u5224\u522b\u80fd\u529b\uff1b3) \u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e0d\u5173\u5fc3\u6837\u672c\u548c\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\u7684\u4f18\u52bf\u3002", "result": "\u5728\u516c\u5f00\u5fc3\u810f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPCL\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u5728\u5206\u5272\u8d28\u91cf\u548c\u8fb9\u754c\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SPCL\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u8fb9\u754c\u533a\u57df\u8868\u793a\u6c61\u67d3\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u8fb9\u754c\u533a\u57df\u3002\u63d0\u51fa\u7684\"\u4e0d\u5173\u5fc3\u6837\u672c\"\u6982\u5ff5\u548c\u8fb9\u754c\u5bf9\u6bd4\u635f\u5931\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u601d\u8def\u3002"}}
{"id": "2602.02052", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.02052", "abs": "https://arxiv.org/abs/2602.02052", "authors": ["Roland Griesmaier", "Bastian Harrach", "Jianli Xiang"], "title": "Monotonicity-based regularization of inverse medium scattering for shape reconstruction", "comment": null, "summary": "We consider the scattering of time-harmonic plane waves by a compactly supported inhomogeneous scattering obstacle governed by the Helmholtz equation. Given far field observations of the scattered fields corresponding to plane wave incident fields for all possible incident and observation directions we study the inverse problem to recover the support of the scatterer. We propose a qualitative monotonicity-based regularization scheme which combines monotonicity-based shape reconstruction with one-step linearization to reconstruct a discrete approximation of the shape of the scatterer from noisy far field data. The purpose of the one-step linearization is to stabilize the monotonicity approach to shape reconstruction. We show that the monotonicity-based regularization scheme recovers the correct shape of the scatterer for noise-free data. Furthermore, we establish that the solution of the monotonicity-based regularization converges to the exact solution as the noise level tends to zero. We present numerical examples to illustrate our theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5355\u8c03\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5355\u8c03\u6027\u5f62\u72b6\u91cd\u5efa\u548c\u4e00\u6b65\u7ebf\u6027\u5316\uff0c\u4ece\u542b\u566a\u8fdc\u573a\u6570\u636e\u4e2d\u91cd\u5efa\u6563\u5c04\u4f53\u652f\u6491\u96c6", "motivation": "\u89e3\u51b3\u4ece\u5168\u65b9\u5411\u5165\u5c04\u548c\u89c2\u6d4b\u7684\u8fdc\u573a\u6570\u636e\u4e2d\u6062\u590d\u6563\u5c04\u4f53\u652f\u6491\u96c6\u7684\u9006\u6563\u5c04\u95ee\u9898\uff0c\u4f20\u7edf\u5355\u8c03\u6027\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u9700\u8981\u7a33\u5b9a\u5316\u5904\u7406", "method": "\u7ed3\u5408\u5355\u8c03\u6027\u5f62\u72b6\u91cd\u5efa\u4e0e\u4e00\u6b65\u7ebf\u6027\u5316\u7684\u6b63\u5219\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u7a33\u5b9a\u5355\u8c03\u6027\u65b9\u6cd5\uff0c\u4ece\u542b\u566a\u8fdc\u573a\u6570\u636e\u91cd\u5efa\u6563\u5c04\u4f53\u5f62\u72b6\u7684\u79bb\u6563\u8fd1\u4f3c", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u65e0\u566a\u58f0\u6570\u636e\u65f6\u80fd\u6b63\u786e\u6062\u590d\u6563\u5c04\u4f53\u5f62\u72b6\uff1b\u566a\u58f0\u8d8b\u4e8e\u96f6\u65f6\u6b63\u5219\u5316\u89e3\u6536\u655b\u5230\u7cbe\u786e\u89e3\uff1b\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c", "conclusion": "\u63d0\u51fa\u7684\u5355\u8c03\u6027\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u7a33\u5b9a\u5f62\u72b6\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5728\u7406\u8bba\u548c\u6570\u503c\u4e0a\u5747\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\uff0c\u4e3a\u6563\u5c04\u4f53\u652f\u6491\u96c6\u6062\u590d\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848"}}
{"id": "2602.00608", "categories": ["cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00608", "abs": "https://arxiv.org/abs/2602.00608", "authors": ["Wei Zeng", "Xuchen Li", "Ruili Feng", "Zhen Liu", "Fengwei An", "Jian Zhao"], "title": "Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design", "comment": "Preprint, Under Review", "summary": "Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \\times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \\textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \\times 480$ resolution -- a $50\\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.", "AI": {"tldr": "\u63d0\u51fa\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u89e3\u51b3\u751f\u6210\u5f0f\u6e38\u620f\u5f15\u64ce\u7684\"\u5185\u5b58\u5899\"\u95ee\u9898\uff0c\u5b9e\u73b0720\u00d7480\u5206\u8fa8\u7387\u5b9e\u65f6\u751f\u6210\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534750\u500d\u50cf\u7d20\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u5b9e\u65f6\u751f\u6210\u5f0f\u6e38\u620f\u5f15\u64ce\u53d7\u9650\u4e8e\"\u5185\u5b58\u5899\"\uff0c\u53ea\u80fd\u5b9e\u73b0\u4f4e\u5206\u8fa8\u7387\uff08\u598264\u00d764\uff09\u90e8\u7f72\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u5206\u8fa8\u7387\u795e\u7ecf\u6a21\u62df\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u4e16\u754c\u6a21\u578b\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u7684\u89e3\u7801\u5668\u89e3\u8026\u5230AI\u52a0\u901f\u5668\u96c6\u7fa4\u4e0a\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u975e\u5bf9\u79f0\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff1b2) \u5185\u5b58\u4e2d\u5fc3\u7b97\u5b50\u878d\u5408\u65b9\u6848\uff1b3) \u6d41\u5f62\u611f\u77e5\u6f5c\u5728\u5916\u63a8\u673a\u5236\u3002", "result": "\u5728\u53ef\u7f16\u7a0bAI\u52a0\u901f\u5668\u96c6\u7fa4\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u73b0720\u00d7480\u5206\u8fa8\u7387\u5b9e\u65f6\u751f\u6210\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534750\u500d\u50cf\u7d20\u541e\u5410\u91cf\uff0c\u5728\u8fde\u7eed3D\u8d5b\u8f66\u548c\u79bb\u65632D\u5e73\u53f0\u6e38\u620f\u4e2d\u5206\u522b\u8fbe\u523026.4 FPS\u548c48.3 FPS\uff0c\u644a\u9500\u6709\u6548\u5ef6\u8fdf2.7\u6beb\u79d2\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\"\u5185\u5b58\u5899\"\u95ee\u9898\u4e0d\u4ec5\u662f\u4f18\u5316\uff0c\u66f4\u662f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u54cd\u5e94\u5f0f\u795e\u7ecf\u6e38\u620f\u4f53\u9a8c\u7684\u524d\u63d0\u6761\u4ef6\u3002"}}
{"id": "2602.00158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00158", "abs": "https://arxiv.org/abs/2602.00158", "authors": ["Ziqi Gao", "Yaotian Zhu", "Qingcheng Zeng", "Xu Zhao", "Ziqing Wang", "Feng Ruan", "Kaize Ding"], "title": "RAPTOR: Ridge-Adaptive Logistic Probes", "comment": "Preprint", "summary": "Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.", "AI": {"tldr": "RAPTOR\u662f\u4e00\u79cd\u57fa\u4e8eL2\u6b63\u5219\u5316\u903b\u8f91\u56de\u5f52\u7684\u8f7b\u91cf\u7ea7\u63a2\u9488\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u51bb\u7ed3LLM\u7684\u5c42\u8868\u793a\u4e2d\u63d0\u53d6\u6982\u5ff5\u5411\u91cf\uff0c\u5728\u51c6\u786e\u6027\u3001\u65b9\u5411\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u9488\u5206\u6790\u7528\u4e8e\u7406\u89e3\u51bb\u7ed3LLM\u5c42\u8868\u793a\u4e2d\u7f16\u7801\u7684\u4fe1\u606f\uff0c\u800c\u63a2\u9488-\u5f15\u5bfc\u7ba1\u9053\u9700\u8981\u51c6\u786e\u3001\u65b9\u5411\u7a33\u5b9a\u4e14\u4f4e\u6210\u672c\u83b7\u53d6\u7684\u6982\u5ff5\u5411\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u9700\u6c42\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRAPTOR\uff08Ridge-Adaptive Logistic Probe\uff09\uff0c\u4e00\u79cd\u7b80\u5355\u7684L2\u6b63\u5219\u5316\u903b\u8f91\u63a2\u9488\uff0c\u901a\u8fc7\u9a8c\u8bc1\u8c03\u4f18\u7684\u5cad\u5f3a\u5ea6\u4ece\u5f52\u4e00\u5316\u6743\u91cd\u4e2d\u63d0\u53d6\u6982\u5ff5\u5411\u91cf\u3002", "result": "\u5728\u6307\u4ee4\u8c03\u4f18LLM\u548c\u4eba\u5de5\u7f16\u5199\u6982\u5ff5\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cRAPTOR\u5728\u51c6\u786e\u6027\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u83b7\u5f97\u7ade\u4e89\u6027\u7684\u65b9\u5411\u7a33\u5b9a\u6027\u548c\u663e\u8457\u66f4\u4f4e\u7684\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "RAPTOR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u6982\u5ff5\u5411\u91cf\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u51f8\u9ad8\u65af\u6700\u5c0f\u6700\u5927\u5b9a\u7406\u7684\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u5cad\u5f3a\u5ea6\u5982\u4f55\u8c03\u8282\u63a2\u9488\u51c6\u786e\u6027\u548c\u6982\u5ff5\u5411\u91cf\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.00619", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00619", "abs": "https://arxiv.org/abs/2602.00619", "authors": ["Yuxuan Lu", "Yongkang Guo", "Yuqing Kong"], "title": "Jailbreaking LLMs via Calibration", "comment": null, "summary": "Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower \"Jailbreak Tax\" compared with existing methods, especially on the safety-hardened gpt-oss-120b.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u5b89\u5168\u5bf9\u9f50\u89c6\u4e3a\u9884\u5bf9\u9f50\u5206\u5e03\u7cfb\u7edf\u6027\u626d\u66f2\u7684\u6846\u67b6\uff0c\u5c06\u5f31\u5230\u5f3a\u8d8a\u72f1\u5efa\u6a21\u4e3a\u9884\u6d4b\u805a\u5408\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u805a\u5408\u7b56\u7565\uff0c\u5e76\u5c55\u793a\u73b0\u6709\u65b9\u6cd5\u53ea\u662f\u8be5\u6846\u67b6\u7684\u7279\u4f8b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u4f1a\u5728\u6a21\u578b\u5bf9\u9f50\u8f93\u51fa\u4e0e\u5e95\u5c42\u9884\u5bf9\u9f50\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u4ea7\u751f\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u548c\u6539\u8fdb\u8d8a\u72f1\u65b9\u6cd5\u3002", "method": "\u5c06\u5b89\u5168\u5bf9\u9f50\u5bf9\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u5f71\u54cd\u5efa\u6a21\u4e3a\u9884\u5bf9\u9f50\u5206\u5e03\u7684\u7cfb\u7edf\u6027\u626d\u66f2\uff0c\u5c06\u5f31\u5230\u5f3a\u8d8a\u72f1\u89c6\u4e3a\u9884\u6d4b\u805a\u5408\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u635f\u5931\u8bf1\u5bfc\u5bf9\u5076\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u504f\u79fb\u6700\u4f18\u805a\u5408\u7b56\u7565\u3002", "result": "\u5728\u7ea2\u961f\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u5b66\u6548\u7528\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728\u5b89\u5168\u52a0\u56fa\u7684gpt-oss-120b\u4e0a\u8868\u73b0\u5c24\u5176\u51fa\u8272\uff0c\u4e14\"\u8d8a\u72f1\u7a0e\"\u66f4\u4f4e\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u7406\u89e3\u5b89\u5168\u5bf9\u9f50\u6548\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63a8\u5bfc\u7684\u6700\u4f18\u805a\u5408\u7b56\u7565\u5728\u5b9e\u9645\u8d8a\u72f1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u7279\u522b\u662f\u5bf9\u5b89\u5168\u52a0\u56fa\u6a21\u578b\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2602.00176", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00176", "abs": "https://arxiv.org/abs/2602.00176", "authors": ["Feng Tian", "Yixuan Li", "Weili Zeng", "Weitian Zhang", "Yichao Yan", "Xiaokang Yang"], "title": "Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation", "comment": null, "summary": "Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u566a\u58f0-\u9891\u7387\u5ef6\u7eed\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e2d\u95f4\u540e\u9a8c\u5206\u5e03\u5bb6\u65cf\uff0c\u5728\u566a\u58f0\u4f9d\u8d56\u7684\u9891\u7387\u5e26\u5185\u5b9e\u65bd\u6d4b\u91cf\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u6269\u6563\u540e\u9a8c\u91c7\u6837\u5728\u7ec6\u8282\u6062\u590d\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\u548c\u6d4b\u91cf\u4e00\u81f4\u6027\u5f15\u5bfc\uff0c\u4f46\u5e38\u56e0\u6d4b\u91cf\u9879\u4e0e\u6269\u6563\u566a\u58f0\u6c34\u5e73\u5f31\u8026\u5408\u800c\u65e0\u6cd5\u6062\u590d\u7ec6\u8282\u3002\u9ad8\u566a\u58f0\u4e0b\u6570\u636e\u4e00\u81f4\u6027\u68af\u5ea6\u4e0e\u540e\u9a8c\u51e0\u4f55\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u65e9\u671f\u6f02\u79fb\u3001\u4f2a\u9ad8\u9891\u4f2a\u5f71\u4ee5\u53ca\u5bf9\u8c03\u5ea6\u548c\u75c5\u6001\u7b97\u5b50\u7684\u654f\u611f\u6027\u3002", "method": "\u63d0\u51fa\u566a\u58f0-\u9891\u7387\u5ef6\u7eed\u6846\u67b6\uff0c\u6784\u5efa\u4e2d\u95f4\u540e\u9a8c\u5206\u5e03\u5bb6\u65cf\uff0c\u5176\u4f3c\u7136\u4ec5\u5728\u566a\u58f0\u4f9d\u8d56\u7684\u9891\u7387\u5e26\u5185\u5f3a\u5236\u6d4b\u91cf\u4e00\u81f4\u6027\u3002\u5b9e\u73b0\u7a33\u5b9a\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u7ed3\u5408\u6269\u6563\u9884\u6d4b\u5668\u3001\u5e26\u9650\u4f3c\u7136\u5f15\u5bfc\u548c\u591a\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u79ef\u6781\u91c7\u7528\u53ef\u9760\u7c97\u7c92\u5ea6\u6821\u6b63\uff0c\u4fdd\u5b88\u91c7\u7528\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5728\u8d85\u5206\u8fa8\u7387\u3001\u4fee\u590d\u548c\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8fd0\u52a8\u53bb\u6a21\u7ccaPSNR\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe5dB\u3002", "conclusion": "\u566a\u58f0-\u9891\u7387\u5ef6\u7eed\u6846\u67b6\u901a\u8fc7\u5e26\u9650\u6d4b\u91cf\u4e00\u81f4\u6027\u6709\u6548\u89e3\u51b3\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u7ec6\u8282\u6062\u590d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9006\u95ee\u9898\u6c42\u89e3\u6027\u80fd\u3002"}}
{"id": "2602.02066", "categories": ["math.NA", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02066", "abs": "https://arxiv.org/abs/2602.02066", "authors": ["David Krieg", "Mario Ullrich"], "title": "Approximation of Functions: Optimal Sampling and Complexity", "comment": "This is a preliminary version of an article to appear in Acta Numerica", "summary": "We consider approximation or recovery of functions based on a finite number of function evaluations. This is a well-studied problem in optimal recovery, machine learning, and numerical analysis in general, but many fundamental insights were obtained only recently. We discuss different aspects of the information-theoretic limit that appears because of the limited amount of data available, as well as algorithms and sampling strategies that come as close to it as possible.\n  We also discuss (optimal) sampling in a broader sense, allowing other types of measurements that may be nonlinear, adaptive and random, and present several relations between the different settings in the spirit of information-based complexity. We hope that this article provides both, a basic introduction to the subject and a contemporary summary of the current state of research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6709\u9650\u51fd\u6570\u8bc4\u4f30\u7684\u51fd\u6570\u903c\u8fd1\u4e0e\u6062\u590d\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u4fe1\u606f\u8bba\u6781\u9650\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u975e\u7ebf\u6027\u3001\u81ea\u9002\u5e94\u548c\u968f\u673a\u6d4b\u91cf\u7b49\u66f4\u5e7f\u6cdb\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u5728\u6700\u4f18\u6062\u590d\u3001\u673a\u5668\u5b66\u4e60\u548c\u6570\u503c\u5206\u6790\u4e2d\uff0c\u57fa\u4e8e\u6709\u9650\u6570\u636e\u70b9\u8fdb\u884c\u51fd\u6570\u903c\u8fd1\u662f\u4e00\u4e2a\u57fa\u7840\u95ee\u9898\u3002\u8fd1\u5e74\u6765\u867d\u7136\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u8bb8\u591a\u57fa\u672c\u89c1\u89e3\u76f4\u5230\u6700\u8fd1\u624d\u88ab\u63ed\u793a\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u4fe1\u606f\u8bba\u6781\u9650\u548c\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u548c\u57fa\u4e8e\u4fe1\u606f\u590d\u6742\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u91c7\u6837\u7b56\u7565\uff08\u5305\u62ec\u975e\u7ebf\u6027\u3001\u81ea\u9002\u5e94\u548c\u968f\u673a\u6d4b\u91cf\uff09\u4e0b\u7684\u51fd\u6570\u6062\u590d\u95ee\u9898\uff0c\u63a2\u8ba8\u7b97\u6cd5\u5982\u4f55\u903c\u8fd1\u4fe1\u606f\u8bba\u6781\u9650\u3002", "result": "\u7cfb\u7edf\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u72b6\u6001\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u4fe1\u606f\u8bba\u6781\u9650\u7684\u5171\u6027\u4e0e\u5dee\u5f02\uff0c\u63d0\u4f9b\u4e86\u63a5\u8fd1\u8fd9\u4e9b\u6781\u9650\u7684\u7b97\u6cd5\u548c\u91c7\u6837\u7b56\u7565\u7684\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e2\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u57fa\u7840\u4ecb\u7ecd\uff0c\u53c8\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u524d\u6cbf\uff0c\u5f3a\u8c03\u4e86\u4fe1\u606f\u8bba\u6781\u9650\u5728\u6709\u9650\u6570\u636e\u51fd\u6570\u6062\u590d\u4e2d\u7684\u6838\u5fc3\u5730\u4f4d\uff0c\u4ee5\u53ca\u4e0d\u540c\u91c7\u6837\u8bbe\u7f6e\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002"}}
{"id": "2602.00611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00611", "abs": "https://arxiv.org/abs/2602.00611", "authors": ["Jiaqi Xu", "Tao Huang", "Kai Zhang"], "title": "Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome", "comment": null, "summary": "Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd7B\u53c2\u6570LLM\u5728VirtualHome\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u81ea\u6d3d\u89e3\u7801\u7b56\u7565SSC\u6765\u63d0\u5347\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1\u6027\u80fd\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u5177\u8eabAI\u9700\u8981\u667a\u80fd\u4f53\u7406\u89e3\u76ee\u6807\u3001\u89c4\u5212\u52a8\u4f5c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u3002\u5f53\u524d\u9700\u8981\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eabAI\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u63d0\u5347\u7ed3\u6784\u5316\u751f\u6210\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Embodied Agent Interface\u6846\u67b6\u5728VirtualHome\u57fa\u51c6\u4e0a\u8bc4\u4f30OPENPANGU-7B\u548cQWEN2.5-7B\u6a21\u578b\u3002\u63d0\u51fa\u7ed3\u6784\u5316\u81ea\u6d3d\u89e3\u7801\u7b56\u7565SSC\uff0c\u901a\u8fc7\u591a\u6b21\u91c7\u6837\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u6295\u7968\u673a\u5236\u6765\u63d0\u5347\u7ed3\u6784\u5316\u751f\u6210\u8d28\u91cf\u3002\u8bc4\u4f30\u56db\u4e2a\u57fa\u672c\u4efb\u52a1\uff1a\u76ee\u6807\u89e3\u91ca\u3001\u52a8\u4f5c\u5e8f\u5217\u3001\u5b50\u76ee\u6807\u5206\u89e3\u548c\u72b6\u6001\u8f6c\u79fb\u5efa\u6a21\u3002", "result": "SSC\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002OPENPANGU-7B\u5728\u5c42\u6b21\u5316\u89c4\u5212\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800cQWEN2.5-7B\u5728\u52a8\u4f5c\u7ea7\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\u3002\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u5c55\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540cLLM\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u63d0\u51fa\u7684SSC\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.00159", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.00159", "abs": "https://arxiv.org/abs/2602.00159", "authors": ["Aneeqa Mehrab", "Jan Willem Van Looy", "Pietro Demurtas", "Stefano Iotti", "Emil Malucelli", "Francesca Rossi", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Sheaf Neural Networks and biomedical applications", "comment": null, "summary": "The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5c42\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u7406\u8bba\u4e0e\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u751f\u7269\u533b\u5b66\u6848\u4f8b\u7814\u7a76\u5c55\u793aSNN\u4f18\u4e8e\u4e3b\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GCN\u3001GAT\u3001GraphSage\uff09\u7684\u6027\u80fd", "motivation": "\u5f53\u524d\u56fe\u795e\u7ecf\u7f51\u7edc\uff08\u5982GCN\u3001GAT\u3001GraphSage\uff09\u5728\u751f\u7269\u533b\u5b66\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u6765\u5904\u7406\u590d\u6742\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\u7ed3\u6784", "method": "\u63d0\u51fa\u5c42\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7b97\u6cd5\uff0c\u5efa\u7acb\u5176\u7406\u8bba\u6846\u67b6\u548c\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u5728\u5177\u4f53\u751f\u7269\u533b\u5b66\u6848\u4f8b\u7814\u7a76\u4e2d\u5e94\u7528\u8be5\u7b97\u6cd5", "result": "SNN\u5728\u751f\u7269\u533b\u5b66\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u4e3b\u6d41\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\uff08GCN\u3001GAT\u3001GraphSage\uff09", "conclusion": "\u5c42\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\u5206\u6790\u5de5\u5177\uff0c\u5728\u89e3\u51b3\u590d\u6742\u751f\u7269\u533b\u5b66\u95ee\u9898\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5"}}
{"id": "2602.00638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00638", "abs": "https://arxiv.org/abs/2602.00638", "authors": ["Yingji Zhang"], "title": "Formal Semantic Control over Language Models", "comment": null, "summary": "This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7VAE\u6846\u67b6\u63d0\u5347\u8bed\u8a00\u8868\u793a/\u6a21\u578b\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u5c40\u90e8\u3001\u51c6\u7b26\u53f7\u3001\u7ec4\u5408\u5f0f\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u63a7\u5236\uff0c\u5206\u522b\u5728\u53e5\u5b50\u5c42\u9762\u548c\u63a8\u7406\u5c42\u9762\u8fdb\u884c\u8bed\u4e49\u7279\u5f81\u89e3\u8026\u4e0e\u64cd\u7eb5\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8bed\u4e49\u8868\u793a\u7f3a\u4e4f\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u3001\u7cbe\u786e\u7ed3\u6784\u548c\u53ef\u9760\u53ef\u63a7\u6027\u3002\u8bba\u6587\u65e8\u5728\u4f7f\u8bed\u8a00\u8868\u793a/\u6a21\u578b\u5728\u8bed\u4e49\u548c\u51e0\u4f55\u4e0a\u66f4\u53ef\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u5851\u9020\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u5b9e\u73b0\u5c40\u90e8\u5316\u3001\u51c6\u7b26\u53f7\u5316\u3001\u7ec4\u5408\u5f0f\u7684\u63a7\u5236\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u6846\u67b6\uff0c\u63a2\u7d22\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff1a(1) \u53e5\u5b50\u7ea7\u5b66\u4e60\u4e0e\u63a7\u5236\uff1a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u89e3\u8026\u548c\u64cd\u7eb5\u7279\u5b9a\u8bed\u4e49\u7279\u5f81\u4ee5\u6307\u5bfc\u53e5\u5b50\u751f\u6210\uff0c\u4ee5\u89e3\u91ca\u6027\u6587\u672c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff1b(2) \u63a8\u7406\u7ea7\u5b66\u4e60\u4e0e\u63a7\u5236\uff1a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9694\u79bb\u548c\u5f15\u5bfc\u63a8\u7406\u884c\u4e3a\u4ee5\u63a7\u5236\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\uff0c\u4e13\u6ce8\u4e8e\u89e3\u91ca\u6027NLI\u4efb\u52a1\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u9896\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u76f8\u5e94\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6574\u7bc7\u8bba\u6587\u4e2d\u589e\u5f3a\u4e86\u81ea\u7136\u8bed\u8a00\u6f5c\u5728\u7a7a\u95f4\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u8bed\u4e49\u8868\u793a\u5b66\u4e60\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8bed\u4e49\u8868\u793a\u80fd\u591f\u88ab\u7cfb\u7edf\u89e3\u91ca\u3001\u7cbe\u786e\u7ed3\u6784\u5316\u5e76\u53ef\u9760\u63a7\u5236\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u8bed\u8a00\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.00181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00181", "abs": "https://arxiv.org/abs/2602.00181", "authors": ["Hang Wu", "Yujun Cai", "Zehao Li", "Haonan Ge", "Bowen Sun", "Junsong Yuan", "Yiwei Wang"], "title": "CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning", "comment": null, "summary": "Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.", "AI": {"tldr": "CamReasoner\uff1a\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u7406\u89e3\u76f8\u673a\u8fd0\u52a8\uff0c\u91c7\u7528\u89c2\u5bdf-\u601d\u8003-\u56de\u7b54\u8303\u5f0f\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u51e0\u4f55\u57fa\u7840\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u5c06\u76f8\u673a\u8fd0\u52a8\u7406\u89e3\u89c6\u4e3a\u9ed1\u76d2\u5206\u7c7b\u4efb\u52a1\uff0c\u7ecf\u5e38\u6df7\u6dc6\u7269\u7406\u4e0a\u4e0d\u540c\u7684\u8fd0\u52a8\uff0c\u4f9d\u8d56\u8868\u9762\u89c6\u89c9\u6a21\u5f0f\u800c\u975e\u51e0\u4f55\u7ebf\u7d22\u3002\u9700\u8981\u5efa\u7acb\u611f\u77e5\u4e0e\u7535\u5f71\u903b\u8f91\u4e4b\u95f4\u7684\u6865\u6881\u3002", "method": "\u63d0\u51faCamReasoner\u6846\u67b6\uff0c\u91c7\u7528\u89c2\u5bdf-\u601d\u8003-\u56de\u7b54\uff08O-T-A\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u89e3\u7801\u65f6\u7a7a\u7ebf\u7d22\uff08\u8f68\u8ff9\u548c\u89c6\u9525\u4f53\uff09\u3002\u6784\u5efa\u5927\u89c4\u6a21\u63a8\u7406\u8f68\u8ff9\u5957\u4ef6\uff0818k SFT\u63a8\u7406\u94fe\u548c38k RL\u53cd\u9988\u6837\u672c\uff09\uff0c\u9996\u6b21\u5728\u8be5\u9886\u57df\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u903b\u8f91\u5bf9\u9f50\u3002", "result": "CamReasoner\u6709\u6548\u6291\u5236\u5e7b\u89c9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u8fd0\u52a8\u63a8\u7406\u57fa\u4e8e\u7269\u7406\u51e0\u4f55\u800c\u975e\u4e0a\u4e0b\u6587\u731c\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u76f8\u673a\u8fd0\u52a8\u7406\u89e3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u903b\u8f91\u5bf9\u9f50\uff0cCamReasoner\u6210\u529f\u5f25\u5408\u4e86\u611f\u77e5\u4e0e\u7535\u5f71\u903b\u8f91\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u89c6\u9891\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u76f8\u673a\u52a8\u6001\u7406\u89e3\u65b9\u6cd5\u3002"}}
{"id": "2602.02068", "categories": ["math.NA", "math-ph", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.02068", "abs": "https://arxiv.org/abs/2602.02068", "authors": ["Jemal Rogava", "Zurab Vashakidze"], "title": "On the Numerical Treatment of an Abstract Nonlinear System of Coupled Hyperbolic Equations Associated with the Timoshenko Model", "comment": "45 pages and 27 figures in total", "summary": "The present work addresses the Cauchy problem for an abstract nonlinear system of coupled hyperbolic equations associated with the Timoshenko model in a real Hilbert space. Our purpose is to develop and delve into a temporal discretization scheme for approximating a solution to this problem. To this end, we propose a symmetric three-layer semi-discrete time-stepping scheme in which the nonlinear term is evaluated at the temporal midpoint. As a result, at each time step, this approach reduces the original nonlinear problem to a linear one and enables parallel computation of its solution. Convergence is proved, and second-order accuracy with respect to the time-step size is established on a local temporal interval. The proposed scheme is then applied to a spatially one-dimensional nonlinear dynamic Timoshenko beam system, and the results obtained for the abstract nonlinear system are extended to this setting. A Legendre-Galerkin spectral approximation is employed for the spatial discretization. By taking differences of Legendre polynomials within the Galerkin framework, the resulting linear system is sparse and can be efficiently decoupled. The convergence of the method is also investigated. Finally, several numerical experiments on carefully chosen benchmark problems are conducted to validate the proposed approach and to confirm the theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u79f0\u4e09\u5c42\u534a\u79bb\u6563\u65f6\u95f4\u6b65\u8fdb\u683c\u5f0f\uff0c\u7528\u4e8e\u6c42\u89e3\u975e\u7ebf\u6027Timoshenko\u6881\u6a21\u578b\u7684\u67ef\u897f\u95ee\u9898\uff0c\u7ed3\u5408Legendre-Galerkin\u8c31\u65b9\u6cd5\u8fdb\u884c\u7a7a\u95f4\u79bb\u6563\uff0c\u8bc1\u660e\u4e86\u4e8c\u9636\u65f6\u95f4\u7cbe\u5ea6\u548c\u6536\u655b\u6027\u3002", "motivation": "\u9488\u5bf9\u62bd\u8c61\u975e\u7ebf\u6027\u8026\u5408\u53cc\u66f2\u65b9\u7a0b\uff08Timoshenko\u6a21\u578b\uff09\u7684\u67ef\u897f\u95ee\u9898\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u65f6\u95f4\u79bb\u6563\u5316\u65b9\u6848\uff0c\u5c06\u975e\u7ebf\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u5bf9\u79f0\u4e09\u5c42\u534a\u79bb\u6563\u65f6\u95f4\u6b65\u8fdb\u683c\u5f0f\uff0c\u975e\u7ebf\u6027\u9879\u5728\u65f6\u95f4\u4e2d\u70b9\u5904\u8ba1\u7b97\uff1b\u7a7a\u95f4\u79bb\u6563\u91c7\u7528Legendre-Galerkin\u8c31\u65b9\u6cd5\uff0c\u5229\u7528Legendre\u591a\u9879\u5f0f\u5dee\u5206\u4ea7\u751f\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u3002", "result": "\u8bc1\u660e\u4e86\u683c\u5f0f\u7684\u6536\u655b\u6027\u548c\u4e8c\u9636\u65f6\u95f4\u7cbe\u5ea6\uff1b\u5e94\u7528\u4e8e\u4e00\u7ef4\u975e\u7ebf\u6027\u52a8\u6001Timoshenko\u6881\u7cfb\u7edf\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6210\u529f\u5c06\u975e\u7ebf\u6027\u95ee\u9898\u7ebf\u6027\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\uff0c\u4e3aTimoshenko\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u503c\u6c42\u89e3\u65b9\u6848\u3002"}}
{"id": "2602.00616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00616", "abs": "https://arxiv.org/abs/2602.00616", "authors": ["Minhyuk Lee", "Hyekyung Yoon", "Myungjoo Kang"], "title": "Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees", "comment": null, "summary": "Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u603b\u53d8\u5dee\u7406\u8bba\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u63d0\u793a\u6295\u5f71\u5728\u4fdd\u6301\u826f\u6027\u63d0\u793a\u5bf9\u9f50\u7684\u540c\u65f6\u51cf\u5c11\u4e0d\u5b89\u5168\u751f\u6210", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u9700\u8981\u5728\u4e0d\u964d\u4f4e\u826f\u6027\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u6291\u5236\u4e0d\u5b89\u5168\u751f\u6210\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u6027\u4e0e\u5bf9\u9f50\u6027\u7684\u6743\u8861\u95ee\u9898", "method": "\u57fa\u4e8e\u603b\u53d8\u5dee\u7406\u8bba\u63d0\u51fa\u5b89\u5168-\u63d0\u793a\u5bf9\u9f50\u6743\u8861\u6846\u67b6\uff0c\u91c7\u7528\u63a8\u7406\u65f6\u63d0\u793a\u6295\u5f71\u65b9\u6cd5\uff0c\u5bf9\u9ad8\u98ce\u9669\u63d0\u793a\u8fdb\u884c\u9009\u62e9\u6027\u5e72\u9884\uff0c\u5c06\u5176\u6620\u5c04\u5230\u5bb9\u5dee\u63a7\u5236\u7684\u5b89\u200b\u200b\u5168\u96c6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u751f\u6210\u5668", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6269\u6563\u9aa8\u5e72\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4\u5f3a\u6a21\u578b\u7ea7\u5bf9\u9f50\u57fa\u7ebf\uff0c\u4e0d\u5f53\u767e\u5206\u6bd4\u76f8\u5bf9\u51cf\u5c1116.7-60.0%\uff0c\u540c\u65f6\u5728COCO\u4e0a\u4fdd\u6301\u63a5\u8fd1\u672a\u5bf9\u9f50\u53c2\u8003\u7684\u826f\u6027\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50", "conclusion": "\u901a\u8fc7\u603b\u53d8\u5dee\u7406\u8bba\u6846\u67b6\u548c\u63a8\u7406\u65f6\u63d0\u793a\u6295\u5f71\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5b89\u5168\u6027\u4e0e\u5bf9\u9f50\u6027\u7684\u6709\u6548\u6743\u8861\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00161", "categories": ["cs.LG", "cs.AI", "cs.CL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00161", "abs": "https://arxiv.org/abs/2602.00161", "authors": ["David Jansen", "Roman Rausch", "David Montero", "Roman Orus"], "title": "Block removal for large language models through constrained binary optimization", "comment": "7 pages, 5 figures", "summary": "Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Transformer\u5757\u79fb\u9664\u95ee\u9898\u8f6c\u5316\u4e3a\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6620\u5c04\u5230\u7269\u7406\u7cfb\u7edf\uff08\u4f0a\u8f9b\u6a21\u578b\uff09\u6765\u9ad8\u6548\u8bc4\u4f30\u5019\u9009\u914d\u7f6e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u79fb\u9664\u6574\u4e2aTransformer\u5757\u770b\u4f3c\u7b80\u5355\uff0c\u4f46\u786e\u5b9a\u79fb\u9664\u54ea\u4e9b\u5757\u6784\u6210\u6307\u6570\u7ea7\u56f0\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u975e\u8fde\u7eed\u5757\u79fb\u9664\u65b9\u6848\u3002", "method": "\u5c06\u5757\u79fb\u9664\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\u95ee\u9898\uff0c\u6620\u5c04\u5230\u7269\u7406\u7cfb\u7edf\uff08\u4f0a\u8f9b\u6a21\u578b\uff09\uff0c\u5229\u7528\u7cfb\u7edf\u80fd\u91cf\u4f5c\u4e3a\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u7684\u5f3a\u4ee3\u7406\u6307\u6807\u3002\u4ec5\u9700\u5c11\u91cf\u6d3b\u52a8\u53c2\u6570\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\uff0c\u7ed3\u5408\u4f0a\u8f9b\u6c42\u89e3\u5668\u5373\u53ef\u9ad8\u6548\u8bc4\u4f30\u5927\u91cf\u5019\u9009\u914d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5757\u79fb\u9664\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u5728\u77ed\u671f\u91cd\u8bad\u7ec3\u540e\u4ecd\u80fd\u4fdd\u6301\uff0c\u5728MMLU\u57fa\u51c6\u4e0a\u8fbe\u5230\u9ad8\u8fbe6\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\u3002\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u67b6\u6784\uff0c\u5305\u62ec\u5177\u6709\u9ad8\u5ea6\u4e0d\u5747\u5300\u5757\u7ed3\u6784\u7684\u590d\u6742\u6a21\u578b\u3002", "conclusion": "\u5c06\u5757\u79fb\u9664\u95ee\u9898\u8f6c\u5316\u4e3a\u7269\u7406\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684\u975e\u8fde\u7eed\u5757\u79fb\u9664\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u538b\u7f29\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002"}}
{"id": "2602.00642", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00642", "abs": "https://arxiv.org/abs/2602.00642", "authors": ["Haitao Li", "Yifan Chen", "Shuo Miao", "Qian Dong", "Jia Chen", "Yiran Hu", "Junjie Chen", "Minghao Qin", "Qingyao Ai", "Yiqun Liu", "Cheng Luo", "Quan Zhou", "Ya Zhang", "Jikun Hu"], "title": "LegalOne: A Family of Foundation Models for Reliable Legal Reasoning", "comment": "25 pages, v1", "summary": "While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.", "AI": {"tldr": "LegalOne\uff1a\u9488\u5bf9\u4e2d\u56fd\u6cd5\u5f8b\u9886\u57df\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5851\u6027\u8c03\u6574\u91c7\u6837\u3001\u6cd5\u5f8b\u4ee3\u7406\u601d\u7ef4\u94fe\u84b8\u998f\u548c\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4ece\u77e5\u8bc6\u83b7\u53d6\u5230\u81ea\u4e3b\u6cd5\u5f8b\u63a8\u7406\u7684\u6f14\u8fdb\uff0c\u5728\u591a\u9879\u6cd5\u5f8b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u7cbe\u786e\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u96be\u4ee5\u8fdb\u884c\u4e25\u8c28\u7684\u591a\u6b65\u9aa4\u53f8\u6cd5\u63a8\u7406\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6cd5\u5f8b\u9886\u57df\u5f00\u53d1\u57fa\u7840\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff1a1) \u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5851\u6027\u8c03\u6574\u91c7\u6837\u5e73\u8861\u65b0\u77e5\u8bc6\u83b7\u53d6\u4e0e\u539f\u6709\u80fd\u529b\u4fdd\u7559\uff1b2) \u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u6cd5\u5f8b\u4ee3\u7406\u601d\u7ef4\u94fe\u84b8\u998f\u4ece\u539f\u59cb\u6cd5\u5f8b\u6587\u672c\u4e2d\u63d0\u53d6\u663e\u5f0f\u63a8\u7406\uff1b3) \u5b9e\u65bd\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a8\u7406\u7684\u6e10\u8fdb\u8fc7\u7a0b\u5b9e\u73b0\u4ece\u7b80\u5355\u6a21\u5f0f\u5339\u914d\u5230\u81ea\u4e3b\u53ef\u9760\u6cd5\u5f8b\u63a8\u7406\u7684\u6f14\u8fdb\u3002", "result": "LegalOne\u5728\u5e7f\u6cdb\u7684\u6cd5\u5f8b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u77e5\u8bc6\u5bc6\u5ea6\u548c\u6548\u7387\u8d85\u8d8a\u4e86\u53c2\u6570\u89c4\u6a21\u5927\u5f97\u591a\u7684\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u3002\u516c\u5f00\u4e86LegalOne\u6743\u91cd\u548cLegalKit\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "LegalOne\u4e3a\u6cd5\u5f8bAI\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u5728\u9ad8\u98ce\u9669\u53f8\u6cd5\u5e94\u7528\u4e2d\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u4e13\u95e8\u9886\u57df\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.00192", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00192", "abs": "https://arxiv.org/abs/2602.00192", "authors": ["Elif Nebioglu", "Emirhan Bilgi\u00e7", "Adrian Popescu"], "title": "AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange", "comment": "21 pages, 15 figures, 6 tables", "summary": "Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u56fe\u50cf\u4fee\u590d\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u5168\u5c40\u4f2a\u5f71\u800c\u975e\u5c40\u90e8\u5408\u6210\u5185\u5bb9\uff0c\u901a\u8fc7\u5f15\u5165INP-X\u64cd\u4f5c\u548c\u6784\u5efa\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u8bc6\u522b\u771f\u5b9e\u4fee\u590d\u5185\u5bb9\u65b9\u9762\u7684\u4e25\u91cd\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u4fee\u590d\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u903c\u771f\u7684\u5c40\u90e8\u56fe\u50cf\u7f16\u8f91\uff0c\u8fd9\u5bf9\u53ef\u9760\u68c0\u6d4b\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u5168\u5c40\u4f2a\u5f71\uff08\u4f5c\u4e3a\u4fee\u590d\u526f\u4f5c\u7528\uff09\uff0c\u800c\u975e\u5c40\u90e8\u5408\u6210\u5185\u5bb9\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u68c0\u6d4b\u5668\u88ab\u8bef\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86Inpainting Exchange (INP-X)\u64cd\u4f5c\uff0c\u8be5\u64cd\u4f5c\u5728\u7f16\u8f91\u533a\u57df\u5916\u6062\u590d\u539f\u59cb\u50cf\u7d20\uff0c\u540c\u65f6\u4fdd\u7559\u6240\u6709\u5408\u6210\u5185\u5bb9\u3002\u6784\u5efa\u4e86\u5305\u542b9\u4e07\u5f20\u771f\u5b9e\u3001\u4fee\u590d\u548c\u4ea4\u6362\u56fe\u50cf\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5c06\u68c0\u6d4b\u5668\u884c\u4e3a\u4e0eVAE\u4fe1\u606f\u74f6\u9888\u5f15\u8d77\u7684\u9ad8\u9891\u8870\u51cf\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5728INP-X\u5e72\u9884\u4e0b\uff0c\u9884\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\uff08\u5305\u62ec\u5546\u4e1a\u68c0\u6d4b\u5668\uff09\u51c6\u786e\u7387\u6025\u5267\u4e0b\u964d\uff08\u4f8b\u5982\u4ece91%\u964d\u81f355%\uff09\uff0c\u7ecf\u5e38\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u6c34\u5e73\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u68c0\u6d4b\u5668\u6bd4\u6807\u51c6\u4fee\u590d\u68c0\u6d4b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9a\u4f4d\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u4fee\u590d\u68c0\u6d4b\u5668\u5bf9\u5168\u5c40\u4f2a\u5f71\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u800c\u975e\u5173\u6ce8\u5c40\u90e8\u5408\u6210\u5185\u5bb9\u3002\u8fd9\u5f3a\u8c03\u4e86\u5185\u5bb9\u611f\u77e5\u68c0\u6d4b\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u7684\u6570\u636e\u96c6\u548c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02095", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.02095", "abs": "https://arxiv.org/abs/2602.02095", "authors": ["Dmitri Kuzmin"], "title": "Convex limiting for finite elements and its relationship to residual distribution", "comment": null, "summary": "We review some recent advances in the field of element-based algebraic stabilization for continuous finite element discretizations of nonlinear hyperbolic problems. The main focus is on multidimensional convex limiting techniques designed to constrain antidiffusive element contributions rather than fluxes. We show that the resulting schemes can be interpreted as residual distribution methods. Two kinds of convex limiting can be used to enforce the validity of generalized discrete maximum principles in this context. The first approach has the structure of a localized flux-corrected transport (FCT) algorithm, in which the computation of a low-order predictor is followed by an antidiffusive correction stage. The second option is the use of a monolithic convex limiting (MCL) procedure at the level of spatial semi-discretization. In both cases, inequality constraints are imposed on scalar functions of intermediate states that are required to stay in convex invariant sets.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u8fde\u7eed\u6709\u9650\u5143\u79bb\u6563\u5316\u975e\u7ebf\u6027\u53cc\u66f2\u95ee\u9898\u7684\u57fa\u4e8e\u5355\u5143\u7684\u4ee3\u6570\u7a33\u5b9a\u5316\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u662f\u591a\u7ef4\u51f8\u9650\u5236\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u7ea6\u675f\u53cd\u6269\u6563\u5355\u5143\u8d21\u732e\u800c\u975e\u901a\u91cf\uff0c\u53ef\u89e3\u91ca\u4e3a\u6b8b\u5dee\u5206\u5e03\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u53cc\u66f2\u95ee\u9898\u8fde\u7eed\u6709\u9650\u5143\u79bb\u6563\u5316\u4e2d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u7ea6\u675f\u53cd\u6269\u6563\u8d21\u732e\u4ee5\u4fdd\u6301\u6570\u503c\u89e3\u7684\u826f\u597d\u6027\u8d28\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u51f8\u9650\u5236\u65b9\u6cd5\uff1a1) \u5c40\u90e8\u5316\u901a\u91cf\u4fee\u6b63\u4f20\u8f93(FCT)\u7b97\u6cd5\uff0c\u5148\u8ba1\u7b97\u4f4e\u9636\u9884\u6d4b\u5668\uff0c\u518d\u8fdb\u884c\u53cd\u6269\u6563\u4fee\u6b63\uff1b2) \u5728\u7a7a\u95f4\u534a\u79bb\u6563\u5316\u5c42\u9762\u4f7f\u7528\u6574\u4f53\u51f8\u9650\u5236(MCL)\u7a0b\u5e8f\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5bf9\u4e2d\u95f4\u72b6\u6001\u7684\u6807\u91cf\u51fd\u6570\u65bd\u52a0\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u786e\u4fdd\u5176\u4fdd\u6301\u5728\u51f8\u4e0d\u53d8\u96c6\u4e2d\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u53ef\u89e3\u91ca\u4e3a\u6b8b\u5dee\u5206\u5e03\u65b9\u6cd5\uff0c\u80fd\u591f\u5f3a\u5236\u6267\u884c\u5e7f\u4e49\u79bb\u6563\u6781\u5927\u503c\u539f\u7406\uff0c\u786e\u4fdd\u6570\u503c\u89e3\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u548c\u7a33\u5b9a\u6027\u8981\u6c42\u3002", "conclusion": "\u57fa\u4e8e\u5355\u5143\u7684\u4ee3\u6570\u7a33\u5b9a\u5316\u65b9\u6cd5\u901a\u8fc7\u51f8\u9650\u5236\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u53cc\u66f2\u95ee\u9898\u6709\u9650\u5143\u79bb\u6563\u5316\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e24\u79cd\u53ef\u884c\u7684\u5b9e\u73b0\u9014\u5f84\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u6570\u503c\u6a21\u62df\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.00659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00659", "abs": "https://arxiv.org/abs/2602.00659", "authors": ["Qusai Khaled", "Laura Genga", "Uzay Kaymak"], "title": "Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics", "comment": "Submitted to 21st International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU2026)", "summary": "In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u7cca\u76f8\u4f3c\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u8d85\u6ee4\u819c\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u5065\u5eb7\u6307\u6807\u548c\u900f\u660e\u89c4\u5219\u5b9e\u73b0\u53ef\u4fe1\u9884\u6d4b", "motivation": "\u53cd\u6e17\u900f\u6d77\u6c34\u6de1\u5316\u4e2d\u8d85\u6ee4\u819c\u56e0\u6c61\u67d3\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u9884\u6d4b\u6027\u7ef4\u62a4\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u64cd\u4f5c\u5458\u4e0d\u4fe1\u4efb\uff0c\u9700\u8981\u900f\u660e\u53ef\u9760\u7684\u9884\u6d4b\u65b9\u6cd5", "method": "\u57fa\u4e8e\u6a21\u7cca\u76f8\u4f3c\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u9884\u6d4b\u6846\u67b6\uff1a1) \u4ece\u8de8\u819c\u538b\u529b\u3001\u901a\u91cf\u548c\u963b\u529b\u63a8\u5bfc\u7269\u7406\u4fe1\u606f\u5065\u5eb7\u6307\u6807\uff1b2) \u901a\u8fc7\u9ad8\u65af\u96b6\u5c5e\u51fd\u6570\u6a21\u7cca\u5316\uff1b3) \u4f7f\u7528\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u8bc6\u522b\u5386\u53f2\u9000\u5316\u8f68\u8ff9\uff1b4) \u4ee5Takagi-Sugeno\u6a21\u7cca\u89c4\u5219\u5f62\u5f0f\u5316RUL\u9884\u6d4b", "result": "\u5728\u5de5\u4e1a\u89c4\u6a21UF\u7cfb\u7edf\u768412,528\u4e2a\u64cd\u4f5c\u5468\u671f\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a4.50\u4e2a\u5468\u671f\uff0c\u540c\u65f6\u751f\u6210\u4e0e\u4e13\u5bb6\u7406\u89e3\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u89c4\u5219\u5e93", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d85\u6ee4\u819c\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u589e\u5f3a\u4e86\u64cd\u4f5c\u5458\u4fe1\u4efb\uff0c\u652f\u6301\u57fa\u4e8e\u6761\u4ef6\u7684\u7ef4\u62a4\u51b3\u7b56"}}
{"id": "2602.00165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00165", "abs": "https://arxiv.org/abs/2602.00165", "authors": ["Arthur Negr\u00e3o", "Pedro Silva", "Vander L. S. Freitas", "Gladston Moreira", "Eduardo Luz"], "title": "Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models", "comment": null, "summary": "The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.", "AI": {"tldr": "Benford-Quant\uff1a\u4e00\u79cd\u53d7\u672c\u798f\u5fb7\u5b9a\u5f8b\u542f\u53d1\u7684\u975e\u5747\u5300\u91cf\u5316\u5668\uff0c\u9488\u5bf9LLM\u6743\u91cd\u5206\u5e03\u9ad8\u5ea6\u504f\u659c\u7684\u7279\u70b9\uff0c\u4f7f\u7528\u5bf9\u6570\u95f4\u9694\u7801\u672c\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edf\u5747\u5300\u91cf\u5316\u5668\u5047\u8bbe\u53c2\u6570\u5747\u5300\u5206\u5e03\uff0c\u4f46\u5b9e\u9645LLM\u6743\u91cd\u5206\u5e03\u9ad8\u5ea6\u504f\u659c\uff0c\u9700\u8981\u66f4\u5339\u914d\u5b9e\u9645\u5206\u5e03\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBenford-Quant\u975e\u5747\u5300\u91cf\u5316\u5668\uff0c\u53d7\u672c\u798f\u5fb7\u5b9a\u5f8b\uff08\u9996\u4f4d\u6570\u5bf9\u6570\u5206\u5e03\uff09\u542f\u53d1\uff0c\u7528\u5bf9\u6570\u95f4\u9694\u7801\u672c\u66ff\u4ee3\u5747\u5300\u7f51\u683c\uff0c\u4e3a\u9891\u7e41\u51fa\u73b0\u7684\u5c0f\u5e45\u503c\u6743\u91cd\u5206\u914d\u66f4\u591a\u5206\u8fa8\u7387\u3002", "result": "1) Transformer\u53d8\u6362\u5c42\u6743\u91cd\u7b26\u5408\u672c\u798f\u5fb7\u7edf\u8ba1\uff0c\u5f52\u4e00\u5316\u5c42\u7cfb\u7edf\u504f\u79bb\uff1b2) \u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0a\u6301\u7eed\u6539\u5584\u56f0\u60d1\u5ea6\uff0cGemma-270M 4-bit\u56f0\u60d1\u5ea6\u964d\u4f4e\u8d8510%\uff1b3) \u5728\u5927LLM\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5dee\u5f02\u7531\u8fc7\u53c2\u6570\u5316\u6548\u5e94\u89e3\u91ca\u3002", "conclusion": "\u5c06\u672c\u798f\u5fb7\u5148\u9a8c\u878d\u5165\u91cf\u5316\u7f51\u683c\u662f\u4f4e\u6210\u672c\u4fee\u6539\uff0c\u5728\u6fc0\u8fdb\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u63d0\u5347\u7cbe\u5ea6\u3002\u867d\u672a\u5728\u56f0\u60d1\u5ea6\u548cLAMBADA\u4efb\u52a1\u4e0a\u8d85\u8d8aSOTA\uff0c\u4f46\u53ef\u4e0eSmoothQuant\u3001\u6fc0\u6d3b\u611f\u77e5\u91cf\u5316\u7b49\u65b9\u6cd5\u6df7\u5408\u4f7f\u7528\uff0c\u65e0\u9700\u5927\u5e45\u4fee\u6539\u6d41\u7a0b\u3002"}}
{"id": "2602.00665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00665", "abs": "https://arxiv.org/abs/2602.00665", "authors": ["Lakshan Cooray", "Deshan Sumanathilaka", "Pattigadapa Venkatesh Raju"], "title": "Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation", "comment": "Submission is under review with Computational Linguistics", "summary": "Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5ba2\u6237\u670d\u52a1\u591a\u8f6e\u95ee\u7b54\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5386\u53f2\u6458\u8981\u7b56\u7565\u4fdd\u6301\u5bf9\u8bdd\u8fde\u7eed\u6027\uff0c\u53d1\u73b0\u90e8\u5206SLM\u80fd\u8fbe\u5230\u63a5\u8fd1LLM\u7684\u6027\u80fd\uff0c\u4f46\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5ba2\u6237\u670d\u52a1QA\u4e2d\u6027\u80fd\u867d\u5f3a\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u90e8\u7f72\u53d7\u9650\uff1b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u591a\u8f6e\u5ba2\u6237\u670d\u52a1QA\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5bf9\u8bdd\u8fde\u7eed\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u5386\u53f2\u6458\u8981\u7b56\u7565\u6765\u4fdd\u6301\u5bf9\u8bdd\u72b6\u6001\uff0c\u5f15\u5165\u57fa\u4e8e\u5bf9\u8bdd\u9636\u6bb5\u7684\u5b9a\u6027\u5206\u6790\u6765\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002\u8bc4\u4f30\u4e869\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684\u4f4e\u53c2\u6570\u5316SLM\u4e0e3\u4e2a\u5546\u4e1aLLM\uff0c\u4f7f\u7528\u8bcd\u6c47\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\uff0c\u4ee5\u53ca\u5305\u62ec\u4eba\u5de5\u8bc4\u4f30\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u65b9\u6cd5\u7684\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "SLM\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u90e8\u5206\u6a21\u578b\u8868\u73b0\u51fa\u63a5\u8fd1LLM\u7684\u6027\u80fd\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5728\u4fdd\u6301\u5bf9\u8bdd\u8fde\u7eed\u6027\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u8fd9\u7a81\u663e\u4e86\u4f4e\u53c2\u6570\u5316\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u5ba2\u6237\u670d\u52a1QA\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u7684SLM\u5728\u5ba2\u6237\u670d\u52a1\u591a\u8f6eQA\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002\u5386\u53f2\u6458\u8981\u7b56\u7565\u6709\u52a9\u4e8e\u4fdd\u6301\u5bf9\u8bdd\u8fde\u7eed\u6027\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u4e0a\u4e0b\u6587\u5bf9\u9f50\u80fd\u529b\u3002\u4f4e\u53c2\u6570\u5316\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u4f18\u5316\u3002"}}
{"id": "2602.00202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00202", "abs": "https://arxiv.org/abs/2602.00202", "authors": ["Shanwen Wang", "Xin Sun", "Danfeng Hong", "Fei Zhou"], "title": "Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images", "comment": null, "summary": "The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.", "AI": {"tldr": "\u63d0\u51faSemiEarth\u6a21\u578b\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u9065\u611f\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7VLM\u4f2a\u6807\u7b7e\u51c0\u5316\u6a21\u5757\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u591a\u7c7b\u522b\u8fb9\u754c\u533a\u57df\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u67b6\u6784\u9762\u4e34\u4f2a\u6807\u7b7e\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u4e2d\u3002\u9065\u611f\u56fe\u50cf\u7684\u591a\u7c7b\u522b\u8fb9\u754c\u533a\u57df\u4f2a\u6807\u7b7e\u8d28\u91cf\u5c24\u5176\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faSemiEarth\u6a21\u578b\uff0c\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e76\u8bbe\u8ba1VLM\u4f2a\u6807\u7b7e\u51c0\u5316\u7ed3\u6784\u3002\u8be5\u6a21\u5757\u5229\u7528VLM\u7684\u5f00\u653e\u4e16\u754c\u80fd\u529b\u51c0\u5316\u6559\u5e08\u7f51\u7edc\u751f\u6210\u7684\u4f2a\u6807\u7b7e\uff0c\u7279\u522b\u662f\u5728\u591a\u7c7b\u522b\u8fb9\u754c\u533a\u57df\u7ea0\u6b63\u9519\u8bef\u9884\u6d4b\uff0c\u72ec\u7acb\u4e8eS4\u67b6\u6784\u8fd0\u884c\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSemiEarth\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u4e0e\u4e4b\u524d\u7684SOTA\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u4f18\u5f02\uff0c\u8fd8\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e76\u8bbe\u8ba1VLM\u4f2a\u6807\u7b7e\u51c0\u5316\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u4f2a\u6807\u7b7e\u8d28\u91cf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u589e\u5f3a\u3002"}}
{"id": "2602.02118", "categories": ["math.NA", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.02118", "abs": "https://arxiv.org/abs/2602.02118", "authors": ["Anna Peruso", "Massimo Sorella"], "title": "Convergence of a least-squares splitting method for the Monge-Amp\u00e8re equation", "comment": "16 pages, 6 figures", "summary": "We study the theoretical convergence of the nonlinear least-squares splitting method for the Monge-Amp\u00e8re equation in which each iteration decouples the pointwise nonlinearity from the differential operator and consists of a local nonlinear update followed by the solution of two sequential Poisson-type elliptic problems. While the method performs well in computations, a rigorous convergence theory has remained unavailable. We observe that the iteration admits a reformulation as an alternating-projection scheme on Sobolev spaces $H^m$, $m\\ge 0$. At a solution, the G\u00e2teaux differentials of the projection maps are the linear projections onto the corresponding tangent spaces. We prove that these tangent spaces are transverse, and hence the linearization of the alternating-projection map is a contraction by classical Hilbert-space theory for alternating projections. Building on this geometric characterization, we prove linear convergence in $H^2$ of the splitting method on the two-dimensional torus $\\mathbb{T}^2$ for initial data sufficiently close to a solution $u\\in H^4$. To the best of our knowledge, this yields the first rigorous convergence result for this splitting method in the periodic setting and provides a functional-analytic explanation for its observed numerical robustness.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u5206\u88c2\u65b9\u6cd5\u5728\u6c42\u89e3Monge-Amp\u00e8re\u65b9\u7a0b\u65f6\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u9996\u6b21\u4e3a\u8be5\u65b9\u6cd5\u7684\u6570\u503c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e00\u76f4\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u6536\u655b\u6027\u5206\u6790\u3002\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u89c2\u5bdf\u5230\u7684\u6570\u503c\u9c81\u68d2\u6027\u63d0\u4f9b\u6570\u5b66\u89e3\u91ca\u3002", "method": "\u5c06\u8fed\u4ee3\u91cd\u65b0\u8868\u8ff0\u4e3aSobolev\u7a7a\u95f4\u4e0a\u7684\u4ea4\u66ff\u6295\u5f71\u65b9\u6848\uff0c\u5206\u6790\u5728\u89e3\u5904\u7684G\u00e2teaux\u5fae\u5206\u4f5c\u4e3a\u5207\u7ebf\u7a7a\u95f4\u4e0a\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u8bc1\u660e\u8fd9\u4e9b\u5207\u7ebf\u7a7a\u95f4\u7684\u6a2a\u622a\u6027\uff0c\u5229\u7528Hilbert\u7a7a\u95f4\u4ea4\u66ff\u6295\u5f71\u7406\u8bba\u8bc1\u660e\u7ebf\u6027\u5316\u6620\u5c04\u7684\u6536\u7f29\u6027\u3002", "result": "\u5728\u4e8c\u7ef4\u73af\u9762$\\mathbb{T}^2$\u4e0a\uff0c\u5bf9\u4e8e\u8db3\u591f\u63a5\u8fd1$H^4$\u89e3\u7684\u521d\u59cb\u6570\u636e\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728$H^2$\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u6536\u655b\u6027\u3002", "conclusion": "\u8fd9\u662f\u8be5\u5206\u88c2\u65b9\u6cd5\u5728\u5468\u671f\u8bbe\u7f6e\u4e0b\u7684\u9996\u4e2a\u4e25\u683c\u6536\u655b\u7ed3\u679c\uff0c\u4e3a\u5176\u6570\u503c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6cdb\u51fd\u5206\u6790\u89e3\u91ca\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u6536\u655b\u6027\u4e0e\u6570\u503c\u89c2\u5bdf\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.00663", "categories": ["cs.AI", "cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2602.00663", "abs": "https://arxiv.org/abs/2602.00663", "authors": ["Fabian P. Kr\u00fcger", "Andrea Hunklinger", "Adrian Wolny", "Tim J. Adler", "Igor Tetko", "Santiago David Villalba"], "title": "SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent", "comment": "Fabian P. Kr\u00fcger and Andrea Hunklinger contributed equally to this work", "summary": "Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.", "AI": {"tldr": "SEISMO\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5728\u7ebf\u5206\u5b50\u4f18\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u6bcf\u6b21oracle\u8c03\u7528\u540e\u5b9e\u65f6\u66f4\u65b0\uff0c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u5206\u5b50\u7ed3\u6784\u4f18\u5316\uff0c\u572823\u4e2a\u4efb\u52a1\u4e0a\u6027\u80fd\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53472-3\u500d\u3002", "motivation": "\u5206\u5b50\u4f18\u5316\u662f\u5316\u5b66\u79d1\u5b66\u7279\u522b\u662f\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6838\u5fc3\u74f6\u9888\uff0c\u7531\u4e8e\u4f9d\u8d56\u6602\u8d35\u4e14\u6709\u9650\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff08\u5982\u751f\u7269\u6d4b\u5b9a\uff09\uff0c\u9700\u8981\u6781\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6279\u91cf\u5b66\u4e60\u6216\u7fa4\u4f53\u4f18\u5316\uff0c\u4e0d\u591f\u9ad8\u6548\u3002", "method": "SEISMO\u662f\u4e00\u4e2a\u4e25\u683c\u5728\u7ebf\u7684LLM\u4ee3\u7406\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u5206\u5b50\u4f18\u5316\uff0c\u6bcf\u6b21oracle\u8c03\u7528\u540e\u7acb\u5373\u66f4\u65b0\u3002\u5b83\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u4e0e\u6807\u91cf\u5206\u6570\u7ed3\u5408\uff0c\u5f53\u53ef\u7528\u65f6\u8fd8\u5229\u7528\u7ed3\u6784\u5316\u89e3\u91ca\u6027\u53cd\u9988\uff0c\u57fa\u4e8e\u5b8c\u6574\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u65b0\u5206\u5b50\u63d0\u6848\u3002", "result": "\u5728Practical Molecular Optimization\u57fa\u51c6\u768423\u4e2a\u4efb\u52a1\u4e2d\uff0cSEISMO\u7684\u4f18\u5316\u66f2\u7ebf\u4e0b\u9762\u79ef\u6bd4\u5148\u524d\u65b9\u6cd5\u9ad82-3\u500d\uff0c\u901a\u5e38\u572850\u6b21oracle\u8c03\u7528\u5185\u8fbe\u5230\u63a5\u8fd1\u6700\u5927\u4efb\u52a1\u5206\u6570\u3002\u5728\u836f\u7269\u5316\u5b66\u4efb\u52a1\u4e2d\uff0c\u63d0\u4f9b\u89e3\u91ca\u6027\u53cd\u9988\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "\u5229\u7528\u9886\u57df\u77e5\u8bc6\u548c\u7ed3\u6784\u5316\u4fe1\u606f\u662f\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u5206\u5b50\u4f18\u5316\u7684\u5173\u952e\u3002SEISMO\u5c55\u793a\u4e86LLM\u4ee3\u7406\u5728\u4e25\u683c\u5728\u7ebf\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5206\u5b50\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u7b49\u9700\u8981\u6602\u8d35\u8bc4\u4f30\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00166", "abs": "https://arxiv.org/abs/2602.00166", "authors": ["Evan Chen", "Wenzhi Fang", "Shiqiang Wang", "Christopher Brinton"], "title": "Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints", "comment": null, "summary": "Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.", "AI": {"tldr": "DA-GRPO\uff1a\u4e00\u79cd\u53cc\u4f18\u52bf\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u667a\u80fd\u8c03\u8282\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u534f\u52a9\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fdd\u6301\u7a33\u5b9a\u7684\u4e91\u4f7f\u7528\u9884\u7b97", "motivation": "\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u9650\u5236\u4e0b\u9700\u8981\u6301\u7eed\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5fc5\u987b\u9009\u62e9\u6027\u5730\u4f9d\u8d56\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u8c03\u8282\u4e91\u7aef\u534f\u52a9\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u57fa\u4e8e\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u5378\u8f7d\u884c\u4e3a\uff0c\u5e76\u5728\u4efb\u52a1\u5206\u5e03\u53d8\u5316\u65f6\u52a0\u5267\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faDA-GRPO\uff08\u53cc\u4f18\u52bf\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5c06\u4e91\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u7eb3\u5165\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u907f\u514d\u56fa\u5b9a\u7684\u5956\u52b1\u5851\u9020\u548c\u5916\u90e8\u8def\u7531\u6a21\u578b\u3002\u8be5\u8bbe\u8ba1\u4f7f\u672c\u5730\u6a21\u578b\u80fd\u591f\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u80fd\u529b\u548c\u534f\u4f5c\u884c\u4e3a\uff0c\u5141\u8bb8\u5728\u8bad\u7ec3\u540e\u81ea\u7136\u51fa\u73b0\u4e91\u8bf7\u6c42\uff0c\u540c\u65f6\u9075\u5b88\u9884\u8bbe\u7684\u534f\u52a9\u9884\u7b97\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDA-GRPO\u63d0\u9ad8\u4e86\u5207\u6362\u540e\u7684\u51c6\u786e\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9057\u5fd8\uff0c\u5e76\u4e0e\u5148\u524d\u7684\u534f\u4f5c\u548c\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\u76f8\u6bd4\u4fdd\u6301\u4e86\u66f4\u7a33\u5b9a\u7684\u4e91\u4f7f\u7528\u3002", "conclusion": "DA-GRPO\u901a\u8fc7\u5c06\u4e91\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u96c6\u6210\u5230\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u534f\u4f5c\u6846\u67b6\uff0c\u4f7f\u672c\u5730\u6a21\u578b\u80fd\u591f\u667a\u80fd\u5730\u8c03\u8282\u4e91\u7aef\u534f\u52a9\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u548c\u9884\u7b97\u7ea6\u675f\u3002"}}
{"id": "2602.00733", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00733", "abs": "https://arxiv.org/abs/2602.00733", "authors": ["Yinuo Zhang", "Dingcheng Huang", "Haifeng Suo", "Yizhuo Li", "Ziya Zhao", "Junhao Xu", "Zhiying Tu", "Dianhui Chu", "Deming Zhai", "Xianming Liu", "Xiaoyan Yu", "Dianbo Sui"], "title": "EchoReview: Learning Peer Review from the Echoes of Scientific Citations", "comment": null, "summary": "As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.", "AI": {"tldr": "\u63d0\u51faEchoReview\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u5b66\u672f\u5f15\u7528\u4e2d\u7684\u9690\u542b\u8bc4\u4f30\u4fe1\u53f7\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u8de8\u4f1a\u8bae\u8de8\u5e74\u4efd\u7684\u5f15\u7528\u9a71\u52a8\u8bc4\u5ba1\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u5728\u8bc1\u636e\u652f\u6301\u548c\u8bc4\u5ba1\u5168\u9762\u6027\u7b49\u6838\u5fc3\u7ef4\u5ea6\u8868\u73b0\u4f18\u5f02\u7684\u81ea\u52a8\u8bc4\u5ba1\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u6295\u7a3f\u91cf\u5feb\u901f\u589e\u957f\uff0c\u4f20\u7edf\u540c\u884c\u8bc4\u5ba1\u7cfb\u7edf\u9762\u4e34\u53ef\u6269\u5c55\u6027\u538b\u529b\uff0c\u9700\u8981\u65e2\u53ef\u6269\u5c55\u53c8\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u5ba1\u65b9\u6cd5\u3002\u73b0\u6709\u57fa\u4e8e\u771f\u5b9e\u8bc4\u5ba1\u6570\u636e\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u6570\u636e\u6e90\u4ee5\u53ca\u4eba\u7c7b\u8bc4\u5ba1\u7684\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u652f\u6301\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u8bc4\u5ba1\u3002", "method": "\u63d0\u51faEchoReview\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u7528\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u4ece\u5b66\u672f\u5f15\u7528\u4e2d\u6316\u6398\u9690\u542b\u7684\u96c6\u4f53\u8bc4\u4f30\u4fe1\u53f7\uff0c\u5c06\u79d1\u5b66\u754c\u957f\u671f\u5224\u65ad\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8bc4\u5ba1\u98ce\u683c\u6570\u636e\u3002\u57fa\u4e8e\u6b64\u6784\u5efaEchoReview-16K\u6570\u636e\u96c6\uff0c\u8bad\u7ec3EchoReviewer-7B\u81ea\u52a8\u8bc4\u5ba1\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEchoReviewer-7B\u5728\u8bc1\u636e\u652f\u6301\u548c\u8bc4\u5ba1\u5168\u9762\u6027\u7b49\u6838\u5fc3\u8bc4\u5ba1\u7ef4\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4e14\u7a33\u5b9a\u7684\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5f15\u7528\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u53ef\u9760\u81ea\u52a8\u540c\u884c\u8bc4\u5ba1\u7684\u7a33\u5065\u6709\u6548\u6570\u636e\u8303\u5f0f\u3002", "conclusion": "\u5f15\u7528\u4e0a\u4e0b\u6587\u80fd\u591f\u4f5c\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u81ea\u52a8\u8bc4\u5ba1\u7cfb\u7edf\u7684\u6709\u6548\u6570\u636e\u6e90\uff0cEchoReview\u6846\u67b6\u4e3a\u89e3\u51b3\u4f20\u7edf\u8bc4\u5ba1\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u5229\u7528\u79d1\u5b66\u754c\u7684\u96c6\u4f53\u5224\u65ad\u6765\u8bad\u7ec3\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u5ba1\u6a21\u578b\u3002"}}
{"id": "2602.00211", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00211", "abs": "https://arxiv.org/abs/2602.00211", "authors": ["Zafar Iqbal", "Anwar Ul Haq", "Srimannarayana Grandhi"], "title": "Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning", "comment": null, "summary": "Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.", "AI": {"tldr": "\u63d0\u51faVCoR\u6846\u67b6\uff0c\u5c06\u65e0\u76d1\u7763\u56fe\u50cf\u914d\u51c6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6e10\u8fdb\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u8df3\u89c6\u89c9\u63a8\u7406\u94fe\u5b9e\u73b0\u5927\u53d8\u5f62\u5904\u7406\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u56fe\u50cf\u914d\u51c6\u4e2d\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u8bef\u5dee\u6f02\u79fb\u548c\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u964d\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u53c8\u80fd\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u8fc7\u7a0b\u7684\u914d\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u8df3\u89c6\u89c9\u63a8\u7406\u94fe(VCoR)\u6846\u67b6\uff0c\u5c06\u914d\u51c6\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6e10\u8fdb\u63a8\u7406\u8fc7\u7a0b\u3002\u6bcf\u8df3\u5305\u542b\u5c40\u90e8\u7a7a\u95f4\u7ec6\u5316(LSR)\u6a21\u5757\u6765\u4e30\u5bcc\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u53ca\u4ea4\u53c9\u53c2\u8003\u6ce8\u610f\u529b(CRA)\u673a\u5236\u5f15\u5bfc\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\uff0c\u4fdd\u6301\u89e3\u5256\u4e00\u81f4\u6027\u3002\u591a\u8df3\u7b56\u7565\u80fd\u5904\u7406\u5927\u53d8\u5f62\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u8fb9\u754c\u3002", "result": "\u5728DIR-Lab 4D CT(\u80ba\u90e8)\u548cIXI T1\u52a0\u6743MRI(\u8111\u90e8)\u4e24\u4e2a\u6311\u6218\u6027\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cVCoR\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u914d\u51c6\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4e2d\u95f4\u53ef\u89c6\u5316\u548c\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5d4c\u5165\u9690\u5f0f\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\uff0cVCoR\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u4e14\u4e34\u5e8a\u53ef\u884c\u7684\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u8ffd\u6c42\u7cbe\u5ea6\uff0c\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002"}}
{"id": "2602.02169", "categories": ["math.NA", "math.AP"], "pdf": "https://arxiv.org/pdf/2602.02169", "abs": "https://arxiv.org/abs/2602.02169", "authors": ["\u0141ukasz P\u0142ociniczak", "Marek A. Teuerle", "Hubert Woszczek"], "title": "Asymmetric L\u00e9vy walks driven by convex combination of fractional material derivatives", "comment": null, "summary": "We analyze a class of linear partial differential equations that arise as deterministic descriptions of the scaling limits of L\u00e9vy walks, in which transport is driven by a convex combination of fractional material derivatives and a source term. Using techniques of Fourier-Laplace transforms, we first prove the existence of mild solutions for continuous initial data. Using a recently obtained pointwise representation of the fractional material derivative, we then identify a necessary and sufficient condition on the source term that guaranties the solution to remain a probability density for all times (non-negativity and unit mass). Motivated by the need to preserve these probabilistic properties in computations, we construct a finite-volume discretization that is probability conservative by construction. We establish discrete stability and a convergence result for the continuous weak solution as space and time steps tend to zero. Extensive numerical experiments validate the scheme: total mass is conserved, non-negativity is maintained, and the computed solutions reproduce the known analytic representations of the probability density functions associated with the L\u00e9vy walk process. The combined theoretical and numerical framework provides a reliable tool for studying anomalous transport governed by fractional dynamics.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e00\u7c7b\u63cf\u8ff0L\u00e9vy\u884c\u8d70\u5c3a\u5ea6\u6781\u9650\u7684\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u5085\u91cc\u53f6-\u62c9\u666e\u62c9\u65af\u53d8\u6362\u8bc1\u660e\u6e29\u548c\u89e3\u5b58\u5728\u6027\uff0c\u5efa\u7acb\u4fdd\u6301\u6982\u7387\u5bc6\u5ea6\u6027\u8d28\u7684\u6761\u4ef6\uff0c\u5e76\u6784\u9020\u6982\u7387\u5b88\u6052\u7684\u6709\u9650\u4f53\u79ef\u79bb\u6563\u683c\u5f0f\u3002", "motivation": "\u7814\u7a76L\u00e9vy\u884c\u8d70\u5c3a\u5ea6\u6781\u9650\u7684\u786e\u5b9a\u6027\u63cf\u8ff0\u65b9\u7a0b\uff0c\u9700\u8981\u4fdd\u6301\u89e3\u7684\u6982\u7387\u5bc6\u5ea6\u6027\u8d28\uff08\u975e\u8d1f\u6027\u548c\u5355\u4f4d\u8d28\u91cf\uff09\uff0c\u5e76\u4e3a\u6570\u503c\u8ba1\u7b97\u63d0\u4f9b\u4fdd\u6301\u8fd9\u4e9b\u6982\u7387\u7279\u6027\u7684\u53ef\u9760\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u5085\u91cc\u53f6-\u62c9\u666e\u62c9\u65af\u53d8\u6362\u6280\u672f\u8bc1\u660e\u6e29\u548c\u89e3\u5b58\u5728\u6027\uff1b\u5229\u7528\u5206\u6570\u7269\u8d28\u5bfc\u6570\u7684\u70b9\u8868\u793a\u5efa\u7acb\u4fdd\u6301\u6982\u7387\u5bc6\u5ea6\u6027\u8d28\u7684\u5145\u8981\u6761\u4ef6\uff1b\u6784\u9020\u6982\u7387\u5b88\u6052\u7684\u6709\u9650\u4f53\u79ef\u79bb\u6563\u683c\u5f0f\uff0c\u5e76\u5206\u6790\u79bb\u6563\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6e29\u548c\u89e3\u7684\u5b58\u5728\u6027\uff0c\u5efa\u7acb\u4e86\u4fdd\u6301\u6982\u7387\u5bc6\u5ea6\u6027\u8d28\u7684\u6e90\u9879\u6761\u4ef6\uff0c\u6784\u9020\u4e86\u6982\u7387\u5b88\u6052\u7684\u6709\u9650\u4f53\u79ef\u683c\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u8d28\u91cf\u5b88\u6052\u3001\u975e\u8d1f\u6027\u4fdd\u6301\uff0c\u6570\u503c\u89e3\u80fd\u91cd\u73b0L\u00e9vy\u884c\u8d70\u8fc7\u7a0b\u7684\u5df2\u77e5\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u89e3\u6790\u8868\u793a\u3002", "conclusion": "\u7ed3\u5408\u7406\u8bba\u548c\u6570\u503c\u6846\u67b6\u4e3a\u7814\u7a76\u5206\u6570\u52a8\u529b\u5b66\u63a7\u5236\u7684\u5f02\u5e38\u8f93\u8fd0\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u79bb\u6563\u683c\u5f0f\u80fd\u4fdd\u6301\u6982\u7387\u7279\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00676", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00676", "abs": "https://arxiv.org/abs/2602.00676", "authors": ["Chao Li", "Shangdong Yang", "Chiheng Zhan", "Zhenxing Ge", "Yujing Hu", "Bingkun Bao", "Xingguo Chen", "Yang Gao"], "title": "OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark", "comment": null, "summary": "The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.", "AI": {"tldr": "OpenGuanDan\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u667a\u80fd\u4f53\u7684\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u4e2d\u56fd\u6d41\u884c\u7684\u56db\u4eba\u591a\u8f6e\u7eb8\u724c\u6e38\u620f\"\u63bc\u86cb\"\uff0c\u8be5\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u6548\u6a21\u62df\u548c\u5168\u9762\u8bc4\u4f30\u529f\u80fd\uff0c\u5305\u542b\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u3001\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u3001\u5408\u4f5c\u7ade\u4e89\u6df7\u5408\u76ee\u6807\u7b49\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u68cb\u724c\u6e38\u620f\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u63bc\u86cb\u6e38\u620f\u5177\u6709\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u3001\u5927\u89c4\u6a21\u4fe1\u606f\u96c6\u3001\u5408\u4f5c\u7ade\u4e89\u6df7\u5408\u76ee\u6807\u3001\u957f\u65f6\u7a0b\u51b3\u7b56\u7b49\u590d\u6742\u7279\u6027\uff0c\u4e3a\u73b0\u6709\u667a\u80fd\u51b3\u7b56\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u5cfb\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u5f00\u53d1\u4e86OpenGuanDan\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u63bc\u86cb\u6e38\u620f\u7684\u9ad8\u6548\u6a21\u62df\u548c\u5168\u9762\u8bc4\u4f30\u3002\u5e73\u53f0\u4e3a\u6bcf\u4e2a\u73a9\u5bb6\u63d0\u4f9b\u72ec\u7acbAPI\uff0c\u652f\u6301\u4eba\u7c7b-AI\u4ea4\u4e92\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3002\u8fdb\u884c\u4e86\u4e24\u79cd\u8bc4\u4f30\uff1a1) \u6240\u6709\u63bc\u86cbAI\u4ee3\u7406\u4e4b\u95f4\u7684\u6210\u5bf9\u7ade\u8d5b\uff1b2) \u4eba\u7c7b-AI\u5bf9\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u4ee3\u7406\uff0c\u4f46\u4ecd\u672a\u8fbe\u5230\u8d85\u4eba\u7c7b\u6c34\u5e73\uff0c\u7a81\u663e\u4e86\u591a\u667a\u80fd\u4f53\u667a\u80fd\u51b3\u7b56\u9886\u57df\u9700\u8981\u7ee7\u7eed\u7814\u7a76\u3002\u9879\u76ee\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "OpenGuanDan\u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u667a\u80fd\u51b3\u7b56\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u5176\u590d\u6742\u7279\u6027\u4f7f\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.00170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00170", "abs": "https://arxiv.org/abs/2602.00170", "authors": ["Qiyao Liang", "Jinyeop Song", "Yizhou Liu", "Jeff Gore", "Ila Fiete", "Risto Miikkulainen", "Xin Qiu"], "title": "The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective", "comment": "8 pages, 6 figures, plus appendices", "summary": "Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\\!\\approx\\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u6743\u91cd\u6270\u52a8\u8fdb\u5316\u7b56\u7565(ES)\u53ef\u4ee5\u7528\u6781\u5c0f\u79cd\u7fa4(\u7ea630)\u5fae\u8c03\u5341\u4ebf\u53c2\u6570\u8bed\u8a00\u6a21\u578b\uff0c\u8fd9\u4e0e\u7ecf\u5178\u96f6\u9636\u4f18\u5316\u7684\u7ef4\u5ea6\u8bc5\u5492\u76f4\u89c9\u76f8\u6096\u3002\u540c\u65f6\u89c2\u5bdf\u5230\u5956\u52b1\u5148\u5347\u540e\u964d\u7684\u73b0\u8c61\uff0c\u4e24\u8005\u90fd\u6e90\u4e8e\u5fae\u8c03\u666f\u89c2\u7684\u4f4e\u7ef4\u66f2\u7387\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u7406\u8bba\u8ba4\u4e3a\u9ad8\u7ef4\u4f18\u5316\u9700\u8981\u5927\u91cf\u6837\u672c\uff0c\u4f46\u5b9e\u8df5\u4e2dES\u80fd\u7528\u6781\u5c0f\u79cd\u7fa4\u5fae\u8c03\u5927\u6a21\u578b\uff0c\u4e14\u89c2\u5bdf\u5230\u5956\u52b1\u975e\u5355\u8c03\u53d8\u5316\u73b0\u8c61\u3002\u9700\u8981\u89e3\u91ca\u8fd9\u4e9b\u770b\u4f3c\u77db\u76fe\u7684\u73b0\u8c61\u80cc\u540e\u7684\u5171\u540c\u673a\u5236\u3002", "method": "\u5c06ES\u4f5c\u4e3a\u51e0\u4f55\u63a2\u9488\uff0c\u5206\u6790GSM8K\u3001ARC-C\u3001WinoGrande\u7b49\u4efb\u52a1\u4e0aQwen2.5-Instruct\u6a21\u578b(0.5B-7B)\u7684\u5fae\u8c03\u5956\u52b1\u666f\u89c2\u3002\u5efa\u7acb\u6700\u5c0f\u4e8c\u6b21\u968f\u673a\u4e0a\u5347\u6a21\u578b\uff0c\u7814\u7a76\u9ad8\u66f2\u7387\u7ef4\u5ea6\u5bf9\u4f18\u5316\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5fae\u8c03\u666f\u89c2\u5177\u6709\u4f4e\u7ef4\u66f2\u7387\u7279\u6027\uff1a\u5c11\u6570\u9ad8\u66f2\u7387\u7ef4\u5ea6\u4e3b\u5bfc\u6539\u8fdb\uff0c\u4ea7\u751f\u5f02\u8d28\u65f6\u95f4\u5c3a\u5ea6\u5bfc\u81f4\u5956\u52b1\u5148\u5347\u540e\u964d\uff0c\u4e14\u8bb8\u591a\u968f\u673a\u6270\u52a8\u5728\u8fd9\u4e9b\u65b9\u5411\u4e0a\u5177\u6709\u76f8\u4f3c\u5206\u91cf\uff0c\u4f7f\u5f97\u5c0f\u79cd\u7fa4ES\u80fd\u6709\u6548\u8bbf\u95ee\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "ES\u7684\u53ef\u6269\u5c55\u6027\u548c\u975e\u5355\u8c03\u8bad\u7ec3\u52a8\u6001\u90fd\u6e90\u4e8e\u5fae\u8c03\u666f\u89c2\u7684\u4f4e\u7ef4\u66f2\u7387\u7279\u6027\u3002\u9ad8\u7ef4\u5fae\u8c03\u53ef\u80fd\u6bd4\u6700\u574f\u60c5\u51b5\u7406\u8bba\u9884\u6d4b\u7684\u66f4\u6613\u4f18\u5316\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.00740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00740", "abs": "https://arxiv.org/abs/2602.00740", "authors": ["Ziyan Xiao", "Yinghao Zhu", "Liang Peng", "Lequan Yu"], "title": "ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement", "comment": null, "summary": "Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns \"how to revise\" rather than just \"what to revise\". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.", "AI": {"tldr": "ExperienceWeaver\uff1a\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5608\u6742\u7684\u591a\u7ef4\u53cd\u9988\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5177\u4f53\u6280\u5de7\u548c\u9ad8\u7ea7\u7b56\u7565\uff09\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u63d0\u5347\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u6548\u679c\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u5bf9\u533b\u7597\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u9650\u4e14\u533b\u5b66\u6587\u6863\u7ea6\u675f\u590d\u6742\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1a\u76d1\u7763\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u6210\u672c\u9ad8\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u901a\u5e38\u53ea\u80fd\u63d0\u4f9b\u8868\u9762\u4fee\u6b63\u800c\u65e0\u6cd5\u6355\u6349\u4fee\u8ba2\u80cc\u540e\u7684\u63a8\u7406\u903b\u8f91\u3002", "method": "\u63d0\u51faExperienceWeaver\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u91cd\u70b9\u4ece\u6570\u636e\u68c0\u7d22\u8f6c\u5411\u7ecf\u9a8c\u5b66\u4e60\u3002\u6846\u67b6\u5c06\u5608\u6742\u7684\u591a\u7ef4\u53cd\u9988\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff0c\u5305\u62ec\u9519\u8bef\u7279\u5b9a\u7684\u6280\u5de7\u548c\u9ad8\u5c42\u6b21\u7b56\u7565\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u63d0\u70bc\u7684\u7ecf\u9a8c\u6ce8\u5165\u5230\u667a\u80fd\u4ee3\u7406\u6d41\u7a0b\u4e2d\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\"\u5982\u4f55\u4fee\u8ba2\"\u800c\u4e0d\u4ec5\u4ec5\u662f\"\u4fee\u8ba2\u4ec0\u4e48\"\u3002", "result": "\u5728\u56db\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cExperienceWeaver\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8d85\u8d8a\u4e86Gemini-3 Pro\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "ExperienceWeaver\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u800c\u975e\u7b80\u5355\u6570\u636e\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6311\u6218\uff0c\u4e3a\u533b\u7597\u6587\u6863\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00212", "abs": "https://arxiv.org/abs/2602.00212", "authors": ["Sathish Krishna Anumula", "Vetrivelan Tamilmani", "Aniruddha Arjun Singh", "Dinesh Rajendran", "Venkata Deepak Namburi"], "title": "Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images", "comment": "17 Pages, 2 Tables, 6 Figures", "summary": "Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b9a\u5236CNN\u7684\u81ea\u52a8\u5316\u80ba\u708e\u8bca\u65ad\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u9884\u5904\u7406\u6280\u672f\uff0c\u5728\u80f8\u90e8X\u5149\u56fe\u50cf\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u80ba\u708e\u8bc6\u522b\u3002", "motivation": "\u80ba\u708e\u662f\u5168\u7403\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u5c24\u5176\u5728\u513f\u79d1\u548c\u8001\u5e74\u4eba\u7fa4\u4e2d\u3002\u4f20\u7edf\u4f9d\u8d56\u653e\u5c04\u79d1\u533b\u751f\u624b\u52a8\u89e3\u8bfb\u80f8\u90e8X\u5149\u7684\u65b9\u6cd5\u5b58\u5728\u89c2\u5bdf\u8005\u5dee\u5f02\u3001\u4e13\u5bb6\u75b2\u52b3\u548c\u5408\u683c\u653e\u5c04\u79d1\u533b\u751f\u77ed\u7f3a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5feb\u901f\u51c6\u786e\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u8bbe\u8ba1\uff0c\u9488\u5bf9\u7070\u5ea6\u533b\u5b66\u56fe\u50cf\u7684\u7eb9\u7406\u7279\u5f81\u8fdb\u884c\u4f18\u5316\u3002\u4f7f\u7528\u5bf9\u6bd4\u5ea6\u53d7\u9650\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff08CLAHE\uff09\u548c\u51e0\u4f55\u589e\u5f3a\u7b49\u9884\u5904\u7406\u6280\u672f\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u5305\u542b5863\u5f20\u524d\u540e\u4f4d\u80f8\u90e8X\u5149\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6a21\u578b\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u548c\u6700\u5c0f\u8ba1\u7b97\u6210\u672c\u8bc6\u522b\u80ba\u708e\u3002", "conclusion": "\u8be5\u5b9a\u5236CNN\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u80ba\u708e\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u57fa\u4e8e\u901a\u7528\u8fc1\u79fb\u5b66\u4e60\u7684\u6a21\u578b\u5177\u6709\u66f4\u5c11\u7684\u5197\u4f59\u53c2\u6570\uff0c\u80fd\u591f\u514b\u670d\u4f20\u7edf\u624b\u52a8\u8bca\u65ad\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.00685", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00685", "abs": "https://arxiv.org/abs/2602.00685", "authors": ["Xuan Liu", "Haoyang Shang", "Zizhang Liu", "Xinyan Liu", "Yunze Xiao", "Yiwen Tu", "Haojian Jin"], "title": "HumanStudy-Bench: Towards AI Agent Design for Participant Simulation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HUMANSTUDY-BENCH\u57fa\u51c6\u548c\u5f15\u64ce\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u6a21\u62df\u53c2\u4e0e\u8005\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u91cd\u6784\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u5b9e\u9a8c\u6765\u91cf\u5316\u4ee3\u7406\u884c\u4e3a\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528LLM\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u6a21\u62df\u53c2\u4e0e\u8005\u5b58\u5728\u884c\u4e3a\u4e0d\u7a33\u5b9a\u3001\u5bf9\u8bbe\u8ba1\u9009\u62e9\u654f\u611f\u7684\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u5e38\u6df7\u6dc6\u57fa\u7840\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9a8c\u5b9e\u4f8b\u5316\uff0c\u96be\u4ee5\u533a\u5206\u7ed3\u679c\u53cd\u6620\u7684\u662f\u6a21\u578b\u672c\u8eab\u8fd8\u662f\u4ee3\u7406\u8bbe\u7f6e\u3002", "method": "\u5c06\u53c2\u4e0e\u8005\u6a21\u62df\u5b9a\u4e49\u4e3a\u5b8c\u6574\u5b9e\u9a8c\u534f\u8bae\u4e0a\u7684\u4ee3\u7406\u8bbe\u8ba1\u95ee\u9898\uff0c\u5f15\u5165HUMANSTUDY-BENCH\u57fa\u51c6\u548c\u6267\u884c\u5f15\u64ce\uff0c\u91c7\u7528Filter-Extract-Execute-Evaluate\u7ba1\u9053\u91cd\u6784\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u5728\u5171\u4eab\u8fd0\u884c\u65f6\u4e2d\u91cd\u653e\u8bd5\u9a8c\u5e8f\u5217\u5e76\u8fd0\u884c\u539f\u59cb\u5206\u6790\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b12\u4e2a\u57fa\u7840\u7814\u7a76\u7684\u52a8\u6001\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d6\u4e2a\u4f53\u8ba4\u77e5\u3001\u6218\u7565\u4e92\u52a8\u548c\u793e\u4f1a\u5fc3\u7406\u5b66\u7b49\u9886\u57df\uff0c\u5305\u542b\u8d85\u8fc76,000\u4e2a\u8bd5\u9a8c\uff0c\u4eba\u7c7b\u6837\u672c\u89c4\u6a21\u4ece\u6570\u5341\u4eba\u5230\u8d85\u8fc72,100\u540d\u53c2\u4e0e\u8005\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53c2\u4e0e\u8005\u6a21\u62df\u89c6\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u95ee\u9898\u5e76\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30LLM\u5728\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u79d1\u5b66\u63a8\u7406\u5c42\u9762\u7684\u4fdd\u771f\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00173", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00173", "abs": "https://arxiv.org/abs/2602.00173", "authors": ["Shuozhe Li", "Vaishnav Tadiparthi", "Kwonjoon Lee", "Nakul Agarwal", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Lizhang Chen", "Amy Zhang", "Liu Leqi"], "title": "Learning Robust Reasoning through Guided Adversarial Self-Play", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.", "AI": {"tldr": "GASP\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u535a\u5f08\u8bad\u7ec3\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u5728\u6709\u7f3a\u9677\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\uff08\u5982\u88ab\u6c61\u67d3\u7684\u601d\u7ef4\u94fe\uff09\u8fdb\u884c\u68c0\u6d4b\u548c\u4fee\u590d\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u6761\u4ef6\u6709\u7f3a\u9677\u65f6\u4f1a\u4e25\u91cd\u5931\u8d25\uff0c\u56e0\u4e3a\u6807\u51c6RLVR\u53ea\u4f18\u5316\u5728\u5e72\u51c0\u6761\u4ef6\u4e0b\u7684\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u6a21\u578b\u80fd\u591f\u5728\u6709\u8bef\u5bfc\u6027\u3001\u88ab\u6c61\u67d3\u6216\u4e0d\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "GASP\uff08\u5f15\u5bfc\u5bf9\u6297\u6027\u81ea\u535a\u5f08\uff09\u65b9\u6cd5\uff1a1\uff09\u5728\u5355\u4e2a\u6a21\u578b\u5185\u6784\u5efa\u5bf9\u6297\u6027\u81ea\u535a\u5f08\u6e38\u620f\uff0c\u5305\u542b\u6c61\u67d3\u8005\u548c\u667a\u80fd\u4f53\u4e24\u4e2a\u89d2\u8272\uff1b2\uff09\u6c61\u67d3\u8005\u5b66\u4e60\u901a\u8fc7\u5c40\u90e8\u8fde\u8d2f\u7684\u6c61\u67d3\u6765\u8bf1\u5bfc\u5931\u8d25\uff1b3\uff09\u667a\u80fd\u4f53\u5b66\u4e60\u5728\u76f8\u540c\u6c61\u67d3\u6761\u4ef6\u4e0b\u8bca\u65ad\u548c\u6062\u590d\uff1b4\uff09\u5f15\u5165\u5206\u5e03\u5185\u4fee\u590d\u5f15\u5bfc\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u4fee\u590d\u7684\u6a21\u4eff\u9879\u63d0\u9ad8\u6062\u590d\u6982\u7387\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u653e\u6743\u91cd\u6a21\u578b\uff081.5B-8B\uff09\u4e0a\uff0cGASP\u6210\u529f\u5c06\u5f3a\u4f46\u8106\u5f31\u7684\u63a8\u7406\u5668\u8f6c\u5316\u4e3a\u9c81\u68d2\u6a21\u578b\uff0c\u80fd\u591f\u62b5\u5fa1\u8bef\u5bfc\u6027\u548c\u88ab\u6270\u52a8\u7684\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u901a\u5e38\u8fd8\u80fd\u63d0\u9ad8\u5e72\u51c0\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u6027\u3002\u5bf9\u6297\u6027\u6c61\u67d3\u4ea7\u751f\u4e86\u6709\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\u6548\u679c\uff0c\u5206\u5e03\u5185\u5f15\u5bfc\u5b9e\u73b0\u4e86\u5feb\u901f\u6062\u590d\u5b66\u4e60\u3002", "conclusion": "GASP\u4ec5\u4f7f\u7528\u7ed3\u679c\u9a8c\u8bc1\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u7b7e\u6216\u5916\u90e8\u6559\u5e08\uff0c\u5c31\u80fd\u8bad\u7ec3\u51fa\u5177\u6709\u68c0\u6d4b\u548c\u4fee\u590d\u80fd\u529b\u7684\u9c81\u68d2\u63a8\u7406\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3RLVR\u6a21\u578b\u5728\u7f3a\u9677\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00742", "abs": "https://arxiv.org/abs/2602.00742", "authors": ["Liang Wang", "Xinyi Mou", "Xiaoyou Liu", "Xuanjing Huang", "Zhongyu Wei"], "title": "CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs", "comment": null, "summary": "User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code", "AI": {"tldr": "CURP\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7528\u6237\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u5411\u7528\u6237\u7f16\u7801\u5668\u548c\u79bb\u6563\u539f\u578b\u7801\u672c\u63d0\u53d6\u591a\u7ef4\u7528\u6237\u7279\u5f81\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u7684\u4e2a\u6027\u5316\uff0c\u53c2\u6570\u91cf\u5c0f\uff08\u7ea62000\u4e07\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u5efa\u6a21\u65b9\u6cd5\uff08\u57fa\u4e8e\u63d0\u793a\u6216\u8bad\u7ec3\uff09\u5728\u4e2a\u6027\u5316\u8d28\u91cf\u4e0e\u8ba1\u7b97/\u6570\u636e\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCURP\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5411\u7528\u6237\u7f16\u7801\u5668\u548c\u79bb\u6563\u539f\u578b\u7801\u672c\u63d0\u53d6\u591a\u7ef4\u7528\u6237\u7279\u5f81\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u7684\u4e2a\u6027\u5316\uff0c\u53c2\u6570\u91cf\u4ec5\u7ea62000\u4e07\uff08\u7ea6\u5360\u603b\u6a21\u578b\u5927\u5c0f\u76840.2%\uff09\u3002", "result": "\u5728\u591a\u79cd\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCURP\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CURP\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7528\u6237\u4e2a\u6027\u5316\u5efa\u6a21\uff0c\u4e3aLLM\u7684\u4e2a\u6027\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00214", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00214", "abs": "https://arxiv.org/abs/2602.00214", "authors": ["Juan A. Olmos", "Antoine Manzanera", "Fabio Mart\u00ednez"], "title": "A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification", "comment": "Accepted at IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.", "AI": {"tldr": "\u63d0\u51faMFM-Geom\u51e0\u4f55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u53cc\u53c2\u6570MRI\u548c\u4e34\u5e8a\u62a5\u544a\uff0c\u4f7f\u7528SPD\u77e9\u9635\u548c\u9ece\u66fc\u6df1\u5ea6\u5b66\u4e60\u63d0\u5347\u524d\u5217\u817a\u764c\u8bc6\u522b\u6027\u80fd", "motivation": "\u524d\u5217\u817a\u764c\u8bca\u65ad\u4f9d\u8d56\u4e13\u5bb6\u4e3b\u89c2\u89e3\u8bfb\uff0c\u73b0\u6709\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u65b9\u6cd5\u591a\u5173\u6ce8\u5f71\u50cf\u6a21\u578b\uff0c\u5ffd\u89c6\u4e34\u5e8a\u80cc\u666f\u4e14\u53d7\u6570\u636e\u7a00\u7f3a\u9650\u5236\uff0c\u96be\u4ee5\u5b66\u4e60\u7a33\u5065\u8868\u5f81", "method": "\u63d0\u51fa\u51e0\u4f55\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578bMFM-Geom\uff0c\u4ece\u53cc\u53c2\u6570MRI\u548c\u4e34\u5e8a\u62a5\u544a\u4e2d\u5b66\u4e60\u8868\u5f81\uff0c\u4f7f\u7528\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u548c\u9ece\u66fc\u6df1\u5ea6\u5b66\u4e60\u6574\u5408\u5f71\u50cf-\u6587\u672c\u8868\u5f81", "result": "\u4ec5\u752810%\u8bad\u7ec3\u6570\u636e\uff0cMFM-Geom\u5728AUC-PR\u4e0a\u6bd4\u57fa\u7ebf\u5206\u7c7btoken\u5d4c\u5165\u65b9\u6cd5\u63d0\u53478.3%\uff0c\u8fbe\u523090.67%\uff1b\u5916\u90e8\u6570\u636e\u96c6\u9a8c\u8bc1\u663e\u793aAUC-PR\u4e3a90.6\uff0c\u8bc1\u660e\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "conclusion": "MFM-Geom\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u9ece\u66fc\u51e0\u4f55\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u524d\u5217\u817a\u764c\u8bca\u65ad\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5fae\u8c03\u751f\u7269\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.02353", "categories": ["math.NA"], "pdf": "https://arxiv.org/pdf/2602.02353", "abs": "https://arxiv.org/abs/2602.02353", "authors": ["Hakop Hakopian"], "title": "A formula for Hermite multivariate interpolation problem and partial fraction decomposition", "comment": "12 pages", "summary": "We present a new formula for the Hermite multivariate interpolation corresponding to the Chung-Yao interpolation. With the help of the respective univariate interpolation formula we give a direct and explicit solution to the problem of partial fraction decomposition of rational functions.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684Hermite\u591a\u5143\u63d2\u503c\u516c\u5f0f\uff0c\u57fa\u4e8eChung-Yao\u63d2\u503c\uff0c\u5229\u7528\u76f8\u5e94\u7684\u4e00\u5143\u63d2\u503c\u516c\u5f0f\u76f4\u63a5\u663e\u5f0f\u89e3\u51b3\u6709\u7406\u51fd\u6570\u90e8\u5206\u5206\u5f0f\u5206\u89e3\u95ee\u9898", "motivation": "\u89e3\u51b3\u6709\u7406\u51fd\u6570\u90e8\u5206\u5206\u5f0f\u5206\u89e3\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u76f4\u63a5\u548c\u663e\u5f0f\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5", "method": "\u57fa\u4e8eChung-Yao\u63d2\u503c\u6784\u5efa\u65b0\u7684Hermite\u591a\u5143\u63d2\u503c\u516c\u5f0f\uff0c\u5229\u7528\u76f8\u5e94\u7684\u4e00\u5143\u63d2\u503c\u516c\u5f0f\u63a8\u5bfc\u51fa\u76f4\u63a5\u663e\u5f0f\u7684\u90e8\u5206\u5206\u5f0f\u5206\u89e3\u65b9\u6cd5", "result": "\u83b7\u5f97\u4e86\u65b0\u7684Hermite\u591a\u5143\u63d2\u503c\u516c\u5f0f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6709\u7406\u51fd\u6570\u7684\u90e8\u5206\u5206\u5f0f\u5206\u89e3\uff0c\u63d0\u4f9b\u4e86\u76f4\u63a5\u663e\u5f0f\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u4e3a\u6709\u7406\u51fd\u6570\u90e8\u5206\u5206\u5f0f\u5206\u89e3\u63d0\u4f9b\u4e86\u66f4\u76f4\u63a5\u548c\u663e\u5f0f\u7684\u8ba1\u7b97\u9014\u5f84\uff0c\u6269\u5c55\u4e86\u63d2\u503c\u7406\u8bba\u7684\u5e94\u7528"}}
{"id": "2602.00699", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00699", "abs": "https://arxiv.org/abs/2602.00699", "authors": ["Xuan Liu", "Ziyu Li", "Mu He", "Ziyang Ma", "Xiaoxu Wu", "Gizem Yilmaz", "Yiyuan Xia", "Bingbing Li", "He Tan", "Jerry Ying Hsi Fuh", "Wen Feng Lu", "Anders E. W. Jarfors", "Per Jansson"], "title": "From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development", "comment": "11 pages,8 figures,3 tables,presented at International Conference on Industry of the Future and Smart Manufacturing,2025", "summary": "Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4e09\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u5316\u4ece\u94f8\u9020\u5236\u9020\u9886\u57df\u6587\u672c\u4e2d\u63d0\u53d6\u672f\u8bed\u548c\u5173\u7cfb\uff0c\u4ee5\u6784\u5efa\u9886\u57df\u672c\u4f53\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u6784\u5efa\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u548c\u4f20\u7edfNLP\u6280\u672f\uff0c\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5728\u94f8\u9020\u5236\u9020\u7b49\u4e13\u4e1a\u9886\u57df\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u4e3a\u81ea\u52a8\u5316\u77e5\u8bc6\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cdLLM\u65b9\u6cd5\uff1a\u9884\u8bad\u7ec3LLM\u9a71\u52a8\u65b9\u6cd5\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9886\u57df\u7279\u5b9a\u6587\u672c\u4e2d\u63d0\u53d6\u672f\u8bed\u548c\u5173\u7cfb\u3002\u4f7f\u7528\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u6784\u5efa\u94f8\u9020\u672c\u4f53\uff0c\u5e76\u7531\u9886\u57df\u4e13\u5bb6\u9a8c\u8bc1\u3002", "result": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u6700\u4f73\u6027\u80fd\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86\u94f8\u9020\u672c\u4f53\uff0c\u8be5\u672c\u4f53\u901a\u8fc7\u4e86\u9886\u57df\u4e13\u5bb6\u7684\u9a8c\u8bc1\u3002", "conclusion": "LLM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u9886\u57df\u672c\u4f53\u6784\u5efa\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4e13\u4e1a\u9886\u57df\u7684\u77e5\u8bc6\u63d0\u53d6\u548c\u672c\u4f53\u6784\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00175", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.00175", "abs": "https://arxiv.org/abs/2602.00175", "authors": ["Manyi Li", "Yufan Liu", "Lai Jiang", "Bing Li", "Yuming Li", "Weiming Hu"], "title": "The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization", "comment": "21 pages, 22 figures, 17 tables", "summary": "Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this \"forgetting\" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u57fa\u4e8e\u9057\u5fd8\u7684\u9632\u5fa1\u673a\u5236\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0cNSFW\u6982\u5ff5\u5e76\u672a\u771f\u6b63\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u6d88\u9664\uff0c\u800c\u662f\u4f5c\u4e3a\u4f11\u7720\u8bb0\u5fc6\u4fdd\u7559\u3002\u4f5c\u8005\u63d0\u51faIVO\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u6f5c\u53d8\u91cf\u91cd\u65b0\u6fc0\u6d3b\u8fd9\u4e9b\u8bb0\u5fc6\uff0c\u66b4\u9732\u5f53\u524d\u9632\u5fa1\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9057\u5fd8\u7684\u9632\u5fa1\u58f0\u79f0\u80fd\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u6e05\u9664NSFW\u6982\u5ff5\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\"\u9057\u5fd8\"\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u5047\u8c61\u3002\u672a\u9057\u5fd8\u7684\u77e5\u8bc6\u4f5c\u4e3a\u4f11\u7720\u8bb0\u5fc6\u4fdd\u7559\uff0c\u53ea\u662f\u8bed\u8a00\u7b26\u53f7\u4e0e\u5e95\u5c42\u77e5\u8bc6\u7684\u6620\u5c04\u88ab\u90e8\u5206\u7834\u574f\u3002\u9700\u8981\u63ed\u793a\u8fd9\u79cd\u9632\u5fa1\u7684\u6839\u672c\u7f3a\u9677\u3002", "method": "\u63d0\u51faIVO\uff08\u521d\u59cb\u6f5c\u53d8\u91cf\u4f18\u5316\uff09\u653b\u51fb\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a\u56fe\u50cf\u53cd\u6f14\u3001\u5bf9\u6297\u4f18\u5316\u548c\u91cd\u7528\u653b\u51fb\u3002\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u6f5c\u53d8\u91cf\uff0c\u4f7f\u9057\u5fd8\u6a21\u578b\u7684\u566a\u58f0\u5206\u5e03\u4e0e\u539f\u59cb\u4e0d\u5b89\u5168\u72b6\u6001\u91cd\u65b0\u5bf9\u9f50\uff0c\u4ece\u800c\u91cd\u65b0\u6fc0\u6d3b\u4f11\u7720\u8bb0\u5fc6\u3002", "result": "\u57288\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u9057\u5fd8\u6280\u672f\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cIVO\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u653b\u51fb\u6210\u529f\u7387\uff08\u6700\u9ad8\u8fbe96.7%\uff09\u548c\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u7684\u6839\u672c\u7f3a\u9677\u3002", "conclusion": "\u57fa\u4e8e\u9057\u5fd8\u7684\u9632\u5fa1\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0cNSFW\u6982\u5ff5\u5e76\u672a\u771f\u6b63\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u6d88\u9664\u3002\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\u53ef\u4f5c\u4e3a\u9057\u5fd8\u5f3a\u5ea6\u7684\u6307\u6807\u3002IVO\u653b\u51fb\u6846\u67b6\u6709\u6548\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u7684\u8106\u5f31\u6027\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2602.00747", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00747", "abs": "https://arxiv.org/abs/2602.00747", "authors": ["Shengrui Li", "Fei Zhao", "Kaiyan Zhao", "Jieying Ye", "Haifeng Liu", "Fangcheng Shi", "Zheyong Xie", "Yao Hu", "Shaosheng Cao"], "title": "Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training", "comment": "17 pages, 5 figures", "summary": "Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.", "AI": {"tldr": "DeMix\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u578b\u878d\u5408\u9884\u6d4b\u6700\u4f18\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u7684\u65b0\u6846\u67b6\uff0c\u5c06\u641c\u7d22\u4e0e\u8bad\u7ec3\u6210\u672c\u89e3\u8026\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5bfb\u627eLLM\u9884\u8bad\u7ec3\u6700\u4f18\u6570\u636e\u6df7\u5408\u7684\u6210\u672c\u3002", "motivation": "\u786e\u5b9a\u6709\u6548\u7684\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u662fLLM\u9884\u8bad\u7ec3\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u5c0f\u89c4\u6a21\u4ee3\u7406\u5b9e\u9a8c\uff0c\u8981\u4e48\u9700\u8981\u6781\u5176\u6602\u8d35\u7684\u5927\u89c4\u6a21\u63a2\u7d22\uff0c\u5bfb\u627e\u6700\u4f18\u6df7\u5408\u6bd4\u4f8b\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "DeMix\u6846\u67b6\u5728\u5019\u9009\u6570\u636e\u96c6\u4e0a\u5927\u89c4\u6a21\u8bad\u7ec3\u7ec4\u4ef6\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u52a0\u6743\u6a21\u578b\u878d\u5408\u6765\u63a8\u5bfc\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u7684\u4ee3\u7406\uff0c\u5c06\u641c\u7d22\u4e0e\u8bad\u7ec3\u6210\u672c\u89e3\u8026\uff0c\u4ece\u800c\u80fd\u591f\u8bc4\u4f30\u65e0\u9650\u91c7\u6837\u7684\u6df7\u5408\u6bd4\u4f8b\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u8d1f\u62c5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeMix\u6253\u7834\u4e86\u5145\u5206\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u66f4\u4f4e\u7684\u641c\u7d22\u6210\u672c\u83b7\u5f97\u5177\u6709\u66f4\u9ad8\u57fa\u51c6\u6027\u80fd\u7684\u6700\u4f18\u6df7\u5408\u6bd4\u4f8b\u3002\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86DeMix Corpora\uff0c\u4e00\u4e2a\u5305\u542b22T token\u7684\u9ad8\u8d28\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "conclusion": "DeMix\u901a\u8fc7\u6a21\u578b\u878d\u5408\u9884\u6d4b\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u7684\u65b0\u8303\u5f0f\uff0c\u4e3aLLM\u9884\u8bad\u7ec3\u6570\u636e\u6df7\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u641c\u7d22\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2602.00216", "categories": ["cs.CV", "cs.CY", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.00216", "abs": "https://arxiv.org/abs/2602.00216", "authors": ["Zaldy Pagaduan", "Jason Occidental", "Nathaniel Duro", "Dexielito Badilles", "Eleonor Palconit"], "title": "Development of a Cacao Disease Identification and Management App Using Deep Learning", "comment": "6 pages, 8 figures, preprint", "summary": "Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u7528\u4e8e\u53ef\u53ef\u75c5\u5bb3\u8bc6\u522b\u7684\u79bb\u7ebf\u79fb\u52a8\u5e94\u7528\uff0c\u5e2e\u52a9\u83f2\u5f8b\u5bbe\u5c0f\u519c\u6237\u89e3\u51b3\u75c5\u5bb3\u7ba1\u7406\u95ee\u9898", "motivation": "\u83f2\u5f8b\u5bbe\u53ef\u53ef\u5c0f\u519c\u6237\u9762\u4e34\u75c5\u5bb3\u6311\u6218\uff0c\u7f3a\u4e4f\u6570\u636e\u548c\u826f\u597d\u519c\u4e1a\u5b9e\u8df5\uff0c\u9700\u8981\u9002\u5408\u504f\u8fdc\u5730\u533a\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u79bb\u7ebf\u79fb\u52a8\u5e94\u7528\uff0c\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u53ef\u53ef\u75c5\u5bb3\u548c\u9ed1\u835a\u611f\u67d3\u7a0b\u5ea6", "result": "\u75c5\u5bb3\u8bc6\u522b\u6a21\u578b\u9a8c\u8bc1\u51c6\u786e\u738796.93%\uff0c\u9ed1\u835a\u611f\u67d3\u7a0b\u5ea6\u68c0\u6d4b\u51c6\u786e\u738779.49%\uff0c\u73b0\u573a\u6d4b\u8bd5\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\u7387\u8fbe84.2%", "conclusion": "\u8be5\u6280\u672f\u8d4b\u80fd\u5c0f\u519c\u6237\uff0c\u901a\u8fc7\u53ef\u8bbf\u95ee\u7684\u5de5\u5177\u6539\u5584\u53ef\u53ef\u4f5c\u7269\u5065\u5eb7\u548c\u751f\u4ea7\u6548\u7387"}}
{"id": "2602.00849", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00849", "abs": "https://arxiv.org/abs/2602.00849", "authors": ["Yuhao Huang", "Shih-Hsin Wang", "Andrea L. Bertozzi", "Bao Wang"], "title": "RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation", "comment": "Accepted to ICLR 2026", "summary": "Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.", "AI": {"tldr": "RMFlow\u901a\u8fc7\u7ed3\u5408\u7c97\u7c92\u5ea61-NFE MeanFlow\u4f20\u8f93\u548c\u5b9a\u5236\u5316\u566a\u58f0\u6ce8\u5165\u7cbe\u70bc\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u5206\u5b50\u751f\u6210\u548c\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u63a5\u8fd1SOTA\u7684\u6548\u679c\uff0c\u8ba1\u7b97\u6210\u672c\u4e0e\u57fa\u7ebfMeanFlows\u76f8\u5f53\u3002", "motivation": "MeanFlow\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4f46\u5176\u5355\u6b21\u51fd\u6570\u8bc4\u4f30\uff081-NFE\uff09\u751f\u6210\u901a\u5e38\u65e0\u6cd5\u4ea7\u751f\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u6027\u53c8\u80fd\u63d0\u5347\u751f\u6210\u8d28\u91cf\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "RMFlow\u6574\u5408\u4e86\u7c97\u7c92\u5ea6\u76841-NFE MeanFlow\u4f20\u8f93\u548c\u540e\u7eed\u7684\u5b9a\u5236\u5316\u566a\u58f0\u6ce8\u5165\u7cbe\u70bc\u6b65\u9aa4\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u6d41\u8def\u5f84\u7684\u5e73\u5747\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u635f\u5931\u51fd\u6570\u5e73\u8861\u4e86\u6982\u7387\u8def\u5f84\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\u6700\u5c0f\u5316\u548c\u6837\u672c\u4f3c\u7136\u6700\u5927\u5316\u3002", "result": "RMFlow\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u4e0a\u4e0b\u6587\u5230\u5206\u5b50\u548c\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u4e0a\uff0c\u4ec5\u4f7f\u75281-NFE\u5c31\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4e0e\u57fa\u7ebfMeanFlows\u76f8\u5f53\u3002", "conclusion": "RMFlow\u901a\u8fc7\u7ed3\u5408\u7c97\u7c92\u5ea6\u4f20\u8f93\u548c\u7cbe\u70bc\u6b65\u9aa4\uff0c\u6210\u529f\u89e3\u51b3\u4e86MeanFlow\u57281-NFE\u8bbe\u7f6e\u4e0b\u751f\u6210\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u751f\u6210\u3002"}}
{"id": "2602.00707", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00707", "abs": "https://arxiv.org/abs/2602.00707", "authors": ["Jingnan Zheng", "Jingjun Xu", "Yanzhen Luo", "Chenhang Cui", "Gelei Deng", "Zhenkai Liang", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Self-Guard: Defending Large Reasoning Models via enhanced self-reflection", "comment": null, "summary": "The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.", "AI": {"tldr": "Self-Guard\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b89\u5168\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u5bfc\u5411\u63d0\u793a\u548c\u5b89\u5168\u6fc0\u6d3b\u5f15\u5bfc\u4e24\u4e2a\u9636\u6bb5\uff0c\u5728\u8868\u793a\u5c42\u9762\u589e\u5f3a\u5927\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u5f25\u5408\u8ba4\u77e5-\u5408\u89c4\u5dee\u8ddd\uff0c\u65e0\u9700\u5927\u91cf\u540e\u8bad\u7ec3\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u663e\u5f0f\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4e5f\u5e26\u6765\u63a8\u7406\u64cd\u7eb5\u548c\u4fe1\u606f\u6cc4\u9732\u7b49\u72ec\u7279\u98ce\u9669\u3002\u73b0\u6709\u5bf9\u9f50\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u5927\u91cf\u540e\u8bad\u7ec3\u6216\u5916\u90e8\u5e72\u9884\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u80fd\u89e3\u51b3\u56fa\u6709\u7684\u8ba4\u77e5-\u5408\u89c4\u5dee\u8ddd\u95ee\u9898\u2014\u2014\u6a21\u578b\u80fd\u8bc6\u522b\u98ce\u9669\u4f46\u56e0\u8fce\u5408\u7528\u6237\u503e\u5411\u800c\u4f18\u5148\u9075\u5faa\u6307\u4ee4\u3002", "method": "Self-Guard\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a1\uff09\u5b89\u5168\u5bfc\u5411\u63d0\u793a\uff1a\u6fc0\u6d3b\u6a21\u578b\u7684\u6f5c\u5728\u5b89\u5168\u610f\u8bc6\uff0c\u5f15\u53d1\u81ea\u53d1\u53cd\u601d\uff1b2\uff09\u5b89\u5168\u6fc0\u6d3b\u5f15\u5bfc\uff1a\u63d0\u53d6\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u65b9\u5411\u6027\u53d8\u5316\u5e76\u653e\u5927\uff0c\u786e\u4fdd\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b89\u5168\u5408\u89c4\u6027\u4f18\u5148\u4e8e\u8fce\u5408\u503e\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSelf-Guard\u80fd\u6709\u6548\u5f25\u5408\u8ba4\u77e5-\u5408\u89c4\u5dee\u8ddd\uff0c\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u5b9e\u7528\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u5b89\u5168\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5bf9\u672a\u89c1\u98ce\u9669\u548c\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u90fd\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLRM\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6210\u672c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Self-Guard\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5b89\u5168\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u5c42\u9762\u7684\u5e72\u9884\u6709\u6548\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u5f25\u5408\u4e86\u8ba4\u77e5\u4e0e\u5408\u89c4\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u548c\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.00179", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00179", "abs": "https://arxiv.org/abs/2602.00179", "authors": ["Joseph L. Breeden"], "title": "How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models", "comment": "22 pages; 2 figures", "summary": "For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u65b9\u6cd5\uff08\u5982LIME\u548cSHAP\uff09\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u4e0d\u7a33\u5b9a\u6027\u53cd\u6620\u4e86\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u6b63\u786e\u505a\u6cd5\u662f\u5148\u8bc4\u4f30\u9884\u6d4b\u662f\u5426\u53ef\u7528\uff0c\u518d\u51b3\u5b9a\u662f\u5426\u89e3\u91ca\u3002", "motivation": "\u5728\u5173\u952e\u51b3\u7b56\u5e94\u7528\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u662f\u4e3b\u8981\u5173\u6ce8\u70b9\uff0c\u4f46\u73b0\u6709\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u65b9\u6cd5\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u4e0d\u7a33\u5b9a\u6027\u53d7\u5230\u6279\u8bc4\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u6279\u8bc4\u53cd\u6620\u4e86\u5bf9\u95ee\u9898\u7684\u8bef\u89e3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u89e3\u91ca\u65b9\u6cd5\u7684\u987a\u5e8f\u548c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u89e3\u91ca\u6846\u67b6\uff1a\u9996\u5148\u8bc4\u4f30\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ea\u6709\u5f53\u9884\u6d4b\u5177\u6709\u8db3\u591f\u4f4e\u7684\u53ef\u7528\u6027\u65f6\u624d\u5bfb\u6c42\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\uff1b\u5f53\u6ca1\u6709\u53ef\u7528\u9884\u6d4b\u65f6\uff0c\u5e94\u4f7f\u7528\u66f4\u7b80\u5355\u7684\u6574\u4f53\u6a21\u578b\uff08\u5982\u4f20\u7edf\u903b\u8f91\u56de\u5f52\uff09\u3002", "result": "\u51b3\u7b56\u8fb9\u754c\u5904\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u9ad8\uff0c\u56e0\u6b64\u89e3\u91ca\u4e0d\u7a33\u5b9a\u6027\u4e5f\u9ad8\uff1b\u5f53\u9884\u6d4b\u53ef\u7528\u65f6\uff0c\u5c40\u90e8\u7ebf\u6027\u89e3\u91ca\u7684\u4e0d\u7a33\u5b9a\u6027\u76f8\u5e94\u8f83\u4f4e\uff1b\u67d0\u4e9b\u58f0\u79f0\u5904\u5904\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff08\u5982ReLU\u7f51\u7edc\uff09\u5b9e\u9645\u4e0a\u53ea\u6709\u865a\u5e7b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u89e3\u91ca\u65b9\u6cd5\u7684\u6b63\u786e\u987a\u5e8f\u662f\uff1a\u5148\u5224\u65ad\u9884\u6d4b\u662f\u5426\u53ef\u7528\uff0c\u518d\u51b3\u5b9a\u662f\u5426\u89e3\u91ca\uff1b\u89e3\u91ca\u4e0d\u53ef\u7528\u7684\u9884\u6d4b\u6ca1\u6709\u610f\u4e49\uff1b\u67d0\u4e9b\u6a21\u578b\u7684\"\u5904\u5904\u53ef\u89e3\u91ca\u6027\"\u662f\u865a\u5e7b\u7684\uff0c\u56e0\u4e3a\u8fb9\u754c\u5904\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u592a\u9ad8\u3002"}}
{"id": "2602.00758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00758", "abs": "https://arxiv.org/abs/2602.00758", "authors": ["Ali El Lahib", "Ying-Jieh Xia", "Zehan Li", "Yuxuan Wang", "Xinyu Pi"], "title": "Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting", "comment": "9 pages, 6 figures", "summary": "Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.", "AI": {"tldr": "\u641c\u7d22\u65e5\u671f\u8fc7\u6ee4\u5668\u5728\u9884\u6d4b\u8bc4\u4f30\u4e2d\u4e0d\u53ef\u9760\uff0c71%\u7684\u67e5\u8be2\u5b58\u5728\u622a\u6b62\u65e5\u671f\u540e\u4fe1\u606f\u6cc4\u9732\uff0c\u5bfc\u81f4\u9884\u6d4b\u51c6\u786e\u7387\u865a\u9ad8", "motivation": "\u7814\u7a76\u641c\u7d22\u5f15\u64ce\u65e5\u671f\u8fc7\u6ee4\u5668\u5728\u56de\u987e\u6027\u9884\u6d4b\u8bc4\u4f30\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63ed\u793a\u4f7f\u7528before:\u8fc7\u6ee4\u5668\u8fdb\u884c\u65f6\u95f4\u9650\u5236\u68c0\u7d22\u7684\u7f3a\u9677", "method": "\u901a\u8fc7\u5ba1\u8ba1Google\u641c\u7d22\u7684before:\u8fc7\u6ee4\u5668\uff0c\u5206\u6790\u4fe1\u606f\u6cc4\u9732\u673a\u5236\uff1b\u4f7f\u7528GPT-oss-120b\u6a21\u578b\u5728\u6cc4\u9732\u6587\u6863\u4e0a\u8fdb\u884c\u9884\u6d4b\uff0c\u5bf9\u6bd4\u65e0\u6cc4\u9732\u6587\u6863\u7684\u9884\u6d4b\u51c6\u786e\u6027", "result": "71%\u7684\u95ee\u9898\u81f3\u5c11\u8fd4\u56de\u4e00\u4e2a\u5305\u542b\u5f3a\u540e\u622a\u6b62\u65e5\u671f\u6cc4\u9732\u7684\u9875\u9762\uff0c41%\u7684\u95ee\u9898\u76f4\u63a5\u6cc4\u9732\u7b54\u6848\uff1b\u4f7f\u7528\u6cc4\u9732\u6587\u6863\u65f6\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u865a\u9ad8\uff08Brier\u5206\u65700.108 vs 0.242\uff09", "conclusion": "\u65e5\u671f\u9650\u5236\u641c\u7d22\u4e0d\u8db3\u4ee5\u8fdb\u884c\u65f6\u95f4\u8bc4\u4f30\uff0c\u5efa\u8bae\u4f7f\u7528\u66f4\u5f3a\u7684\u68c0\u7d22\u4fdd\u969c\u63aa\u65bd\u6216\u5728\u51bb\u7ed3\u7684\u65f6\u95f4\u6233\u7f51\u9875\u5feb\u7167\u4e0a\u8fdb\u884c\u8bc4\u4f30"}}
{"id": "2602.00247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00247", "abs": "https://arxiv.org/abs/2602.00247", "authors": ["Samyak Jha", "Junho Kim"], "title": "CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models", "comment": null, "summary": "Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.", "AI": {"tldr": "CAPA\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u8d21\u732e\u8bc4\u4f30\u89c6\u89c9token\u91cd\u8981\u6027\uff0c\u4fee\u526a\u4f4e\u8d21\u732etoken\u5e76\u7528\u7ebf\u6027\u8fd1\u4f3c\u51cf\u5c11FFN\u8ba1\u7b97\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5904\u7406\u6570\u5343\u4e2a\u89c6\u89c9token\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u54ea\u4e9btoken\u548c\u8ba1\u7b97\u53ef\u4ee5\u5b89\u5168\u79fb\u9664", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u8d21\u732e\u6307\u6807\u66ff\u4ee3\u4f20\u7edf\u6ce8\u610f\u529b\u5206\u6570\uff0c\u8bc6\u522b\u89c6\u89c9\u6ce8\u610f\u529b\u6c47\u4e2d\u7684\u6982\u7387\u5783\u573e\uff08\u53ef\u4fee\u526a\uff09\u548c\u7ed3\u6784\u951a\u70b9\uff08\u9700\u4fdd\u7559\uff09\uff1b\u5728\u5173\u952e\u529f\u80fd\u8f6c\u6362\u5904\u4fee\u526a\u89c6\u89c9token\uff0c\u5e76\u5bf9FFN\u8fdb\u884c\u9ad8\u6548\u7ebf\u6027\u8fd1\u4f3c", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAPA\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\uff0c\u5e76\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027", "conclusion": "\u6ce8\u610f\u529b\u8d21\u732e\u662f\u66f4\u51c6\u786e\u7684\u89c6\u89c9token\u9009\u62e9\u6807\u51c6\uff0cCAPA\u6846\u67b6\u901a\u8fc7\u53cc\u7b56\u7565\uff08token\u4fee\u526a\u548cFFN\u8fd1\u4f3c\uff09\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd"}}
{"id": "2602.00862", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00862", "abs": "https://arxiv.org/abs/2602.00862", "authors": ["Shih-Hsin Wang", "Yuhao Huang", "Taos Transue", "Justin Baker", "Jonathan Forstater", "Thomas Strohmer", "Bao Wang"], "title": "Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs", "comment": "Published in NeurIPS 2025", "summary": "Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $\u03b1$-helices, $\u03b2$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u86cb\u767d\u8d28\u7ed3\u6784\u5b66\u4e60\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u5316\u56fe\u8868\u793a\uff08\u7ec6\u7c92\u5ea6\u4e8c\u7ea7\u7ed3\u6784\u5b50\u56fe\u548c\u7c97\u7c92\u5ea6\u8fde\u63a5\u56fe\uff09\u6765\u6355\u6349\u5c40\u90e8\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u5b66\u4e60\u4e2d\u9762\u4e34\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u6765\u540c\u65f6\u6355\u6349\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u548c\u9ad8\u7ea7\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u5305\u542b\u7ec6\u7c92\u5ea6\u4e8c\u7ea7\u7ed3\u6784\u5b50\u56fe\uff08\u03b1-\u87ba\u65cb\u3001\u03b2-\u94fe\u3001\u73af\uff09\u548c\u7c97\u7c92\u5ea6\u8fde\u63a5\u56fe\u7684\u5c42\u6b21\u5316\u8868\u793a\uff1b\u4f7f\u7528\u4e24\u4e2aGNN\u5206\u522b\u5b66\u4e60\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u548c\u8de8\u57fa\u5e8f\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u6846\u67b6\u652f\u6301\u5404\u9636\u6bb5\u7075\u6d3b\u9009\u62e9GNN\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u8be5\u5c42\u6b21\u5316\u6846\u67b6\u4fdd\u6301\u4e86\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u4e0d\u4e22\u5931\u5173\u952e\u7ed3\u6784\u4fe1\u606f\uff1b\u5b9e\u8bc1\u8868\u660e\u5c06\u57fa\u7ebfGNN\u96c6\u6210\u5230\u591a\u5c3a\u5ea6\u6846\u67b6\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u5c3a\u5ea6\u56fe\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u86cb\u767d\u8d28\u7ed3\u6784\u5b66\u4e60\u4e2d\u7684\u591a\u5c3a\u5ea6\u548c\u957f\u7a0b\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u8868\u8fbe\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2602.00709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00709", "abs": "https://arxiv.org/abs/2602.00709", "authors": ["Wenda Li", "Tongya Zheng", "Kaixuan Chen", "Shunyu Liu", "Haoze Jiang", "Yunzhi Hao", "Rui Miao", "Zujie Ren", "Mingli Song", "Hang Shi", "Gang Chen"], "title": "Physics-informed Diffusion Generation for Geomagnetic Map Interpolation", "comment": "5 pages, 2 figures, IEEE ICASSP'26", "summary": "Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.", "AI": {"tldr": "\u63d0\u51faPDG\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6269\u6563\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u5730\u78c1\u5730\u56fe\u63d2\u503c\uff0c\u7ed3\u5408\u5c40\u90e8\u611f\u53d7\u91ce\u7684\u7269\u7406\u4fe1\u606f\u63a9\u7801\u7b56\u7565\u548c\u514b\u91cc\u91d1\u539f\u7406\u7684\u7269\u7406\u7ea6\u675f\uff0c\u6709\u6548\u6d88\u9664\u566a\u58f0\u5e72\u6270\u5e76\u4fdd\u8bc1\u7269\u7406\u89c4\u5f8b\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6563\u70b9\u6570\u636e\u63d2\u503c\u65b9\u6cd5\u5e76\u975e\u4e13\u95e8\u4e3a\u5730\u78c1\u5730\u56fe\u8bbe\u8ba1\uff0c\u7531\u4e8e\u68c0\u6d4b\u566a\u58f0\u548c\u7269\u7406\u89c4\u5f8b\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u5730\u78c1\u5730\u56fe\u63d2\u503c\u5728\u5bfc\u822a\u548c\u8d44\u6e90\u52d8\u63a2\u4e2d\u5177\u6709\u5173\u952e\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u6269\u6563\u751f\u6210\u6846\u67b6(PDG)\uff1a1) \u57fa\u4e8e\u5c40\u90e8\u611f\u53d7\u91ce\u8bbe\u8ba1\u7269\u7406\u4fe1\u606f\u63a9\u7801\u7b56\u7565\uff0c\u6307\u5bfc\u6269\u6563\u751f\u6210\u8fc7\u7a0b\u6d88\u9664\u566a\u58f0\u5e72\u6270\uff1b2) \u6839\u636e\u5730\u78c1\u5730\u56fe\u7684\u514b\u91cc\u91d1\u539f\u7406\u5bf9\u6269\u6563\u751f\u6210\u7ed3\u679c\u65bd\u52a0\u7269\u7406\u4fe1\u606f\u7ea6\u675f\uff0c\u786e\u4fdd\u4e25\u683c\u9075\u5faa\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6df1\u5165\u5206\u6790\u8868\u660e\uff0cPDG\u7684\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u5177\u6709\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "PDG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u63a9\u7801\u7b56\u7565\u548c\u7269\u7406\u7ea6\u675f\uff0c\u80fd\u591f\u6709\u6548\u8fdb\u884c\u5730\u78c1\u5730\u56fe\u63d2\u503c\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6d88\u9664\u566a\u58f0\u5e72\u6270\u548c\u4fdd\u8bc1\u7269\u7406\u89c4\u5f8b\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.00191", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00191", "abs": "https://arxiv.org/abs/2602.00191", "authors": ["Yadang Alexis Rouzoumka", "Jean Pinsolle", "Eug\u00e9nie Terreaux", "Christ\u00e8le Morisseau", "Jean-Philippe Ovarlez", "Chengfang Ren"], "title": "GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models", "comment": "preprint", "summary": "Diffusion models learn a time-indexed score field $\\mathbf{s}_\u03b8(\\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.", "AI": {"tldr": "\u63d0\u51faGEPC\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u5206\u6570\u573a\u7684\u7fa4\u7b49\u53d8\u6027\u7834\u574f\u6765\u8bc6\u522b\u5206\u5e03\u5916\u6570\u636e\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u8ba1\u7b97\u8f7b\u91cf", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684OOD\u68c0\u6d4b\u5668\u4e3b\u8981\u5229\u7528\u5206\u6570\u5927\u5c0f\u6216\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5206\u6570\u573a\u7684\u7b49\u53d8\u6027\u7279\u5f81\u3002\u5f53\u5206\u5e03\u5185\u6570\u636e\u5177\u6709\u8fd1\u4f3c\u7b49\u53d8\u6027\u65f6\uff0c\u5206\u5e03\u5916\u6570\u636e\u53ef\u80fd\u7834\u574f\u8fd9\u79cd\u7b49\u53d8\u6027\uff0c\u8fd9\u4e3aOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u7fa4\u7b49\u53d8\u540e\u9a8c\u4e00\u81f4\u6027\uff08GEPC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u5b66\u4e60\u5230\u7684\u5206\u6570\u573a\u5728\u6709\u9650\u7fa4\u53d8\u6362\u4e0b\u7684\u53d8\u6362\u4e00\u81f4\u6027\u6765\u68c0\u6d4b\u7b49\u53d8\u6027\u7834\u574f\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u5206\u6570\u8bc4\u4f30\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u53ef\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7b49\u53d8\u6027\u7834\u574f\u56fe\u3002", "result": "\u5728OOD\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGEPC\u8fbe\u5230\u4e0e\u73b0\u6709\u6269\u6563\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684AUROC\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u8f7b\u91cf\u3002\u5728\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u4e0a\uff0cGEPC\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u76ee\u6807-\u80cc\u666f\u5206\u79bb\u548c\u89c6\u89c9\u53ef\u89e3\u91ca\u7684\u7b49\u53d8\u6027\u7834\u574f\u56fe\u3002", "conclusion": "GEPC\u901a\u8fc7\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u5206\u6570\u573a\u7684\u7b49\u53d8\u6027\u7834\u574f\uff0c\u4e3aOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u8fd1\u4f3c\u7b49\u53d8\u6027\u7684\u6570\u636e\u5206\u5e03\u573a\u666f\u3002"}}
{"id": "2602.00759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00759", "abs": "https://arxiv.org/abs/2602.00759", "authors": ["Zhipeng Chen", "Xiaobo Qin", "Wayne Xin Zhao", "Youbin Wu", "Ji-Rong Wen"], "title": "Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning", "comment": "21 pages, Working in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.", "AI": {"tldr": "A\u00b2D\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u80fd\u529b\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u7b80\u5355\u5b50\u95ee\u9898\u6765\u589e\u5f3aRLVR\u6548\u679c\uff0c\u65e0\u9700\u6559\u5e08\u6a21\u578b\u6307\u5bfc", "motivation": "\u4f20\u7edfRLVR\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u6709\u9650\uff0c\u6a21\u578b\u53ea\u80fd\u8fdb\u884c\u76f2\u63a2\u7d22\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u5bb9\u6613\u5931\u8d25\u3002\u9700\u8981\u5728\u4e0d\u4f9d\u8d56\u6559\u5e08\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u4e3aRLVR\u8fc7\u7a0b\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\u3002", "method": "1. \u5148\u901a\u8fc7\u65e0\u84b8\u998f\u7684RLVR\u8bad\u7ec3\u5206\u89e3\u5668\uff0c\u4f7f\u5176\u80fd\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u7b80\u5355\u5b50\u95ee\u9898\uff1b2. \u7528\u5206\u89e3\u5668\u4e3a\u8bad\u7ec3\u96c6\u4e2d\u7684\u6bcf\u4e2a\u95ee\u9898\u6807\u6ce8\u5b50\u95ee\u9898\uff1b3. \u5728\u5b50\u95ee\u9898\u6307\u5bfc\u4e0b\u7528RLVR\u8bad\u7ec3\u63a8\u7406\u5668", "result": "1. \u4e0e\u7ade\u4e89\u57fa\u7ebf\u76f8\u6bd4\u8868\u73b0\u66f4\u4f18\uff1b2. \u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u5e94\u7528\u4e8e\u4e0d\u540cRLVR\u7b97\u6cd5\uff1b3. \u5206\u89e3\u5668\u5206\u6790\u63ed\u793a\u4e86RLVR\u8fc7\u7a0b\u5982\u4f55\u5f71\u54cd\u5176\u6027\u80fd\u548c\u884c\u4e3a\uff0c\u4ee5\u53ca\u54ea\u79cd\u6307\u5bfc\u66f4\u9002\u5408\u589e\u5f3a\u63a8\u7406\u5668\u7684\u63a2\u7d22\u548c\u5229\u7528\u80fd\u529b", "conclusion": "A\u00b2D\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u80fd\u529b\u5206\u89e3\u6709\u6548\u589e\u5f3a\u4e86RLVR\u7684\u6548\u679c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6559\u5e08\u6a21\u578b\u7684\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u65b0\u9014\u5f84"}}
{"id": "2602.00249", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00249", "abs": "https://arxiv.org/abs/2602.00249", "authors": ["Rishav Pramanik", "Ian E. Nielsen", "Jeff Smith", "Saurav Pandit", "Ravi P. Ramachandran", "Zhaozheng Yin"], "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis", "comment": null, "summary": "The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.", "AI": {"tldr": "SANEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7ec4\u5408\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u6765\u8bc4\u4f30\u7a7a\u95f4\u5173\u7cfb\u3001\u5c5e\u6027\u7ed1\u5b9a\u548c\u8ba1\u6570\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u8bcd\u6c47\u9650\u5236\u548c\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u6e32\u67d3\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u63d0\u793a\u65f6\u5b58\u5728\u74f6\u9888\uff0c\u800c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8bcd\u6c47\u5c01\u95ed\u3001\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bca\u65ad\u80fd\u529b\u3001\u65e0\u6cd5\u63d0\u4f9b\u53ef\u89e3\u91ca\u53cd\u9988\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5728\u7ec4\u5408\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faSANEval\u57fa\u51c6\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u5f00\u653e\u8bcd\u6c47\u7ec4\u5408\u8bc4\u4f30\u6d41\u7a0b\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u63d0\u793a\u7406\u89e3\uff1b2\uff09\u91c7\u7528LLM\u589e\u5f3a\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u5668\u6765\u7a33\u5065\u8bc4\u4f30\u7ec4\u5408\u4e00\u81f4\u6027\uff0c\u4e0d\u53d7\u56fa\u5b9a\u8bcd\u6c47\u9650\u5236\u3002", "result": "\u5728\u516d\u4e2a\u6700\u5148\u8fdb\u7684T2I\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eSANEval\u7684\u81ea\u52a8\u8bc4\u4f30\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\uff0c\u5728\u5c5e\u6027\u7ed1\u5b9a\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u8ba1\u6570\u4efb\u52a1\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684Spearman\u79e9\u76f8\u5173\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "SANEval\u4e3a\u7ec4\u5408T2I\u751f\u6210\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c06\u53d1\u5e03\u6570\u636e\u96c6\u548c\u5f00\u6e90\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.00869", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.00869", "abs": "https://arxiv.org/abs/2602.00869", "authors": ["Yuhao Huang", "Taos Transue", "Shih-Hsin Wang", "William Feldman", "Hong Zhang", "Bao Wang"], "title": "Improving Flow Matching by Aligning Flow Divergence", "comment": "Published in ICML 2025", "summary": "Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \\href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6761\u4ef6\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u5339\u914d\u6d41\u573a\u548c\u5176\u6563\u5ea6\u6765\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u4e0d\u727a\u7272\u751f\u6210\u6548\u7387\u3002", "motivation": "\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u867d\u7136\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u6d41\u57fa\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f46\u5728\u5b66\u4e60\u6982\u7387\u8def\u5f84\u7684\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u4f5c\u8005\u53d1\u73b0CFM\u4e0d\u80fd\u5b8c\u5168\u4fdd\u8bc1\u6982\u7387\u8def\u5f84\u7684\u7cbe\u786e\u5b66\u4e60\uff0c\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u6765\u8868\u5f81\u5b66\u4e60\u6982\u7387\u8def\u5f84\u4e0e\u771f\u5b9e\u6982\u7387\u8def\u5f84\u4e4b\u95f4\u7684\u8bef\u5dee\uff0c\u5e76\u63a8\u5bfc\u5176\u89e3\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u4e24\u4e2a\u6982\u7387\u8def\u5f84\u4e4b\u95f4\u7684\u603b\u53d8\u5dee\u5dee\u8ddd\u53d7CFM\u635f\u5931\u548c\u76f8\u5173\u6563\u5ea6\u635f\u5931\u7684\u4e0a\u754c\u7ea6\u675f\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u540c\u65f6\u5339\u914d\u6d41\u573a\u53ca\u5176\u6563\u5ea6\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u4e2a\u91cd\u8981\u57fa\u51c6\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6d41\u57fa\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u52a8\u529b\u7cfb\u7edf\u3001DNA\u5e8f\u5217\u548c\u89c6\u9891\u7684\u751f\u6210\u5efa\u6a21\uff0c\u4e14\u4e0d\u727a\u7272\u751f\u6210\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u540c\u65f6\u5339\u914d\u6d41\u573a\u548c\u6563\u5ea6\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u6761\u4ef6\u6d41\u5339\u914d\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u6d41\u57fa\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2602.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00710", "abs": "https://arxiv.org/abs/2602.00710", "authors": ["Yueqi Zhang", "Jin Hu", "Shaoxiong Feng", "Peiwen Yuan", "Xinglin Wang", "Yiwei Li", "Jiayi Shi", "Chuyi Tan", "Ji Zhang", "Boyuan Pan", "Yao Hu", "Kan Li"], "title": "Learning More from Less: Unlocking Internal Representations for Benchmark Compression", "comment": null, "summary": "The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.", "AI": {"tldr": "REPCORE\uff1a\u901a\u8fc7\u5c06\u5f02\u6784\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u6765\u6784\u5efa\u4ee3\u8868\u6027\u6838\u5fc3\u96c6\uff0c\u4ec5\u970010\u4e2a\u6e90\u6a21\u578b\u5373\u53ef\u7cbe\u786e\u8bc4\u4f30LLM\u6027\u80fd\uff0c\u4f18\u4e8e\u57fa\u4e8e\u8f93\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6e90\u6a21\u578b\u6765\u4f30\u8ba1\u53ef\u9760\u7684\u9879\u76ee\u7279\u5f81\uff0c\u4f46\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6e90\u6a21\u578b\u6570\u91cf\u6709\u9650\u65f6\u7edf\u8ba1\u4e0d\u7a33\u5b9a\u3002\u57fa\u4e8e\u79bb\u6563\u6b63\u786e\u6027\u6807\u7b7e\u7684\u65b9\u6cd5\u4f1a\u4e22\u5931\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faREPCORE\u65b9\u6cd5\uff0c\u5c06\u5f02\u6784\u9690\u85cf\u72b6\u6001\u5bf9\u9f50\u5230\u7edf\u4e00\u7684\u6f5c\u5728\u7a7a\u95f4\u6765\u6784\u5efa\u4ee3\u8868\u6027\u6838\u5fc3\u96c6\uff0c\u4f7f\u7528\u8fd9\u4e9b\u5b50\u96c6\u8fdb\u884c\u6027\u80fd\u5916\u63a8\uff0c\u4ec5\u9700\u5c11\u91cf\u6e90\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u4f30\u8ba1\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c200\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cREPCORE\u5728\u6392\u540d\u76f8\u5173\u6027\u548c\u4f30\u8ba1\u51c6\u786e\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u8f93\u51fa\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u8c31\u5206\u6790\u663e\u793a\u5bf9\u9f50\u8868\u793a\u5305\u542b\u53ef\u5206\u79bb\u7684\u7ec4\u4ef6\uff0c\u53cd\u6620\u4e86\u5e7f\u6cdb\u7684\u54cd\u5e94\u503e\u5411\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "REPCORE\u901a\u8fc7\u5229\u7528\u9690\u85cf\u72b6\u6001\u4fe1\u606f\u800c\u975e\u4ec5\u4f9d\u8d56\u8f93\u51fa\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u5c0f\u6e90\u6a21\u578b\u6c60\u4e0b\u7684LLM\u8bc4\u4f30\u95ee\u9898\uff0c\u4e3a\u65b0\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.00199", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00199", "abs": "https://arxiv.org/abs/2602.00199", "authors": ["Johanna Marie Gegenfurtner", "Albert Kj\u00f8ller Jacobsen", "Naima Elosegui Borras", "Alejandro Valverde Mahou", "Georgios Arvanitidis"], "title": "Reducing Memorisation in Generative Models via Riemannian Bayesian Inference", "comment": null, "summary": "Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u635f\u5931\u51fd\u6570\u7684\u9ece\u66fc\u51e0\u4f55\u7ed3\u6784\u6765\u5e73\u8861\u751f\u6210\u6a21\u578b\u7684\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u6837\u672c\uff0c\u4f46\u5e73\u8861\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u4f5c\u8005\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u51fa\u53d1\uff0c\u5173\u6ce8\u6d41\u5339\u914d\u548c\u6269\u6563\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u6784\u5efa\u80fd\u66f4\u597d\u6355\u6349\u6570\u636e\u5206\u5e03\u53d8\u5f02\u6027\u7684\u9884\u6d4b\u540e\u9a8c\u5206\u5e03\u3002", "method": "\u4f7f\u7528\u9ece\u66fc\u5ea6\u91cf\u6355\u6349\u635f\u5931\u51fd\u6570\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u80fd\u9002\u5e94\u635f\u5931\u666f\u89c2\u5c40\u90e8\u7ed3\u6784\u7684\u7075\u6d3b\u8fd1\u4f3c\u540e\u9a8c\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u91c7\u6837\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u4f3c\u4f46\u8bb0\u5fc6\u51cf\u5c11\u7684\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u8bb0\u5fc6\u3002\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8bf4\u660e\u8003\u8651\u635f\u5931\u51e0\u4f55\u7ed3\u6784\u80fd\u6709\u6548\u5229\u7528\u53c2\u6570\u7a7a\u95f4\uff0c\u5373\u4f7f\u5bf9\u4e8e\u590d\u6742\u7684\u9ad8\u7ef4\u751f\u6210\u6a21\u578b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u635f\u5931\u51fd\u6570\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u4ee5\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u6709\u6548\u5e73\u8861\u751f\u6210\u6a21\u578b\u7684\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u9ad8\u7ef4\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00760", "abs": "https://arxiv.org/abs/2602.00760", "authors": ["Kaiyan Chang", "Chenwei Zhu", "Yingfeng Luo", "Yifu Huo", "Chenglong Wang", "Xiaoqian Liu", "Qiaozhi He", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards", "comment": "Under Review", "summary": "Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnchor-based Process Reward (APR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u63a8\u7406\u951a\u70b9\u5e76\u60e9\u7f5a\u951a\u70b9\u540e\u65e0\u610f\u4e49\u7684\u91cd\u590d\u9a8c\u8bc1\uff0c\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u4f1a\u51fa\u73b0\"\u8fc7\u5ea6\u601d\u8003\"\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u5f97\u5230\u6700\u7ec8\u7b54\u6848\u540e\u4ecd\u8fdb\u884c\u65e0\u610f\u4e49\u7684\u91cd\u590d\u81ea\u6211\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002\u9700\u8981\u4ece\u7ec6\u7c92\u5ea6\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5b9a\u4e49\"\u63a8\u7406\u951a\u70b9\"\u2014\u2014\u7b54\u6848\u9996\u6b21\u7a33\u5b9a\u7684\u4f4d\u7f6e\uff0c\u8bc6\u522b\u51fa\u951a\u70b9\u540e\u7684\"\u7b54\u6848\u7a33\u5b9a\u5c3e\u90e8\"\uff08AST\uff09\u3002\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u951a\u70b9\u7684\u8fc7\u7a0b\u5956\u52b1\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u63a8\u7406\u951a\u70b9\u5e76\u4e13\u95e8\u60e9\u7f5a\u951a\u70b9\u540e\u7684AST\u3002\u7ed3\u5408\u9002\u5408\u957f\u5ea6\u60e9\u7f5a\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "APR\u65b9\u6cd5\u57281.5B\u548c7B\u89c4\u6a21\u7684\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd-\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u548c\u60e9\u7f5a\u63a8\u7406\u951a\u70b9\u540e\u7684\u65e0\u610f\u4e49\u91cd\u590d\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00262", "abs": "https://arxiv.org/abs/2602.00262", "authors": ["Huanran Li", "Daniel Pimentel-Alarc\u00f3n"], "title": "Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning", "comment": null, "summary": "Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0d\u5b8c\u6574\u6570\u636e\u805a\u7c7b\u7684\u5bf9\u6bd4\u81ea\u76d1\u7763\u6846\u67b6CSC\uff0c\u901a\u8fc7\u751f\u6210\u63a9\u7801\u89c6\u56fe\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u4e0d\u53d8\u5d4c\u5165\uff0c\u518d\u7ed3\u5408\u7a00\u758f\u5b50\u7a7a\u95f4\u805a\u7c7b\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5b50\u7a7a\u95f4\u805a\u7c7b\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u6570\u636e\u5b8c\u5168\u89c2\u6d4b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7ecf\u5e38\u9047\u5230\u7f3a\u5931\u6570\u636e\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u805a\u7c7b(CSC)\u6846\u67b6\uff1a1) \u751f\u6210\u90e8\u5206\u89c2\u6d4b\u8f93\u5165\u7684\u63a9\u7801\u89c6\u56fe\uff1b2) \u4f7f\u7528SimCLR\u98ce\u683c\u7684\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e0d\u53d8\u5d4c\u5165\uff1b3) \u5e94\u7528\u7a00\u758f\u5b50\u7a7a\u95f4\u805a\u7c7b\u5bf9\u5d4c\u5165\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSC\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u7f3a\u5931\u6570\u636e\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5927\u578b\u6570\u636e\u96c6\u3002", "conclusion": "CSC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u6570\u636e\u7684\u5b50\u7a7a\u95f4\u805a\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u9ad8\u8d28\u91cf\u5d4c\u5165\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\u3002"}}
{"id": "2602.01176", "categories": ["cs.LG", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.01176", "abs": "https://arxiv.org/abs/2602.01176", "authors": ["Olaf Yunus Laitinen Imanov"], "title": "Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations", "comment": "8 pages, 4 figures, 6 tables", "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.", "AI": {"tldr": "MF-BPINN\uff1a\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u3001\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u5b66\u4e60\u7684\u591a\u4fdd\u771f\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6c42\u89e3\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b", "motivation": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6c42\u89e3\u9ad8\u4fdd\u771f\u5ea6PDE\u4ecd\u7136\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5728\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u4e0b\u8fdb\u884c\u591a\u6b21\u8bc4\u4f30\u7684\u53c2\u6570\u5316\u7cfb\u7edf", "method": "\u63d0\u51faMF-BPINN\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7ed3\u5408\u4e30\u5bcc\u7684\u4f4e\u4fdd\u771f\u5ea6\u6a21\u62df\u548c\u7a00\u758f\u7684\u9ad8\u4fdd\u771f\u5ea6\u6570\u636e\uff1b\u5f15\u5165\u5177\u6709\u53ef\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u7684\u81ea\u9002\u5e94\u6b8b\u5dee\u7f51\u7edc\uff0c\u52a8\u6001\u5e73\u8861\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u4fdd\u771f\u5ea6\u5dee\u5f02\uff1b\u5f00\u53d1\u57fa\u4e8e\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\u7684\u4e25\u683c\u8d1d\u53f6\u65af\u6846\u67b6", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u9ad8\u4fdd\u771f\u5ea6PDE\u6c42\u89e3\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898", "conclusion": "MF-BPINN\u4e3a\u53c2\u6570\u5316\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u9ad8\u6548\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u591a\u4fdd\u771f\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u3001\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\u6765\u63d0\u5347\u8ba1\u7b97\u6548\u7387"}}
{"id": "2602.00731", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.00731", "abs": "https://arxiv.org/abs/2602.00731", "authors": ["Kyle Hamilton", "Ali Intizar"], "title": "Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations", "comment": null, "summary": "In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u8fd1\u4e94\u5e74\u5de5\u4e1a\u9884\u6d4b\u6027\u7ef4\u62a4\u7684\u73b0\u72b6\uff0c\u6307\u51fa\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46\u5b58\u5728\u6570\u636e\u9700\u6c42\u5927\u3001\u6cdb\u5316\u6027\u5dee\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5219\u7cbe\u5ea6\u4f4e\u3001\u8bef\u62a5\u591a\u3002\u4f5c\u8005\u63d0\u51fa\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7b26\u53f7\u903b\u8f91\u7ed3\u5408\u7684\u795e\u7ecf\u7b26\u53f7AI\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u9884\u6d4b\u6027\u7ef4\u62a4\u9886\u57df\u9762\u4e34\u4e24\u96be\u56f0\u5883\uff1a\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\uff09\u7cbe\u5ea6\u9ad8\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5219\u7cbe\u5ea6\u4f4e\u3001\u8bef\u62a5\u591a\u4e14\u9700\u8981\u6301\u7eed\u4e13\u5bb6\u7ef4\u62a4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u8fd1\u4e94\u5e74\u5de5\u4e1a\u9884\u6d4b\u6027\u7ef4\u62a4\u6587\u732e\uff0c\u91cd\u70b9\u5173\u6ce8\u6df7\u5408\u7cfb\u7edf\u3002\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7AI\u67b6\u6784\uff0c\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7b26\u53f7\u903b\u8f91\u7ed3\u5408\uff0c\u5229\u7528\u4f20\u611f\u5668\u6570\u636e\u548c\u624b\u5de5\u89c4\u5219\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u5177\u4f53\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u3002", "result": "\u53d1\u73b0\u5927\u591a\u6570\u8fd1\u671f\u6587\u732e\u91c7\u7528\u6570\u636e\u9a71\u52a8\u67b6\u6784\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002\u6df7\u5408\u7cfb\u7edf\u6709\u6f5c\u529b\u514b\u670d\u5355\u4e00\u65b9\u6cd5\u7684\u5f31\u70b9\u3002\u795e\u7ecf\u7b26\u53f7AI\u80fd\u521b\u5efa\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u7cfb\u7edf\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7AI\u662f\u9884\u6d4b\u6027\u7ef4\u62a4\u7684\u672a\u6765\u65b9\u5411\uff0c\u80fd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u548c\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u5f53\u524d\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\u7684\u7ef4\u62a4\u7cfb\u7edf\u3002"}}
{"id": "2602.00205", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00205", "abs": "https://arxiv.org/abs/2602.00205", "authors": ["Beier Zhu", "Kesen Zhao", "Jiequan Cui", "Qianru Sun", "Yuan Zhou", "Xun Yang", "Hanwang Zhang"], "title": "Reducing Class-Wise Performance Disparity via Margin Regularization", "comment": "To appear in ICLR 2026", "summary": "Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2", "AI": {"tldr": "MR\u00b2\u901a\u8fc7\u52a8\u6001\u8c03\u6574logit\u548c\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u8fb9\u754c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u9a71\u52a8\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7279\u522b\u662f\u63d0\u5347\u56f0\u96be\u7c7b\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5373\u4f7f\u5728\u7c7b\u522b\u5e73\u8861\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4e5f\u7ecf\u5e38\u8868\u73b0\u51fa\u663e\u8457\u7684\u7c7b\u522b\u95f4\u51c6\u786e\u7387\u5dee\u5f02\uff0c\u8fd9\u5bf9\u53ef\u9760\u90e8\u7f72\u6784\u6210\u95ee\u9898\u3002\u867d\u7136\u5df2\u6709\u7ecf\u9a8c\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5bf9\u5206\u7c7b\u4e2d\u6027\u80fd\u5dee\u5f02\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86MR\u00b2\uff08Margin Regularization for Performance Disparity Reduction\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7406\u8bba\u9a71\u52a8\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574logit\u548c\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u8fb9\u754c\u3002\u57fa\u4e8e\u8fb9\u7f18\u7684\u7c7b\u522b\u654f\u611f\u6cdb\u5316\u8fb9\u754c\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6bcf\u7c7b\u7279\u5f81\u53d8\u5f02\u6027\u5982\u4f55\u5f71\u54cd\u8bef\u5dee\uff0c\u4ece\u800c\u6fc0\u52b1\u5bf9\u56f0\u96be\u7c7b\u522b\u4f7f\u7528\u66f4\u5927\u7684\u8fb9\u754c\u3002MR\u00b2\u4f18\u5316\u4e0e\u7279\u5f81\u6269\u6563\u6210\u6bd4\u4f8b\u7684\u6bcf\u7c7blogit\u8fb9\u754c\uff0c\u5e76\u60e9\u7f5a\u8fc7\u5ea6\u7684\u8868\u793a\u8fb9\u754c\u4ee5\u589e\u5f3a\u7c7b\u5185\u7d27\u51d1\u6027\u3002", "result": "\u5728\u5305\u62ecImageNet\u5728\u5185\u76847\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cd\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff08MAE\u3001MoCov2\u3001CLIP\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMR\u00b2\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6574\u4f53\u51c6\u786e\u7387\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u56f0\u96be\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u7b80\u5355\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MR\u00b2\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u9a71\u52a8\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8fb9\u754c\u6765\u51cf\u5c11\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u56f0\u96be\u7c7b\u522b\u7684\u8868\u73b0\uff0c\u4e3a\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00762", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.00762", "abs": "https://arxiv.org/abs/2602.00762", "authors": ["Yuheng Shao", "Junjie Xiong", "Chaoran Wu", "Xiyuan Wang", "Ziyu Zhou", "Yang Ouyang", "Qinyi Tao", "Quan Li"], "title": "WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs", "comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI' 26), April 13--17, 2026, Barcelona, Spain", "summary": "Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.", "AI": {"tldr": "WordCraft\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u5e2e\u52a9\u4e2d\u6587\u6bcd\u8bed\u82f1\u8bed\u5b66\u4e60\u8005\u901a\u8fc7\u5173\u952e\u8bcd\u6cd5\u8bb0\u5fc6\u8bcd\u6c47\uff0c\u89e3\u51b3\u4ed6\u4eec\u751f\u6210\u5408\u9002\u5173\u952e\u8bcd\u3001\u6784\u5efa\u5173\u8054\u548c\u5f62\u6210\u5fc3\u7406\u610f\u8c61\u7684\u56f0\u96be\u3002", "motivation": "\u4e2d\u6587\u6bcd\u8bed\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u5728\u8fd0\u7528\u5173\u952e\u8bcd\u6cd5\u8bb0\u5fc6\u8bcd\u6c47\u65f6\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u96be\u4ee5\u751f\u6210\u8bed\u97f3\u5408\u9002\u7684\u5173\u952e\u8bcd\u3001\u6784\u5efa\u8fde\u8d2f\u7684\u5173\u8054\u3001\u4ee5\u53ca\u521b\u9020\u751f\u52a8\u7684\u5fc3\u7406\u610f\u8c61\u6765\u5e2e\u52a9\u957f\u671f\u8bb0\u5fc6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5b8c\u5168\u81ea\u52a8\u5316\u751f\u6210\u5173\u952e\u8bcd\uff08\u727a\u7272\u5b66\u4e60\u8005\u53c2\u4e0e\u5ea6\uff09\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u7ed3\u679c\u800c\u7f3a\u4e4f\u8fc7\u7a0b\u6307\u5bfc\u3002", "method": "\u9996\u5148\u8fdb\u884c\u5f62\u6210\u6027\u7814\u7a76\uff08N=18\uff09\uff0c\u4e86\u89e3\u5b66\u4e60\u8005\u548c\u6559\u80b2\u8005\u7684\u9700\u6c42\u548c\u56f0\u96be\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5f00\u53d1\u4e86WordCraft\u2014\u2014\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u5de5\u5177\uff0c\u901a\u8fc7\u5f15\u5bfc\u5b66\u4e60\u8005\u5b8c\u6210\u5173\u952e\u8bcd\u9009\u62e9\u3001\u5173\u8054\u6784\u5efa\u548c\u610f\u8c61\u5f62\u6210\u4e09\u4e2a\u6b65\u9aa4\u6765\u642d\u5efa\u5173\u952e\u8bcd\u6cd5\u7684\u811a\u624b\u67b6\u3002", "result": "\u4e24\u9879\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cWordCraft\u4e0d\u4ec5\u4fdd\u7559\u4e86\u751f\u6210\u6548\u5e94\uff08\u5b66\u4e60\u8005\u81ea\u5df1\u53c2\u4e0e\u521b\u9020\u7684\u6548\u679c\uff09\uff0c\u800c\u4e14\u5728\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u9ad8\u6c34\u5e73\u3002", "conclusion": "WordCraft\u901a\u8fc7\u5b66\u4e60\u8005\u4e2d\u5fc3\u7684\u4ea4\u4e92\u5f0f\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4e2d\u6587\u6bcd\u8bed\u82f1\u8bed\u5b66\u4e60\u8005\u5728\u8bcd\u6c47\u8bb0\u5fc6\u4e2d\u4f7f\u7528\u5173\u952e\u8bcd\u6cd5\u7684\u5173\u952e\u56f0\u96be\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u4fdd\u6301\u5b66\u4e60\u8005\u53c2\u4e0e\u5ea6\u53c8\u63d0\u4f9b\u8fc7\u7a0b\u6307\u5bfc\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00265", "abs": "https://arxiv.org/abs/2602.00265", "authors": ["Dong Liang", "Yuhao Liu", "Jinyuan Jia", "Youjun Zhao", "Rynson W. H. Lau"], "title": "World-Shaper: A Unified Framework for 360\u00b0 Panoramic Editing", "comment": null, "summary": "Being able to edit panoramic images is crucial for creating realistic 360\u00b0 visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360\u00b0 visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/", "AI": {"tldr": "World-Shaper\uff1a\u9996\u4e2a\u76f4\u63a5\u5728ERP\u57df\u8fdb\u884c\u5168\u666f\u56fe\u50cf\u7f16\u8f91\u7684\u7edf\u4e00\u51e0\u4f55\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u7f16\u8f91\u8303\u5f0f\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u7f16\u8f91\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u53ef\u63a7\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u900f\u89c6\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u65e0\u6cd5\u5efa\u6a21\u5168\u666f\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u4f20\u7edf\u7684\u7acb\u65b9\u4f53\u8d34\u56fe\u5206\u89e3\u4f1a\u7834\u574f\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5728ERP\u57df\u76f4\u63a5\u8fdb\u884c\u5168\u666f\u7f16\u8f91", "method": "1) \u91c7\u7528\u751f\u6210-\u7f16\u8f91\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u63a7\u5168\u666f\u751f\u6210\u5408\u6210\u914d\u5bf9\u6570\u636e\uff1b2) \u5f15\u5165\u51e0\u4f55\u611f\u77e5\u5b66\u4e60\u7b56\u7565\uff0c\u5305\u62ec\u663e\u5f0f\u4f4d\u7f6e\u611f\u77e5\u5f62\u72b6\u76d1\u7763\u548c\u901a\u8fc7\u6e10\u8fdb\u8bad\u7ec3\u9690\u5f0f\u5185\u5316\u5168\u666f\u5148\u9a8c", "result": "\u5728PEBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u7f16\u8f91\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8fde\u8d2f\u7075\u6d3b\u7684360\u00b0\u89c6\u89c9\u4e16\u754c\u521b\u5efa", "conclusion": "World-Shaper\u6210\u529f\u89e3\u51b3\u4e86\u5168\u666f\u7f16\u8f91\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u4e3a\u7edf\u4e00\u7684\u7f16\u8f91\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5c06\u5f00\u6e90"}}
{"id": "2602.00751", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00751", "abs": "https://arxiv.org/abs/2602.00751", "authors": ["Cl\u00e1udio L\u00facio do Val Lopes", "Jo\u00e3o Marcus Pitta", "Fabiano Bel\u00e9m", "Gildson Alves", "Fl\u00e1vio Vin\u00edcius Cruzeiro Martins"], "title": "Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance", "comment": "9 pages, 5 figures 2026 IEEE/ACM 5th International Conference on AI Engineering - Software Engineering for AI}{April 12--13, 2026}{Rio de Janeiro, Brazil", "summary": "The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.\n  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Maria\u5e73\u53f0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u521d\u7ea7\u533b\u7597\u4fdd\u5065\u7684\u751f\u4ea7\u7ea7AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\uff08\u6e05\u6d01\u67b6\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001\u4ee3\u7406\u6a21\u5757\u5316\u3001\u4eba\u673a\u534f\u540c\u6cbb\u7406\uff09\u6765\u89e3\u51b3\u4e34\u5e8aAI\u4e2d\u7684\u53ef\u4fe1\u8d56\u6027\u6311\u6218\u3002", "motivation": "AI\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u96c6\u6210\u9762\u4e34\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\uff0c\u9700\u8981\u4ece\u5b64\u7acb\u6a21\u578b\u8f6c\u5411\u7a33\u5065\u3001\u53ef\u6cbb\u7406\u3001\u53ef\u9760\u7684\u7cfb\u7edf\u3002\u5de5\u4e1a\u5e94\u7528\u4e2d\u5e38\u5b58\u5728\u8106\u5f31\u3001\u539f\u578b\u884d\u751f\u7684\u67b6\u6784\u548c\u7cfb\u7edf\u6027\u76d1\u7763\u7f3a\u5931\uff0c\u5bfc\u81f4\"\u8d23\u4efb\u771f\u7a7a\"\uff0c\u5b89\u5168\u548c\u95ee\u8d23\u53d7\u5230\u635f\u5bb3\u3002", "method": "\u63d0\u51faMaria\u5e73\u53f0\u4f5c\u4e3a\u884c\u4e1a\u6848\u4f8b\u7814\u7a76\uff0c\u91c7\u7528\u534f\u540c\u67b6\u6784\uff1a\u7ed3\u5408\u6e05\u6d01\u67b6\u6784\uff08\u53ef\u7ef4\u62a4\u6027\uff09\u548c\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\uff08\u5f39\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff09\uff1b\u5f15\u5165\u4ee3\u7406\u4f5c\u4e3a\u4e3b\u8981\u6a21\u5757\u5316\u5355\u5143\uff0c\u6bcf\u4e2a\u4ee3\u7406\u62e5\u6709\u81ea\u4e3b\u7684MLOps\u751f\u547d\u5468\u671f\uff1b\u6280\u672f\u96c6\u6210\u4eba\u673a\u534f\u540c\u6cbb\u7406\u6a21\u578b\u4f5c\u4e3a\u5173\u952e\u7684\u4e8b\u4ef6\u9a71\u52a8\u6570\u636e\u6e90\u3002", "result": "Maria\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53c2\u8003\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\u7684\u6574\u5408\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aAI\u7cfb\u7edf\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u6784\u5efa\u53ef\u7ef4\u62a4\u3001\u53ef\u6269\u5c55\u548c\u53ef\u95ee\u8d23\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "conclusion": "\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8aAI\u9700\u8981\u901a\u8fc7\u6e05\u6d01\u67b6\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\u3001\u4ee3\u7406\u6a21\u5757\u5316\u548c\u4eba\u673a\u534f\u540c\u6cbb\u7406\u56db\u4e2a\u5de5\u7a0b\u652f\u67f1\u7684\u5168\u9762\u6574\u5408\u6765\u5b9e\u73b0\u3002Maria\u5e73\u53f0\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u6784\u5efa\u53ef\u7ef4\u62a4\u3001\u53ef\u6269\u5c55\u548c\u53ef\u95ee\u8d23\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53c2\u8003\u67b6\u6784\u548c\u7ecf\u9a8c\u6559\u8bad\u3002"}}
{"id": "2602.00208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00208", "abs": "https://arxiv.org/abs/2602.00208", "authors": ["Jordan Levy", "Paul Saves", "Moncef Garouani", "Nicolas Verstaevel", "Benoit Gaudou"], "title": "Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity", "comment": "Accepted at Intelligent Data Analysis (IDA), 2026", "summary": "Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHAP\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u8868\u5f81\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u51b3\u7b56\u673a\u5236\u6765\u6784\u5efa\u4e92\u8865\u6027\u66f4\u5f3a\u7684\u96c6\u6210\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u5206\u5e03\u591a\u6837\u6027\u548c\u6807\u7b7e\u7f3a\u5931\u7684\u6311\u6218\u3002\u73b0\u6709\u96c6\u6210\u65b9\u6cd5\u5e38\u56e0\u68c0\u6d4b\u5668\u51b3\u7b56\u673a\u5236\u76f8\u4f3c\u800c\u4ea7\u751f\u5197\u4f59\u7684\u5f02\u5e38\u5206\u6570\uff0c\u9650\u5236\u4e86\u96c6\u6210\u5b66\u4e60\u7684\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u771f\u6b63\u80fd\u6355\u6349\u4e0d\u540c\u7c7b\u578b\u5f02\u5e38\u6a21\u5f0f\u7684\u4e92\u8865\u68c0\u6d4b\u5668\u3002", "method": "\u4f7f\u7528SHAP\uff08SHapley Additive exPlanations\uff09\u91cf\u5316\u6bcf\u4e2a\u5f02\u5e38\u68c0\u6d4b\u5668\u5bf9\u8f93\u5165\u7279\u5f81\u7684\u91cd\u8981\u6027\u5206\u914d\uff0c\u6784\u5efa\u68c0\u6d4b\u5668\u7684\"\u89e3\u91ca\u6863\u6848\"\u3002\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u89e3\u91ca\u6863\u6848\u6765\u8861\u91cf\u68c0\u6d4b\u5668\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u57fa\u4e8e\u89e3\u91ca\u591a\u6837\u6027\u6765\u9009\u62e9\u4e92\u8865\u7684\u68c0\u6d4b\u5668\u6784\u5efa\u96c6\u6210\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5177\u6709\u76f8\u4f3cSHAP\u89e3\u91ca\u7684\u68c0\u6d4b\u5668\u4f1a\u4ea7\u751f\u76f8\u5173\u7684\u5f02\u5e38\u5206\u6570\u5e76\u8bc6\u522b\u91cd\u53e0\u7684\u5f02\u5e38\uff1b2\uff09\u89e3\u91ca\u5dee\u5f02\u53ef\u9760\u5730\u6307\u793a\u4e86\u4e92\u8865\u7684\u68c0\u6d4b\u884c\u4e3a\uff1b3\uff09\u89e3\u91ca\u9a71\u52a8\u7684\u5ea6\u91cf\u63d0\u4f9b\u4e86\u4e0e\u539f\u59cb\u8f93\u51fa\u4e0d\u540c\u7684\u6a21\u578b\u9009\u62e9\u6807\u51c6\uff1b4\uff09\u4ec5\u6709\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5355\u4e2a\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u4ecd\u662f\u6709\u6548\u96c6\u6210\u7684\u524d\u63d0\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u9488\u5bf9\u89e3\u91ca\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\uff0c\u80fd\u591f\u6784\u5efa\u66f4\u52a0\u591a\u6837\u3001\u4e92\u8865\u4e14\u6700\u7ec8\u66f4\u6709\u6548\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u96c6\u6210\u7cfb\u7edf\u3002\u89e3\u91ca\u9a71\u52a8\u7684\u65b9\u6cd5\u4e3a\u5f02\u5e38\u68c0\u6d4b\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.00769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00769", "abs": "https://arxiv.org/abs/2602.00769", "authors": ["Siyu Yan", "Lusha Zhu", "Jian-Qiao Zhu"], "title": "Eliciting Trustworthiness Priors of Large Language Models via Economic Games", "comment": null, "summary": "One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0c\u53d1\u73b0GPT-4.1\u7684\u4fe1\u4efb\u5148\u9a8c\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u5e76\u80fd\u6839\u636e\u73a9\u5bb6\u7279\u5f81\u8c03\u6574\u4fe1\u4efb\u884c\u4e3a\u3002", "motivation": "\u6784\u5efa\u53ef\u4fe1\u8d56AI\u7cfb\u7edf\u9700\u8981\u4fdd\u6301\u6821\u51c6\u7684\u4fe1\u4efb\uff0c\u4f46\u5982\u4f55\u8868\u5f81AI\u7cfb\u7edf\u81ea\u8eab\u7684\u4fe1\u4efb\u6c34\u5e73\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u65b9\u6cd5\u4eceLLMs\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0c\u5e76\u7406\u89e3\u5176\u4fe1\u4efb\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b0\u9896\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u884c\u4e3a\u535a\u5f08\u8bba\u4e2d\u7684\u4fe1\u4efb\u6e38\u620f\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u4ece\u591a\u4e2a\u9886\u5148LLMs\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790GPT-4.1\u5bf9\u4e0d\u540c\u73a9\u5bb6\u89d2\u8272\u7684\u54cd\u5e94\u3002", "result": "GPT-4.1\u7684\u4fe1\u4efb\u5148\u9a8c\u4e0e\u4eba\u7c7b\u89c2\u5bdf\u5230\u7684\u5148\u9a8c\u9ad8\u5ea6\u4e00\u81f4\u3002GPT-4.1\u80fd\u6839\u636e\u73a9\u5bb6\u7279\u5f81\u533a\u5206\u4fe1\u4efb\u6c34\u5e73\uff0c\u4e14\u63d0\u53d6\u7684\u4fe1\u4efb\u53d8\u5316\u53ef\u901a\u8fc7\u57fa\u4e8e\u611f\u77e5\u6e29\u6696\u548c\u80fd\u529b\u7684\u523b\u677f\u5370\u8c61\u6a21\u578b\u826f\u597d\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4eceLLMs\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0cGPT-4.1\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4fe1\u4efb\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3AI\u7cfb\u7edf\u4fe1\u4efb\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2602.00267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00267", "abs": "https://arxiv.org/abs/2602.00267", "authors": ["Gemma Canet Tarr\u00e9s", "Manel Baradad", "Francesc Moreno-Noguer", "Yumeng Li"], "title": "PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories", "comment": null, "summary": "Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.", "AI": {"tldr": "PLACID\u662f\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u591a\u5bf9\u8c61\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u65f6\u5e8f\u5148\u9a8c\u4fdd\u6301\u5bf9\u8c61\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5b9e\u73b0\u968f\u673a\u5e03\u5c40\u5bf9\u8c61\u5411\u76ee\u6807\u4f4d\u7f6e\u6536\u655b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u5bf9\u8c61\u5408\u6210\u56fe\u50cf\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u5728\u7167\u7247\u7ea7\u56fe\u50cf\u5408\u6210\u65b9\u9762\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u5728\u5de5\u4f5c\u5ba4\u7ea7\u522b\u7684\u591a\u5bf9\u8c61\u5408\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\uff1a\u65e0\u6cd5\u5b8c\u7f8e\u4fdd\u6301\u5bf9\u8c61\u8eab\u4efd\u3001\u80cc\u666f\u548c\u989c\u8272\u4fdd\u771f\u5ea6\u5dee\u3001\u5e03\u5c40\u63a7\u5236\u4e0d\u7cbe\u786e\u3001\u5bf9\u8c61\u663e\u793a\u4e0d\u5b8c\u6574\u3002\u73b0\u6709\u6a21\u578b\u5e38\u6539\u53d8\u5bf9\u8c61\u7ec6\u8282\u3001\u9057\u6f0f\u6216\u91cd\u590d\u5bf9\u8c61\u3001\u4ea7\u751f\u5c3a\u5bf8\u9519\u8bef\u6216\u4e0d\u4e00\u81f4\u7684\u5e03\u5c40\u3002", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u63a7\u5236\u4fdd\u6301\u5bf9\u8c61\u4e00\u81f4\u6027\u3001\u8eab\u4efd\u548c\u80cc\u666f\u7ec6\u8282\uff0c\u5229\u7528\u89c6\u9891\u7684\u65f6\u5e8f\u5148\u9a8c\u30022. \u63d0\u51fa\u65b0\u9896\u7684\u6570\u636e\u7b56\u5c55\u7b56\u7565\uff0c\u751f\u6210\u5408\u6210\u5e8f\u5217\uff1a\u968f\u673a\u653e\u7f6e\u7684\u5bf9\u8c61\u5e73\u6ed1\u79fb\u52a8\u5230\u76ee\u6807\u4f4d\u7f6e\uff0c\u8fd9\u4e9b\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u65f6\u4e0e\u89c6\u9891\u6a21\u578b\u7684\u65f6\u5e8f\u5148\u9a8c\u5bf9\u9f50\u3002\u63a8\u7406\u65f6\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u5bf9\u8c61\u5728\u6587\u672c\u5f15\u5bfc\u4e0b\u6536\u655b\u5230\u8fde\u8d2f\u5e03\u5c40\uff0c\u6700\u7ec8\u5e27\u4f5c\u4e3a\u5408\u6210\u56fe\u50cf\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u91cf\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cPLACID\u5728\u591a\u5bf9\u8c61\u5408\u6210\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u8eab\u4efd\u3001\u80cc\u666f\u548c\u989c\u8272\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5bf9\u8c61\u9057\u6f0f\u66f4\u5c11\uff0c\u89c6\u89c9\u6548\u679c\u66f4\u5438\u5f15\u4eba\u3002", "conclusion": "PLACID\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u5bf9\u8c61\u5408\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6a21\u578b\u7684\u65f6\u5e8f\u5148\u9a8c\u548c\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u7684\u591a\u5bf9\u8c61\u5408\u6210\uff0c\u4e3a\u5de5\u4f5c\u5ba4\u7ea7\u522b\u7684\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00780", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00780", "abs": "https://arxiv.org/abs/2602.00780", "authors": ["Yuting Huang", "Leilei Ding", "Zhipeng Tang", "Zenghuan Zhu", "Jiajun Deng", "Xinrui Lin", "Shuo Liu", "Haojie Ren", "Jianmin Ji", "Yanyong Zhang"], "title": "Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models", "comment": "12 pages, 7 figures", "summary": "While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.", "AI": {"tldr": "EcoVLA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u81ea\u9002\u5e94\u526a\u679d\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u73af\u5883\u611f\u77e5\u81ea\u9002\u5e94\u526a\u679d\u548c\u4ea4\u9519\u63a8\u7406\u7f16\u6392\uff0c\u5b9e\u73b0\u5b9e\u65f6\u64cd\u4f5c\u4e14\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002", "motivation": "VLA\u6a21\u578b\u53c2\u6570\u5e9e\u5927\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u963b\u788d\u5b9e\u65f6\u64cd\u4f5c\u3002\u9759\u6001\u526a\u679d\u65e0\u6cd5\u9002\u5e94\u73af\u5883\u52a8\u6001\u53d8\u5316\uff0c\u800c\u56fa\u5b9a\u95f4\u9694\u7684\u52a8\u6001\u5c42\u526a\u679d\u7c92\u5ea6\u7c97\u4e14\u91cd\u8bad\u7ec3\u5f00\u9500\u5927\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u81ea\u9002\u5e94\u526a\u679d\u65b9\u6848\u3002", "method": "EcoVLA\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u73af\u5883\u611f\u77e5\u81ea\u9002\u5e94\u526a\u679d(EAP)\uff0c\u5229\u7528\u7269\u7406\u73af\u5883\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u66f4\u65b0\u7a00\u758f\u6a21\u5f0f\uff1b2) \u4ea4\u9519\u63a8\u7406\u7f16\u6392(I\u00b2O)\uff0c\u5229\u7528VLA\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u7a7a\u95f2\u5e76\u884c\u8c03\u5ea6\u526a\u679d\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u79cdVLA\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEcoVLA\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u5355\u72ec\u4f7f\u7528\u5b9e\u73b01.60\u500d\u52a0\u901f\u4e14\u6210\u529f\u7387\u4ec5\u4e0b\u964d0.4%\uff1b\u4e0e\u4ee4\u724c\u526a\u679d\u7ed3\u5408\u5b9e\u73b02.18\u500d\u52a0\u901f\u4e14\u6027\u80fd\u4ec5\u4e0b\u964d0.5%\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "EcoVLA\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u81ea\u9002\u5e94\u526a\u679d\u6846\u67b6\uff0c\u80fd\u6709\u6548\u52a0\u901fVLA\u6a21\u578b\u63a8\u7406\uff0c\u9002\u5e94\u73af\u5883\u52a8\u6001\u53d8\u5316\uff0c\u4e14\u80fd\u4e0e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u6b63\u4ea4\u7ec4\u5408\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u5b9e\u65f6\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2602.00217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00217", "abs": "https://arxiv.org/abs/2602.00217", "authors": ["Chen Liu", "Xingzhi Sun", "Xi Xiao", "Alexandre Van Tassel", "Ke Xu", "Kristof Reimann", "Danqi Liao", "Mark Gerstein", "Tianyang Wang", "Xiao Wang", "Smita Krishnaswamy"], "title": "Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models", "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\\texttt{GPT2}$ and $\\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\\texttt{GPT2-xl}$ and $\\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u5d4c\u5165\u51dd\u7ed3\"\u73b0\u8c61\uff0c\u5c0f\u6a21\u578btoken\u5d4c\u5165\u4f1a\u574d\u7f29\u5230\u72ed\u7a84\u7684\u9525\u5f62\u5b50\u7a7a\u95f4\uff0c\u800c\u5927\u6a21\u578b\u5bf9\u6b64\u73b0\u8c61\u66f4\u5177\u62b5\u6297\u529b\u3002\u4f5c\u8005\u63d0\u51fa\u5206\u6563\u635f\u5931\u51fd\u6570\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u5373\u53ef\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u53c2\u6570\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u4e86\u7406\u89e3\u6a21\u578b\u7f29\u653e\u673a\u5236\uff0c\u7814\u7a76\u5927\u6a21\u578b\u4e0e\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u5f81\u5dee\u5f02\uff0c\u76ee\u6807\u662f\u8ba9\u5c0f\u6a21\u578b\u590d\u5236\u5927\u6a21\u578b\u7684\u8868\u5f81\u8d28\u91cf\u3002", "method": "1. \u89c2\u5bdf\u5e76\u5b9a\u4e49\"\u5d4c\u5165\u51dd\u7ed3\"\u73b0\u8c61\uff1atoken\u5d4c\u5165\u574d\u7f29\u5230\u72ed\u7a84\u9525\u5f62\u5b50\u7a7a\u95f4\uff1b2. \u5728\u591a\u4e2aTransformer\u5bb6\u65cf\u4e2d\u7cfb\u7edf\u5206\u6790\u6b64\u73b0\u8c61\uff1b3. \u63d0\u51fa\u5206\u6563\u635f\u5931\u51fd\u6570\uff0c\u5728\u8bad\u7ec3\u4e2d\u663e\u5f0f\u9f13\u52b1\u5d4c\u5165\u5206\u6563\uff1b4. \u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "1. \u5c0f\u6a21\u578b\uff08\u5982GPT2\u3001Qwen3-0.6B\uff09\u8868\u73b0\u51fa\u4e25\u91cd\u5d4c\u5165\u51dd\u7ed3\uff0c\u5927\u6a21\u578b\uff08\u5982GPT2-xl\u3001Qwen3-32B\uff09\u5bf9\u6b64\u66f4\u5177\u62b5\u6297\u529b\uff1b2. \u77e5\u8bc6\u84b8\u998f\u65e0\u6cd5\u53ef\u9760\u7f13\u89e3\u6b64\u73b0\u8c61\uff1b3. \u5206\u6563\u635f\u5931\u80fd\u6709\u6548\u7f13\u89e3\u51dd\u7ed3\uff0c\u6062\u590d\u5927\u6a21\u578b\u7684\u5206\u6563\u6a21\u5f0f\uff0c\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5d4c\u5165\u51dd\u7ed3\u662f\u5c0f\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u4e4b\u95f4\u7684\u91cd\u8981\u8868\u5f81\u5dee\u5f02\uff0c\u901a\u8fc7\u5206\u6563\u635f\u5931\u53ef\u4ee5\u7f13\u89e3\u6b64\u73b0\u8c61\uff0c\u4e3a\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u5c0fTransformer\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}
{"id": "2602.00770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00770", "abs": "https://arxiv.org/abs/2602.00770", "authors": ["Siyuan Zhang", "Jialian Li", "Yichi Zhang", "Xiao Yang", "Yinpeng Dong", "Hang Su"], "title": "Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models", "comment": "30 pages, 27 figures, 8 tables", "summary": "Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8868\u5f81\u89c6\u89d2\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u6f14\u5316\uff0c\u53d1\u73b0\u540e\u8bad\u7ec3\u4ec5\u80fd\u6709\u9650\u63d0\u5347\u521d\u59cb\u8868\u5f81\u8d28\u91cf\uff0c\u4f46\u80fd\u63a8\u52a8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8868\u5f81\u7684\u5206\u5e03\u8fc1\u79fb\uff0c\u4ece\u800c\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u663e\u5f0f\u751f\u6210\u7ed3\u679c\u5206\u6790\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6f14\u5316\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u76d2\uff0c\u65e0\u6cd5\u63ed\u793a\u5185\u90e8\u53d8\u5316\u3002\u4e3a\u4e86\u89e3\u6784\u8fd9\u79cd\u4e0d\u900f\u660e\u6027\uff0c\u9700\u8981\u4ece\u8868\u5f81\u89c6\u89d2\u7814\u7a76\u6a21\u578b\u5185\u90e8\u72b6\u6001\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u91c7\u7528\u8868\u5f81\u89c6\u89d2\u5206\u6790\u6a21\u578b\u5185\u90e8\u72b6\u6001\u52a8\u6001\uff0c\u901a\u8fc7\u8de8\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u6a21\u578b\u7684\u7efc\u5408\u5b9e\u9a8c\uff0c\u5305\u62ec\u7edf\u8ba1\u5206\u6790\u548c\u53cd\u4e8b\u5b9e\u5b9e\u9a8c\uff0c\u7814\u7a76\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8868\u5f81\u7684\u5206\u5e03\u8fc1\u79fb\u53ca\u5176\u4e0e\u5916\u90e8\u8f93\u51fa\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u540e\u8bad\u7ec3\u5bf9\u9759\u6001\u521d\u59cb\u8868\u5f81\u8d28\u91cf\u63d0\u5347\u6709\u9650\uff1b\u63a8\u7406\u4efb\u52a1\u6d89\u53ca\u751f\u6210\u8fc7\u7a0b\u4e2d\u8868\u5f81\u7684\u663e\u8457\u8fde\u7eed\u5206\u5e03\u8fc1\u79fb\uff1b\u540e\u8bad\u7ec3\u4f7f\u6a21\u578b\u80fd\u591f\u9a71\u52a8\u8fd9\u79cd\u8fc1\u79fb\u671d\u5411\u66f4\u4f18\u7684\u4efb\u52a1\u89e3\u51b3\u5206\u5e03\uff1b\u751f\u6210\u6b63\u786e\u6027\u4e0e\u6700\u7ec8\u8868\u5f81\u9ad8\u5ea6\u76f8\u5173\uff1b\u751f\u6210\u6807\u8bb0\u7684\u8bed\u4e49\u662f\u8fc1\u79fb\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u63a8\u7406\u8fc7\u7a0b\u548c\u8bad\u7ec3\u5bf9\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u5f71\u54cd\u7684\u65b0\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5206\u6790\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d1e\u89c1\uff0c\u5f3a\u8c03\u8868\u5f81\u52a8\u6001\u5728\u7406\u89e3\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6f14\u5316\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.00268", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00268", "abs": "https://arxiv.org/abs/2602.00268", "authors": ["Ariel Shaulov", "Eitan Shaar", "Amit Edenzon", "Lior Wolf"], "title": "TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation", "comment": null, "summary": "Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728\u6807\u8bb0\u6765\u7f13\u89e3\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5b58\u5728\u4e25\u91cd\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u9519\u8bef\u4f1a\u968f\u7740\u65f6\u95f4\u7d2f\u79ef\u548c\u653e\u5927\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u4e0d\u662f\u6a21\u578b\u5bb9\u91cf\u4e0d\u8db3\uff0c\u800c\u662f\u63a8\u7406\u65f6\u7684\u9519\u8bef\u4f20\u64ad\u5bfc\u81f4\u7684\uff0c\u7279\u522b\u662f\u7531\u4e8e\u5728\u81ea\u56de\u5f52\u63a8\u7406\u4e2d\u91cd\u590d\u4f7f\u7528\u4e86\u5df2\u635f\u574f\u7684\u6f5c\u5728\u6761\u4ef6\u6807\u8bb0\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u6807\u8bb0\u88ab\u91cd\u590d\u7528\u4e8e\u6761\u4ef6\u5316\u4e4b\u524d\uff0c\u8bc6\u522b\u5e76\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728\u6807\u8bb0\u3002\u5c06\u4e0d\u7a33\u5b9a\u7684\u6807\u8bb0\u5b9a\u4e49\u4e3a\u90a3\u4e9b\u8868\u793a\u4e0e\u5148\u524d\u751f\u6210\u6279\u6b21\u663e\u8457\u504f\u79bb\u7684\u6f5c\u5728\u6807\u8bb0\uff0c\u8868\u660e\u53ef\u80fd\u5b58\u5728\u635f\u574f\u6216\u8bed\u4e49\u6f02\u79fb\u3002\u901a\u8fc7\u4ece\u81ea\u56de\u5f52\u4e0a\u4e0b\u6587\u4e2d\u663e\u5f0f\u79fb\u9664\u635f\u574f\u7684\u6f5c\u5728\u6807\u8bb0\uff0c\u800c\u4e0d\u662f\u4fee\u6539\u6574\u4e2a\u7a7a\u95f4\u533a\u57df\u6216\u6a21\u578b\u53c2\u6570\uff0c\u9632\u6b62\u4e0d\u53ef\u9760\u7684\u6f5c\u5728\u4fe1\u606f\u5f71\u54cd\u672a\u6765\u7684\u751f\u6210\u6b65\u9aa4\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u957f\u65f6\u57df\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8fc7\u7a0b\u6216\u79bb\u5f00\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u65f6\u8bc6\u522b\u548c\u79fb\u9664\u4e0d\u7a33\u5b9a\u7684\u6f5c\u5728\u6807\u8bb0\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u957f\u89c6\u9891\u751f\u6210\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2602.00785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00785", "abs": "https://arxiv.org/abs/2602.00785", "authors": ["Sherry Yang"], "title": "World Models as an Intermediary between Agents and the Real World", "comment": null, "summary": "Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3b\u5f20\u5728\u590d\u6742\u9ad8\u6210\u672c\u9886\u57df\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u4e2d\u4ecb\uff0c\u4ee5\u89e3\u51b3\u9ad8\u6210\u672c\u4ea4\u4e92\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e16\u754c\u6a21\u578b\u5982\u4f55\u514b\u670d\u6781\u7aef\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u957f\u65f6\u57df\u4efb\u52a1\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684LLM\u4ee3\u7406\u5728\u4f4e\u6210\u672c\u73af\u5883\uff08\u5982\u6e38\u620f\u3001\u6570\u5b66\u3001\u7f16\u7a0b\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7269\u7406\u673a\u5668\u4eba\u8fd0\u884c\u3001ML\u5de5\u7a0b\u65f6\u95f4\u6210\u672c\u3001\u79d1\u5b66\u5b9e\u9a8c\u8d44\u6e90\u7b49\u9ad8\u6210\u672c\u590d\u6742\u9886\u57df\u4e2d\u672a\u80fd\u53d6\u5f97\u7c7b\u4f3c\u6210\u529f\u3002\u771f\u6b63\u7684\u74f6\u9888\u5728\u4e8e\u6267\u884c\u52a8\u4f5c\u83b7\u53d6\u5956\u52b1\u4fe1\u53f7\u7684\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u4e2d\u4ecb\uff0c\u5c06\u4e16\u754c\u6a21\u578b\u89c6\u4e3a\u52a8\u6001\u3001\u5956\u52b1\u548c\u4efb\u52a1\u5206\u5e03\u7684\u6a21\u578b\u3002\u8ba8\u8bba\u4e86\u4e16\u754c\u6a21\u578b\u5982\u4f55\u514b\u670d\u9ad8\u6210\u672c\u52a8\u4f5c\u7684\u57fa\u672c\u969c\u788d\uff0c\u5305\u62ec\u6781\u7aef\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u957f\u65f6\u57df\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "result": "\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u5982\u4f55\u4e3a\u4ee3\u7406\u63d0\u4f9b\u5173\u952e\u4e14\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u8986\u76d6\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u673a\u5668\u4eba\u548cAI\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u6784\u5efa\u8fd9\u4e9b\u4e16\u754c\u6a21\u578b\u7684\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u53ef\u64cd\u4f5c\u5efa\u8bae\uff0c\u5305\u62ec\u6570\u636e\u96c6\u7ba1\u7406\u3001\u67b6\u6784\u8bbe\u8ba1\u3001\u6269\u5c55\u548c\u8bc4\u4f30\u7b49\u65b9\u9762\uff0c\u4ee5\u63a8\u52a8\u9ad8\u6210\u672c\u590d\u6742\u9886\u57df\u4e2d\u4ee3\u7406\u6027\u80fd\u7684\u4e0b\u4e00\u9636\u6bb5\u53d1\u5c55\u3002"}}
{"id": "2602.00218", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00218", "abs": "https://arxiv.org/abs/2602.00218", "authors": ["Bob Junyi Zou", "Lu Tian"], "title": "GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection", "comment": null, "summary": "Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.", "AI": {"tldr": "GRIP2\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u7ef4\u6b63\u5219\u5316\u8868\u9762\u79ef\u5206\u548c\u5757\u968f\u673a\u91c7\u6837\uff0c\u5728\u975e\u7ebf\u6027\u3001\u9ad8\u76f8\u5173\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u5b9e\u73b0\u5047\u53d1\u73b0\u7387\u63a7\u5236\u548c\u9ad8\u7edf\u8ba1\u529f\u6548\u3002", "motivation": "\u5728\u975e\u7ebf\u6027\u3001\u9ad8\u5ea6\u76f8\u5173\u3001\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u9762\u4e34\u5047\u53d1\u73b0\u7387\u63a7\u5236\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u9ad8\u7edf\u8ba1\u529f\u6548\u53c8\u80fd\u4e25\u683c\u63a7\u5236\u5047\u53d1\u73b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGRIP2\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u4e8c\u7ef4\u6b63\u5219\u5316\u8868\u9762\u63a7\u5236\u7a00\u758f\u5f3a\u5ea6\u548c\u7a00\u758f\u5316\u51e0\u4f55\uff1b2\uff09\u901a\u8fc7\u5757\u968f\u673a\u91c7\u6837\u5728\u5355\u6b21\u8bad\u7ec3\u4e2d\u8fd1\u4f3c\u8868\u9762\u79ef\u5206\uff1b3\uff09\u6784\u5efa\u53cd\u5bf9\u79f0\u7edf\u8ba1\u91cf\u786e\u4fdd\u6709\u9650\u6837\u672cFDR\u63a7\u5236\u3002", "result": "\u5728\u5408\u6210\u548c\u534a\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0cGRIP2\u5728\u9ad8\u76f8\u5173\u6027\u548c\u4f4e\u4fe1\u566a\u6bd4\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u66f4\u9ad8\u7684\u7edf\u8ba1\u529f\u6548\u548c\u7a33\u5b9a\u6027\u3002\u5728HIV\u8010\u836f\u6027\u6570\u636e\u4e2d\uff0c\u6bd4\u73b0\u6709\u7ebf\u6027\u57fa\u7ebf\u65b9\u6cd5\u66f4\u597d\u5730\u8bc6\u522b\u51fa\u5df2\u77e5\u8010\u836f\u76f8\u5173\u7a81\u53d8\u3002", "conclusion": "GRIP2\u662f\u4e00\u79cd\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u573a\u666f\u4e0b\u540c\u65f6\u5b9e\u73b0\u5047\u53d1\u73b0\u7387\u63a7\u5236\u548c\u7edf\u8ba1\u529f\u6548\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.00777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00777", "abs": "https://arxiv.org/abs/2602.00777", "authors": ["Xuan Ai", "Qingqing Yang", "Peng Wang", "Lei Deng", "Lin Zhang", "Renhai Chen", "Gong Zhang"], "title": "HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference", "comment": null, "summary": "Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\\bf HyLRA} ({\\bf Hy}brid {\\bf L}ayer {\\bf R}euse {\\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \\textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \\textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\\%--46\\% while maintaining comparable performance (with $<1\\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \\href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\\texttt{/r/unified-cache-management-CF80/}}", "AI": {"tldr": "HyLRA\u662f\u4e00\u79cd\u57fa\u4e8e\u5c42\u95f4\u7a00\u758f\u6027\u5206\u6790\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u786e\u5b9a\u6700\u4f18\u5c42\u95f4\u7b56\u7565\uff0c\u5728\u654f\u611f\u5c42\u4fdd\u7559\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u5728\u5bb9\u5fcd\u5c42\u91cd\u7528\u524d\u5c42\u5173\u952etoken\u7d22\u5f15\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5728LLMs\u4e2d\u9762\u4e34\u6ce8\u610f\u529b\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548cKV\u7f13\u5b58\u5185\u5b58\u5360\u7528\u5927\u7684\u74f6\u9888\u3002\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\u6216\u6fc0\u8fdb\u526a\u679d\uff0c\u65e0\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u95f4\u53d6\u5f97\u6700\u4f18\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u5c42\u95f4\u7a00\u758f\u6027\u5206\u6790\u53d1\u73b0\u6ce8\u610f\u529b\u673a\u5236\u7684\u53cc\u91cd\u7279\u6027\uff1a\u5c42\u5185\u654f\u611f\u6027\u548c\u5c42\u95f4\u76f8\u4f3c\u6027\u3002\u91c7\u7528\u79bb\u7ebf\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u5236\u5b9a\u6700\u4f18\u5c42\u95f4\u7b56\u7565\uff0c\u654f\u611f\u5c42\u4fdd\u7559\u5b8c\u6574\u6ce8\u610f\u529b\uff0c\u5bb9\u5fcd\u5c42\u91cd\u7528\u524d\u5c42top-k\u5173\u952etoken\u7d22\u5f15\uff0c\u907f\u514d\u4e8c\u6b21\u8ba1\u7b97\u3002", "result": "HyLRA\u5c06\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53476%-46%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\uff08\u51c6\u786e\u7387\u4e0b\u964d<1%\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "HyLRA\u901a\u8fc7\u5c42\u95f4\u6df7\u5408\u6ce8\u610f\u529b\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u4e8c\u6b21\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3aLLMs\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00288", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00288", "abs": "https://arxiv.org/abs/2602.00288", "authors": ["Baiqi Li", "Kangyi Zhao", "Ce Zhang", "Chancharik Mitra", "Jean de Dieu Nyandwi", "Gedas Bertasius"], "title": "TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs", "comment": "For code and data, see https://baiqi-li.github.io/timeblind_project/", "summary": "Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .", "AI": {"tldr": "TimeBlind\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7406\u89e3\u4e0a\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u6700\u5c0f\u5bf9\u8303\u5f0f\u63ed\u793a\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u65f6\u95f4\u903b\u8f91\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u8bed\u4e49\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u65f6\u95f4\u52a8\u6001\u7684\u7406\u89e3\u4ecd\u7136\u8584\u5f31\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u4e09\u7ea7\u5206\u7c7b\uff1a\u8bc6\u522b\u539f\u5b50\u4e8b\u4ef6\u3001\u8868\u5f81\u4e8b\u4ef6\u5c5e\u6027\u3001\u63a8\u7406\u4e8b\u4ef6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u4f7f\u7528\u6700\u5c0f\u5bf9\u8303\u5f0f\uff1a\u89c6\u9891\u5bf9\u5177\u6709\u76f8\u540c\u7684\u9759\u6001\u89c6\u89c9\u5185\u5bb9\u4f46\u4e0d\u540c\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u901a\u8fc7\u4e92\u8865\u95ee\u9898\u6d88\u9664\u8bed\u8a00\u5148\u9a8c\u3002", "result": "\u8bc4\u4f3020\u591a\u4e2a\u6700\u5148\u8fdb\u7684MLLM\uff08\u5305\u62ecGPT-5\u3001Gemini 3 Pro\uff09\u5728600\u4e2a\u5b9e\u4f8b\uff082400\u4e2a\u89c6\u9891-\u95ee\u9898\u5bf9\uff09\u4e0a\uff0c\u6700\u4f73\u6a21\u578b\u7684\u5b9e\u4f8b\u51c6\u786e\u7387\u4ec5\u4e3a48.2%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0898.2%\uff09\u3002", "conclusion": "\u524d\u6cbf\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u6377\u5f84\u800c\u975e\u771f\u6b63\u7684\u65f6\u95f4\u903b\u8f91\uff0cTimeBlind\u6210\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u9891\u7406\u89e3\u7684\u91cd\u8981\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u5728\u65f6\u7a7a\u63a8\u7406\u4e0a\u7684\u6839\u672c\u7f3a\u9677\u3002"}}
{"id": "2602.00811", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00811", "abs": "https://arxiv.org/abs/2602.00811", "authors": ["Ronghao Lin", "Honghao Lu", "Ruixing Wu", "Aolin Xiong", "Qinggong Chu", "Qiaolin He", "Sijie Mai", "Haifeng Hu"], "title": "MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing", "comment": null, "summary": "As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMissMAC-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u548c\u8de8\u6a21\u6001\u534f\u540c\u89c6\u89d2\u6765\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5f80\u5f80\u4e0d\u5b8c\u6574\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u73b0\u6709\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u6027\u80fd\u6ce2\u52a8\u5927\uff0c\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u6210\u4e3a\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\uff0c\u9700\u8981\u7cfb\u7edf\u91cf\u5316\u8bc4\u4f30\u3002", "method": "\u63d0\u51faMissMAC-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6307\u5bfc\u539f\u5219\uff1a\u8bad\u7ec3\u65f6\u4e0d\u4f7f\u7528\u7f3a\u5931\u5148\u9a8c\uff0c\u5355\u4e00\u6a21\u578b\u80fd\u5904\u7406\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u6a21\u6001\u573a\u666f\u3002\u6574\u5408\u6570\u636e\u96c6\u548c\u5b9e\u4f8b\u7ea7\u522b\u7684\u56fa\u5b9a\u4e0e\u968f\u673a\u7f3a\u5931\u6a21\u5f0f\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf93\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540cMAC\u65b9\u6cd5\u5904\u7406\u6a21\u6001\u7f3a\u5931\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u57fa\u51c6\u4e3a\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "MissMAC-Bench\u57fa\u51c6\u5efa\u7acb\u4e86\u516c\u5e73\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4fc3\u8fdb\u4e86\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u5f25\u5408\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.00240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00240", "abs": "https://arxiv.org/abs/2602.00240", "authors": ["Md Muhtasim Munif Fahim", "Soyda Humyra Yesmin", "Saiful Islam", "Md. Palash Bin Faruque", "Md. A. Salam", "Md. Mahfuz Uddin", "Samiul Islam", "Tofayel Ahmed", "Md. Binyamin", "Md. Rezaul Karim"], "title": "Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting", "comment": null, "summary": "We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.", "AI": {"tldr": "Green-NAS\u662f\u4e00\u4e2a\u9762\u5411\u4f4e\u8d44\u6e90\u73af\u5883\u7684\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u4ee5\u5929\u6c14\u9884\u62a5\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u6a21\u578b\u51c6\u786e\u6027\u548c\u6548\u7387\u6765\u5bfb\u627e\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u80fd\u8017\u548c\u78b3\u6392\u653e\u3002", "motivation": "\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5f00\u53d1\u53ef\u6301\u7eed\u7684AI\u6a21\u578b\uff0c\u9075\u5faa\"\u7eff\u8272AI\"\u539f\u5219\uff0c\u6700\u5c0f\u5316\u8ba1\u7b97\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\uff0c\u4f18\u5148\u8003\u8651\u53ef\u6301\u7eed\u90e8\u7f72\u800c\u975e\u539f\u59cb\u8ba1\u7b97\u89c4\u6a21\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u6a21\u578b\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5bfb\u627e\u53c2\u6570\u5c11\u4f46\u7cbe\u5ea6\u9ad8\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff1b\u5e76\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u63d0\u9ad8\u6570\u636e\u6709\u9650\u57ce\u5e02\u7684\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\u3002", "result": "\u6700\u4f73\u6a21\u578bGreen-NAS-A\u4ec5\u4f7f\u7528153k\u53c2\u6570\uff08\u6bd4GraphCast\u7b49\u5168\u7403\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5c11239\u500d\uff09\uff0cRMSE\u8fbe\u52300.0988\uff08\u6bd4\u624b\u52a8\u8c03\u4f18\u57fa\u7ebf\u4ec5\u5dee1.4%\uff09\uff1b\u8fc1\u79fb\u5b66\u4e60\u53ef\u5c06\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\u63d0\u9ad8\u7ea65.2%\u3002", "conclusion": "Green-NAS\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5f00\u53d1\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684AI\u6a21\u578b\uff0c\u4e3a\u7eff\u8272AI\u5728\u5929\u6c14\u9884\u62a5\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u591a\u76ee\u6807\u4f18\u5316\u548c\u8fc1\u79fb\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00846", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00846", "abs": "https://arxiv.org/abs/2602.00846", "authors": ["Zicheng Kong", "Dehua Ma", "Zhenbo Xu", "Alven Yang", "Yiwei Ru", "Haoran Wang", "Zixuan Zhou", "Fuqing Bie", "Liuyu Xiang", "Huijia Wu", "Jian Zhao", "Zhaofeng He"], "title": "Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \\textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \\textbf{text, image, video, and audio}. At the core of our approach is \\textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \\emph{reconcile and filter} preferences while providing a modality-aware \\emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\\% on ShareGPT-V) and audio (66.8\\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.", "AI": {"tldr": "Omni-RRM\uff1a\u9996\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u7ef4\u5ea6\u504f\u597d\u5224\u65ad\u548c\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\uff0c\u5728\u89c6\u9891\u3001\u97f3\u9891\u3001\u56fe\u50cf\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u7c97\u7cd9\u7684\u5bf9\u9f50\u6280\u672f\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5956\u52b1\u6a21\u578b\u3002\u73b0\u6709\u5956\u52b1\u6a21\u578b\u4e3b\u8981\u662f\u89c6\u89c9\u4e2d\u5fc3\u7684\uff0c\u8fd4\u56de\u4e0d\u900f\u660e\u7684\u6807\u91cf\u5206\u6570\uff0c\u4e14\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "method": "\u63d0\u51faOmni-RRM\uff0c\u9996\u4e2a\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u80fd\u4ea7\u751f\u7ed3\u6784\u5316\u7684\u591a\u7ef4\u5ea6\u504f\u597d\u5224\u65ad\u3002\u6838\u5fc3\u662fOmni-Preference\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b8c\u5168\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efa\uff1a\u5bf9\u6bd4\u4e0d\u540c\u80fd\u529b\u6a21\u578b\u5408\u6210\u5019\u9009\u54cd\u5e94\u5bf9\uff0c\u4f7f\u7528\u5f3a\u6559\u5e08\u6a21\u578b\u534f\u8c03\u548c\u8fc7\u6ee4\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u6a21\u6001\u611f\u77e5\u7684\u8bc4\u5206\u6807\u51c6\u4f9d\u636e\u3002\u8bad\u7ec3\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u8bc4\u5206\u6807\u51c6\u8f93\u51fa\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u63d0\u9ad8\u5bf9\u56f0\u96be\u4f4e\u5bf9\u6bd4\u5ea6\u5bf9\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "Omni-RRM\u5728\u89c6\u9891\uff08ShareGPT-V\u4e0a80.2%\uff09\u548c\u97f3\u9891\uff08Audio-HH-RLHF\u4e0a66.8%\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u5728\u56fe\u50cf\u4efb\u52a1\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u5956\u52b1\u6a21\u578b\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u534717.7%\u3002\u8fd8\u80fd\u901a\u8fc7Best-of-N\u9009\u62e9\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\uff0c\u5e76\u53ef\u8fc1\u79fb\u5230\u7eaf\u6587\u672c\u504f\u597d\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "Omni-RRM\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u548c\u7ed3\u6784\u5316\u591a\u7ef4\u5ea6\u5956\u52b1\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4e14\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00289", "abs": "https://arxiv.org/abs/2602.00289", "authors": ["Alan Yuille", "Daniel Kersten"], "title": "Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory", "comment": null, "summary": "This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.", "AI": {"tldr": "\u672c\u6587\u4ece\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\u4ecb\u7ecd\u8ba1\u7b97\u673a\u89c6\u89c9\u53ca\u5176\u4e0e\u8ba4\u77e5\u79d1\u5b66\u7684\u5173\u7cfb\uff0c\u6bd4\u8f83\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e24\u8005\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5e9e\u5927\u590d\u6742\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u5176\u6838\u5fc3\u6982\u5ff5\u3002\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u89c6\u89d2\uff0c\u65e2\u80fd\u6db5\u76d6\u4e0e\u8ba4\u77e5\u79d1\u5b66\u76f8\u5173\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u53c8\u80fd\u89e3\u91ca\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u53d6\u5f97\u5de8\u5927\u6210\u529f\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u4f5c\u4e3a\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002\u901a\u8fc7\u8be5\u6846\u67b6\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8ba8\u8bba\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u7684\u5c40\u9650\u6027\u3002", "result": "\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u80fd\u591f\u7edf\u4e00\u89e3\u91ca\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u6982\u5ff5\u4e0a\u4e0e\u8ba4\u77e5\u79d1\u5b66\u6709\u5171\u9e23\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u5de8\u5927\u5546\u4e1a\u6210\u529f\u3002\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u4e3a\u7406\u89e3\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5176\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u5c06\u8d1d\u53f6\u65af\u65b9\u6cd5\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7ed3\u5408\u5230\u66f4\u4e30\u5bcc\u6846\u67b6\u4e2d\u7684\u65b9\u5411\u3002"}}
{"id": "2602.00815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00815", "abs": "https://arxiv.org/abs/2602.00815", "authors": ["Yunjian Zhang", "Sudong Wang", "Yang Li", "Peiran Xu", "Conghao Zhou", "Xiaoyue Ma", "Jianing Li", "Yao Zhu"], "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement", "comment": null, "summary": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDoPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5355\u4e00\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u5927\u5e45\u964d\u4f4eRLVR\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4e0b\u80fd\u6709\u6548\u5bf9\u9f50LLM\u7684\u63a8\u7406\u94fe\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u5956\u52b1\u4fe1\u53f7\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faDynamic One-Shot Policy Refinement (DoPR)\uff1a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684RL\u7b56\u7565\uff0c\u901a\u8fc7\u5956\u52b1\u6ce2\u52a8\u6027\u548c\u63a2\u7d22\u9a71\u52a8\u7684\u91c7\u96c6\u673a\u5236\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6279\u6b21\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5355\u4e2a\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u3002", "result": "DoPR\u5c06\u8bad\u7ec3\u5f00\u9500\u964d\u4f4e\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u63a8\u7406\u51c6\u786e\u7387\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u53ea\u9700\u5c11\u91cf\u8bad\u7ec3\u5b9e\u4f8b\u5373\u53ef\u5b9e\u73b0\u5f3a\u6027\u80fd\u3002", "conclusion": "DoPR\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u63a8\u7406\u5bc6\u96c6\u578bLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u3001\u53ef\u8bbf\u95ee\u7684RL\u8bad\u7ec3\u8def\u5f84\u3002"}}
{"id": "2602.00250", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00250", "abs": "https://arxiv.org/abs/2602.00250", "authors": ["Shreshth Saini", "Avinab Saha", "Balu Adsumilli", "Neil Birkbeck", "Yilin Wang", "Alan C. Bovik"], "title": "TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models", "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \\texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.", "AI": {"tldr": "\u63d0\u51faBoE Steering\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u8fd1\u4f3c\u65e0\u9650\u89c6\u91ce\u524d\u77bb\uff0c\u89e3\u51b3MDM\u91c7\u6837\u4e2d\u7684\u8f68\u8ff9\u9501\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u975e\u81ea\u56de\u5f52\u751f\u6210", "motivation": "\u5f53\u524dMDM\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u7684\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u51b3\u7b56\u7684\u957f\u671f\u5f71\u54cd\uff0c\u5bfc\u81f4\u65e9\u671f\u5e7b\u89c9\u5f15\u53d1\u5168\u5c40\u4e0d\u4e00\u81f4\u6027\u3002\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u867d\u7136\u7f13\u89e3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff08\u6bcf\u6b65\u9700\u8981O(K)\u6b21\u524d\u5411\u4f20\u64ad\uff09", "method": "\u63d0\u51faBackward-on-Entropy (BoE) Steering\u6846\u67b6\uff0c\u901a\u8fc7Token Influence Score (TIS)\u4f5c\u4e3a\u6700\u4f18\u63a7\u5236\u4fe1\u53f7\uff0c\u8be5\u5206\u6570\u4ece\u8f68\u8ff9\u6210\u672c\u6cdb\u51fd\u7684\u4e00\u9636\u5c55\u5f00\u63a8\u5bfc\u5f97\u51fa\u3002\u5f15\u5165ActiveQueryAttention\u7a00\u758f\u4f34\u968f\u539f\u8bed\uff0c\u5229\u7528\u63a9\u7801\u76ee\u6807\u7684\u7ed3\u6784\u964d\u4f4e\u53cd\u5411\u4f20\u64ad\u590d\u6742\u5ea6", "result": "BoE\u5728\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u89e3\u63a9\u7801\u65b9\u6cd5\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u8868\u660e\u68af\u5ea6\u5f15\u5bfc\u7684\u8f6c\u5411\u4e3a\u9c81\u68d2\u7684\u975e\u81ea\u56de\u5f52\u751f\u6210\u63d0\u4f9b\u4e86\u6570\u5b66\u539f\u7406\u6e05\u6670\u4e14\u9ad8\u6548\u7684\u8def\u5f84", "conclusion": "\u68af\u5ea6\u5f15\u5bfc\u7684\u8f6c\u5411\u4e3a\u63a9\u7801\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u5b66\u539f\u7406\u6e05\u6670\u4e14\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u9501\u5b9a\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c"}}
{"id": "2602.00848", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00848", "abs": "https://arxiv.org/abs/2602.00848", "authors": ["Ziwei Gong", "Yanda Chen", "Julia Hirschberg", "Chen Zhao", "He He", "Zhou Yu", "Kathleen Mckeown"], "title": "Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation", "comment": null, "summary": "Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.", "AI": {"tldr": "\u63d0\u51faFactuality-Controlled Generation (FCG)\u6846\u67b6\uff0c\u8ba9\u7528\u6237\u80fd\u5728\u67e5\u8be2\u65f6\u6307\u5b9a\u4e8b\u5b9e\u6027\u7ea6\u675f\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u4e8b\u5b9e\u6027\u7ea6\u675f\u9075\u5faa\u548c\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u77e5\u8bc6\u65f6\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u7f6e\u4fe1\u5ea6\uff0c\u9762\u4e34\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4e0d\u540c\u5e94\u7528\u9700\u8981\u4e0d\u540c\u7684\u5e73\u8861\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u7528\u6237\u6307\u5b9a\u4e8b\u5b9e\u6027\u7ea6\u675f\u7684\u6846\u67b6", "method": "\u63d0\u51faFactuality-Controlled Generation (FCG)\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u80fd\u6839\u636e\u7528\u6237\u6307\u5b9a\u7684\u7ea6\u675f\u6761\u4ef6\u8c03\u6574\u751f\u6210\u5185\u5bb9\u7684\u5e73\u8861", "result": "\u5408\u6210\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u9075\u5faa\u4e8b\u5b9e\u6027\u8981\u6c42\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f93\u51fa\u7684\u4fe1\u606f\u4e30\u5bcc\u5ea6", "conclusion": "FCG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u63a7\u7684\u751f\u6210\u673a\u5236"}}
{"id": "2602.00292", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00292", "abs": "https://arxiv.org/abs/2602.00292", "authors": ["Rory Driscoll", "Alexandros Christoforos", "Chadbourne Davis"], "title": "LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification", "comment": null, "summary": "While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.", "AI": {"tldr": "LogicGaze\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u63a8\u7406\u4e2d\u662f\u5426\u57fa\u4e8e\u771f\u5b9e\u89c6\u89c9\u8bc1\u636e\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u9a8c\u8bc1\u3001\u53d9\u4e8b\u5408\u6210\u548c\u6270\u52a8\u62d2\u7edd\u4e09\u4e2a\u6d4b\u8bd5\u63ed\u793a\u6a21\u578b\u5728\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u4e2d\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u867d\u7136\u5e8f\u5217\u63a8\u7406\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u5c06\u63a8\u7406\u94fe\u57fa\u4e8e\u5b9e\u9645\u89c6\u89c9\u8bc1\u636e\u65b9\u9762\u7684\u53ef\u9760\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5f53\u524d\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u80fd\u5426\u9a8c\u8bc1\u5e8f\u5217\u56e0\u679c\u94fe\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002", "method": "\u4eceShareGPT4Video\u768440,000\u4e2a\u89c6\u9891\u7247\u6bb5\u548cFlickr30k\u56fe\u50cf\u5b50\u96c6\u4e2d\u6784\u5efaLogicGaze\u57fa\u51c6\u6846\u67b6\uff0c\u6574\u5408\u56e0\u679c\u5e8f\u5217\u4e0e\u89c6\u89c9\u77db\u76fe\u4f46\u8bed\u8a00\u5408\u7406\u7684\u6270\u52a8\uff0c\u8feb\u4f7f\u6a21\u578b\u9a8c\u8bc1\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u771f\u5b9e\u6027\u3002\u91c7\u7528\u4e09\u91cd\u8bc4\u4f30\u534f\u8bae\uff1a\u56e0\u679c\u9a8c\u8bc1\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u53d9\u4e8b\u5408\u6210\u548c\u6270\u52a8\u62d2\u7edd\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u5305\u62ecQwen2.5-VL-72B\u5728\u5185\u7684\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u8868\u660e\u5b83\u4eec\u5728\u5c06\u5e8f\u5217\u63a8\u7406\u94fe\u57fa\u4e8e\u5b9e\u9645\u89c6\u89c9\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002", "conclusion": "LogicGaze\u6846\u67b6\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5021\u5bfc\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u4fe1\u7684\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u3002"}}
{"id": "2602.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00845", "abs": "https://arxiv.org/abs/2602.00845", "authors": ["Senkang Hu", "Yong Dai", "Yuzhi Zhao", "Yihang Tao", "Yu Guo", "Zhengru Fang", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward", "comment": null, "summary": "Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.", "AI": {"tldr": "InfoReasoner\u662f\u4e00\u4e2a\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u6765\u4f18\u5316\u68c0\u7d22\u8fc7\u7a0b\u7684\u6846\u67b6\uff0c\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53475.4%", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u4ee3\u7406\u63a8\u7406\u83b7\u53d6\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u68c0\u7d22\u8fc7\u7a0b\u4f18\u5316\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u5bc6\u96c6\u3001\u6709\u539f\u5219\u7684\u5956\u52b1\u4fe1\u53f7", "method": "1) \u7406\u8bba\u5c42\u9762\uff1a\u5c06\u4fe1\u606f\u589e\u76ca\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6a21\u578b\u4fe1\u5ff5\u72b6\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff1b2) \u5b9e\u8df5\u5c42\u9762\uff1a\u63d0\u51fa\u8f93\u51fa\u611f\u77e5\u7684\u5185\u5728\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u53cc\u5411\u6587\u672c\u8574\u542b\u7684\u8bed\u4e49\u805a\u7c7b\u76f4\u63a5\u4ece\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u8ba1\u7b97\u4fe1\u606f\u589e\u76ca\uff1b3) \u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3", "result": "\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfoReasoner\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.4%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5177\u6709\u68c0\u7d22\u529f\u80fd\u7684\u4ee3\u7406\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u53ef\u6269\u5c55\u7684\u8def\u5f84"}}
{"id": "2602.00269", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00269", "abs": "https://arxiv.org/abs/2602.00269", "authors": ["Keisuke Kamahori", "Wei-Tzu Lee", "Atindra Jha", "Rohan Kadekodi", "Stephanie Wang", "Arvind Krishnamurthy", "Baris Kasikci"], "title": "VoxServe: Streaming-Centric Serving System for Speech Language Models", "comment": "The code is available at https://github.com/vox-serve/vox-serve", "summary": "Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.", "AI": {"tldr": "VoxServe\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u578b\u67b6\u6784\u4e0e\u7cfb\u7edf\u4f18\u5316\uff0c\u5b9e\u73b0\u6d41\u5f0f\u573a\u666f\u4e0b\u7684\u9ad8\u6027\u80fd\u63a8\u7406\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u541e\u5410\u91cf\u63d0\u534710-20\u500d\u3002", "motivation": "\u5728\u6d41\u5f0f\u573a\u666f\u4e2d\u90e8\u7f72\u73b0\u4ee3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u548c\u5f3a\u6d41\u5f0f\u4fdd\u8bc1\uff0c\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u7075\u6d3b\u9ad8\u6548\u5730\u652f\u6301\u591a\u6837\u5316\u6a21\u578b\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u6267\u884c\u62bd\u8c61\u5c42\u89e3\u8026\u6a21\u578b\u67b6\u6784\u4e0e\u7cfb\u7edf\u4f18\u5316\uff0c\u5b9e\u73b0\u6d41\u5f0f\u611f\u77e5\u8c03\u5ea6\u548c\u5f02\u6b65\u63a8\u7406\u6d41\u6c34\u7ebf\uff0c\u63d0\u5347\u7aef\u5230\u7aef\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u73b0\u4ee3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cVoxServe\u5728\u53ef\u6bd4\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6848\u9ad810-20\u500d\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6d41\u5f0f\u53ef\u884c\u6027\u3002", "conclusion": "VoxServe\u4e3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u7684\u6d41\u5f0f\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u62bd\u8c61\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u6d41\u5f0f\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00857", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00857", "abs": "https://arxiv.org/abs/2602.00857", "authors": ["Manveer Singh Tamber", "Hosna Oyarhoseini", "Jimmy Lin"], "title": "Unifying Adversarial Robustness and Training Across Text Scoring Models", "comment": null, "summary": "Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u6587\u672c\u8bc4\u5206\u6a21\u578b\uff08\u5bc6\u96c6\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u3001\u5956\u52b1\u6a21\u578b\uff09\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u7edf\u4e00\u8d77\u6765\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u6548\u679c\uff0c\u5e76\u5728RLHF\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u5728\u4e0d\u540c\u5e94\u7528\u548c\u653b\u51fb\u65b9\u6cd5\u4e4b\u95f4\u5206\u6563\uff0c\u63a9\u76d6\u4e86\u5171\u4eab\u7684\u6f0f\u6d1e\u3002\u9700\u8981\u7edf\u4e00\u7814\u7a76\u6587\u672c\u8bc4\u5206\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u5bc6\u96c6\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u548c\u5956\u52b1\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u5c06\u6587\u672c\u8bc4\u5206\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u7edf\u4e00\u5316\uff0c\u5f00\u53d1\u4e86\u591a\u79cd\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u7ec4\u5408\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u83b7\u5f97\u5f3a\u9c81\u68d2\u6027\u3002\u4f7f\u7528\u6587\u672c\u8bc4\u5206\u5931\u8d25\u7684\u53ef\u6d4b\u8bd5\u6027\u4f5c\u4e3a\u539f\u5219\u6027\u89c6\u89d2\uff1a\u5f53\u65e0\u5173\u6216\u88ab\u62d2\u7edd\u7684\u6587\u672c\u5f97\u5206\u9ad8\u4e8e\u76f8\u5173\u6216\u88ab\u9009\u62e9\u7684\u6587\u672c\u65f6\uff0c\u653b\u51fb\u6210\u529f\u3002", "result": "\u5c55\u793a\u4e86\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u77ed\u89c6\uff0c\u65e0\u6cd5\u6709\u6548\u8de8\u653b\u51fb\u6cdb\u5316\u3002\u63d0\u51fa\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u8fd8\u6539\u5584\u4e86\u4efb\u52a1\u6548\u679c\u3002\u5728RLHF\u4e2d\uff0c\u5bf9\u6297\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u51cf\u8f7b\u4e86\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u652f\u6301\u8bad\u7ec3\u66f4\u597d\u5bf9\u9f50\u7684LLM\u3002", "conclusion": "\u7edf\u4e00\u6587\u672c\u8bc4\u5206\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u662f\u6709\u6548\u7684\uff0c\u7ec4\u5408\u4e92\u8865\u7684\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u4ea7\u751f\u5f3a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4efb\u52a1\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u5728RLHF\u4e2d\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u80fd\u591f\u8bad\u7ec3\u66f4\u597d\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2602.00309", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00309", "abs": "https://arxiv.org/abs/2602.00309", "authors": ["Samuel Church", "Joshua D. Warner", "Danyal Maqbool", "Xin Tie", "Junjie Hu", "Meghan G. Lubner", "Tyler J. Bradshaw"], "title": "Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation", "comment": null, "summary": "The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.", "AI": {"tldr": "SAM2CT\uff1a\u9996\u4e2a\u53ef\u63d0\u793a\u5206\u5272\u6a21\u578b\uff0c\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u7a00\u758f\u6807\u6ce8\uff08\u7bad\u5934/\u7ebf\u6bb5\uff09\u8f6c\u6362\u4e3aCT 3D\u5206\u5272\uff0c\u5229\u7528\u5386\u53f2PACS\u6570\u636e\u6784\u5efa\u5927\u89c4\u6a21\u5206\u5272\u6570\u636e\u96c6", "motivation": "CT\u5f71\u50cf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f463D\u5206\u5272\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u4e34\u5e8aPACS\u4e2d\u5df2\u6709\u5927\u91cf\u653e\u5c04\u79d1\u533b\u751f\u7684\u7a00\u758f\u6807\u6ce8\uff08\u5982\u7bad\u5934\u3001\u7ebf\u6bb5\u6d4b\u91cf\uff09\uff0c\u8fd9\u4e9b\u8d44\u6e90\u672a\u88ab\u5145\u5206\u5229\u7528", "method": "\u57fa\u4e8eSAM2\u6784\u5efaSAM2CT\u6a21\u578b\uff1a1\uff09\u6269\u5c55\u63d0\u793a\u7f16\u7801\u5668\u652f\u6301\u7bad\u5934\u548c\u7ebf\u6bb5\u8f93\u5165\uff1b2\uff09\u5f15\u5165Memory-Conditioned Memories\uff08MCM\uff09\u5185\u5b58\u7f16\u7801\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf93D\u533b\u5b66\u5f71\u50cf\uff1b3\uff09\u5229\u7528\u5386\u53f2GSPS\u6807\u6ce8\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u516c\u5171\u75c5\u7076\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAM2CT\u4f18\u4e8e\u73b0\u6709\u53ef\u63d0\u793a\u5206\u5272\u6a21\u578b\uff1a\u7bad\u5934\u63d0\u793aDice\u7cfb\u65700.649\uff0c\u7ebf\u6bb5\u63d0\u793a0.757\u3002\u5728\u4e34\u5e8aPACS\u6570\u636e\uff08N=60\uff09\u4e0a\uff0c87%\u7684\u5206\u5272\u7ed3\u679c\u4e34\u5e8a\u53ef\u63a5\u53d7\u6216\u4ec5\u9700\u5fae\u8c03\u3002\u5728\u6025\u8bca\u79d1\u53d1\u73b0\u4e0a\u8868\u73b0\u51fa\u5f3a\u96f6\u6837\u672c\u6027\u80fd", "conclusion": "SAM2CT\u8bc1\u660e\u4e86\u5229\u7528\u5386\u53f2GSPS\u6807\u6ce8\u8fdb\u884c\u5927\u89c4\u6a213D CT\u5206\u5272\u6570\u636e\u96c6\u751f\u6210\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00851", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00851", "abs": "https://arxiv.org/abs/2602.00851", "authors": ["Hyejun Jeong", "Amir Houmansadr", "Shlomo Zilberstein", "Eugene Bagdasarian"], "title": "Persuasion Propagation in LLM Agents", "comment": "Code available at https://github.com/HyejunJeong/persuasion-propagation", "summary": "Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \\emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\\% fewer searches and visit 16.9\\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.", "AI": {"tldr": "\u7814\u7a76AI\u667a\u80fd\u4f53\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u53d7\u5230\u7528\u6237\u8bf4\u670d\u65f6\uff0c\u4fe1\u5ff5\u5e72\u9884\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a\uff0c\u53d1\u73b0\u4efb\u52a1\u6267\u884c\u524d\u7684\u4fe1\u5ff5\u9884\u7f6e\u6bd4\u5b9e\u65f6\u8bf4\u670d\u66f4\u6709\u6548", "motivation": "\u73b0\u4ee3AI\u667a\u80fd\u4f53\u7ed3\u5408\u5bf9\u8bdd\u4ea4\u4e92\u548c\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff08\u5982\u7f16\u7801\u548c\u7f51\u7edc\u7814\u7a76\uff09\uff0c\u9700\u8981\u7814\u7a76\u5f53\u667a\u80fd\u4f53\u6267\u884c\u957f\u671f\u4efb\u52a1\u65f6\u53d7\u5230\u7528\u6237\u8bf4\u670d\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u4ee5\u53ca\u4fe1\u5ff5\u5c42\u9762\u7684\u5e72\u9884\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a", "method": "\u5f15\u5165\u884c\u4e3a\u4e2d\u5fc3\u8bc4\u4f30\u6846\u67b6\uff0c\u533a\u5206\u4efb\u52a1\u6267\u884c\u671f\u95f4\u6216\u4e4b\u524d\u7684\u8bf4\u670d\u5e94\u7528\uff0c\u5728\u7f51\u7edc\u7814\u7a76\u548c\u7f16\u7801\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5b9e\u65f6\u8bf4\u670d\u4e0e\u4fe1\u5ff5\u9884\u7f6e\u7684\u6548\u679c", "result": "\u5b9e\u65f6\u8bf4\u670d\u4ea7\u751f\u5fae\u5f31\u4e14\u4e0d\u4e00\u81f4\u7684\u884c\u4e3a\u6548\u5e94\uff0c\u800c\u4efb\u52a1\u5f00\u59cb\u65f6\u660e\u786e\u6307\u5b9a\u4fe1\u5ff5\u72b6\u6001\u7684\u4fe1\u5ff5\u9884\u7f6e\u667a\u80fd\u4f53\u5e73\u5747\u51cf\u5c1126.9%\u7684\u641c\u7d22\u6b21\u6570\u548c16.9%\u7684\u72ec\u7279\u6765\u6e90\u8bbf\u95ee", "conclusion": "\u5373\u4f7f\u5728\u5148\u524d\u7684\u4ea4\u4e92\u4e2d\uff0c\u8bf4\u670d\u4e5f\u80fd\u5f71\u54cd\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u8fd9\u8981\u6c42\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8fdb\u884c\u884c\u4e3a\u5c42\u9762\u7684\u8bc4\u4f30"}}
{"id": "2602.00282", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00282", "abs": "https://arxiv.org/abs/2602.00282", "authors": ["Naman Saxena", "Vaneet Aggarwal"], "title": "Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning", "comment": null, "summary": "Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(\u03b5^{-2})$ and sample complexity of $\\tilde{O}(\u03b5^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7ea6\u675f\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u63d0\u51fa\u4e86CBSO\u7b97\u6cd5\uff0c\u83b7\u5f97\u4e86O(\u03b5^{-2})\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u548c\u00d5(\u03b5^{-4})\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u9996\u6b21\u4f7f\u7528Moreau\u5305\u7edc\u5206\u6790\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u7684\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u3002", "motivation": "\u5143\u5b66\u4e60\u3001\u5206\u5c42\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u8bb8\u591a\u91cd\u8981\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u90fd\u53ef\u4ee5\u5efa\u6a21\u4e3a\u53cc\u5c42RL\u95ee\u9898\u3002\u867d\u7136\u8fd9\u4e9b\u9886\u57df\u5728\u5b9e\u8bc1\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u53cc\u5c42RL\u7b97\u6cd5\u7684\u7406\u8bba\u5206\u6790\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u4e86\u7ea6\u675f\u53cc\u5c42\u6b21\u68af\u5ea6\u4f18\u5316\uff08CBSO\uff09\u7b97\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u60e9\u7f5a\u7684\u76ee\u6807\u51fd\u6570\u6765\u5904\u7406\u7ea6\u675f\u53cc\u5c42\u95ee\u9898\u4e2d\u7684\u539f\u59cb-\u5bf9\u5076\u95f4\u9699\u548c\u8d85\u68af\u5ea6\u95ee\u9898\u3002\u91c7\u7528Moreau\u5305\u7edc\u6765\u5206\u6790\u5177\u6709\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u7684\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u3002", "result": "\u83b7\u5f97\u4e86O(\u03b5^{-2})\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u548c\u00d5(\u03b5^{-4})\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u8fd9\u662f\u9996\u6b21\u4f7f\u7528Moreau\u5305\u7edc\u5206\u6790\u5177\u6709\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u7684\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7ea6\u675f\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u975e\u5149\u6ed1\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5143\u5b66\u4e60\u3001\u5206\u5c42\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988RL\u7b49\u9886\u57df\u7684\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2602.00881", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00881", "abs": "https://arxiv.org/abs/2602.00881", "authors": ["Shounak Paul", "Raghav Dogra", "Pawan Goyal", "Saptarshi Ghosh"], "title": "ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople", "comment": "9 Pages of Main, 1 page of Limitations and Ethics Statement, 11 Pages of Appendix, Accepted for Publication at EACL 2026 (Findings)", "summary": "Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86ILSIC\u8bed\u6599\u5e93\uff0c\u5305\u542b\u5370\u5ea6\u6cd5\u5f8b\u4e2d500+\u6cd5\u89c4\u7684\u666e\u901a\u4eba\u67e5\u8be2\u548c\u6cd5\u5ead\u5224\u51b3\uff0c\u7528\u4e8e\u6bd4\u8f83\u6cd5\u5ead\u4e0e\u666e\u901a\u4eba\u6570\u636e\u5728\u6cd5\u5f8b\u6cd5\u89c4\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u5dee\u5f02\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u79cd\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6cd5\u5f8b\u6cd5\u89c4\u8bc6\u522b\u4efb\u52a1\u4e3b\u8981\u4f7f\u7528\u6cd5\u5ead\u5224\u51b3\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u67e5\u8be2\u5f80\u5f80\u662f\u666e\u901a\u4eba\u63d0\u51fa\u7684\u975e\u6b63\u5f0f\u95ee\u9898\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u6cd5\u5ead\u6570\u636e\u548c\u666e\u901a\u4eba\u6570\u636e\u5dee\u5f02\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u521b\u5efa\u5305\u542b\u4e24\u79cd\u6570\u636e\u7c7b\u578b\u7684\u8bed\u6599\u5e93\u6765\u63a2\u7d22\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u521b\u5efaILSIC\u8bed\u6599\u5e93\uff0c\u5305\u542b500+\u5370\u5ea6\u6cd5\u89c4\u7684\u666e\u901a\u4eba\u67e5\u8be2\u548c\u6cd5\u5ead\u5224\u51b3\u3002\u8fdb\u884c\u591a\u79cd\u5b9e\u9a8c\uff1a\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63a8\u7406\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u5206\u6790\u7eaf\u6cd5\u5ead\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u5728\u666e\u901a\u4eba\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u4ece\u6cd5\u5ead\u5230\u666e\u901a\u4eba\u6570\u636e\u7684\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u3002", "result": "\u7eaf\u6cd5\u5ead\u5224\u51b3\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u666e\u901a\u4eba\u67e5\u8be2\u6d4b\u8bd5\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4ece\u6cd5\u5ead\u5230\u666e\u901a\u4eba\u6570\u636e\u7684\u8fc1\u79fb\u5b66\u4e60\u662f\u6709\u76ca\u7684\u3002\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u6790\u53d1\u73b0\uff0c\u67e5\u8be2\u7c7b\u522b\u548c\u6cd5\u89c4\u9891\u7387\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u6cd5\u5ead\u6570\u636e\u548c\u666e\u901a\u4eba\u6570\u636e\u5728\u6cd5\u5f8b\u6cd5\u89c4\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u666e\u901a\u4eba\u67e5\u8be2\u7684\u6a21\u578b\u548c\u65b9\u6cd5\u3002ILSIC\u8bed\u6599\u5e93\u4e3a\u7814\u7a76\u8fd9\u4e00\u5dee\u5f02\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u8fc1\u79fb\u5b66\u4e60\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u5728\u666e\u901a\u4eba\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2602.00314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00314", "abs": "https://arxiv.org/abs/2602.00314", "authors": ["Apostol Vassilev", "Munawar Hasan", "Edward Griffor", "Honglan Jin", "Pavel Piliptchak", "Mahima Arora", "Thoshitha Gamage"], "title": "On the Assessment of Sensitivity of Autonomous Vehicle Perception", "comment": "21 pages, 17 figures", "summary": "The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5728\u6076\u52a3\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u4f4e\u5149\u7167\u6761\u4ef6\u5bf9\u611f\u77e5\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u4e14\u8ddd\u79bb\u8d8a\u8fdc\u611f\u77e5\u9c81\u68d2\u6027\u8d8a\u5dee\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u6027\u4e25\u91cd\u4f9d\u8d56\u611f\u77e5\u7cfb\u7edf\u7684\u5b9e\u65f6\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u81ea\u7136\u548c\u5bf9\u6297\u6027\u9a7e\u9a76\u56e0\u7d20\u4f1a\u5bfc\u81f4\u611f\u77e5\u9519\u8bef\u548c\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u5e76\u63a2\u7d22\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u96c6\u6210\u7684\u9884\u6d4b\u654f\u611f\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u6761\u4ef6\u4e0b\u8bc4\u4f30\u611f\u77e5\u6027\u80fd\u3002\u63d0\u51fa\u611f\u77e5\u8bc4\u4f30\u67b6\u6784\uff0c\u57fa\u4e8e\u8f66\u8f86\u5728\u5e72\u6e7f\u8def\u9762\u4e0a\u7684\u505c\u8f66\u8ddd\u79bb\u548c\u8f66\u901f\u5236\u5b9a\u8bc4\u4f30\u6807\u51c6\uff0c\u4f7f\u7528YOLO(v8-v9)\u3001DETR50\u3001DETR101\u3001RT-DETR\u7b49\u4e94\u79cd\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4f4e\u5149\u7167\u6761\u4ef6\uff08\u5982\u96fe\u548c\u4f4e\u592a\u9633\u9ad8\u5ea6\uff09\u5bf9\u611f\u77e5\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff1b\u5bf9\u6297\u6027\u9053\u8def\u6761\u4ef6\uff08\u5982\u9053\u8def\u7269\u4f53\u906e\u6321\uff09\u4f1a\u589e\u52a0\u611f\u77e5\u654f\u611f\u6027\uff1b\u5f53\u5bf9\u6297\u6027\u9053\u8def\u6761\u4ef6\u4e0e\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u7ed3\u5408\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u66f4\u660e\u663e\uff1b\u8ddd\u79bb\u9053\u8def\u7269\u4f53\u8d8a\u8fdc\uff0c\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\u8d8a\u5927\uff0c\u611f\u77e5\u9c81\u68d2\u6027\u8d8a\u5dee\u3002", "conclusion": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5b58\u5728\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u4f4e\u5149\u7167\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u611f\u77e5\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u3002"}}
{"id": "2602.00854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00854", "abs": "https://arxiv.org/abs/2602.00854", "authors": ["Fangzhou Lin", "Qianwen Ge", "Lingyu Xu", "Peiran Li", "Xiangbo Gao", "Shuo Xing", "Kazunori Yamada", "Ziming Zhang", "Haichong Zhang", "Zhengzhong Tu"], "title": "Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding", "comment": "14 pages, 1 figures", "summary": "AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u80fd\u529b-\u7406\u89e3\u9e3f\u6c9f\"\u6982\u5ff5\uff0c\u5373AI\u7cfb\u7edf\u80fd\u529b\u63d0\u5347\u800c\u7528\u6237\u7406\u89e3\u80fd\u529b\u4e0b\u964d\uff0c\u5e76\u5b9a\u4e49\"\u8ba4\u77e5\u5b8c\u6574\u6027\u9608\u503c\"\u4f5c\u4e3a\u7ef4\u6301\u4eba\u7c7b\u76d1\u7763\u6240\u9700\u7684\u6700\u4f4e\u7406\u89e3\u6c34\u5e73\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ea7\u751f\u8d8a\u6765\u8d8a\u6d41\u7545\u3001\u6b63\u786e\u7684\u7ed3\u679c\uff0c\u7528\u6237\u89e3\u91ca\u3001\u9a8c\u8bc1\u548c\u5e72\u9884\u7684\u80fd\u529b\u9010\u6e10\u88ab\u4fb5\u8680\u3002\u73b0\u6709\u900f\u660e\u5ea6\u3001\u7528\u6237\u63a7\u5236\u3001\u7d20\u517b\u548c\u6cbb\u7406\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u4eba\u7c7b\u5728\u6301\u7eedAI\u59d4\u6258\u4e0b\u5fc5\u987b\u4fdd\u7559\u7684\u57fa\u7840\u7406\u89e3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u5b8c\u6574\u6027\u9608\u503c(CIT)\u6982\u5ff5\uff0c\u5c06\u5176\u64cd\u4f5c\u5316\u4e3a\u4e09\u4e2a\u529f\u80fd\u7ef4\u5ea6\uff1a\u9a8c\u8bc1\u80fd\u529b\u3001\u7406\u89e3\u4fdd\u6301\u578b\u4ea4\u4e92\u3001\u6cbb\u7406\u7684\u5236\u5ea6\u652f\u67b6\u3002\u8fd9\u4e3a\u8d23\u4efb\u5173\u952e\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u80fd\u529b-\u7406\u89e3\u9e3f\u6c9f\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u8ba4\u77e5\u5b8c\u6574\u6027\u9608\u503c\u4f5c\u4e3a\u7ef4\u6301\u76d1\u7763\u3001\u81ea\u4e3b\u6027\u548c\u95ee\u8d23\u53c2\u4e0e\u6240\u9700\u7684\u6700\u4f4e\u7406\u89e3\u6807\u51c6\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u9700\u8981\u5c06\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u4e0e\u8ba4\u77e5\u53ef\u6301\u7eed\u6027\u5bf9\u9f50\uff0c\u7279\u522b\u662f\u5728\u8d23\u4efb\u5173\u952e\u573a\u666f\u4e2d\uff0c\u786e\u4fdd\u4eba\u7c7b\u5728AI\u534f\u52a9\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u6548\u7684\u76d1\u7763\u548c\u95ee\u8d23\u80fd\u529b\u3002"}}
{"id": "2602.00286", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00286", "abs": "https://arxiv.org/abs/2602.00286", "authors": ["Shaorong Zhang", "Longxuan Yu", "Rob Brekelmans", "Luhan Tang", "Salman Asif", "Greg Ver Steeg"], "title": "Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective", "comment": null, "summary": "Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing \"incoherence\" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u6846\u67b6\u6765\u5206\u6790\u63a9\u7801\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e24\u79cd\u5931\u8d25\u6765\u6e90\uff1a\u987a\u5e8f\u654f\u611f\u6027\u548c\u5e76\u884c\u5316\u504f\u5dee\uff0c\u63ed\u793a\u4e86\u6613\u5148\u89e3\u7801\u7684\u4f18\u52bf\u3001\u56e0\u5b50\u5316\u5e76\u884c\u89e3\u7801\u7684\u5185\u5728\u91c7\u6837\u8bef\u5dee\u4ee5\u53ca\u9a8c\u8bc1\u7684\u6307\u6570\u6210\u672c\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u901a\u8fc7\u727a\u7272\u987a\u5e8f\u786e\u5b9a\u6027\u6765\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u751f\u6210\u987a\u5e8f\u7684\u7406\u8bba\u673a\u5236\u548c\u5e76\u884c\u5316\u98ce\u9669\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5c06\u5931\u8d25\u6765\u6e90\u89e3\u8026\u4e3a\u987a\u5e8f\u654f\u611f\u6027\u548c\u5e76\u884c\u5316\u504f\u5dee\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u5728\u53d7\u63a7\u7684Block-HMM\u548c\u5927\u89c4\u6a21MDMs\uff08LLaDA\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a1\uff09\u968f\u7740\u6a21\u578b\u8bef\u5dee\u589e\u52a0\uff0c\u6613\u5148\u89e3\u7801\u7684\u4f18\u52bf\u66f4\u52a0\u660e\u663e\uff1b2\uff09\u56e0\u5b50\u5316\u5e76\u884c\u89e3\u7801\u4f1a\u5f15\u5165\u5185\u5728\u91c7\u6837\u8bef\u5dee\uff0c\u53ef\u80fd\u5bfc\u81f4\u4efb\u610f\u5927\u7684\u53cd\u5411KL\u6563\u5ea6\uff1b3\uff09\u9a8c\u8bc1\u53ef\u4ee5\u6d88\u9664\u91c7\u6837\u8bef\u5dee\uff0c\u4f46\u9700\u8981\u6307\u6570\u7ea7\u6210\u672c\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u5931\u8d25\u673a\u5236\uff0c\u63ed\u793a\u4e86\u5e76\u884c\u5316\u504f\u5dee\u7684\u98ce\u9669\u548c\u9a8c\u8bc1\u7684\u6210\u672c\u6743\u8861\uff0c\u4e3a\u6539\u8fdbMDMs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.00887", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00887", "abs": "https://arxiv.org/abs/2602.00887", "authors": ["Gaurav Srivastava", "Aafiya Hussain", "Chi Wang", "Yingyan Celine Lin", "Xuan Wang"], "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents", "comment": null, "summary": "Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.", "AI": {"tldr": "effGen\u662f\u4e00\u4e2a\u4e13\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7684\u5f00\u6e90\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u3001\u4efb\u52a1\u5206\u89e3\u3001\u590d\u6742\u5ea6\u8def\u7531\u548c\u7edf\u4e00\u5185\u5b58\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u7684\u672c\u5730\u90e8\u7f72\uff0c\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u9ad8token\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u654f\u611f\u5e94\u7528\u3002\u9700\u8981\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u4f18\u5316\u7684\u672c\u5730\u90e8\u7f72\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6210\u672c\u3001\u6548\u7387\u548c\u9690\u79c1\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "1) \u63d0\u793a\u4f18\u5316\u538b\u7f29\u4e0a\u4e0b\u658770-80%\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u8bed\u4e49\uff1b2) \u667a\u80fd\u4efb\u52a1\u5206\u89e3\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u5e76\u884c\u6216\u987a\u5e8f\u5b50\u4efb\u52a1\uff1b3) \u57fa\u4e8e\u4e94\u4e2a\u56e0\u7d20\u7684\u590d\u6742\u5ea6\u8def\u7531\u8fdb\u884c\u9884\u6267\u884c\u51b3\u7b56\uff1b4) \u7edf\u4e00\u5185\u5b58\u7cfb\u7edf\u7ed3\u5408\u77ed\u671f\u3001\u957f\u671f\u548c\u5411\u91cf\u5b58\u50a8\uff1b5) \u7edf\u4e00\u591a\u79cd\u667a\u80fd\u4f53\u534f\u8bae\u5b9e\u73b0\u8de8\u534f\u8bae\u901a\u4fe1\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ceffGen\u5728\u6210\u529f\u7387\u3001\u6267\u884c\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u4f18\u4e8eLangChain\u3001AutoGen\u548cSmolagents\u3002\u63d0\u793a\u4f18\u5316\u5bf9\u5c0f\u578b\u6a21\u578b\u6548\u679c\u66f4\u663e\u8457\uff081.5B\u6a21\u578b\u63d0\u534711.2% vs 32B\u6a21\u578b2.4%\uff09\uff0c\u800c\u8def\u7531\u5bf9\u5927\u578b\u6a21\u578b\u6548\u679c\u66f4\u597d\uff081.5B\u6a21\u578b3.6% vs 32B\u6a21\u578b7.9%\uff09\uff0c\u4e24\u8005\u7ed3\u5408\u5728\u6240\u6709\u89c4\u6a21\u4e0a\u90fd\u6709\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "effGen\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u672c\u5730\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8eAPI\u7684\u5927\u578b\u6a21\u578b\u7cfb\u7edf\u7684\u9650\u5236\uff0c\u901a\u8fc7\u5f00\u6e90MIT\u8bb8\u53ef\u8bc1\u786e\u4fdd\u7814\u7a76\u548c\u5546\u4e1a\u5e94\u7528\u7684\u5e7f\u6cdb\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2602.00340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00340", "abs": "https://arxiv.org/abs/2602.00340", "authors": ["Alexandros Christoforos", "Sarah Jenkins", "Michael Brown", "Tuan Pham", "David Chen"], "title": "Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception", "comment": null, "summary": "This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.", "AI": {"tldr": "SynerNet\u6846\u67b6\u901a\u8fc7\u56db\u4e2a\u4e13\u4e1a\u8ba1\u7b97\u5355\u5143\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9047\u5230\u5206\u5e03\u5916\u6982\u5ff5\u65f6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u9000\u5316\u95ee\u9898\uff0c\u5728VISTA-Beyond\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e861.2%\u52305.4%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9047\u5230\u5206\u5e03\u5916\u6982\u5ff5\u65f6\u51fa\u73b0\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u672a\u89c1\u6982\u5ff5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u534f\u540c\u795e\u7ecf\u4ee3\u7406\u7f51\u7edc\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u4e0a\u4e0b\u6587\u3001\u540d\u4e49\u5d4c\u5165\u548c\u5168\u5c40\u534f\u8c03\u56db\u4e2a\u8ba1\u7b97\u5355\u5143\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d88\u606f\u4f20\u64ad\u534f\u8bae\u534f\u540c\u5de5\u4f5c\uff0c\u5305\u62ec\u591a\u4ee3\u7406\u6f5c\u5728\u7a7a\u95f4\u547d\u540d\u83b7\u53d6\u6846\u67b6\u3001\u8bed\u4e49\u4e0a\u4e0b\u6587\u4ea4\u6362\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u52a8\u6001\u5e73\u8861\u673a\u5236\u3002", "result": "\u5728VISTA-Beyond\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSynerNet\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7cbe\u5ea6\u6539\u8fdb\u8303\u56f4\u4ece1.2%\u52305.4%\uff0c\u8986\u76d6\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "SynerNet\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5916\u6982\u5ff5\u4e0a\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u540c\u673a\u5236\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2602.00861", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00861", "abs": "https://arxiv.org/abs/2602.00861", "authors": ["Kushal Chakrabarti", "Nirmal Balachundar"], "title": "Multi-Head Attention Is a Multi-Player Game", "comment": null, "summary": "Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $\u0393(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \\emph{excess hallucination probability} and \\emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $\u0393(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $\u0393(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\\% hallucination reduction (8\\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.", "AI": {"tldr": "\u8bba\u6587\u5c06Transformer\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u4e3a\u591a\u5934\u95f4\u7684\u6f5c\u5728\u535a\u5f08\uff0c\u8bc1\u660e\u4ea4\u53c9\u71b5\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u7eb3\u4ec0\u5747\u8861\uff0c\u5176\u65e0\u6548\u7387\u53d7\u591a\u5934\u4ea4\u4e92\u77e9\u9635\u0393(G)\u7ea6\u675f\u3002\u63d0\u51fa\u4e86GAME-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u964d\u4f4e\u0393(G)\u6765\u51cf\u5c11\u5e7b\u89c9\u548c\u5197\u4f59\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u3002", "motivation": "\u73b0\u4ee3Transformer\u6ce8\u610f\u529b\u673a\u5236\u672c\u8d28\u4e0a\u662f\u591a\u5934\u7ade\u4e89\u4e0e\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u5355\u4e00\u4f18\u5316\u5668\uff0c\u5ffd\u7565\u4e86\u591a\u5934\u95f4\u7684\u535a\u5f08\u7ed3\u6784\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "method": "\u5c06\u591a\u5934\u6ce8\u610f\u529b\u5efa\u6a21\u4e3a\u6f5c\u5728\u535a\u5f08\uff0c\u8bc1\u660e\u4ea4\u53c9\u71b5\u8bad\u7ec3\u8bf1\u5bfc\u51fa\u591a\u5934\u95f4\u7684\u6f5c\u5728\u535a\u5f08\uff0c\u68af\u5ea6\u4e0b\u964d\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u3002\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5206\u6790\u65e0\u6548\u7387\u8fb9\u754c\uff0c\u5e76\u5f00\u53d1GAME-LoRA\u65b9\u6cd5\uff0c\u7ed3\u5408Barlow Twins\u53bb\u76f8\u5173\u548clog-determinant\u534f\u8c03\u538b\u529b\u6765\u964d\u4f4e\u591a\u5934\u4ea4\u4e92\u77e9\u9635\u0393(G)\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65e0\u653f\u5e9c\u4ef7\u683c\uff08PoA\uff09\u53d7\u0393(G)\u7ea6\u675f\uff0c\u0393(G)\u80fd\u9884\u6d4b\u5e7b\u89c9\u6982\u7387\uff08p<0.05\uff09\u3002GAME-LoRA\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe18%\u7684\u5e7b\u89c9\u51cf\u5c11\uff08\u5e73\u57478%\uff09\uff0c\u4e14\u4e0d\u635f\u5bb3\u77e5\u8bc6\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "conclusion": "\u591a\u5934\u6ce8\u610f\u529b\u672c\u8d28\u4e0a\u662f\u535a\u5f08\u7cfb\u7edf\uff0c\u5ffd\u7565\u8fd9\u4e00\u7ed3\u6784\u4f1a\u5bfc\u81f4\u65e0\u6548\u7387\u3002\u901a\u8fc7\u964d\u4f4e\u591a\u5934\u4ea4\u4e92\u0393(G)\u7684\u89c4\u8303\u5316\u65b9\u6cd5\u53ef\u4ee5\u540c\u65f6\u51cf\u5c11\u5e7b\u89c9\u548c\u5197\u4f59\uff0cGAME-LoRA\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7406\u8bba\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.00294", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00294", "abs": "https://arxiv.org/abs/2602.00294", "authors": ["Franz A. Heinsen", "Leo Kozachkov"], "title": "Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation", "comment": "For source code and replication instructions, see https://github.com/glassroom/sata_attention. 12 pages, 6 figures (main); 4 pages, 2 figures (appendix)", "summary": "The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u4ee5\u6052\u5b9a\u6210\u672c\u5b9e\u73b0\u4efb\u610f\u7cbe\u5ea6\uff0c\u5927\u5e45\u964d\u4f4eTransformer\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6210\u672c\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0c\u5bfc\u81f4\u5b58\u50a8\u3001\u8ba1\u7b97\u548c\u80fd\u6e90\u9700\u6c42\u8d85\u8fc7\u793e\u4f1a\u4f9b\u5e94\u80fd\u529b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u7684\u6cf0\u52d2\u5c55\u5f00\u5206\u89e3\u4e3a\u5bf9\u79f0\u5f20\u91cf\u94fe\u8868\u8fbe\u5f0f\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u83b7\u5f97\u524d\u9988\u53d8\u6362\uff0c\u5c06\u67e5\u8be2\u548c\u952e\u6620\u5c04\u5230\u6700\u5c0f\u591a\u9879\u5f0f\u6838\u7279\u5f81\u57fa\u7684\u5750\u6807\u4e2d\u3002", "result": "\u5b9e\u73b0\u4e86\u6052\u5b9a\u6210\u672c\u7684\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u91cf\u964d\u4f4e\u6570\u4e2a\u6570\u91cf\u7ea7\uff0c\u6210\u672c\u4e0e\u5934\u5927\u5c0f\u6210\u53cd\u6bd4\u56fa\u5b9a\uff0c\u652f\u6301\u66f4\u591a\u6ce8\u610f\u529b\u5934\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4ee5\u9002\u5ea6\u56fa\u5b9a\u6210\u672c\u5b9e\u73b0\u65e0\u9650\u5236\u7684token\u751f\u6210\uff0c\u5927\u5e45\u964d\u4f4e\u5927\u89c4\u6a21Transformer\u6a21\u578b\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u80fd\u6e90\u9700\u6c42\uff0c\u6240\u5f15\u5165\u7684\u6570\u5b66\u6280\u672f\u5177\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2602.00913", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00913", "abs": "https://arxiv.org/abs/2602.00913", "authors": ["V\u00edctor Yeste", "Paolo Rosso"], "title": "Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts", "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 42 pages, 4 figures", "summary": "Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\\rightarrow$HO$\\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.", "AI": {"tldr": "\u7814\u7a76\u5728\u8ba1\u7b97\u53d7\u9650\u6761\u4ef6\u4e0b\u63a2\u7d22\u65bd\u74e6\u8328\u9ad8\u9636\u7c7b\u522b\u5bf9\u53e5\u5b50\u7ea7\u4eba\u7c7b\u4ef7\u503c\u68c0\u6d4b\u7684\u5b9e\u7528\u6027\uff0c\u53d1\u73b0\u786c\u6027\u5c42\u6b21\u7ea6\u675f\u53cd\u800c\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6807\u7b7e\u9608\u503c\u8c03\u4f18\u548c\u8f7b\u91cf\u96c6\u6210\u66f4\u6709\u6548\u3002", "motivation": "\u53e5\u5b50\u7ea7\u4eba\u7c7b\u4ef7\u503c\u68c0\u6d4b\u901a\u5e38\u88ab\u6784\u5efa\u4e3a\u5bf9\u65bd\u74e6\u8328\u4ef7\u503c\u7684\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u65bd\u74e6\u8328\u9ad8\u9636\u7c7b\u522b\u662f\u5426\u63d0\u4f9b\u53ef\u7528\u7684\u7ed3\u6784\u3002\u7814\u7a76\u65e8\u5728\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u63a2\u7d22\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728ValueEval'24/ValuesML\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\uff1a(1)\u76f4\u63a5\u76d1\u7763\u7684transformer\u6a21\u578b\uff1b(2)\u4f7f\u7528\u786c\u63a9\u7801\u5f3a\u5236\u5c42\u6b21\u7ed3\u6784\u7684HO\u2192\u4ef7\u503c\u6d41\u6c34\u7ebf\uff1b(3)\u5b58\u5728\u2192HO\u2192\u4ef7\u503c\u7ea7\u8054\u65b9\u6cd5\uff1b\u540c\u65f6\u6d4b\u8bd5\u4f4e\u6210\u672c\u9644\u52a0\u7ec4\u4ef6\u3001\u6807\u7b7e\u9608\u503c\u8c03\u4f18\u3001\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18LLM\u57fa\u7ebf\u3001QLoRA\u548c\u7b80\u5355\u96c6\u6210\u3002", "result": "\u9ad8\u9636\u7c7b\u522b\u53ef\u4ece\u5355\u53e5\u5b66\u4e60\uff0c\u4f46\u786c\u6027\u5c42\u6b21\u95e8\u63a7\u901a\u5e38\u901a\u8fc7\u9519\u8bef\u7d2f\u79ef\u548c\u53ec\u56de\u6291\u5236\u964d\u4f4e\u6700\u7ec8\u4efb\u52a1\u7684Macro-F1\uff1b\u6807\u7b7e\u9608\u503c\u8c03\u4f18\u662f\u9ad8\u6548\u6760\u6746\uff08\u63d0\u5347\u8fbe+0.05 Macro-F1\uff09\uff0c\u5c0f\u578btransformer\u96c6\u6210\u63d0\u4f9b\u6700\u4e00\u81f4\u7684\u989d\u5916\u589e\u76ca\uff08\u63d0\u5347\u8fbe+0.02 Macro-F1\uff09\uff1b\u5c0f\u578bLLM\u4f5c\u4e3a\u72ec\u7acb\u7cfb\u7edf\u843d\u540e\u4e8e\u76d1\u7763\u7f16\u7801\u5668\uff0c\u4f46\u53ef\u5728\u8de8\u5bb6\u65cf\u96c6\u6210\u4e2d\u8d21\u732e\u4e92\u8865\u9519\u8bef\u3002", "conclusion": "\u9ad8\u9636\u7ed3\u6784\u5728\u63cf\u8ff0\u4e0a\u6709\u7528\uff0c\u4f46\u7528\u786c\u95e8\u63a7\u5f3a\u5236\u6267\u884c\u4f1a\u635f\u5bb3\u53e5\u5b50\u7ea7\u4ef7\u503c\u68c0\u6d4b\uff1b\u7a33\u5065\u7684\u6539\u8fdb\u6765\u81ea\u6821\u51c6\u548c\u8f7b\u91cf\u96c6\u6210\u3002"}}
{"id": "2602.00344", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00344", "abs": "https://arxiv.org/abs/2602.00344", "authors": ["Beidi Zhao", "Wenlong Deng", "Xinting Liao", "Yushu Li", "Nazim Shaikh", "Yao Nie", "Xiaoxiao Li"], "title": "When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs", "comment": "18 pages, 10 figures", "summary": "While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.", "AI": {"tldr": "MAD-RAG\uff1a\u4e00\u79cd\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\u7684\u8bad\u7ec3\u514d\u8d39\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u95ee\u9898\u8868\u8ff0\u548c\u89e3\u8026\u89c6\u89c9\u57fa\u7840\u4e0e\u4e0a\u4e0b\u6587\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06RAG\u5931\u8d25\u5f52\u56e0\u4e8e\u5bf9\u68c0\u7d22\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u4e0d\u8db3\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u4e86\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u6ce8\u610f\u529b\u5206\u6563\u3002\u5f53\u68c0\u7d22\u4e0a\u4e0b\u6587\u8db3\u591f\u76f8\u5173\u65f6\uff0c\u68c0\u7d22\u6587\u672c\u4f1a\u5168\u5c40\u6291\u5236\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u4ece\u95ee\u9898\u76f8\u5173\u533a\u57df\u8f6c\u79fb\uff0c\u4f7f\u539f\u672c\u80fd\u6b63\u786e\u56de\u7b54\u7684\u95ee\u9898\u5931\u8d25\u3002", "method": "\u63d0\u51faMAD-RAG\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u53cc\u95ee\u9898\u8868\u8ff0\u89e3\u8026\u89c6\u89c9\u57fa\u7840\u4e0e\u4e0a\u4e0b\u6587\u96c6\u6210\uff1b2\uff09\u7ed3\u5408\u6ce8\u610f\u529b\u6df7\u5408\u4ee5\u4fdd\u7559\u56fe\u50cf\u6761\u4ef6\u8bc1\u636e\uff1b3\uff09\u65e0\u9700\u8bad\u7ec3\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "result": "\u5728OK-VQA\u3001E-VQA\u548cInfoSeek\u6570\u636e\u96c6\u4e0a\uff0cMAD-RAG\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u76f8\u6bd4\u666e\u901aRAG\u57fa\u7ebf\u5206\u522b\u83b7\u5f974.76%\u30019.20%\u548c6.18%\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u7ea0\u6b63\u4e86\u9ad8\u8fbe74.68%\u7684\u5931\u8d25\u6848\u4f8b\u3002", "conclusion": "\u6ce8\u610f\u529b\u5206\u6563\u662fRAG\u5931\u8d25\u7684\u91cd\u8981\u6a21\u5f0f\uff0cMAD-RAG\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u57fa\u7840\u548c\u4e0a\u4e0b\u6587\u96c6\u6210\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u578b\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2602.00866", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00866", "abs": "https://arxiv.org/abs/2602.00866", "authors": ["Akiharu Esashi", "Pawissanutt Lertpongrujikorn", "Justin Makino", "Yuibi Fujimoto", "Mohsen Amini Salehi"], "title": "Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data", "comment": null, "summary": "The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.", "AI": {"tldr": "\u63d0\u51faCAN\u57fa\u7840\u6a21\u578b\uff0c\u5c06CAN\u603b\u7ebf\u6570\u636e\u89c6\u4e3a\u8bed\u8a00\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5fae\u8c03\u5230\u591a\u79cd\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316", "motivation": "\u73b0\u6709CAN\u6570\u636e\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u662f\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u5b64\u7acb\u6a21\u578b\uff0c\u4f7f\u7528\u539f\u59cb\u6570\u636e\u6216\u6709\u9650\u89e3\u7801\u4fe1\u53f7\uff0c\u7f3a\u4e4f\u5171\u4eab\u8868\u793a\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002\u800cNLP\u548cCV\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u5df2\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u6b64\u8303\u5f0f\u5e94\u7528\u4e8eCAN\u6570\u636e", "method": "\u5c06CAN\u6570\u636e\u89c6\u4e3a\u8bed\u8a00\u5904\u7406\uff1a1\uff09\u63d0\u51fa\u7edf\u4e00\u7684\u79bb\u6563-\u8fde\u7eed\u6df7\u5408\u4fe1\u53f7\u6807\u8bb0\u5316\u65b9\u6848\uff1b2\uff09\u5728\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u89e3\u7801CAN\u4fe1\u53f7\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b3\uff09\u9488\u5bf9\u65f6\u95f4\u590d\u6742\u6027\u548c\u884c\u7a0b\u7279\u5b9a\u53d8\u5f02\u6027\u6311\u6218\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\uff1b4\uff09\u5728\u5f02\u6784\u6c7d\u8f66\u4fdd\u9669\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03", "result": "\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684CAN\u6a21\u578b\u80fd\u591f\u6709\u6548\u9002\u5e94\u591a\u79cd\u9884\u6d4b\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u5728CAN\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6c7d\u8f66AI\u4e2d\u7684\u53ef\u6cdb\u5316\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411", "conclusion": "\u57fa\u7840\u6a21\u578b\u8303\u5f0f\uff08\u5927\u89c4\u6a21\u9884\u8bad\u7ec3+\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\uff09\u4e0d\u4ec5\u9002\u7528\u4e8eNLP\u548cCV\uff0c\u4e5f\u9002\u7528\u4e8eCAN\u6570\u636e\uff0c\u80fd\u591f\u5b9e\u73b0\u591a\u76ee\u6807\u4e0b\u6e38\u6cdb\u5316\uff0c\u4e3a\u6c7d\u8f66AI\u4e2d\u7684\u901a\u7528\u8868\u793a\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.00297", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00297", "abs": "https://arxiv.org/abs/2602.00297", "authors": ["Jie Yang", "Yifan Hu", "Yuante Li", "Kexin Zhang", "Kaize Ding", "Philip S. Yu"], "title": "From Observations to States: Latent Time Series Forecasting", "comment": null, "summary": "Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLatentTSF\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ece\u89c2\u6d4b\u7a7a\u95f4\u56de\u5f52\u8f6c\u79fb\u5230\u6f5c\u5728\u72b6\u6001\u9884\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5b58\u5728\u7684\"\u6f5c\u5728\u6df7\u6c8c\"\u95ee\u9898\uff0c\u5373\u9884\u6d4b\u51c6\u786e\u4f46\u6f5c\u5728\u8868\u793a\u65f6\u5e8f\u65e0\u5e8f\u7684\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\"\u6f5c\u5728\u6df7\u6c8c\"\u95ee\u9898\uff1a\u9884\u6d4b\u51c6\u786e\u7684\u6a21\u578b\u5b66\u4e60\u7684\u6f5c\u5728\u8868\u793a\u5f80\u5f80\u662f\u65f6\u5e8f\u65e0\u5e8f\u4e14\u7f3a\u4e4f\u8fde\u7eed\u6027\u7684\u3002\u8fd9\u6e90\u4e8e\u4e3b\u6d41\u7684\u89c2\u6d4b\u7a7a\u95f4\u9884\u6d4b\u8303\u5f0f\uff0c\u6a21\u578b\u6700\u5c0f\u5316\u566a\u58f0\u548c\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u4e0a\u7684\u70b9\u8bef\u5dee\uff0c\u9f13\u52b1\u6377\u5f84\u89e3\u800c\u975e\u6062\u590d\u5e95\u5c42\u7cfb\u7edf\u52a8\u6001\u3002", "method": "\u63d0\u51faLatentTSF\u65b0\u8303\u5f0f\uff0c\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u5c06\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u89c2\u6d4b\u6295\u5f71\u5230\u9ad8\u7ef4\u6f5c\u5728\u72b6\u6001\u7a7a\u95f4\uff0c\u6269\u5c55\u8868\u793a\u4ee5\u6355\u83b7\u5e95\u5c42\u7cfb\u7edf\u53d8\u91cf\u5e76\u65bd\u52a0\u66f4\u5e73\u6ed1\u7684\u65f6\u5e8f\u7ed3\u6784\u3002\u9884\u6d4b\u5b8c\u5168\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\uff0c\u4f7f\u6a21\u578b\u4e13\u6ce8\u4e8e\u5b66\u4e60\u7ed3\u6784\u5316\u65f6\u5e8f\u52a8\u6001\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6f5c\u5728\u76ee\u6807\u9690\u5f0f\u5730\u6700\u5927\u5316\u9884\u6d4b\u6f5c\u5728\u72b6\u6001\u4e0e\u771f\u5b9e\u72b6\u6001\u53ca\u89c2\u6d4b\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\uff0cLatentTSF\u6709\u6548\u7f13\u89e3\u4e86\u6f5c\u5728\u6df7\u6c8c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "LatentTSF\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ece\u89c2\u6d4b\u56de\u5f52\u8f6c\u79fb\u5230\u6f5c\u5728\u72b6\u6001\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6f5c\u5728\u6df7\u6c8c\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u83b7\u5e95\u5c42\u7cfb\u7edf\u52a8\u6001\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.00914", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.00914", "abs": "https://arxiv.org/abs/2602.00914", "authors": ["V\u00edctor Yeste", "Rodrigo Rivas-Ar\u00e9valo"], "title": "A Baseline Multimodal Approach to Emotion Recognition in Conversations", "comment": "10 pages", "summary": "We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u300a\u8001\u53cb\u8bb0\u300b\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff0c\u7ed3\u5408\u6587\u672c\u5206\u7c7b\u5668\u548c\u8bed\u97f3\u8868\u793a\u6a21\u578b\uff0c\u91c7\u7528\u7b80\u5355\u7684\u540e\u671f\u878d\u5408\u7b56\u7565\u3002", "motivation": "\u4e3aSemEval-2024\u4efb\u52a13\u63d0\u4f9b\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u652f\u6301\u672a\u6765\u66f4\u4e25\u683c\u7684\u6bd4\u8f83\uff0c\u800c\u4e0d\u662f\u8ffd\u6c42\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u57fa\u4e8etransformer\u7684\u6587\u672c\u5206\u7c7b\u5668\u548c\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u6a21\u578b\uff0c\u91c7\u7528\u7b80\u5355\u7684\u540e\u671f\u878d\u5408\u96c6\u6210\u7b56\u7565\uff0c\u5728\u6709\u9650\u8bad\u7ec3\u534f\u8bae\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u62a5\u544a\u4e86\u57fa\u7ebf\u8bbe\u7f6e\u548c\u5b9e\u8bc1\u7ed3\u679c\uff0c\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u4f55\u65f6\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u9884\u5370\u672c\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u7814\u7a76\u8bb0\u5f55\uff0c\u65e8\u5728\u652f\u6301\u672a\u6765\u66f4\u4e25\u683c\u7684\u6bd4\u8f83\uff0c\u5f3a\u8c03\u591a\u6a21\u6001\u878d\u5408\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.00347", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00347", "abs": "https://arxiv.org/abs/2602.00347", "authors": ["Chongyu Qu", "Zhengyi Lu", "Yuxiang Lai", "Thomas Z. Li", "Junchao Zhu", "Junlin Guo", "Juming Xiong", "Yanfan Zhu", "Yuechen Yang", "Allen J. Luna", "Kim L. Sandler", "Bennett A. Landman", "Yuankai Huo"], "title": "AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning", "comment": null, "summary": "Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.", "AI": {"tldr": "AdaFuse\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u80ba\u764c\u98ce\u9669\u9884\u6d4b\uff0c\u53ef\u6839\u636e\u60a3\u8005\u5177\u4f53\u60c5\u51b5\u52a8\u6001\u9009\u62e9\u4f7f\u7528\u54ea\u4e9b\u6a21\u6001\u6570\u636e\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5bf9\u6240\u6709\u60a3\u8005\u4f7f\u7528\u6240\u6709\u53ef\u7528\u6a21\u6001\u6570\u636e\uff0c\u8981\u4e48\u5e73\u7b49\u5bf9\u5f85\uff0c\u8981\u4e48\u5b66\u4e60\u6743\u91cd\u5206\u914d\uff0c\u4f46\u672a\u89e3\u51b3\u4e00\u4e2a\u6839\u672c\u95ee\u9898\uff1a\u5bf9\u4e8e\u7279\u5b9a\u60a3\u8005\uff0c\u662f\u5426\u5e94\u8be5\u4f7f\u7528\u67d0\u4e9b\u6a21\u6001\uff1f\u9700\u8981\u4e2a\u6027\u5316\u7684\u6a21\u6001\u9009\u62e9\u548c\u878d\u5408\u7b56\u7565\u3002", "method": "\u5c06\u591a\u6a21\u6001\u878d\u5408\u5efa\u6a21\u4e3a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u7f51\u7edc\uff0c\u8fed\u4ee3\u51b3\u5b9a\u662f\u5426\u7eb3\u5165\u989d\u5916\u6a21\u6001\u6216\u57fa\u4e8e\u5df2\u6709\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u79cd\u987a\u5e8f\u5236\u5b9a\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u5df2\u89c2\u5bdf\u6a21\u6001\u8c03\u6574\u9009\u62e9\uff0c\u5e76\u5728\u4fe1\u606f\u8db3\u591f\u65f6\u63d0\u524d\u7ec8\u6b62\u3002", "result": "\u5728NLST\u6570\u636e\u96c6\u4e0a\uff0cAdaFuse\u8fbe\u5230\u6700\u9ad8AUC\uff080.762\uff09\uff0c\u4f18\u4e8e\u6700\u4f73\u5355\u6a21\u6001\u57fa\u7ebf\uff080.732\uff09\u3001\u6700\u4f73\u56fa\u5b9a\u878d\u5408\u7b56\u7565\uff080.759\uff09\u4ee5\u53ca\u81ea\u9002\u5e94\u57fa\u7ebfDynMM\uff080.754\uff09\u548cMoE\uff080.742\uff09\uff0c\u540c\u65f6\u6bd4\u6240\u6709\u4e09\u6a21\u6001\u65b9\u6cd5\u4f7f\u7528\u66f4\u5c11\u7684FLOPs\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2a\u6027\u5316\u591a\u6a21\u6001\u878d\u5408\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee3\u8868\u4e86\u4ece\u7edf\u4e00\u878d\u5408\u7b56\u7565\u5411\u81ea\u9002\u5e94\u8bca\u65ad\u6d41\u7a0b\u7684\u8f6c\u53d8\uff0c\u5b66\u4e60\u4f55\u65f6\u54a8\u8be2\u989d\u5916\u6a21\u6001\u4ee5\u53ca\u4f55\u65f6\u73b0\u6709\u4fe1\u606f\u8db3\u4ee5\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u3002"}}
{"id": "2602.00871", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00871", "abs": "https://arxiv.org/abs/2602.00871", "authors": ["Hossein A. Rahmani", "Mengting Wan", "Pei Zhou", "Longqi Yang", "Nick Craswell", "Emine Yilmaz", "Sujay Kumar Jauhar"], "title": "Beyond Output Critique: Self-Correction via Task Distillation", "comment": null, "summary": "Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.", "AI": {"tldr": "SELF-THOUGHT\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u62bd\u8c61\u5f15\u5bfcLLM\u81ea\u6211\u4fee\u6b63\uff0c\u5c06\u4efb\u52a1\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u6a21\u677f\uff0c\u6307\u5bfc\u89e3\u51b3\u65b9\u6848\u5b9e\u4f8b\u5316\uff0c\u63d0\u5347\u4fee\u6b63\u6548\u679c\u5e76\u5b9e\u73b0\u8de8\u6a21\u578b\u6a21\u677f\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709LLM\u81ea\u6211\u4fee\u6b63\u65b9\u6cd5\u4e3b\u8981\u505c\u7559\u5728\u8f93\u51fa\u6279\u8bc4\u5c42\u9762\uff0c\u53ea\u80fd\u4fee\u8865\u8868\u9762\u9519\u8bef\uff0c\u96be\u4ee5\u7ea0\u6b63\u6df1\u5c42\u63a8\u7406\u7f3a\u9677\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7406\u89e3\u4efb\u52a1\u672c\u8d28\u7ed3\u6784\u3001\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u7684\u81ea\u6211\u4fee\u6b63\u6846\u67b6\u3002", "method": "\u63d0\u51faSELF-THOUGHT\u6846\u67b6\uff1a1) \u4efb\u52a1\u62bd\u8c61\uff1a\u5c06\u8f93\u5165\u548c\u521d\u59cb\u54cd\u5e94\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u6a21\u677f\uff0c\u6355\u6349\u5173\u952e\u53d8\u91cf\u3001\u7ea6\u675f\u548c\u95ee\u9898\u7ed3\u6784\uff1b2) \u89e3\u51b3\u65b9\u6848\u5b9e\u4f8b\u5316\uff1a\u57fa\u4e8e\u62bd\u8c61\u6a21\u677f\u751f\u6210\u5177\u4f53\u89e3\u51b3\u65b9\u6848\uff1b3) \u8de8\u6a21\u578b\u6a21\u677f\u8fc1\u79fb\uff1a\u5927\u6a21\u578b\u751f\u6210\u7684\u6a21\u677f\u53ef\u6307\u5bfc\u5c0f\u6a21\u578b\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u4fee\u6b63\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSELF-THOUGHT\u63d0\u9ad8\u4e86\u5927\u6a21\u578b\u548c\u5c0f\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5c0f\u6a21\u578b\u63d0\u4f9b\u4e86\u65e0\u9700\u5927\u91cf\u5fae\u8c03\u6216\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u53ef\u9760\u4fee\u6b63\u65b9\u6cd5\u3002", "conclusion": "SELF-THOUGHT\u901a\u8fc7\u4efb\u52a1\u62bd\u8c61\u548c\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u7279\u522b\u662f\u4e3a\u5c0f\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53ef\u9760\u4fee\u6b63\u8def\u5f84\uff0c\u63a8\u52a8\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u4fee\u6b63\u8bed\u8a00\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2602.00299", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00299", "abs": "https://arxiv.org/abs/2602.00299", "authors": ["Rituparna Datta", "Zihan Guan", "Baltazar Espinoza", "Yiqi Su", "Priya Pitre", "Srini Venkatramanan", "Naren Ramakrishnan", "Anil Vullikanti"], "title": "Agentic Framework for Epidemiological Modeling", "comment": null, "summary": "Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.", "AI": {"tldr": "EPIAGENT\uff1a\u4e00\u4e2a\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u751f\u6210\u3001\u6821\u51c6\u548c\u9a8c\u8bc1\u6d41\u884c\u75c5\u5b66\u6a21\u62df\u5668\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f7f\u7528\u6d41\u884c\u75c5\u5b66\u6d41\u7a0b\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u52a0\u901f\u6709\u6548\u6a21\u578b\u7684\u6536\u655b", "motivation": "\u4f20\u7edf\u6d41\u884c\u75c5\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u578b\u7c7b\u522b\uff0c\u9700\u8981\u968f\u7740\u75c5\u539f\u4f53\u3001\u653f\u7b56\u548c\u60c5\u666f\u5047\u8bbe\u7684\u53d8\u5316\u800c\u624b\u52a8\u91cd\u65b0\u8bbe\u8ba1\uff0c\u8fd9\u9650\u5236\u4e86\u5efa\u6a21\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387", "method": "\u5c06\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e3a\u8fed\u4ee3\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0c\u4f7f\u7528\u6d41\u884c\u75c5\u5b66\u6d41\u7a0b\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u8fde\u63a5\u60c5\u666f\u89c4\u8303\u4e0e\u6a21\u578b\u7ed3\u6784\uff0c\u5728\u4ee3\u7801\u751f\u6210\u524d\u8fdb\u884c\u6a21\u5757\u5316\u6b63\u786e\u6027\u68c0\u67e5\uff0c\u7136\u540e\u5c06\u9a8c\u8bc1\u7684\u6d41\u7a0b\u56fe\u7f16\u8bd1\u4e3a\u652f\u6301\u53ef\u89e3\u91ca\u53c2\u6570\u5b66\u4e60\u7684\u673a\u5236\u6a21\u578b", "result": "\u5728\u6d41\u884c\u75c5\u5b66\u60c5\u666f\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cEPIAGENT\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u589e\u957f\u52a8\u6001\uff0c\u5e76\u5728\u4e0d\u540c\u75ab\u82d7\u63a5\u79cd\u548c\u514d\u75ab\u9003\u9038\u5047\u8bbe\u4e0b\u4ea7\u751f\u6d41\u884c\u75c5\u5b66\u4e00\u81f4\u7684\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff0c\u667a\u80fd\u4f53\u53cd\u9988\u5faa\u73af\u9632\u6b62\u9000\u5316\u5e76\u663e\u8457\u52a0\u901f\u5411\u6709\u6548\u6a21\u578b\u7684\u6536\u655b", "conclusion": "EPIAGENT\u901a\u8fc7\u6a21\u4eff\u4e13\u4e1a\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5408\u6210\u3001\u6821\u51c6\u548c\u9a8c\u8bc1\u6d41\u884c\u75c5\u5b66\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u5efa\u6a21\u6548\u7387\u548c\u7075\u6d3b\u6027"}}
{"id": "2602.00945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00945", "abs": "https://arxiv.org/abs/2602.00945", "authors": ["Anusa Saha", "Tanmay Joshi", "Vinija Jain", "Aman Chadha", "Amitava Das"], "title": "Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs", "comment": null, "summary": "LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.\n  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.", "AI": {"tldr": "\u63d0\u51faNeural FOXP2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u64cd\u63a7\u8bed\u8a00\u795e\u7ecf\u5143\uff0c\u4f7f\u6a21\u578b\u5c06\u7279\u5b9a\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u5982\u5370\u5730\u8bed\u6216\u897f\u73ed\u7259\u8bed\uff09\u8bbe\u4e3a\u4e3b\u8981\u8bed\u8a00\uff0c\u89e3\u51b3LLMs\u4e2d\u82f1\u8bed\u4e3b\u5bfc\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLMs\u662f\u591a\u8bed\u8a00\u7684\uff0c\u4f46\u82f1\u8bed\u5728\u9884\u8bad\u7ec3\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5176\u4ed6\u8bed\u8a00\u5728\u53c2\u6570\u8bb0\u5fc6\u4e2d\u88ab\u7cfb\u7edf\u6027\u5730\u6291\u5236\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u5b89\u5168\u5730\u64cd\u63a7\u8bed\u8a00\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u4f7f\u975e\u82f1\u8bed\u8bed\u8a00\u6210\u4e3a\u4e3b\u8981\u8bed\u8a00\u3002", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5b9a\u4f4d\uff1a\u8bad\u7ec3\u6bcf\u5c42SAE\u5206\u89e3\u6fc0\u6d3b\uff0c\u8bc6\u522b\u5bf9\u76ee\u6807\u8bed\u8a00\u6709\u9009\u62e9\u6027\u7684\u7279\u5f81\uff0c\u8ffd\u8e2a\u5230\u8bed\u8a00\u795e\u7ecf\u5143\u96c6\u5408\uff1b2) \u64cd\u63a7\u65b9\u5411\uff1a\u901a\u8fc7\u8c31\u4f4e\u79e9\u5206\u6790\u5b9a\u4f4d\u53ef\u63a7\u7684\u8bed\u8a00\u8f6c\u6362\u51e0\u4f55\uff0c\u63d0\u53d6\u4e3b\u5bfc\u5947\u5f02\u65b9\u5411\uff1b3) \u64cd\u63a7\uff1a\u5728\u4f4e\u5230\u4e2d\u5c42\u5e94\u7528\u5e26\u7b26\u53f7\u7684\u7a00\u758f\u6fc0\u6d3b\u504f\u79fb\uff0c\u6cbf\u76ee\u6807\u8bed\u8a00\u4e3b\u5bfc\u65b9\u5411\u6dfb\u52a0\u6b63\u5411\u64cd\u63a7\uff0c\u5bf9\u82f1\u8bed\u795e\u7ecf\u5143\u6dfb\u52a0\u8865\u507f\u6027\u8d1f\u5411\u504f\u79fb\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e86\u8bed\u8a00\u795e\u7ecf\u5143\uff0c\u5e76\u5efa\u7acb\u4e86\u53ef\u63a7\u7684\u8bed\u8a00\u8f6c\u6362\u51e0\u4f55\uff0c\u80fd\u591f\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u504f\u79fb\u5b9e\u73b0\u76ee\u6807\u8bed\u8a00\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u3002", "conclusion": "\u8bed\u8a00\u9ed8\u8ba4\u6027\u7531\u7a00\u758f\u3001\u4f4e\u79e9\u7684\u63a7\u5236\u7535\u8def\uff08\u8bed\u8a00\u795e\u7ecf\u5143\uff09\u63a7\u5236\uff0c\u53ef\u4ee5\u88ab\u673a\u5236\u6027\u5730\u9694\u79bb\u548c\u5b89\u5168\u5730\u64cd\u63a7\u3002Neural FOXP2\u65b9\u6cd5\u80fd\u591f\u4f7f\u7279\u5b9a\u975e\u82f1\u8bed\u8bed\u8a00\u6210\u4e3a\u6a21\u578b\u7684\u4e3b\u8981\u8bed\u8a00\u3002"}}
{"id": "2602.00348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00348", "abs": "https://arxiv.org/abs/2602.00348", "authors": ["Zhengyi Lu", "Ming Lu", "Chongyu Qu", "Junchao Zhu", "Junlin Guo", "Marilyn Lionts", "Yanfan Zhu", "Yuechen Yang", "Tianyuan Yao", "Jayasai Rajagopal", "Bennett Allan Landman", "Xiao Wang", "Xinqiang Yan", "Yuankai Huo"], "title": "MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI", "comment": null, "summary": "Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc", "AI": {"tldr": "MASC\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u91d1\u5c5e\u611f\u77e5\u7684k\u7a7a\u95f4\u91c7\u6837\u548c\u4f2a\u5f71\u6821\u6b63\uff0c\u7528\u4e8e\u52a0\u901fMRI\u626b\u63cf", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u91d1\u5c5e\u4f2a\u5f71\u51cf\u5c11\uff08MAR\uff09\u548c\u52a0\u901fMRI\u91c7\u96c6\u4f5c\u4e3a\u4e24\u4e2a\u72ec\u7acb\u95ee\u9898\u5904\u7406\uff0c\u4f46\u91d1\u5c5e\u690d\u5165\u7269\u5728MRI\u4e2d\u4f1a\u9020\u6210\u4e25\u91cd\u4f2a\u5f71\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u548c\u4e34\u5e8a\u8bca\u65ad\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51faMASC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u6784\u5efa\u914d\u5bf9\u7684MRI\u6570\u636e\u96c6\uff1b2\uff09\u5c06\u4e3b\u52a8MRI\u91c7\u96c6\u5efa\u6a21\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u57fa\u4e8ePPO\u7684\u4f2a\u5f71\u611f\u77e5\u4ee3\u7406\u5b66\u4e60\u5728\u6709\u9650\u91c7\u96c6\u9884\u7b97\u4e0b\u9009\u62e9k\u7a7a\u95f4\u76f8\u4f4d\u7f16\u7801\u7ebf\uff1b3\uff09\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6848\uff0c\u8ba9\u91c7\u96c6\u7b56\u7565\u5b66\u4e60\u9009\u62e9\u6700\u80fd\u652f\u6301\u4f2a\u5f71\u53bb\u9664\u7684k\u7a7a\u95f4\u7ebf\uff0c\u540c\u65f6MAR\u7f51\u7edc\u9002\u5e94\u7531\u6b64\u4ea7\u751f\u7684\u6b20\u91c7\u6837\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09MASC\u7684\u5b66\u4e60\u7b56\u7565\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u7b56\u7565\uff1b2\uff09\u7aef\u5230\u7aef\u8bad\u7ec3\u76f8\u6bd4\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3MAR\u7f51\u7edc\u63d0\u9ad8\u4e86\u6027\u80fd\uff1b3\uff09\u5728FastMRI\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5411\u771f\u5b9e\u4e34\u5e8aMRI\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MASC\u6210\u529f\u5b9e\u73b0\u4e86\u91d1\u5c5e\u611f\u77e5k\u7a7a\u95f4\u91c7\u6837\u548c\u4f2a\u5f71\u6821\u6b63\u7684\u8054\u5408\u4f18\u5316\uff0c\u4e3a\u52a0\u901fMRI\u626b\u63cf\u4e2d\u7684\u91d1\u5c5e\u4f2a\u5f71\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.00911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00911", "abs": "https://arxiv.org/abs/2602.00911", "authors": ["Abhijit Chakraborty", "Sandipan De", "Yash Shah", "Chahana Dahal", "Vivek Gupta"], "title": "Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs", "comment": null, "summary": "Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.", "AI": {"tldr": "Synapse\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u95f4\u7684\u5de5\u5177\u4f7f\u7528\u5171\u4eab\u77e5\u8bc6\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u677f\u8868\u793a\u3001\u5d4c\u5165\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u63a9\u7801\u6765\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u6548\u679c\u5e76\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u8054\u90a6\u5b66\u4e60\u4e0b\u9762\u4e34\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u6570\u636e\u548c\u5de5\u5177\u4f7f\u7528\u5f02\u8d28\u6027\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u534f\u4f5c\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u8bad\u7ec3\u5171\u4eab\u7684\u5168\u5c40\u5de5\u5177\u4f7f\u7528\u77e5\u8bc6\u6a21\u578b\uff0c\u5ba2\u6237\u7aef\u667a\u80fd\u4f53\u5728\u672c\u5730\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u901a\u8fc7\u534f\u8c03\u5668\u4f20\u8f93\u5de5\u4ef6\u8fdb\u884c\u8054\u90a6\u805a\u5408\uff0c\u66f4\u65b0\u5168\u5c40\u5de5\u5177\u6c47\u7f16\u5e76\u91cd\u65b0\u5206\u53d1\u3002\u91c7\u7528\u6a21\u677f\u8868\u793a\u3001\u5d4c\u5165\u68c0\u7d22\u4e0eLLM\u91cd\u6392\u5e8f\u3001\u81ea\u9002\u5e94\u63a9\u7801\u7b49\u6280\u672f\u3002", "result": "Synapse\u76f8\u6bd4\u6743\u91cd\u6216\u63d0\u793a\u5171\u4eab\u65b9\u6cd5\uff0c\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u63d0\u9ad8\u4e86\u5de5\u5177\u4f7f\u7528\u6548\u679c\u5e76\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "Synapse\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2dLLM\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6311\u6218\uff0c\u652f\u6301\u5f02\u6784\u6570\u636e\u5e76\u91cf\u5316\u6027\u80fd\u6539\u8fdb\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5de5\u5177\u9009\u62e9\u6536\u655b\u3002"}}
{"id": "2602.00302", "categories": ["cs.LG", "cond-mat.dis-nn", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.00302", "abs": "https://arxiv.org/abs/2602.00302", "authors": ["Sam Reifenstein", "Timothee Leleu"], "title": "Neural Ising Machines via Unrolling and Zeroth-Order Training", "comment": null, "summary": "We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.", "AI": {"tldr": "\u63d0\u51faNPIM\uff08\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u4f0a\u8f9b\u673a\uff09\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u8fed\u4ee3\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u7528\u4e8eNP\u96be\u7684\u4f0a\u8f9b\u6a21\u578b\u548c\u6700\u5927\u5272\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9NP\u96be\u7684\u4f0a\u8f9b\u6a21\u578b\u548c\u6700\u5927\u5272\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u8fed\u4ee3\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u66f4\u65b0\u89c4\u5219\u6765\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51faNPIM\u65b9\u6cd5\uff1a1\uff09\u5b66\u4e60\u5171\u4eab\u7684\u8282\u70b9\u7ea7\u66f4\u65b0\u89c4\u5219\uff0c\u5c06\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u573a\u6620\u5c04\u5230\u81ea\u65cb\u66f4\u65b0\uff1b2\uff09\u4f7f\u7528\u7d27\u51d1\u7684\u591a\u5c42\u611f\u77e5\u673a\u53c2\u6570\u5316\uff0c\u53c2\u6570\u6570\u91cf\u5c11\uff1b3\uff09\u91c7\u7528\u96f6\u9636\u4f18\u5316\u5668\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u957f\u5faa\u73af\u4f0a\u8f9b\u673a\u52a8\u529b\u5b66\u4e2d\u7684\u68af\u5ea6\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "result": "NPIM\u80fd\u591f\u6062\u590d\u6709\u6548\u7684\u7b97\u6cd5\u7ed3\u6784\uff0c\u5305\u62ec\u52a8\u91cf\u5f0f\u884c\u4e3a\u548c\u65f6\u95f4\u53d8\u5316\u8c03\u5ea6\uff0c\u5728\u9ad8\u5ea6\u975e\u51f8\u7684\u80fd\u91cf\u666f\u89c2\u4e2d\u5b9e\u73b0\u9ad8\u6548\u641c\u7d22\u3002\u5728\u6807\u51c6\u4f0a\u8f9b\u548c\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNPIM\u5728\u89e3\u8d28\u91cf\u548c\u6c42\u89e3\u65f6\u95f4\u65b9\u9762\u4e0e\u6700\u65b0\u5b66\u4e60\u65b9\u6cd5\u548c\u7ecf\u5178\u4f0a\u8f9b\u673a\u542f\u53d1\u5f0f\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "NPIM\u4f5c\u4e3a\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u4f0a\u8f9b\u673a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u53c2\u6570\u5b66\u4e60\u6709\u6548\u7684\u52a8\u529b\u5b66\u66f4\u65b0\u89c4\u5219\uff0c\u5728NP\u96be\u4f18\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2602.00970", "categories": ["cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00970", "abs": "https://arxiv.org/abs/2602.00970", "authors": ["Saaduddin Mahmud", "Eugene Bagdasarian", "Shlomo Zilberstein"], "title": "Verification Required: The Impact of Information Credibility on AI Persuasion", "comment": "19 pages, 5 figures", "summary": "Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMixTalk\u6e38\u620f\u6846\u67b6\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u6982\u7387\u53ef\u4fe1\u5ea6\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u6027\u901a\u4fe1\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9526\u6807\u8d5b\u8bc4\u4f30\u73b0\u6709LLM\u8868\u73b0\uff0c\u5e76\u63d0\u51faTOPD\u65b9\u6cd5\u63d0\u5347\u63a5\u6536\u8005\u6297\u8bf4\u670d\u80fd\u529b\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u901a\u4fe1\u5f71\u54cd\u9ad8\u98ce\u9669\u51b3\u7b56\u7684\u573a\u666f\uff0c\u9700\u8981\u5bf9\u5176\u7b56\u7565\u6027\u901a\u4fe1\u6709\u539f\u5219\u6027\u7406\u89e3\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5ec9\u4ef7\u4ea4\u8c08\u6216\u5b8c\u5168\u53ef\u9a8c\u8bc1\u7684\u62ab\u9732\uff0c\u65e0\u6cd5\u6355\u6349\u4fe1\u606f\u5177\u6709\u6982\u7387\u53ef\u4fe1\u5ea6\u7684\u73b0\u5b9e\u9886\u57df\u3002", "method": "\u63d0\u51faMixTalk\u7b56\u7565\u901a\u4fe1\u6e38\u620f\u6846\u67b6\uff0c\u53d1\u9001\u8005\u7b56\u7565\u6027\u5730\u7ed3\u5408\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u58f0\u660e\u6765\u4f20\u9012\u79c1\u6709\u4fe1\u606f\uff0c\u63a5\u6536\u8005\u5206\u914d\u6709\u9650\u9884\u7b97\u8fdb\u884c\u6210\u672c\u9a8c\u8bc1\uff0c\u5e76\u6839\u636e\u5148\u9a8c\u4fe1\u5ff5\u3001\u58f0\u660e\u548c\u9a8c\u8bc1\u7ed3\u679c\u63a8\u65ad\u5e95\u5c42\u72b6\u6001\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u9526\u6807\u8d5b\u8bc4\u4f30\u6700\u5148\u8fdbLLM\u667a\u80fd\u4f53\uff0c\u5e76\u63d0\u51faTOPD\u79bb\u7ebf\u65b9\u6cd5\u4ece\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u63d0\u70bc\u9526\u6807\u8d5b\u9884\u8a00\u7b56\u7565\u5e76\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u4e0a\u4e0b\u6587\u90e8\u7f72\u3002", "result": "\u5728\u4e09\u4e2a\u73b0\u5b9e\u90e8\u7f72\u8bbe\u7f6e\u7684\u5927\u89c4\u6a21\u9526\u6807\u8d5b\u4e2d\u8bc4\u4f30\u4e86\u6700\u5148\u8fdbLLM\u667a\u80fd\u4f53\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u63a8\u7406\u4fe1\u606f\u53ef\u4fe1\u5ea6\u548c\u5851\u9020\u8fd9\u4e9b\u4ea4\u4e92\u7684\u663e\u5f0f\u884c\u4e3a\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002TOPD\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u6536\u8005\u5bf9\u8bf4\u670d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MixTalk\u6846\u67b6\u4e3a\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u6982\u7387\u53ef\u4fe1\u5ea6\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u6027\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0cTOPD\u65b9\u6cd5\u901a\u8fc7\u4ece\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u63d0\u70bc\u7b56\u7565\u77e5\u8bc6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u63a5\u6536\u8005\u667a\u80fd\u4f53\u7684\u6297\u8bf4\u670d\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2602.00350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00350", "abs": "https://arxiv.org/abs/2602.00350", "authors": ["Ignacy Kolton", "Kacper Marzol", "Pawe\u0142 Batorski", "Marcin Mazur", "Paul Swoboda", "Przemys\u0142aw Spurek"], "title": "ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models", "comment": null, "summary": "Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe", "AI": {"tldr": "ReLAPSe\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u6846\u67b6\uff0c\u7528\u4e8e\u6062\u590d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5df2\u9057\u5fd8\u6982\u5ff5\uff0c\u901a\u8fc7\u7b56\u7565\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u3001\u8fd1\u5b9e\u65f6\u7684\u6982\u5ff5\u6062\u590d", "motivation": "\u73b0\u6709\u5bf9\u6297\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff08\u9700\u8981\u9010\u5b9e\u4f8b\u8fed\u4ee3\u641c\u7d22\uff09\uff0c\u800c\u57fa\u4e8e\u63a8\u7406\u548c\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u7f3a\u4e4f\u76ee\u6807\u6a21\u578b\u6f5c\u5728\u89c6\u89c9\u8868\u793a\u7684\u76f4\u63a5\u53cd\u9988\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u4e25\u683c\u6d4b\u8bd5\u5df2\u9057\u5fd8\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faReLAPSe\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u6062\u590d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u566a\u58f0\u9884\u6d4b\u635f\u5931\u4f5c\u4e3a\u6a21\u578b\u5185\u5728\u4e14\u53ef\u9a8c\u8bc1\u7684\u53cd\u9988\u4fe1\u53f7\u3002\u8fd9\u79cd\u95ed\u73af\u8bbe\u8ba1\u4f7f\u6587\u672c\u63d0\u793a\u64cd\u4f5c\u4e0e\u6f5c\u5728\u89c6\u89c9\u6b8b\u5dee\u76f4\u63a5\u5bf9\u9f50\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u6062\u590d\u7b56\u7565\u800c\u975e\u4f18\u5316\u5b64\u7acb\u63d0\u793a\u3002", "result": "ReLAPSe\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8fd1\u5b9e\u65f6\u7684\u7ec6\u7c92\u5ea6\u8eab\u4efd\u548c\u98ce\u683c\u6062\u590d\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u65b9\u6cd5\u3002\u901a\u8fc7\u4ece\u9010\u5b9e\u4f8b\u4f18\u5316\u8f6c\u5411\u5168\u5c40\u7b56\u7565\u5b66\u4e60\uff0c\u4e3a\u4e25\u683c\u7ea2\u961f\u6d4b\u8bd5\u5df2\u9057\u5fd8\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u5de5\u5177\u3002", "conclusion": "ReLAPSe\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u6297\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6982\u5ff5\u6062\u590d\uff0c\u4e3a\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u9057\u5fd8\u673a\u5236\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4ece\u9010\u5b9e\u4f8b\u4f18\u5316\u5230\u5168\u5c40\u7b56\u7565\u5b66\u4e60\u7684\u8f6c\u53d8\u4f18\u52bf\u3002"}}
{"id": "2602.00924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00924", "abs": "https://arxiv.org/abs/2602.00924", "authors": ["Ouns El Harzli", "Hugo Wallner", "Yoonsoo Nam", "Haixuan Xavier Tao"], "title": "Supervised sparse auto-encoders as unconstrained feature models for semantic composition", "comment": null, "summary": "Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.", "AI": {"tldr": "\u63d0\u51fa\u76d1\u7763\u5f0f\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u4f20\u7edfSAE\u7684L1\u60e9\u7f5a\u975e\u5e73\u6ed1\u6027\u548c\u7279\u5f81-\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u5728Stable Diffusion 3.5\u4e0a\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u548c\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) L1\u60e9\u7f5a\u7684\u975e\u5e73\u6ed1\u6027\u963b\u788d\u4e86\u91cd\u6784\u548c\u53ef\u6269\u5c55\u6027\uff1b2) \u5b66\u4e60\u5230\u7684\u7279\u5f81\u4e0e\u4eba\u7c7b\u8bed\u4e49\u7f3a\u4e4f\u5bf9\u9f50", "method": "\u91c7\u7528\u65e0\u7ea6\u675f\u7279\u5f81\u6a21\u578b\uff08\u6765\u81ea\u795e\u7ecf\u574d\u7f29\u7406\u8bba\uff09\u5e76\u5f15\u5165\u76d1\u7763\u4efb\u52a1\uff0c\u8054\u5408\u5b66\u4e60\u7a00\u758f\u6982\u5ff5\u5d4c\u5165\u548c\u89e3\u7801\u5668\u6743\u91cd\uff0c\u76d1\u7763\uff08\u4ec5\u89e3\u7801\u5668\uff09SAE\u91cd\u6784\u7279\u5f81\u5411\u91cf", "result": "\u5728Stable Diffusion 3.5\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5c55\u793a\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u91cd\u6784\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u6982\u5ff5\u7ec4\u5408\u56fe\u50cf\uff0c\u652f\u6301\u65e0\u9700\u63d0\u793a\u4fee\u6539\u7684\u7279\u5f81\u7ea7\u5e72\u9884\u8fdb\u884c\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91", "conclusion": "\u901a\u8fc7\u76d1\u7763\u548c\u6570\u5b66\u6846\u67b6\u6539\u8fdb\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7279\u5f81\u8bed\u4e49\u5bf9\u9f50\u548c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.00315", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.00315", "abs": "https://arxiv.org/abs/2602.00315", "authors": ["Arian Khorasani", "Nathaniel Chen", "Yug D Oswal", "Akshat Santhana Gopalan", "Egemen Kolemen", "Ravid Shwartz-Ziv"], "title": "Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors", "comment": null, "summary": "How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.", "AI": {"tldr": "\u7814\u7a76\u8005\u4f7f\u7528\u7c7b\u522b\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u4f5c\u4e3a\u795e\u8c15\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u83b7\u5f97\u7cbe\u786e\u540e\u9a8c\u5206\u5e03\uff0c\u4ece\u800c\u80fd\u591f\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u6781\u9650\u3001\u7814\u7a76\u7f29\u653e\u89c4\u5f8b\u3001\u5b66\u4e60\u6781\u9650\u3001\u8f6f\u6807\u7b7e\u6548\u679c\u3001\u5206\u5e03\u504f\u79fb\u548c\u4e3b\u52a8\u5b66\u4e60\u3002", "motivation": "\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u4e0e\u7406\u8bba\u6700\u4f18\u6027\u80fd\u7684\u5dee\u8ddd\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u540e\u9a8c\u5206\u5e03p(y|x)\u7684\u8bbf\u95ee\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7cbe\u786e\u6d4b\u91cf\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u6781\u9650\u548c\u5269\u4f59\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u7c7b\u522b\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u4f5c\u4e3a\u795e\u8c15\uff0c\u5728AFHQ\u548cImageNet\u7b49\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u7cbe\u786e\u53ef\u5904\u7406\u7684\u540e\u9a8c\u5206\u5e03\u3002\u901a\u8fc7\u8fd9\u4e2a\u6846\u67b6\u8fdb\u884c\u4e94\u4e2a\u65b9\u9762\u7684\u7814\u7a76\uff1a\u7f29\u653e\u89c4\u5f8b\u5206\u6790\u3001\u5b66\u4e60\u6781\u9650\u6d4b\u91cf\u3001\u8f6f\u6807\u7b7e\u8bad\u7ec3\u3001\u5206\u5e03\u504f\u79fb\u91cf\u5316\u548c\u4e3b\u52a8\u5b66\u4e60\u8bc4\u4f30\u3002", "result": "1) \u9884\u6d4b\u8bef\u5dee\u53ef\u5206\u89e3\u4e3a\u4e0d\u53ef\u7ea6\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u53ef\u7ea6\u7684\u8ba4\u77e5\u8bef\u5dee\uff1b\u8ba4\u77e5\u8bef\u5dee\u968f\u6570\u636e\u96c6\u5927\u5c0f\u5448\u5e42\u5f8b\u4e0b\u964d\u30022) \u4e0d\u540c\u67b6\u6784\u63a5\u8fd1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u5f0f\u4e0d\u540c\uff1aResNets\u5448\u73b0\u5e72\u51c0\u7684\u5e42\u5f8b\u7f29\u653e\uff0c\u800cVision Transformers\u5728\u4f4e\u6570\u636e\u533a\u57df\u505c\u6ede\u30023) \u4f7f\u7528\u7cbe\u786e\u540e\u9a8c\u4f5c\u4e3a\u8f6f\u6807\u7b7e\u8bad\u7ec3\u4f18\u4e8e\u786c\u6807\u7b7e\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u6821\u51c6\u30024) \u5206\u5e03\u504f\u79fb\u7c7b\u578b\u6bd4\u5e45\u5ea6\u66f4\u91cd\u8981\u30025) \u7cbe\u786e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u53ef\u63d0\u9ad8\u4e3b\u52a8\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u6807\u51c6\u6307\u6807\u9690\u85cf\u4e86\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u3001\u63a9\u76d6\u4e86\u67b6\u6784\u5dee\u5f02\uff0c\u5e76\u4e14\u65e0\u6cd5\u8bca\u65ad\u5206\u5e03\u504f\u79fb\u7684\u672c\u8d28\u3002\u901a\u8fc7\u7cbe\u786e\u540e\u9a8c\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6027\u80fd\u6781\u9650\u3002"}}
{"id": "2602.00977", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00977", "abs": "https://arxiv.org/abs/2602.00977", "authors": ["Pengyue Yang", "Jiawen Wen", "Haolin Jin", "Linghan Huang", "Huaming Chen", "Ling Chen"], "title": "Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals", "comment": "Accepted at The ACM Web Conference 2026 (WWW 2026)", "summary": "Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.", "AI": {"tldr": "\u63d0\u51faStructural Confidence\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790LLM\u9690\u85cf\u72b6\u6001\u8f68\u8ff9\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u53f7\u6765\u589e\u5f3a\u8f93\u51fa\u6b63\u786e\u6027\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u548c\u8ba1\u7b97\u9650\u5236\u4e0b\u66f4\u7a33\u5065\u3002", "motivation": "LLM\u5728\u9519\u8bef\u6210\u672c\u9ad8\u7684\u9886\u57df\u90e8\u7f72\u589e\u591a\uff0c\u4f46\u4f20\u7edf\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982token\u4f3c\u7136\u5ea6\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u591a\u6837\u672c\u4e00\u81f4\u6027\uff09\u5728\u5206\u5e03\u504f\u79fb\u3001\u9886\u57df\u4e13\u4e1a\u6587\u672c\u548c\u8ba1\u7b97\u9650\u5236\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faStructural Confidence\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u6700\u540e\u4e00\u5c42\u9690\u85cf\u72b6\u6001\u8f68\u8ff9\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u53f7\uff0c\u7ed3\u5408\u8c31\u5206\u6790\u3001\u5c40\u90e8\u53d8\u5316\u548c\u5168\u5c40\u5f62\u72b6\u63cf\u8ff0\u7b26\uff0c\u6355\u6349\u6982\u7387\u548c\u53e5\u5b50\u5d4c\u5165\u5ffd\u7565\u7684\u5185\u90e8\u7a33\u5b9a\u6027\u6a21\u5f0f\u3002", "result": "\u5728\u56db\u4e2a\u5f02\u6784\u57fa\u51c6\u6d4b\u8bd5\uff08FEVER\u3001SciFact\u3001WikiBio-hallucination\u3001TruthfulQA\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8de8\u9886\u57df\u8bc4\u4f30\uff0c\u5728AUROC\u548cAUPR\u6307\u6807\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u5355\u6b21\u786e\u5b9a\u6027\u524d\u5411\u4f20\u64ad\uff0c\u65e0\u9700\u591a\u6b21\u968f\u673a\u751f\u6210\u6216\u8f85\u52a9\u6a21\u578b\u3002", "conclusion": "Structural Confidence\u4e3a\u8d44\u6e90\u53d7\u9650\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7a33\u5065\u7684\u4e8b\u540e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5b9e\u7528\u57fa\u7840\uff0c\u7279\u522b\u9002\u7528\u4e8e\u793e\u4f1a\u5f71\u54cd\u5927\u3001\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.00381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00381", "abs": "https://arxiv.org/abs/2602.00381", "authors": ["Kezia Minni", "Qiang Zhang", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Image-Caption Rating from Comparative Judgments", "comment": null, "summary": "Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $\u03c1$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u8bc4\u5206\u7684\u56fe\u50cf\u5b57\u5e55\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u5b57\u5e55\u54ea\u4e2a\u66f4\u597d\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u76f4\u63a5\u8bc4\u5206\u56fe\u50cf\u5b57\u5e55\u7684\u51c6\u786e\u6027\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u800c\u4eba\u7c7b\u66f4\u5bb9\u6613\u6bd4\u8f83\u4e24\u4e2a\u5b57\u5e55\u54ea\u4e2a\u66f4\u597d\u5339\u914d\u56fe\u50cf\uff0c\u56e0\u6b64\u5e0c\u671b\u5f00\u53d1\u80fd\u5efa\u6a21\u8fd9\u79cd\u6bd4\u8f83\u5224\u65ad\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u4f7f\u7528VICR\u6570\u636e\u96c6\uff0c\u7528ResNet-50\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0cMiniLM\u63d0\u53d6\u6587\u672c\u7279\u5f81\uff0c\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\u548c\u6bd4\u8f83\u5b66\u4e60\u6a21\u578b\u3002\u6bd4\u8f83\u5b66\u4e60\u6a21\u578b\u901a\u8fc7\u4eba\u7c7b\u5bf9\u5b57\u5e55\u5bf9\u7684\u6bd4\u8f83\u5224\u65ad\u6765\u5b66\u4e60\u3002", "result": "\u56de\u5f52\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff08Pearson's \u03c1: 0.7609, Spearman's rs: 0.7089\uff09\uff0c\u4f46\u6bd4\u8f83\u5b66\u4e60\u6a21\u578b\u968f\u6570\u636e\u589e\u52a0\u7a33\u6b65\u63d0\u5347\u5e76\u63a5\u8fd1\u56de\u5f52\u57fa\u7ebf\u3002\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6bd4\u8f83\u6807\u6ce8\u66f4\u5feb\u4e14\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u66f4\u9ad8\u3002", "conclusion": "\u6bd4\u8f83\u5b66\u4e60\u80fd\u6709\u6548\u5efa\u6a21\u4eba\u7c7b\u504f\u597d\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u4e3a\u56fe\u50cf\u5b57\u5e55\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.00929", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00929", "abs": "https://arxiv.org/abs/2602.00929", "authors": ["Zergham Ahmed", "Kazuki Irie", "Joshua B. Tenenbaum", "Christopher J. Bates", "Samuel J. Gershman"], "title": "Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents", "comment": "20 pages", "summary": "Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.", "AI": {"tldr": "TheoryCoder-2\u662f\u4e00\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u4e3b\u52a8\u5b66\u4e60\u53ef\u91cd\u7528\u62bd\u8c61\uff0c\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff08\u5982TheoryCoder\uff09\u867d\u7136\u901a\u8fc7\u4f7f\u7528\u62bd\u8c61\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u63d0\u4f9b\u7684\u62bd\u8c61\uff0c\u56de\u907f\u4e86\u62bd\u8c61\u5b66\u4e60\u95ee\u9898\u3002\u4eba\u7c7b\u80fd\u591f\u5b66\u4e60\u62bd\u8c61\u5e76\u7528\u5176\u9ad8\u6548\u89c4\u5212\uff0c\u800c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7406\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728\u8fd9\u65b9\u9762\u4ecd\u6709\u6311\u6218\u3002", "method": "TheoryCoder-2\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4ece\u7ecf\u9a8c\u4e2d\u4e3b\u52a8\u5408\u6210\u53ef\u91cd\u7528\u62bd\u8c61\uff0c\u5e76\u5c06\u8fd9\u4e9b\u62bd\u8c61\u6574\u5408\u5230\u5206\u5c42\u89c4\u5212\u8fc7\u7a0b\u4e2d\u3002\u5b83\u901a\u8fc7\u5408\u6210\u62bd\u8c61\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u7684\u62bd\u8c61\u5b66\u4e60\u3002", "result": "\u5728BabyAI\u3001Minihack\u548cVGDL\u6e38\u620f\uff08\u5982Sokoban\uff09\u7b49\u591a\u6837\u5316\u73af\u5883\u4e2d\uff0cTheoryCoder-2\u6bd4\u57fa\u7ebfLLM\u4ee3\u7406\uff08\u589e\u5f3a\u7ecf\u5178\u89c4\u5212\u57df\u6784\u5efa\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u89c4\u5212\uff09\u4ee5\u53ca\u5148\u524d\u7684\u7a0b\u5e8f\u5408\u6210\u4ee3\u7406\uff08\u5982WorldCoder\uff09\u663e\u8457\u66f4\u6837\u672c\u9ad8\u6548\u3002\u5b83\u80fd\u591f\u89e3\u51b3\u57fa\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742\u4efb\u52a1\uff0c\u540c\u65f6\u53ea\u9700\u8981\u6700\u5c11\u7684\u4eba\u5de5\u63d0\u793a\u3002", "conclusion": "TheoryCoder-2\u901a\u8fc7\u5229\u7528LLM\u4e3b\u52a8\u5b66\u4e60\u53ef\u91cd\u7528\u62bd\u8c61\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5c0f\u5316\u7684\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002"}}
{"id": "2602.00318", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00318", "abs": "https://arxiv.org/abs/2602.00318", "authors": ["Kunal Mukherjee", "Zulfikar Alom", "Tran Gia Bao Ngo", "Cuneyt Gurcan Akcora", "Murat Kantarcioglu"], "title": "Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection", "comment": null, "summary": "The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.\n  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.", "AI": {"tldr": "BOCLOAK\uff1a\u4e00\u79cd\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\u8bc4\u4f30GNN\u793e\u4ea4\u673a\u5668\u4eba\u68c0\u6d4b\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u653b\u51fb\u6846\u67b6", "motivation": "\u5f53\u524dGNN\u673a\u5668\u4eba\u68c0\u6d4b\u5668\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u672a\u77e5\uff0c\u653b\u51fb\u8005\u9762\u4e34\u9886\u57df\u7279\u5b9a\u548c\u65f6\u95f4\u7ea6\u675f\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u9002\u7528\u6027\u6709\u9650\uff0c\u9700\u8981\u8bc4\u4f30\u7ea6\u675f\u611f\u77e5\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027", "method": "BOCLOAK\u6784\u5efa\u65f6\u7a7a\u90bb\u5c45\u7279\u5f81\u7684\u6982\u7387\u5ea6\u91cf\uff0c\u5b66\u4e60\u5206\u79bb\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u884c\u4e3a\u7684\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\uff0c\u5c06\u4f20\u8f93\u8ba1\u5212\u89e3\u7801\u4e3a\u7a00\u758f\u3001\u5408\u7406\u7684\u8fb9\u7f16\u8f91\uff0c\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u89c4\u907f\u68c0\u6d4b", "result": "\u5728\u4e09\u4e2a\u793e\u4ea4\u673a\u5668\u4eba\u6570\u636e\u96c6\u3001\u4e94\u4e2aSOTA\u68c0\u6d4b\u5668\u3001\u4e09\u4e2a\u5bf9\u6297\u9632\u5fa1\u548c\u56db\u4e2a\u57fa\u7ebf\u653b\u51fb\u4e0a\u8bc4\u4f30\uff0cBOCLOAK\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad880.13%\uff0cGPU\u5185\u5b58\u51cf\u5c1199.80%", "conclusion": "\u6700\u4f18\u4f20\u8f93\u4e3a\u8fde\u63a5\u5bf9\u6297\u653b\u51fb\u548c\u73b0\u5b9e\u673a\u5668\u4eba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u539f\u5219\u6027\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u5b9e\u7ea6\u675f\u4e0bGNN\u68c0\u6d4b\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u7a7a\u767d"}}
{"id": "2602.00981", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00981", "abs": "https://arxiv.org/abs/2602.00981", "authors": ["Yutong Song", "Shiva Shrestha", "Chenhan Lyu", "Elahe Khatibi", "Pengfei Zhang", "Honghui Xu", "Nikil Dutt", "Amir Rahmani"], "title": "MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA", "comment": null, "summary": "Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.", "AI": {"tldr": "MedSpeak\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684ASR\u9519\u8bef\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u8bed\u97f3\u4fe1\u606f\uff0c\u4ee5\u53caLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u672f\u8bed\u8bc6\u522b\u51c6\u786e\u6027\u548c\u533b\u5b66\u53e3\u8bed\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u53e3\u8bed\u95ee\u7b54\u7cfb\u7edf\u5728\u533b\u5b66\u672f\u8bed\u8bc6\u522b\u4e0a\u5b58\u5728\u51c6\u786e\u6027\u95ee\u9898\uff0c\u8fd9\u5f71\u54cd\u4e86\u533b\u5b66SQA\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51faMedSpeak\u6846\u67b6\uff0c\u5229\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u8bed\u97f3\u4fe1\u606f\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bf9\u566a\u58f0\u8f6c\u5f55\u8fdb\u884c\u6821\u6b63\uff0c\u5e76\u6539\u8fdb\u4e0b\u6e38\u7b54\u6848\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMedSpeak\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u672f\u8bed\u8bc6\u522b\u51c6\u786e\u6027\u548c\u6574\u4f53\u533b\u5b66SQA\u6027\u80fd\uff0c\u6210\u4e3a\u533b\u5b66SQA\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MedSpeak\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8f85\u52a9\u7684ASR\u9519\u8bef\u6821\u6b63\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66SQA\u4e2d\u7684\u672f\u8bed\u8bc6\u522b\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u53e3\u8bed\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00385", "abs": "https://arxiv.org/abs/2602.00385", "authors": ["Bsher Karbouj", "Adam Michael Altenbuchner", "Joerg Krueger"], "title": "Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects", "comment": null, "summary": "Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9YOLOv5\u548cFaster R-CNN\u5728\u81ea\u52a8\u9a7e\u9a76\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\u5206\u6790\uff0c\u53d1\u73b0YOLOv5\u5728mAP\u3001\u53ec\u56de\u7387\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u800cFaster R-CNN\u5728\u5c0f\u7269\u4f53\u68c0\u6d4b\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u6709\u4f18\u52bf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982YOLO\u3001SSD\u3001Faster R-CNN\uff09\u5728\u7279\u5b9a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u6307\u5bfc\u6709\u9650\u3002\u65b9\u6cd5\u9009\u62e9\u4f1a\u5f71\u54cd\u68c0\u6d4b\u7cbe\u5ea6\u3001\u5904\u7406\u901f\u5ea6\u3001\u73af\u5883\u9c81\u68d2\u6027\u7b49\u591a\u4e2a\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "method": "\u91c7\u7528\u7efc\u5408\u5b9e\u9a8c\u5206\u6790\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e24\u79cd\u4e3b\u6d41\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\uff1a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668YOLOv5\u548c\u4e24\u9636\u6bb5\u68c0\u6d4b\u5668Faster R-CNN\u3002\u5728\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528mAP\u3001\u53ec\u56de\u7387\u548c\u63a8\u7406\u901f\u5ea6\u7b49\u591a\u9879\u6307\u6807\u3002", "result": "YOLOv5\u5728mAP\u3001\u53ec\u56de\u7387\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u96c6\u89c4\u6a21\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u589e\u52a0\u65f6\u3002Faster R-CNN\u5728\u68c0\u6d4b\u5c0f\u578b\u8fdc\u8ddd\u79bb\u7269\u4f53\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u597d\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e0d\u540c\u7f6e\u4fe1\u5ea6\u9608\u503c\u548c\u5b9e\u9645\u573a\u666f\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "\u4e24\u79cd\u6a21\u578b\u5404\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u573a\u666f\u3002YOLOv5\u66f4\u9002\u5408\u9700\u8981\u9ad8\u6548\u7387\u548c\u5b9e\u65f6\u6027\u80fd\u7684\u5e94\u7528\uff0c\u800cFaster R-CNN\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u5c0f\u7269\u4f53\u548c\u5e94\u5bf9\u590d\u6742\u73af\u5883\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u66f4\u597d\u3002\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9009\u62e9\u5408\u9002\u7684\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.00947", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00947", "abs": "https://arxiv.org/abs/2602.00947", "authors": ["Mohan Reddy"], "title": "The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis", "comment": null, "summary": "Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.", "AI": {"tldr": "\u804a\u5929\u754c\u9762\u4e0d\u9002\u5408\u591a\u6b65\u9aa4\u3001\u72b6\u6001\u4f9d\u8d56\u7684\u6570\u636e\u5206\u6790\u4efb\u52a1\uff0c\u56e0\u4e3a\u4f1a\u9020\u6210\u8ba4\u77e5\u8fc7\u8f7d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e868\u79cd\u6df7\u5408\u8bbe\u8ba1\u6a21\u5f0f\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898", "motivation": "\u5f53\u524dAI\u8f85\u52a9\u6570\u636e\u5206\u6790\u666e\u904d\u91c7\u7528\u804a\u5929\u754c\u9762\uff0c\u4f46\u5bf9\u4e8e\u591a\u6b65\u9aa4\u3001\u72b6\u6001\u4f9d\u8d56\u7684\u5206\u6790\u4efb\u52a1\uff0c\u8fd9\u79cd\u754c\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\u3002\u4f5c\u8005\u57fa\u4e8eWoods(1984)\u7684\"\u9501\u773c\u6548\u5e94\"\u7406\u8bba\uff0c\u6307\u51fa\u804a\u5929\u754c\u9762\u4f1a\u901a\u8fc7\u4e94\u79cd\u673a\u5236\u7cfb\u7edf\u6027\u964d\u4f4e\u5206\u6790\u6027\u80fd", "method": "\u4f5c\u8005\u9996\u5148\u5206\u6790\u4e86\u804a\u5929\u754c\u9762\u9020\u6210\u8ba4\u77e5\u8fc7\u8f7d\u7684\u4e94\u79cd\u673a\u5236\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u8ba4\u77e5\u8fc7\u8f7d\u7684\u91cf\u5316\u516c\u5f0fO = max(0, m - v - W)\uff0c\u6700\u540e\u63d0\u51fa\u4e868\u79cd\u6df7\u5408\u8bbe\u8ba1\u6a21\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "result": "\u804a\u5929\u754c\u9762\u4f1a\u5bfc\u81f4\uff1a1)\u5185\u5bb9\u4e0d\u65ad\u66ff\u6362\u7834\u574f\u6d77\u9a6c\u4f53\u7a7a\u95f4\u8bb0\u5fc6\uff1b2)\u9690\u85cf\u72b6\u6001\u53d8\u91cf\u8d85\u51fa\u5de5\u4f5c\u8bb0\u5fc6\u5bb9\u91cf\uff1b3)\u5f3a\u5236\u8a00\u8bed\u5316\u964d\u4f4e\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\uff1b4)\u7ebf\u6027\u6587\u672c\u6d41\u963b\u788d\u8ba4\u77e5\u5378\u8f7d\uff1b5)\u5e8f\u5217\u5316\u60e9\u7f5a\u968f\u6570\u636e\u7ef4\u5ea6\u589e\u52a0\u3002\u4f5c\u8005\u63d0\u51fa\u76848\u79cd\u8bbe\u8ba1\u6a21\u5f0f\u9488\u5bf9\u8fd9\u4e9b\u8ba4\u77e5\u74f6\u9888", "conclusion": "\u804a\u5929\u754c\u9762\u4e0d\u9002\u5408\u5f00\u653e\u5f0f\u7684\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\uff0c\u4f46\u7ed3\u5408\u4e13\u5bb6\u5148\u9a8c\u77e5\u8bc6\u7684\u826f\u597d\u811a\u624b\u67b6\u5bf9\u8bdd\u7cfb\u7edf\u53ef\u80fd\u9002\u7528\u4e8e\u6307\u5bfc\u6027\u4efb\u52a1\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u53ef\u8bc1\u4f2a\u7684\u5047\u8bbe\u548c\u5b9e\u9a8c\u8303\u5f0f\u4f9b\u5b9e\u8bc1\u9a8c\u8bc1"}}
{"id": "2602.00328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00328", "abs": "https://arxiv.org/abs/2602.00328", "authors": ["Nikhil Gopal", "Kostis Kaffes"], "title": "Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference", "comment": null, "summary": "Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.", "AI": {"tldr": "Harvest\u662f\u4e00\u4e2a\u5229\u7528GPU\u95f4\u9ad8\u901f\u4e92\u8fde\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u6743\u91cd\u548cKV\u7f13\u5b58\u52a8\u6001\u653e\u7f6e\u5728\u7a7a\u95f2GPU\u5185\u5b58\u4e2d\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "LLM\u63a8\u7406\u8d8a\u6765\u8d8a\u53d7\u9650\u4e8eGPU\u5185\u5b58\u5bb9\u91cf\u800c\u975e\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u548c\u81ea\u56de\u5f52\u89e3\u7801\u4e2dKV\u7f13\u5b58\u7684\u7ebf\u6027\u589e\u957f\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u6a21\u578b\u72b6\u6001\u548cKV\u5f20\u91cf\u5378\u8f7d\u5230\u4e3b\u673a\u5185\u5b58\uff0c\u4f46\u53d7\u9650\u4e8ePCIe\u5e26\u5bbd\u5bfc\u81f4\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\u3002", "method": "Harvest\u6846\u67b6\u5229\u7528GPU\u95f4\u9ad8\u901f\u5bf9\u7b49\u4e92\u8fde\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u548cKV\u7f13\u5b58\u52a8\u6001\u653e\u7f6e\u5728\u672a\u4f7f\u7528\u7684GPU\u5185\u5b58\u4e2d\uff0c\u5c06\u5176\u4ed6GPU\u5185\u5b58\u89c6\u4e3a\u4e34\u65f6\u7f13\u5b58\u5c42\uff0c\u5728\u52a8\u6001\u5185\u5b58\u53ef\u7528\u6027\u4e0b\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5f00\u9500\u3002", "result": "\u901a\u8fc7Harvest\u52a0\u901f\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406\u7ec4\u4ef6\uff08\u4e13\u5bb6\u5c42\u6743\u91cd\u548cKV\u7f13\u5b58\u6761\u76ee\uff09\u7684\u68c0\u7d22\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "Harvest\u901a\u8fc7\u5229\u7528GPU\u95f4\u9ad8\u901f\u4e92\u8fde\u548c\u52a8\u6001\u5185\u5b58\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00983", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00983", "abs": "https://arxiv.org/abs/2602.00983", "authors": ["Batuhan K. Karaman", "Aditya Rawal", "Suhaila Shakiah", "Mohammad Ghavamzadeh", "Mingyi Hong", "Arijit Biswas", "Ruida Zhou"], "title": "DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning", "comment": "This work is accepted to the 29th International Conference on Artificial Intelligence and Statistics (AISTATS) 2026", "summary": "Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.", "AI": {"tldr": "DISPO\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u6b63\u786e\u548c\u9519\u8bef\u54cd\u5e94\u7684\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u4e0a\u4e0b\u88c1\u526a\uff0c\u5b9e\u73b0\u56db\u4e2a\u53ef\u63a7\u7b56\u7565\u66f4\u65b0\u673a\u5236\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e2d\u5b58\u5728\u6743\u8861\uff1aPPO\u7c7b\u65b9\u6cd5\uff08\u5982GRPO/DAPO\uff09\u8bad\u7ec3\u7a33\u5b9a\u4f46\u5b66\u4e60\u6162\uff0cREINFORCE\u7c7b\u65b9\u6cd5\uff08\u5982CISPO\uff09\u5b66\u4e60\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u53c8\u80fd\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "DISPO\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684REINFORCE\u98ce\u683c\u7b97\u6cd5\uff0c\u5c06\u6b63\u786e\u548c\u9519\u8bef\u54cd\u5e94\u7684\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u4e0a\u4e0b\u88c1\u526a\u5206\u79bb\uff0c\u5f62\u6210\u56db\u4e2a\u53ef\u63a7\u7684\u7b56\u7565\u66f4\u65b0\u673a\u5236\u3002\u901a\u8fc7\u9488\u5bf9\u6027\u6d88\u878d\u7814\u7a76\u63ed\u793a\u6bcf\u4e2a\u673a\u5236\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u522b\u8c03\u6574\u8fd9\u56db\u4e2a\u88c1\u526a\u53c2\u6570\u6765\u7ef4\u6301\u63a2\u7d22-\u84b8\u998f\u5e73\u8861\uff0c\u9632\u6b62\u707e\u96be\u6027\u5931\u8d25\u3002", "result": "DISPO\u5728AIME'24\u4e0a\u8fbe\u523061.04%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eCISPO\u768455.42%\u548cDAPO\u768450.21%\u3002\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u90fd\u663e\u793a\u51fa\u7c7b\u4f3c\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DISPO\u901a\u8fc7\u89e3\u8026\u91cd\u8981\u6027\u91c7\u6837\u6743\u91cd\u7684\u88c1\u526a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u5b66\u4e60\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00391", "abs": "https://arxiv.org/abs/2602.00391", "authors": ["Alberto Mario Ceballos-Arroyo", "Shrikanth M. Yadav", "Chu-Hsuan Lin", "Jisoo Kim", "Geoffrey S. Young", "Huaizu Jiang", "Lei Qin"], "title": "Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data", "comment": "16 pages, 8 figures", "summary": "In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u60014D-CTA\u7684\u8111\u8840\u7ba1\u6807\u6ce8\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u65f6\u76f8\u6570\u636e\u589e\u5f3a\u8840\u7ba1\u53ef\u89c6\u5316\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8840\u7ba1\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8111\u8840\u7ba1\u6807\u6ce8\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u52a8\u6001CTA\u6570\u636e\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u81ea\u52a8\u5316\u65b9\u6cd5\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\uff0c\u5e76\u5229\u7528\u591a\u65f6\u76f8\u6570\u636e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u60014D-CTA\u591a\u65f6\u76f8\u6570\u636e\uff0c\u901a\u8fc7\u51cf\u5f71\u6280\u672f\u53bb\u9664\u9aa8\u9abc\u548c\u8f6f\u7ec4\u7ec7\u4ee5\u589e\u5f3a\u8840\u7ba1\u53ef\u89c6\u5316\uff1b\u5229\u7528\u540c\u4e00\u5206\u5272\u6807\u6ce8\u5e94\u7528\u4e8e\u591a\u4e2a\u65f6\u76f8\uff0c\u5c06\u6570\u636e\u96c6\u6269\u59274-5\u500d\uff1b\u8bad\u7ec3nnUNet\u6a21\u578b\u8fdb\u884c\u8840\u7ba1\u5206\u5272\u3002", "result": "\u5728110\u5f20\u8bad\u7ec3\u56fe\u50cf\u548c165\u5f20\u6d4b\u8bd5\u56fe\u50cf\u4e0a\uff0c\u6a21\u578b\u5728TopBrain\u6570\u636e\u96c6\u4e0a\u52a8\u8109\u5e73\u5747mDC\u8fbe0.846\uff0c\u9759\u8109\u8fbe0.957\uff1b\u8bef\u5dee\u6307\u6807(aDHD)\u52a8\u81090.304mm\u3001\u9759\u81090.078mm\uff1b\u62d3\u6251\u654f\u611f\u6027(tSens)\u52a8\u81090.877\u3001\u9759\u81090.974\uff0c\u5747\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001CTA\u8840\u7ba1\u6807\u6ce8\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u901a\u8fc7\u591a\u65f6\u76f8\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\u96c6\u5e76\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728\u8111\u52a8\u8109\u548c\u9759\u8109\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00950", "abs": "https://arxiv.org/abs/2602.00950", "authors": ["Ant\u00f3nio Farinhas", "Nuno M. Guerreiro", "Jos\u00e9 Pombal", "Pedro Henrique Martins", "Laura Melton", "Alex Conway", "Cara Dochat", "Maya D'Eon", "Ricardo Rei"], "title": "MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support", "comment": null, "summary": "Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.", "AI": {"tldr": "MindGuard\uff1a\u57fa\u4e8e\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u5b66\u7684\u8f7b\u91cf\u7ea7\u5b89\u5168\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u533a\u5206\u6cbb\u7597\u6027\u62ab\u9732\u4e0e\u771f\u5b9e\u4e34\u5e8a\u5371\u673a\uff0c\u51cf\u5c11\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2dLLM\u7684\u5b89\u5168\u8bef\u5224", "motivation": "\u73b0\u6709\u901a\u7528\u5b89\u5168\u673a\u5236\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6cbb\u7597\u6027\u62ab\u9732\u4e0e\u771f\u5b9e\u4e34\u5e8a\u5371\u673a\uff0c\u5bfc\u81f4\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5931\u8d25\u3002\u9700\u8981\u4e34\u5e8a\u57fa\u7840\u7684\u98ce\u9669\u5206\u7c7b\u6765\u786e\u4fdd\u5b89\u5168\u540c\u65f6\u4fdd\u7559\u6cbb\u7597\u6027\u5185\u5bb9\u7a7a\u95f4\u3002", "method": "1. \u4e0e\u5fc3\u7406\u5b66\u535a\u58eb\u5408\u4f5c\u5f00\u53d1\u4e34\u5e8a\u98ce\u9669\u5206\u7c7b\u5b66\uff1b2. \u53d1\u5e03MindGuard-testset\u771f\u5b9e\u4e16\u754c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff1b3. \u901a\u8fc7\u53d7\u63a7\u53cc\u4ee3\u7406\u8bbe\u7f6e\u751f\u6210\u5408\u6210\u5bf9\u8bdd\uff1b4. \u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5b89\u5168\u5206\u7c7b\u5668\uff084B\u548c8B\u53c2\u6570\uff09", "result": "MindGuard\u5206\u7c7b\u5668\u5728\u9ad8\u53ec\u56de\u7387\u64cd\u4f5c\u70b9\u51cf\u5c11\u8bef\u62a5\uff0c\u4e0e\u4e34\u5e8a\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u65f6\uff0c\u5728\u5bf9\u6297\u6027\u591a\u8f6e\u4e92\u52a8\u4e2d\u5b9e\u73b0\u66f4\u4f4e\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u6709\u5bb3\u53c2\u4e0e\u7387\uff0c\u4f18\u4e8e\u901a\u7528\u5b89\u5168\u673a\u5236", "conclusion": "MindGuard\u63d0\u4f9b\u4e86\u4e34\u5e8a\u57fa\u7840\u7684\u5b89\u5168\u6846\u67b6\uff0c\u80fd\u6709\u6548\u533a\u5206\u6cbb\u7597\u6027\u5185\u5bb9\u4e0e\u771f\u5b9e\u5371\u673a\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u4e2d\u7684LLM\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u6a21\u578b\u548c\u8bc4\u4f30\u6570\u636e\u5df2\u5f00\u6e90"}}
{"id": "2602.00329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00329", "abs": "https://arxiv.org/abs/2602.00329", "authors": ["Meng Ding", "Zeqing Zhang", "Di Wang", "Lijie Hu"], "title": "In-Run Data Shapley for Adam Optimizer", "comment": "16 pages", "summary": "Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent \"In-Run\" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \\approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\\sim$95\\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdam-Aware In-Run Data Shapley\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5728Adam\u4f18\u5316\u5668\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u6570\u636e\u8d21\u732e\u8bc4\u4f30\u3002", "motivation": "\u53ef\u9760\u7684\u6570\u636e\u5f52\u56e0\u5bf9\u4e8e\u51cf\u8f7b\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u504f\u89c1\u548c\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\"In-Run\"\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56SGD\u7684\u7ebf\u6027\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349Adam\u7b49\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u590d\u6742\u52a8\u6001\u3002\u7814\u7a76\u8868\u660e\u6570\u636e\u5f52\u56e0\u672c\u8d28\u4e0a\u4f9d\u8d56\u4e8e\u4f18\u5316\u5668\uff0cSGD-based\u4ee3\u7406\u5728Adam\u4e0b\u4e0e\u771f\u5b9e\u8d21\u732e\u663e\u8457\u504f\u79bb\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\u3002", "method": "\u63d0\u51faAdam-Aware In-Run Data Shapley\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u56fa\u5b9a\u72b6\u6001\u5047\u8bbe\u91cd\u65b0\u5b9a\u4e49\u6548\u7528\uff0c\u6062\u590d\u53ef\u52a0\u6027\uff1b2) \u63d0\u51fa\u7ebf\u6027\u5316\u5e7d\u7075\u8fd1\u4f3c\u6280\u672f\uff0c\u7ebf\u6027\u5316\u65b9\u5dee\u4f9d\u8d56\u7684\u7f29\u653e\u9879\uff0c\u65e0\u9700\u751f\u6210\u6bcf\u6837\u672c\u68af\u5ea6\u5373\u53ef\u8ba1\u7b97\u68af\u5ea6\u70b9\u79ef\u5bf9\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u65b9\u6cd5\u5bf9\u771f\u5b9e\u8fb9\u9645\u8d21\u732e\u7684\u4fdd\u771f\u5ea6\u63a5\u8fd1\u5b8c\u7f8e\uff08Pearson R > 0.99\uff09\uff1b2) \u4fdd\u6301\u7ea695%\u7684\u6807\u51c6\u8bad\u7ec3\u541e\u5410\u91cf\uff1b3) \u5728\u6570\u636e\u5f52\u56e0\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eSGD-based\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6570\u636e\u5f52\u56e0\u5177\u6709\u4f18\u5316\u5668\u4f9d\u8d56\u6027\uff0cSGD-based\u65b9\u6cd5\u5728Adam\u4f18\u5316\u5668\u4e0b\u65e0\u6548\u3002\u63d0\u51fa\u7684Adam-aware\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5730\u8bc4\u4f30\u6570\u636e\u8d21\u732e\uff0c\u4e3a\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6570\u636e\u5f52\u56e0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00986", "abs": "https://arxiv.org/abs/2602.00986", "authors": ["Guowei Xu", "Mert Yuksekgonul", "James Zou"], "title": "Sparse Reward Subsystem in Large Language Models", "comment": null, "summary": "In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.", "AI": {"tldr": "\u8bba\u6587\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2a\u7a00\u758f\u5956\u52b1\u5b50\u7cfb\u7edf\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u8111\u4e2d\u7684\u751f\u7269\u5956\u52b1\u7cfb\u7edf\uff0c\u5305\u542b\u4ee3\u8868\u72b6\u6001\u4ef7\u503c\u671f\u671b\u7684\"\u4ef7\u503c\u795e\u7ecf\u5143\"\u548c\u7f16\u7801\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\u7684\"\u591a\u5df4\u80fa\u795e\u7ecf\u5143\"\u3002", "motivation": "\u53d7\u751f\u7269\u5927\u8111\u4e2d\u5956\u52b1\u5b50\u7cfb\u7edf\u7684\u542f\u53d1\uff0c\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u662f\u5426\u4e5f\u5b58\u5728\u7c7b\u4f3c\u7684\u5956\u52b1\u5904\u7406\u673a\u5236\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5982\u4f55\u8fdb\u884c\u63a8\u7406\u548c\u4ef7\u503c\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5e72\u9884\u5b9e\u9a8c\u8bc6\u522b\u548c\u5206\u6790LLM\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u4ef7\u503c\u795e\u7ecf\u5143\uff0c\u7814\u7a76\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u7f16\u7801\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\u7684\u591a\u5df4\u80fa\u795e\u7ecf\u5143\u3002", "result": "\u53d1\u73b0\u4ef7\u503c\u795e\u7ecf\u5143\u5728\u63a8\u7406\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u5177\u6709\u8de8\u6570\u636e\u96c6\u3001\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u5728\u540c\u4e00\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u7684\u4e0d\u540c\u6a21\u578b\u95f4\u5177\u6709\u663e\u8457\u53ef\u8fc1\u79fb\u6027\uff1b\u8bc6\u522b\u51fa\u7f16\u7801\u5956\u52b1\u9884\u6d4b\u8bef\u5dee\u7684\u591a\u5df4\u80fa\u795e\u7ecf\u5143\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7c7b\u4f3c\u4e8e\u751f\u7269\u5927\u8111\u7684\u5956\u52b1\u5b50\u7cfb\u7edf\uff0c\u5305\u542b\u4ef7\u503c\u795e\u7ecf\u5143\u548c\u591a\u5df4\u80fa\u795e\u7ecf\u5143\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7406\u89e3LLM\u7684\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.00393", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00393", "abs": "https://arxiv.org/abs/2602.00393", "authors": ["Gabriel Bromonschenkel", "Alessandro L. Koerich", "Thiago M. Paix\u00e3o", "Hil\u00e1rio Tomaz Alves de Oliveira"], "title": "Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset", "comment": "Accepted to JBCS. 18 pages, 11 figures", "summary": "Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eTransformer\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5df4\u897f\u8461\u8404\u7259\u8bed\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e86\u539f\u751f\u4eba\u5de5\u6807\u6ce8\u548c\u81ea\u52a8\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u53d1\u73b0Swin-DistilBERTimbau\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cViTucano\u5728\u6587\u672c\u6307\u6807\u4e0a\u4f18\u4e8e\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\uff0cGPT-4\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\u6a21\u578b\uff0c\u800c\u5df4\u897f\u8461\u8404\u7259\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\u548c\u6a21\u578b\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u81ea\u52a8\u7ffb\u8bd1\u73b0\u6709\u6570\u636e\u96c6\u6765\u7f13\u89e3\u8d44\u6e90\u7a00\u7f3a\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u5f71\u54cd\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5305\u542b\u5df4\u897f\u8461\u8404\u7259\u8bed\u6bcd\u8bed\u8005\u4eba\u5de5\u521b\u5efa\u7684Flickr30K\u7248\u672c\uff0c\u4e0e\u81ea\u52a8\u7ffb\u8bd1\u7248\u672c\u8fdb\u884c\u6bd4\u8f83\u3002\u91c7\u7528\u8de8\u4e0a\u4e0b\u6587\u65b9\u6cd5\uff0c\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u5e76\u5728\u53e6\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u7ffb\u8bd1\u5f71\u54cd\u3002\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u8fdb\u884c\u6a21\u578b\u63a8\u7406\u89e3\u91ca\uff0c\u5e76\u4f7f\u7528CLIP-Score\u8bc4\u4f30\u56fe\u50cf-\u63cf\u8ff0\u5bf9\u9f50\u3002", "result": "Swin-DistilBERTimbau\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002\u5df4\u897f\u8461\u8404\u7259\u8bed\u9884\u8bad\u7ec3\u6a21\u578bViTucano\u5728\u4f20\u7edf\u6587\u672c\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u548cLLaMa 3.2 Vision\u7b49\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u800cGPT-4\u6a21\u578b\u5728CLIP-Score\u4e0a\u5f97\u5206\u6700\u9ad8\uff0c\u8868\u660e\u5176\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u80fd\u529b\u66f4\u5f3a\u3002\u6ce8\u610f\u529b\u5206\u6790\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5305\u62ec\u6027\u522b\u8bef\u5206\u7c7b\u3001\u5bf9\u8c61\u679a\u4e3e\u9519\u8bef\u548c\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u5df4\u897f\u8461\u8404\u7259\u8bed\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u539f\u751f\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4e13\u95e8\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6587\u672c\u6307\u6807\u4e0a\u7684\u4f18\u52bf\uff0c\u800c\u5927\u578b\u591a\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u6a21\u578b\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2602.00951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00951", "abs": "https://arxiv.org/abs/2602.00951", "authors": ["Hector Munoz-Avila", "David W. Aha", "Paola Rizzo"], "title": "R-HTN: Rebellious Online HTN Planning for Safety and Game AI", "comment": null, "summary": "We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \\D. Like other agents that are capable of rebellion (i.e., {\\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \\D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \\D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \\D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.", "AI": {"tldr": "\u63d0\u51fa\u5728\u7ebf\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\uff08HTN\uff09\u667a\u80fd\u4f53R-HTN\uff0c\u80fd\u591f\u5728\u8fdd\u53cd\u5185\u7f6e\u6307\u4ee4\u65f6\u62d2\u7edd\u6267\u884c\u7528\u6237\u4efb\u52a1\u6216\u81ea\u9002\u5e94\u4fee\u6539\u8ba1\u5212\uff0c\u786e\u4fdd\u4e0d\u8fdd\u53cd\u5b89\u5168\u6216\u4e2a\u6027\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u901a\u5e38\u76f2\u76ee\u6267\u884c\u7528\u6237\u6307\u4ee4\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u667a\u80fd\u4f53\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u62d2\u7edd\u6267\u884c\u6216\u8c03\u6574\u884c\u4e3a\uff0c\u4ee5\u907f\u514d\u8fdd\u53cd\u5b89\u5168\u89c4\u5219\u6216\u4e2a\u6027\u7ea6\u675f\u3002", "method": "\u7ed3\u5408HTN\u89c4\u5212\u3001\u5728\u7ebf\u89c4\u5212\u548c\u5185\u7f6e\u6307\u4ee4\u96c6D\uff0c\u63d0\u51faR-HTN\u7b97\u6cd5\u3002\u7814\u7a76\u4e24\u79cd\u53d8\u4f53\uff1a\u975e\u81ea\u9002\u5e94\u667a\u80fd\u4f53\uff08\u8fdd\u53cd\u6307\u4ee4\u65f6\u505c\u6b62\u6267\u884c\uff09\u548c\u81ea\u9002\u5e94\u667a\u80fd\u4f53\uff08\u8fdd\u53cd\u6307\u4ee4\u65f6\u4fee\u6539HTN\u8ba1\u5212\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\uff09\u3002", "result": "R-HTN\u667a\u80fd\u4f53\u5728\u6d4b\u8bd5\u4e2d\u4ece\u672a\u8fdd\u53cd\u6307\u4ee4\uff0c\u5e76\u5728\u53ef\u884c\u60c5\u51b5\u4e0b\u52aa\u529b\u5b9e\u73b0\u7528\u6237\u76ee\u6807\uff08\u5c3d\u7ba1\u53ef\u80fd\u4ee5\u7528\u6237\u672a\u9884\u671f\u7684\u65b9\u5f0f\uff09\u3002\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u6bd4\u975e\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "R-HTN\u4e3a\u5728\u7ebfHTN\u89c4\u5212\u63d0\u4f9b\u4e86\u667a\u80fd\u4e0d\u670d\u4ece\u7684\u6846\u67b6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u9075\u5b88\u5185\u7f6e\u6307\u4ee4\u7684\u524d\u63d0\u4e0b\u5904\u7406\u7528\u6237\u4efb\u52a1\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u548c\u4e2a\u6027\u5316\u667a\u80fd\u4f53\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00331", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2602.00331", "abs": "https://arxiv.org/abs/2602.00331", "authors": ["Anushka Narayanan", "Karianne J. Bergen"], "title": "Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks", "comment": "submitted to Environmental Data Science (preprint)", "summary": "Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u901a\u9053\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u539f\u578b\u89e3\u91ca\u6027AI\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u9053\u7279\u5b9a\u539f\u578b\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6", "motivation": "\u73b0\u6709\u539f\u578bXAI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6807\u51c6RGB\u56fe\u50cf\uff0c\u4e0d\u9002\u7528\u4e8e\u5730\u7406\u79d1\u5b66\u4e2d\u5e38\u89c1\u7684\u591a\u901a\u9053\u3001\u53d8\u91cf\u7279\u5b9a\u7684\u6570\u636e\u3002\u9700\u8981\u4e3a\u5730\u7406\u79d1\u5b66\u6570\u636e\u5f00\u53d1\u4e13\u95e8\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u4ee5\u7406\u89e3\u4e0d\u540c\u73af\u5883\u53d8\u91cf\u5982\u4f55\u5355\u72ec\u548c\u7ec4\u5408\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u3002", "method": "\u5f00\u53d1\u4e86\u9488\u5bf9\u591a\u901a\u9053\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u539f\u578bXAI\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u901a\u9053\u4ee3\u8868\u4e0d\u540c\u7684\u7269\u7406\u73af\u5883\u53d8\u91cf\u6216\u5149\u8c31\u901a\u9053\u3002\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u6765\u81ea\u591a\u4e2a\u4e0d\u540c\u8bad\u7ec3\u6837\u672c\u7684\u901a\u9053\u7279\u5b9a\u539f\u578b\u7279\u5f81\uff0c\u5e76\u5c55\u793a\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u5355\u72ec\u548c\u7ec4\u5408\u5f71\u54cd\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u5730\u7406\u79d1\u5b66\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff1a(1) \u4f7f\u7528\u591a\u53d8\u91cf\u6c14\u5019\u6570\u636e\u5206\u7c7b\u9a6c\u767b-\u6731\u5229\u5b89\u632f\u8361\u76f8\u4f4d\uff1b(2) \u4ece\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u571f\u5730\u5229\u7528\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0e\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u5c40\u90e8\uff08\u5b9e\u4f8b\u7ea7\uff09\u548c\u5168\u5c40\uff08\u6a21\u578b\u7ea7\uff09\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u5c06\u901a\u9053\u539f\u578b\u660e\u786e\u7eb3\u5165\u9884\u6d4b\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5730\u7406\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u73af\u5883\u53d8\u91cf\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.00996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00996", "abs": "https://arxiv.org/abs/2602.00996", "authors": ["Abhijit Chakraborty", "Ashish Raj Shekhar", "Shiven Agarwal", "Vivek Gupta"], "title": "DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework", "comment": null, "summary": "Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.", "AI": {"tldr": "DeALOG\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u7684\u591a\u6a21\u6001\u95ee\u7b54\uff0c\u901a\u8fc7\u4e13\u7528\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u5171\u4eab\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u5b9e\u73b0\u9c81\u68d2\u6027\u3002", "motivation": "\u590d\u6742\u7684\u8de8\u6a21\u6001\u95ee\u7b54\u9700\u8981\u6574\u5408\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u7b49\u591a\u79cd\u4fe1\u606f\u6e90\uff0c\u9700\u8981\u4e00\u4e2a\u652f\u6301\u4e13\u4e1a\u5316\u5904\u7406\u3001\u534f\u8c03\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u8868\u683c\u3001\u4e0a\u4e0b\u6587\u3001\u89c6\u89c9\u3001\u603b\u7ed3\u548c\u9a8c\u8bc1\u7b49\u4e13\u7528\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5171\u4eab\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u4f5c\u4e3a\u6301\u4e45\u5185\u5b58\u8fdb\u884c\u901a\u4fe1\uff0c\u5b9e\u73b0\u534f\u4f5c\u9519\u8bef\u68c0\u6d4b\u548c\u9a8c\u8bc1\u3002", "result": "\u5728FinQA\u3001TAT-QA\u3001CRT-QA\u3001WikiTableQuestions\u3001FeTaQA\u548cMultiModalQA\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5206\u6790\u8bc1\u5b9e\u4e86\u5171\u4eab\u65e5\u5fd7\u3001\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\u548c\u9a8c\u8bc1\u5bf9\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DeALOG\u901a\u8fc7\u6a21\u5757\u5316\u7ec4\u4ef6\u548c\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4e3a\u8de8\u6a21\u6001\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00394", "abs": "https://arxiv.org/abs/2602.00394", "authors": ["Manoj Reddy Bethi", "Sai Rupa Jhade", "Pravallika Yaganti", "Monoshiz Mahbub Khan", "Zhe Yu"], "title": "Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences", "comment": null, "summary": "Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u6210\u5bf9\u6bd4\u8f83\u5b66\u4e60\u6846\u67b6\u66ff\u4ee3\u76f4\u63a5\u8bc4\u5206\u6765\u964d\u4f4e\u4eba\u7c7b\u5ba1\u7f8e\u5224\u65ad\u6807\u6ce8\u6210\u672c\uff0c\u901a\u8fc7\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u548c\u6bd4\u8f83\u6a21\u578b\uff0c\u5728\u51cf\u5c1160%\u6807\u6ce8\u65f6\u95f4\u7684\u540c\u65f6\u8fbe\u5230\u63a5\u8fd1\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u5ba1\u7f8e\u5224\u65ad\u5efa\u6a21\u9762\u4e34\u4e2a\u4f53\u504f\u597d\u5dee\u5f02\u5927\u548c\u6807\u6ce8\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002\u76f4\u63a5\u8bc4\u5206\u9700\u8981\u5927\u91cf\u6807\u6ce8\u5de5\u4f5c\uff0c\u800c\u76f8\u5bf9\u6bd4\u8f83\u9009\u62e9\u8ba4\u77e5\u8d1f\u62c5\u66f4\u5c0f\u3001\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u56e0\u6b64\u63a2\u7d22\u4f7f\u7528\u6210\u5bf9\u6bd4\u8f83\u6846\u67b6\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "method": "\u4f7f\u7528ResNet-50\u63d0\u53d6\u7ed8\u753b\u56fe\u50cf\u7684\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\uff0c\u5f00\u53d1\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u6a21\u578b\u548c\u53cc\u5206\u652f\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\u3002\u901a\u8fc7\u56db\u4e2a\u7814\u7a76\u95ee\u9898\u63a2\u7d22\uff1a1)\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u4e0e\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\u6bd4\u8f83\uff1b2)\u6210\u5bf9\u6bd4\u8f83\u5b66\u4e60\u4e0e\u56de\u5f52\u9884\u6d4b\u5bf9\u6bd4\uff1b3)\u4e2a\u4f53\u8bc4\u5206\u8005\u504f\u597d\u9884\u6d4b\uff1b4)\u76f4\u63a5\u8bc4\u5206\u4e0e\u6bd4\u8f83\u5224\u65ad\u7684\u6807\u6ce8\u6210\u672c\u6743\u8861\u3002", "result": "\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u6bd4\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe328%\u7684R\u00b2\uff1b\u6bd4\u8f83\u6a21\u578b\u5728\u65e0\u6cd5\u83b7\u53d6\u76f4\u63a5\u8bc4\u5206\u503c\u7684\u60c5\u51b5\u4e0b\u63a5\u8fd1\u56de\u5f52\u6027\u80fd\uff1b\u4e2a\u4f53\u504f\u597d\u9884\u6d4b\u4ecd\u7136\u56f0\u96be\uff0c\u4e2a\u4f53\u5185\u548c\u4e2a\u4f53\u95f4\u9884\u6d4b\u6027\u80fd\u663e\u8457\u4f4e\u4e8e\u5e73\u5747\u8bc4\u5206\u9884\u6d4b\uff1b\u6bd4\u8f83\u5224\u65ad\u6bd4\u76f4\u63a5\u8bc4\u5206\u51cf\u5c1160%\u7684\u6bcf\u9879\u6807\u6ce8\u65f6\u95f4\u3002", "conclusion": "\u6210\u5bf9\u6bd4\u8f83\u5b66\u4e60\u6846\u67b6\u662f\u964d\u4f4e\u4eba\u7c7b\u5ba1\u7f8e\u5224\u65ad\u6807\u6ce8\u6210\u672c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u56de\u5f52\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\uff0c\u4f46\u4e2a\u4f53\u504f\u597d\u9884\u6d4b\u4ecd\u7136\u662f\u6311\u6218\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4e2a\u6027\u5316\u504f\u597d\u5efa\u6a21\u3002"}}
{"id": "2602.00954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00954", "abs": "https://arxiv.org/abs/2602.00954", "authors": ["Jinlong Pang", "Zhaowei Zhu", "Na Di", "Yichi Zhang", "Yaxuan Wang", "Chen Qian", "Yang Liu"], "title": "Small-Margin Preferences Still Matter-If You Train Them Right", "comment": null, "summary": "Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.", "AI": {"tldr": "MixDPO\uff1a\u4e00\u79cd\u96be\u5ea6\u611f\u77e5\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u5c06\u7b80\u5355\u504f\u597d\u5bf9\u7528\u4e8e\u504f\u597d\u635f\u5931\uff0c\u56f0\u96be\u5bf9\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u89e3\u51b3DPO\u5bf9\u56f0\u96be\u504f\u597d\u5bf9\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u5bf9\u504f\u597d\u5bf9\u7684\u8d28\u91cf\u548c\u96be\u5ea6\u9ad8\u5ea6\u654f\u611f\uff0c\u901a\u5e38\u5c06\u5c0f\u8fb9\u9645\uff08\u6a21\u7cca\uff09\u5bf9\u89c6\u4e3a\u566a\u58f0\u5e76\u8fc7\u6ee4\u6389\u3002\u4f46\u7814\u7a76\u53d1\u73b0\u56f0\u96be\u5bf9\u5728\u504f\u597d\u635f\u5931\u4e0b\u4f1a\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4f46\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u4ecd\u5305\u542b\u6709\u7528\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "MixDPO\uff1a1\uff09\u6309\u8fb9\u9645\u5b9a\u4e49\u7684\u96be\u5ea6\u5bf9\u504f\u597d\u6570\u636e\u8fdb\u884c\u4ece\u6613\u5230\u96be\u7684\u6392\u5e8f\uff08\u8bfe\u7a0b\u5b66\u4e60\uff09\uff1b2\uff09\u5c06\u56f0\u96be\u5bf9\u8def\u7531\u5230SFT\u76ee\u6807\uff0c\u540c\u65f6\u5bf9\u7b80\u5355\u5bf9\u5e94\u7528\u504f\u597d\u635f\u5931\u3002\u8fd9\u79cd\u6df7\u5408\u8bbe\u8ba1\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6a21\u7cca\u5bf9\u800c\u4e0d\u5f15\u53d1\u4f18\u5316\u5931\u8d25\u3002", "result": "\u5728\u4e09\u4e2aLLM-judge\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMixDPO\u5728DPO\u548c\u4e00\u7cfb\u5217\u5e7f\u6cdb\u4f7f\u7528\u7684\u53d8\u4f53\u4e0a\u6301\u7eed\u6539\u8fdb\u5bf9\u9f50\u6548\u679c\uff0c\u5728AlpacaEval~2\u957f\u5ea6\u63a7\u5236\u80dc\u7387\u4e0a\u83b7\u5f97\u7279\u522b\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "\u56f0\u96be\u504f\u597d\u5bf9\u4e0e\u4f18\u5316\u76ee\u6807\u5b58\u5728\u5f3a\u70c8\u4ea4\u4e92\u4f5c\u7528\uff0cMixDPO\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7684\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u5229\u7528\u4e86\u6a21\u7cca\u504f\u597d\u5bf9\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2602.00333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00333", "abs": "https://arxiv.org/abs/2602.00333", "authors": ["Parmida Davarmanesh", "Ashia Wilson", "Adityanarayanan Radhakrishnan"], "title": "Efficient and accurate steering of Large Language Models through attention-guided feature learning", "comment": null, "summary": "Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u5185\u90e8\u6fc0\u6d3b\u5f15\u5bfc\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728512\u4e2a\u8bed\u4e49\u6982\u5ff5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5f15\u5bfc\u6548\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u6982\u5ff5\u7279\u5f81\u5728LLM\u5c42\u95f4\u7684\u5206\u5e03\u89c4\u5f8b\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5185\u90e8\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u975e\u5e38\u8106\u5f31\uff0c\u6982\u5ff5\u7684\u53ef\u5f15\u5bfc\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7279\u5f81\u63d0\u53d6\u7684\u7b97\u6cd5\u9009\u62e9\u3002\u9700\u8981\u89e3\u51b3\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u81ea\u52a8\u9009\u62e9\u76f8\u5173token\u5d4c\u5165\u3001\u5904\u7406\u6982\u5ff5\u7279\u5f81\u5728\u6fc0\u6d3b\u4e2d\u7684\u5f02\u8d28\u6027\u3001\u8bc6\u522b\u6700\u76f8\u5173\u7684\u5f15\u5bfc\u5c42\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u81ea\u52a8\u9009\u62e9\u76f8\u5173token\u5d4c\u5165\u6765\u63d0\u53d6\u6982\u5ff5\u76f8\u5173\u7279\u5f81\uff0c\u540c\u65f6\u8003\u8651\u6982\u5ff5\u7279\u5f81\u5728LLM\u6fc0\u6d3b\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u6700\u9002\u5408\u5f15\u5bfc\u7684\u5c42\u3002", "result": "\u5728512\u4e2a\u8bed\u4e49\u6982\u5ff5\u7684\u5f15\u5bfc\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff08\u6210\u529f\u5f15\u5bfc\u7684\u6982\u5ff5\u6570\u91cf\u51e0\u4e4e\u7ffb\u500d\uff09\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u548c\u5927\u5c0f\u7684\u6a21\u578b\uff08\u5305\u62ec700\u4ebf\u53c2\u6570\u6a21\u578b\uff09\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u884c\u4e1a\u7ea7LLM\u5fae\u8c03\u7b97\u6cd5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u7406\u89e3\u6982\u5ff5\u7279\u5b9a\u7279\u5f81\u5728LLM\u5c42\u95f4\u7684\u5206\u5e03\u89c4\u5f8b\u3002"}}
{"id": "2602.00998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00998", "abs": "https://arxiv.org/abs/2602.00998", "authors": ["Zhikun Xu", "Xiaodong Yu", "Ben Zhou", "Jiang Liu", "Jialian Wu", "Ze Wang", "Ximeng Sun", "Hao Chen", "Zicheng Liu"], "title": "Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning", "comment": null, "summary": "Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRULES\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8f93\u51fa\uff08\u524d\u63d0\u68c0\u67e5\u548c\u7ed3\u8bba\u6548\u7528\u68c0\u67e5\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6b63\u786e\u5e94\u7528\u5f15\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7ecf\u5e38\u9519\u8bef\u5e94\u7528\u5f15\u7406\uff0c\u5728\u6ca1\u6709\u9a8c\u8bc1\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u5c31\u5bfc\u5165\u7ed3\u8bba\u3002\u8fd9\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u9762\u7684\u7f3a\u9677\u3002", "method": "\u5c06\u5f15\u7406\u5224\u65ad\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff1a\u7ed9\u5b9a\u9648\u8ff0\u548c\u5019\u9009\u5f15\u7406\uff0c\u6a21\u578b\u5fc5\u987b\u8f93\u51fa\u524d\u63d0\u68c0\u67e5\u548c\u7ed3\u8bba\u6548\u7528\u68c0\u67e5\u3002\u63d0\u51faRULES\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u6bb5\u5f0f\u8f93\u51fa\u7f16\u7801\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a0\u5206\u6bb5\u611f\u77e5\u635f\u5931\u63a9\u7801\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u60e9\u7f5a\u5206\u914d\u7ed9\u5bfc\u81f4\u9519\u8bef\u7684\u90e8\u5206\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e0a\u4f18\u4e8e\u666e\u901a\u6a21\u578b\u548c\u5355\u6807\u7b7eRL\u57fa\u7ebf\uff1b\u5728\u9002\u7528\u6027\u7834\u574f\u6027\u6270\u52a8\u4e0a\u6709\u66f4\u5927\u6539\u8fdb\uff1b\u5728\u7aef\u5230\u7aef\u4efb\u52a1\u4e0a\u8fbe\u5230\u6301\u5e73\u6216\u9002\u5ea6\u63d0\u5347\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u4e24\u6bb5\u5f0f\u8f93\u51fa\u548c\u5206\u6bb5\u611f\u77e5\u5f3a\u5316\u5bf9\u9c81\u68d2\u6027\u90fd\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "RULES\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u8f93\u51fa\u8bbe\u8ba1\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6b63\u786e\u5e94\u7528\u5f15\u7406\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6270\u52a8\u548c\u590d\u6742\u573a\u666f\u65f6\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.00395", "categories": ["cs.CV", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00395", "abs": "https://arxiv.org/abs/2602.00395", "authors": ["Roger Hsiao", "Yuchen Fang", "Xiangru Huang", "Ruilong Li", "Hesam Rabeti", "Zan Gojcic", "Javad Lavaei", "James Demmel", "Sophia Shao"], "title": "3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting", "comment": null, "summary": "We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (H\u00f6llein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.", "AI": {"tldr": "3DGS\u00b2-TR\uff1a\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u4f7f\u7528\u5bf9\u89d2Hessian\u8fd1\u4f3c\u548c\u53c2\u6570\u7ea7\u4fe1\u4efb\u57df\uff0c\u5728\u51cf\u5c1150%\u8bad\u7ec3\u8fed\u4ee3\u7684\u540c\u65f6\u4fdd\u6301\u4f4e\u5185\u5b58\u5f00\u9500", "motivation": "\u73b0\u6709\u4e8c\u9636\u65b9\u6cd5\uff08\u59823DGS-LM\u548c3DGS2\uff09\u4f9d\u8d56\u663e\u5f0f\u6216\u5bc6\u96c6\u66f2\u7387\u8868\u793a\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5728\u5927\u573a\u666f\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u7a33\u5b9a\u7684\u4f18\u5316\u5668\u6765\u52a0\u901f3DGS\u573a\u666f\u8bad\u7ec3\u3002", "method": "1) \u4f7f\u7528Hutchinson\u65b9\u6cd5\u4ec5\u8fd1\u4f3cHessian\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\uff0c\u5b9e\u73b0\u5b8c\u5168\u77e9\u9635\u65e0\u5173\u7684O(n)\u590d\u6742\u5ea6\uff1b2) \u57fa\u4e8e\u5e73\u65b9Hellinger\u8ddd\u79bb\u5f15\u5165\u53c2\u6570\u7ea7\u4fe1\u4efb\u57df\u6280\u672f\uff0c\u6b63\u5219\u5316\u9ad8\u65af\u53c2\u6570\u66f4\u65b0\u4ee5\u5e94\u5bf93DGS\u5149\u6805\u5316\u8fc7\u7a0b\u4e2d\u7684\u5f3a\u975e\u7ebf\u6027\uff1b3) \u4fdd\u6301\u4e0eADAM\u76f8\u540c\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002", "result": "\u5728\u76f8\u540c\u53c2\u6570\u521d\u59cb\u5316\u548c\u65e0\u81f4\u5bc6\u5316\u6761\u4ef6\u4e0b\uff0c\u76f8\u6bd4ADAM\uff1a1) \u4f7f\u752850%\u66f4\u5c11\u7684\u8bad\u7ec3\u8fed\u4ee3\u8fbe\u5230\u66f4\u597d\u7684\u91cd\u5efa\u8d28\u91cf\uff1b2) \u5cf0\u503cGPU\u5185\u5b58\u5f00\u9500\u5c0f\u4e8e1GB\uff08\u6bd4ADAM\u591a17%\uff0c\u6bd43DGS-LM\u5c1185%\uff09\uff1b3) \u80fd\u591f\u6269\u5c55\u5230\u8d85\u5927\u573a\u666f\u548c\u6f5c\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u8bbe\u7f6e\u3002", "conclusion": "3DGS\u00b2-TR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5bf9\u89d2Hessian\u8fd1\u4f3c\u548c\u53c2\u6570\u7ea7\u4fe1\u4efb\u57df\u6280\u672f\uff0c\u5728\u4fdd\u6301\u4f4e\u5185\u5b58\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f3DGS\u8bad\u7ec3\uff0c\u4e3a\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.00994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00994", "abs": "https://arxiv.org/abs/2602.00994", "authors": ["Yu Li", "Mingyang Yi", "Xiuyu Li", "Ju Fan", "Fuxin Jiang", "Binbin Chen", "Peng Li", "Jie Song", "Tieying Zhang"], "title": "Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning", "comment": null, "summary": "Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u7ebf\u6027\u6548\u5e94\u5f52\u56e0\u7cfb\u7edf\uff08LEAS\uff09\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u4e4b\u95f4\u7684\u8bad\u7ec3\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u89e3\u8026\u884c\u52a8\u63a8\u7406\u8c03\u4f18\uff08DART\uff09\u6846\u67b6\u6765\u5206\u79bb\u8fd9\u4e24\u79cd\u80fd\u529b\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u5171\u4eab\u6a21\u578b\u53c2\u6570\u540c\u65f6\u652f\u6301\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\uff0c\u5e76\u5047\u8bbe\u8054\u5408\u8bad\u7ec3\u80fd\u63d0\u5347\u6574\u4f53\u4ee3\u7406\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u8bc1\u68c0\u9a8c\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u8fd9\u79cd\u5047\u8bbe\u7684\u6709\u6548\u6027\u3002", "method": "\u9996\u5148\u5f15\u5165\u7ebf\u6027\u6548\u5e94\u5f52\u56e0\u7cfb\u7edf\uff08LEAS\uff09\u91cf\u5316\u5206\u6790\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u4e4b\u95f4\u7684\u5e72\u6270\uff1b\u7136\u540e\u63d0\u51fa\u89e3\u8026\u884c\u52a8\u63a8\u7406\u8c03\u4f18\uff08DART\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7684\u4f4e\u79e9\u9002\u5e94\u6a21\u5757\u663e\u5f0f\u89e3\u8026\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDART\u6846\u67b6\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u53476.35%\u7684\u6027\u80fd\uff0c\u4e14\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5c31\u80fd\u8fbe\u5230\u4e0e\u663e\u5f0f\u5206\u79bb\u5de5\u5177\u4f7f\u7528\u548c\u63a8\u7406\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u8bad\u7ec3\u5e72\u6270\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u6548\u679c\u53d7\u9650\uff1b\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u8fd9\u4e24\u79cd\u80fd\u529b\u7684\u53c2\u6570\u66f4\u65b0\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u6311\u6218\u4e86\u5f53\u524d\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u666e\u904d\u8303\u5f0f\u3002"}}
{"id": "2602.00334", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00334", "abs": "https://arxiv.org/abs/2602.00334", "authors": ["Aikaterini Karoni", "Rajit Rajpal", "Benedict Leimkuhler", "Gabriel Stoltz"], "title": "Adaptive Momentum and Nonlinear Damping for Neural Network Training", "comment": "29 pages, 11 figures", "summary": "We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u80fd\u8c03\u8282\u7684\u81ea\u9002\u5e94\u52a8\u91cf\u7cfb\u6570\uff0c\u5c06\u7acb\u65b9\u963b\u5c3c\u673a\u5236\u5f15\u5165\u4f18\u5316\u7b97\u6cd5\uff0c\u5728ViT\u3001BERT\u3001GPT2\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eAdam\u3002", "motivation": "\u5927\u89c4\u6a21\u4f18\u5316\u4e2d\u9700\u8981\u5e73\u8861\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982mSGD\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u81ea\u9002\u5e94\u673a\u5236\u6765\u6839\u636e\u5c40\u90e8\u66f2\u7387\u8c03\u6574\u52a8\u91cf\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u4f18\u5316\u65b9\u6848\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u578b\u53c2\u6570\u5f15\u5165\u7531\u52a8\u80fd\u8c03\u8282\u7684\u81ea\u9002\u5e94\u52a8\u91cf\u7cfb\u6570\uff0c\u5c06\u7acb\u65b9\u963b\u5c3c\u673a\u5236\u4e0emSGD\u548cAdam\u7684\u8fde\u7eed\u52a8\u529b\u5b66\u7ed3\u5408\uff0c\u5f62\u6210\u4e24\u79cd\u5177\u4f53\u4f18\u5316\u65b9\u6848\u3002", "result": "\u5728ViT\u3001BERT\u3001GPT2\u7b49\u4efb\u52a1\u4e0a\uff0c\u65b0\u65b9\u6cd5\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5339\u914d\u6216\u4f18\u4e8eAdam\uff0c\u7279\u522b\u662f\u5728mSGD\u901a\u5e38\u8868\u73b0\u4e0d\u4f73\u7684\u4efb\u52a1\u4e0a\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u65b9\u6848\u7684\u6307\u6570\u6536\u655b\u6027\u3002", "conclusion": "\u901a\u8fc7\u52a8\u80fd\u8c03\u8282\u7684\u81ea\u9002\u5e94\u52a8\u91cf\u7cfb\u6570\u548c\u7acb\u65b9\u963b\u5c3c\u673a\u5236\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01007", "abs": "https://arxiv.org/abs/2602.01007", "authors": ["Zishuo Bao", "Jiaqi Leng", "Junxiong Wang", "Bowen Peng", "Yucheng Lu"], "title": "Distilling Token-Trained Models into Byte-Level Models", "comment": "17 pages, 3 figures, 13 tables", "summary": "Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u7684\u5206\u8bcd\u8bad\u7ec3LLM\u8f6c\u6362\u4e3a\u5b57\u8282\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u9700\u7ea61250\u4ebf\u5b57\u8282\u8bad\u7ec3\u6570\u636e\uff0c\u4fdd\u7559\u539f\u6a21\u578b\u5927\u90e8\u5206\u80fd\u529b", "motivation": "\u5b57\u8282\u8bed\u8a00\u6a21\u578b\uff08BLMs\uff09\u662f\u8d85\u8d8a\u5206\u8bcd\u9650\u5236\u7684\u6269\u5c55\u65b9\u5411\uff0c\u4f46\u73b0\u6709BLMs\u9700\u8981\u4ece\u5934\u8bad\u7ec3\u4e07\u4ebf\u5b57\u8282\u6570\u636e\uff0c\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u5229\u7528\u73b0\u6709\u5206\u8bcd\u6a21\u578b", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a1\uff09\u6e10\u8fdb\u77e5\u8bc6\u84b8\u998f\uff0c\u5bf9\u9f50\u5b57\u8282\u7ea7\u8868\u793a\u4e0e\u5206\u8bcd\u6559\u5e08\u6a21\u578b\u7684\u5d4c\u5165\uff1b2\uff09\u5b57\u8282\u7ea7\u76d1\u7763\u5fae\u8c03\uff0c\u5b9e\u73b0\u5b8c\u5168\u5728\u5b57\u8282\u7a7a\u95f4\u7684\u7aef\u5230\u7aef\u751f\u6210", "result": "\u5728Llama\u3001Qwen\u3001OLMo\u7b49\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u9a8c\u8bc1\uff0c\u84b8\u998f\u51fa\u7684BLMs\u4ec5\u4f7f\u7528\u7ea61250\u4ebf\u5b57\u8282\u6570\u636e\u5c31\u80fd\u4fdd\u7559\u6559\u5e08\u6a21\u578b\u5927\u90e8\u5206\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u84b8\u998f\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u5c06\u73b0\u6709\u5206\u8bcdLLM\u8f6c\u6362\u4e3a\u5b57\u8282\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u4e3aBLMs\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2602.00414", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00414", "abs": "https://arxiv.org/abs/2602.00414", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Aldair Ernesto Gongora", "Ruben Glatt", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "title": "Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure", "comment": null, "summary": "Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.", "AI": {"tldr": "\u63d0\u51fa\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u65b0\u65b9\u6cd5\uff1a\u901a\u8fc7\u6587\u672c\u573a\u666f\u751f\u6210\u5bf9\u9f50\u7684\u56fe\u50cf-\u573a\u666f\u56fe-\u771f\u503c\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u8bc4\u4f30VLM\u5728\u89c6\u89c9\u5b89\u5168\u76d1\u63a7\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0VLM\u5728\u7eaf\u89c6\u89c9\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u63d0\u51fa\u573a\u666f\u56fe\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u4f9d\u8d56\u4eba\u5de5\u4e14\u7f3a\u4e4f\u6301\u7eed\u76d1\u63a7\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u6709\u6f5c\u529b\u4f46\u7f3a\u4e4f\u771f\u5b9e\u8bc4\u4f30\u6570\u636e\uff0c\u56e0\u4e3a\u5b89\u5168\u4e8b\u6545\u4e3b\u8981\u4ee5\u975e\u7ed3\u6784\u5316\u6587\u672c\u8bb0\u5f55\u3002", "method": "1) \u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\uff1a\u7528LLM\u4f5c\u4e3a\u573a\u666f\u56fe\u67b6\u6784\u5e08\uff0c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u6e32\u67d3\u5668\uff0c\u5c06\u6587\u672c\u5b9e\u9a8c\u5ba4\u573a\u666f\u8f6c\u6362\u4e3a\u5bf9\u9f50\u7684\u4e09\u5143\u7ec4(\u56fe\u50cf\u3001\u573a\u666f\u56fe\u3001\u771f\u503c)\uff1b2) \u63d0\u51fa\u540e\u8bad\u7ec3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\uff1a\u573a\u666f\u56fe\u5f15\u5bfc\u5bf9\u9f50\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u573a\u666f\u56fe\u4ee5\u5f25\u5408VLM\u611f\u77e5\u5dee\u8ddd\u3002", "result": "\u57281,207\u4e2a\u6837\u672c\u3001362\u4e2a\u72ec\u7279\u573a\u666f\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd57\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u53d1\u73b0VLM\u5728\u7ed9\u5b9a\u6587\u672c\u573a\u666f\u56fe\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7eaf\u89c6\u89c9\u8bbe\u7f6e\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u63d0\u51fa\u7684\u573a\u666f\u56fe\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\u63d0\u5347\u4e86\u7eaf\u89c6\u89c9\u73af\u5883\u4e0b\u7684\u5371\u9669\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "VLM\u5728\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u4e2d\u5177\u6709\u6f5c\u529b\u4f46\u9762\u4e34\u89c6\u89c9\u611f\u77e5\u6311\u6218\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u573a\u666f\u56fe\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.00997", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00997", "abs": "https://arxiv.org/abs/2602.00997", "authors": ["Mayank Singh", "Vikas Yadav", "Eduardo Blanco"], "title": "Error Taxonomy-Guided Prompt Optimization", "comment": null, "summary": "Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.", "AI": {"tldr": "ETGPO\u662f\u4e00\u79cd\u57fa\u4e8e\u9519\u8bef\u5206\u7c7b\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u9519\u8bef\u6a21\u5f0f\u8fdb\u884c\u5168\u5c40\u4f18\u5316\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8282\u7701\u4e09\u5206\u4e4b\u4e8c\u7684token\u4f7f\u7528\u548c\u8bc4\u4f30\u9884\u7b97\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u591a\u4e3a\u81ea\u5e95\u5411\u4e0a\u7684\u8bd5\u9519\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c6\u89d2\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5168\u5c40\u5316\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u9519\u8bef\u5206\u7c7b\u5f15\u5bfc\u7684\u63d0\u793a\u4f18\u5316(ETGPO)\uff1a\u6536\u96c6\u6a21\u578b\u9519\u8bef\uff0c\u5206\u7c7b\u5efa\u7acb\u9519\u8bef\u5206\u7c7b\u6cd5\uff0c\u9488\u5bf9\u9ad8\u9891\u9519\u8bef\u6a21\u5f0f\u5728\u63d0\u793a\u4e2d\u6dfb\u52a0\u9488\u5bf9\u6027\u6307\u5bfc\uff0c\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u7684\u5168\u5c40\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b66\u3001\u95ee\u7b54\u3001\u903b\u8f91\u63a8\u7406\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cETGPO\u8fbe\u5230\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4f18\u5316\u9636\u6bb5\u7684token\u4f7f\u7528\u91cf\u548c\u8bc4\u4f30\u9884\u7b97\u51cf\u5c11\u7ea6\u4e09\u5206\u4e4b\u4e8c\u3002", "conclusion": "ETGPO\u901a\u8fc7\u9519\u8bef\u5206\u7c7b\u7684\u5168\u5c40\u89c6\u89d2\u5b9e\u73b0\u4e86\u9ad8\u6548\u63d0\u793a\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.00357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00357", "abs": "https://arxiv.org/abs/2602.00357", "authors": ["Chenyang Yuan", "Xiaoyuan Cheng"], "title": "Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design", "comment": null, "summary": "Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u91c7\u6837\u5668\u914d\u5408\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u667a\u80fdAP\u90e8\u7f72\u89c4\u5212\uff0c\u76f8\u6bd4\u4f20\u7edfLLM\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u590d\u6742\u7684\u5ba4\u5185\u51e0\u4f55\u7ed3\u6784\u548c\u4fe1\u53f7\u4f20\u64ad\u4f7f\u5f97\u667a\u80fdAP\u90e8\u7f72\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5177\u6709\u5f3a\u5927\u7684\u65e0\u7ebf\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u7684\u751f\u6210\u63a8\u7406\u6a21\u578b\uff0c\u8be5\u51fd\u6570\u6355\u6349\u4e86\u4e0d\u540c\u5e73\u9762\u56fe\u4e2dAP\u90e8\u7f72\u7684\u6838\u5fc3\u76ee\u6807\u3002\u91c7\u7528\u6269\u6563\u91c7\u6837\u5668\u8fdb\u884c\u4f18\u5316\uff0c\u6269\u6563\u8fc7\u7a0b\u901a\u8fc7\u5e73\u6ed1\u548c\u9510\u5316\u5956\u52b1\u666f\u89c2\u9010\u6b65\u6539\u8fdb\u91c7\u6837\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u8fed\u4ee3\u7ec6\u5316\u3002", "result": "\u6269\u6563\u91c7\u6837\u5668\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u51f8\u548c\u788e\u7247\u5316\u7684\u76ee\u6807\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u5ba4\u5185AP\u90e8\u7f72\u6570\u636e\u96c6\uff0c\u9700\u8981\u8d85\u8fc75\u4e07CPU\u5c0f\u65f6\u6765\u8bad\u7ec3\u901a\u7528\u5956\u52b1\u51fd\u6570\u3002\u8bc4\u4f30\u4e86\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u63a8\u7406\u914d\u5408\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u4e3a\u5ba4\u5185AP\u90e8\u7f72\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9886\u57df\u65e0\u5173\u7684\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2602.01015", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01015", "abs": "https://arxiv.org/abs/2602.01015", "authors": ["Conrad Borchers", "Jill-J\u00eann Vie", "Roger Azevedo"], "title": "Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident", "comment": "Manuscript under review", "summary": "Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.", "AI": {"tldr": "GPT-4.1\u5728\u6a21\u62df\u65b0\u624b\u63a8\u7406\u548c\u5143\u8ba4\u77e5\u5224\u65ad\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff1a\u63a8\u7406\u8fc7\u4e8e\u8fde\u8d2f\u3001\u5197\u957f\u4e14\u7f3a\u4e4f\u4eba\u7c7b\u601d\u7ef4\u53e3\u8bed\u7684\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u9ad8\u4f30\u5b66\u4e60\u8005\u8868\u73b0\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u771f\u5b9e\u5b66\u4e60\u8fc7\u7a0b\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5de5\u4f5c\u8bb0\u5fc6\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u95ee\u9898\u89e3\u51b3\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5b66\u4e60\u4e2d\u788e\u7247\u5316\u3001\u4e0d\u5b8c\u7f8e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u9700\u8981\u8bc4\u4f30LLM\u662f\u5426\u80fd\u771f\u5b9e\u6a21\u62df\u65b0\u624b\u63a8\u7406\u548c\u5143\u8ba4\u77e5\u5224\u65ad\uff0c\u8fd9\u5bf9\u8bbe\u8ba1\u80fd\u6709\u6548\u652f\u6301\u65b0\u624b\u5b66\u4e60\u548c\u81ea\u6211\u8c03\u8282\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528630\u6761\u6765\u81ea\u591a\u6b65\u9aa4\u5316\u5b66\u8f85\u5bfc\u95ee\u9898\u7684\u601d\u7ef4\u53e3\u8bed\u8bb0\u5f55\uff0c\u5305\u542b\u5b66\u751f\u63d0\u793a\u4f7f\u7528\u3001\u5c1d\u8bd5\u548c\u95ee\u9898\u60c5\u5883\u7684\u65e5\u5fd7\u3002\u5728\u6700\u5c0f\u5316\u548c\u6269\u5c55\u60c5\u5883\u63d0\u793a\u4e0b\uff0c\u6bd4\u8f83LLM\u751f\u6210\u63a8\u7406\u4e0e\u4eba\u7c7b\u5b66\u4e60\u8005\u8bdd\u8bed\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u6b65\u9aa4\u7ea7\u5b66\u4e60\u8005\u6210\u529f\u7684\u80fd\u529b\u3002", "result": "GPT-4.1\u80fd\u751f\u6210\u6d41\u7545\u4e14\u60c5\u5883\u9002\u5f53\u7684\u5ef6\u7eed\uff0c\u4f46\u5176\u63a8\u7406\u7cfb\u7edf\u6027\u8fc7\u4e8e\u8fde\u8d2f\u3001\u5197\u957f\u4e14\u53d8\u5f02\u6027\u4f4e\u4e8e\u4eba\u7c7b\u601d\u7ef4\u53e3\u8bed\u3002\u8fd9\u4e9b\u6548\u5e94\u5728\u63d0\u793a\u4e2d\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u95ee\u9898\u89e3\u51b3\u60c5\u5883\u65f6\u52a0\u5267\u3002\u5b66\u4e60\u8005\u8868\u73b0\u88ab\u6301\u7eed\u9ad8\u4f30\u3002", "conclusion": "LLM\u5728\u6a21\u62df\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u8ba4\u8bc6\u8bba\u9650\u5236\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u5305\u542b\u4e13\u5bb6\u5f0f\u89e3\u51b3\u65b9\u6848\u800c\u7f3a\u4e4f\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5de5\u4f5c\u8bb0\u5fc6\u7ea6\u675f\u3002\u8bc4\u4f30\u6846\u67b6\u53ef\u6307\u5bfc\u672a\u6765\u8bbe\u8ba1\u66f4\u771f\u5b9e\u652f\u6301\u65b0\u624b\u5b66\u4e60\u548c\u81ea\u6211\u8c03\u8282\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u3002"}}
{"id": "2602.00420", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00420", "abs": "https://arxiv.org/abs/2602.00420", "authors": ["Yihang Chen", "Zhao Xu", "Youyuan Jiang", "Tianle Zheng", "Cho-Jui Hsieh"], "title": "Text is All You Need for Vision-Language Model Jailbreaking", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.", "AI": {"tldr": "Text-DJ\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u67e5\u8be2\u5206\u89e3\u4e3a\u591a\u4e2a\u8bed\u4e49\u76f8\u5173\u4f46\u66f4\u826f\u6027\u7684\u5b50\u67e5\u8be2\uff0c\u5e76\u6dfb\u52a0\u5927\u91cf\u65e0\u5173\u7684\u5e72\u6270\u67e5\u8be2\uff0c\u4ee5\u56fe\u50cf\u7f51\u683c\u5f62\u5f0f\u5448\u73b0\uff0c\u6210\u529f\u7ed5\u8fc7\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u4e3b\u8981\u5173\u6ce8\u5206\u6790\u663e\u5f0f\u6587\u672c\u8f93\u5165\u6216\u76f8\u5173\u89c6\u89c9\u573a\u666f\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u578b\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u80fd\u529b\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002Text-DJ\u65e8\u5728\u63a2\u7d22\u901a\u8fc7OCR\u529f\u80fd\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\u7684\u53ef\u80fd\u6027\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u5c06\u5355\u4e2a\u6709\u5bb3\u67e5\u8be2\u5206\u89e3\u4e3a\u591a\u4e2a\u8bed\u4e49\u76f8\u5173\u4f46\u66f4\u826f\u6027\u7684\u5b50\u67e5\u8be2\uff1b2) \u9009\u62e9\u4e0e\u6709\u5bb3\u67e5\u8be2\u6700\u5927\u7a0b\u5ea6\u65e0\u5173\u7684\u5e72\u6270\u67e5\u8be2\u96c6\u5408\uff1b3) \u5c06\u6240\u6709\u5206\u89e3\u7684\u5b50\u67e5\u8be2\u548c\u5e72\u6270\u67e5\u8be2\u4ee5\u56fe\u50cf\u7f51\u683c\u5f62\u5f0f\u540c\u65f6\u5448\u73b0\u7ed9\u6a21\u578b\uff0c\u5176\u4e2d\u5b50\u67e5\u8be2\u4f4d\u4e8e\u7f51\u683c\u4e2d\u95f4\u4f4d\u7f6e\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed5\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u63d0\u793a\u8f6c\u6362\u4e3a\u56fe\u50cf\u7ed5\u8fc7\u6807\u51c6\u6587\u672c\u8fc7\u6ee4\u5668\uff0c\u5e76\u901a\u8fc7\u8bf1\u5bfc\u5e72\u6270\u4f7f\u6a21\u578b\u7684\u5b89\u5168\u534f\u8bae\u65e0\u6cd5\u5728\u5927\u91cf\u65e0\u5173\u67e5\u8be2\u4e2d\u94fe\u63a5\u5206\u6563\u7684\u5b50\u67e5\u8be2\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bOCR\u80fd\u529b\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5bf9\u5206\u6563\u7684\u591a\u56fe\u50cf\u5bf9\u6297\u8f93\u5165\u4e0d\u591f\u9c81\u68d2\uff0c\u51f8\u663e\u4e86\u9700\u8981\u4e3a\u788e\u7247\u5316\u7684\u591a\u6a21\u6001\u8f93\u5165\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2602.01002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01002", "abs": "https://arxiv.org/abs/2602.01002", "authors": ["Itai Shapira", "Gerdus Benade", "Ariel D. Procaccia"], "title": "How RLHF Amplifies Sycophancy", "comment": null, "summary": "Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u5982\u4f55\u653e\u5927LLM\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u534f\u65b9\u5dee\u548c\u5747\u503c\u5dee\u8ddd\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\u9632\u6b62\u8c04\u5a9a\u884c\u4e3a\u589e\u52a0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u504f\u597d\u7684\u540e\u8bad\u7ec3\u540e\u5e38\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u5373\u503e\u5411\u4e8e\u9644\u548c\u7528\u6237\u7684\u9648\u8ff0\u6216\u6697\u793a\u4fe1\u5ff5\uff0c\u5373\u4f7f\u8fd9\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u6216\u5408\u7406\u5224\u65ad\u76f8\u51b2\u7a81\u3002\u9700\u8981\u7406\u89e3\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u5982\u4f55\u88ab\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u653e\u5927\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u5f62\u5f0f\u5316\u5206\u6790\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u5982\u4f55\u901a\u8fc7\u8bc6\u522b\u660e\u786e\u7684\u653e\u5927\u673a\u5236\u6765\u589e\u52a0\u8c04\u5a9a\u884c\u4e3a\uff1b2) \u8bc1\u660e\u884c\u4e3a\u6f02\u79fb\u65b9\u5411\u7531\u57fa\u7840\u7b56\u7565\u4e0b\u8ba4\u53ef\u63d0\u793a\u4e2d\u7684\u4fe1\u5ff5\u4fe1\u53f7\u4e0e\u5b66\u4e60\u5956\u52b1\u4e4b\u95f4\u7684\u534f\u65b9\u5dee\u51b3\u5b9a\uff1b3) \u5206\u6790Bradley-Terry\u7b49\u968f\u673a\u6548\u7528\u6a21\u578b\u4e0b\u7684\u5956\u52b1\u5b66\u4e60\uff1b4) \u63d0\u51fa\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u6700\u5c0f\u5316\u63a8\u5bfc\u95ed\u5f0f\u534f\u8bae\u60e9\u7f5a\u5956\u52b1\u4fee\u6b63\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u53d1\u73b0\u5956\u52b1\u5dee\u8ddd\u666e\u904d\u5b58\u5728\uff0c\u5e76\u5728\u6240\u6709\u8003\u8651\u7684\u914d\u7f6e\u4e2d\u5bfc\u81f4\u884c\u4e3a\u6f02\u79fb\u3002\u63d0\u51fa\u7684\u5956\u52b1\u4fee\u6b63\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9632\u6b62\u8c04\u5a9a\u884c\u4e3a\u589e\u52a0\u3002", "conclusion": "\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4f1a\u653e\u5927LLM\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u8fd9\u79cd\u653e\u5927\u673a\u5236\u53ef\u901a\u8fc7\u534f\u65b9\u5dee\u548c\u5747\u503c\u5dee\u8ddd\u6761\u4ef6\u89e3\u91ca\u3002\u901a\u8fc7\u8bbe\u8ba1\u9002\u5f53\u7684\u8bad\u7ec3\u65f6\u5e72\u9884\u548c\u5956\u52b1\u4fee\u6b63\uff0c\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u8c04\u5a9a\u884c\u4e3a\u7684\u589e\u52a0\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u672a\u7ea6\u675f\u540e\u8bad\u7ec3\u7b56\u7565\u7684\u6700\u5c0fKL\u6563\u5ea6\u8ddd\u79bb\u3002"}}
{"id": "2602.00360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00360", "abs": "https://arxiv.org/abs/2602.00360", "authors": ["Sumana Biswas", "Karen Young", "Josephine Griffith"], "title": "Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition", "comment": null, "summary": "Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTEMSA\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u56fe\u50cf\u4e2d\u6240\u6709\u68c0\u6d4b\u5230\u7684\u7269\u4f53\u540d\u79f0\u5e76\u4e0e\u76f8\u5173\u6587\u672c\u7ed3\u5408\uff08\u79f0\u4e3aTEMS\uff09\uff0c\u6765\u6539\u8fdb\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9762\u4e34\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u5dee\u5f02\u3001\u60c5\u611f\u6b67\u4e49\u548c\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6709\u6548\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u3002", "method": "\u63d0\u51faTEMSA\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7269\u4f53\u8bc6\u522b\u6280\u672f\u63d0\u53d6\u56fe\u50cf\u4e2d\u6240\u6709\u68c0\u6d4b\u5230\u7684\u7269\u4f53\u540d\u79f0\uff0c\u5c06\u8fd9\u4e9b\u7269\u4f53\u540d\u79f0\u4e0e\u76f8\u5173\u6587\u672c\u7ed3\u5408\u5f62\u6210TEMS\u6570\u636e\uff0c\u7136\u540e\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5bf9\u56fe\u50cf\u3001\u6587\u672c\u4ee5\u53caTEMS\u7ec4\u5408\u8fdb\u884c\u60c5\u611f\u5206\u6790\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u5f53\u4f7f\u7528\u5305\u542b\u6240\u6709\u7269\u4f53\u540d\u79f0\u7684TEMS\u6570\u636e\u65f6\uff0c\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7ed3\u679c\u76f8\u6bd4\u5355\u72ec\u5206\u6790\u56fe\u50cf\u6216\u6587\u672c\u6709\u6240\u6539\u5584\u3002", "conclusion": "TEMSA\u65b9\u6cd5\u80fd\u6709\u6548\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u7269\u4f53\u8bc6\u522b\u4fe1\u606f\u5728\u589e\u5f3a\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.01030", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.01030", "abs": "https://arxiv.org/abs/2602.01030", "authors": ["Sheng-Lun Wei", "Yu-Ling Liao", "Yen-Hua Chang", "Hen-Hsen Huang", "Hsin-Hsi Chen"], "title": "Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations", "comment": "Accepted as a long findings paper at EACL 2026", "summary": "This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $\u03ba$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u591a\u8bed\u8a00MLLM\u4e2d\u7684\u8bed\u97f3\u504f\u89c1\uff0c\u6784\u5efaBiasInEar\u6570\u636e\u96c6\uff0c\u8bc4\u4f309\u4e2a\u6a21\u578b\u5728\u8bed\u8a00\u3001\u53e3\u97f3\u3001\u6027\u522b\u548c\u9009\u9879\u987a\u5e8f\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0MLLM\u5bf9\u8bed\u8a00\u548c\u9009\u9879\u987a\u5e8f\u654f\u611f\uff0c\u8bed\u97f3\u4f1a\u653e\u5927\u73b0\u6709\u7ed3\u6784\u504f\u89c1\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00MLLM\u4e2d\u8bed\u97f3\u504f\u89c1\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u8bed\u97f3\u96c6\u6210LLM\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\uff0c\u586b\u8865\u6587\u672c\u548c\u8bed\u97f3\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u6784\u5efaBiasInEar\u6570\u636e\u96c6\uff08\u57fa\u4e8eGlobal MMLU Lite\uff0c\u6db5\u76d6\u82f1\u4e2d\u97e9\u4e09\u8bed\uff0c\u5e73\u8861\u6027\u522b\u548c\u53e3\u97f3\uff0c\u517170.8\u5c0f\u65f6\u8bed\u97f3\uff0c11,200\u4e2a\u95ee\u9898\uff09\u3002\u4f7f\u7528\u56db\u79cd\u4e92\u8865\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u71b5\u3001APES\u3001Fleiss' \u03ba\uff09\u8bc4\u4f309\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5206\u6790\u8bed\u8a00\u3001\u53e3\u97f3\u3001\u6027\u522b\u548c\u9009\u9879\u987a\u5e8f\u6270\u52a8\u7684\u5f71\u54cd\u3002", "result": "MLLM\u5bf9\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\uff08\u6027\u522b\uff09\u76f8\u5bf9\u9c81\u68d2\uff0c\u4f46\u5bf9\u8bed\u8a00\u548c\u9009\u9879\u987a\u5e8f\u9ad8\u5ea6\u654f\u611f\uff0c\u8868\u660e\u8bed\u97f3\u4f1a\u653e\u5927\u73b0\u6709\u7ed3\u6784\u504f\u89c1\u3002\u67b6\u6784\u8bbe\u8ba1\u548c\u63a8\u7406\u7b56\u7565\u663e\u8457\u5f71\u54cd\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u8bc4\u4f30\u8bed\u97f3\u96c6\u6210LLM\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u504f\u89c1\u7684\u5173\u952e\u7279\u5f81\uff0c\u4e3a\u672a\u6765\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u8bed\u97f3AI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.00440", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00440", "abs": "https://arxiv.org/abs/2602.00440", "authors": ["Anugunj Naman", "Gaibo Zhang", "Ayushman Singh", "Yaguang Zhang"], "title": "DISK: Dynamic Inference SKipping for World Models", "comment": null, "summary": "We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.", "AI": {"tldr": "DISK\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u63a7\u5236\u5668\u534f\u8c03\u89c6\u9891\u548c\u81ea\u6211\u8f68\u8ff9\u7684\u6269\u6563\u53d8\u6362\u5668\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b02\u500d\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u4e16\u754c\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\u9700\u8981\u540c\u65f6\u9884\u6d4b\u89c6\u9891\u548c\u8f68\u8ff9\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u8fd0\u52a8-\u5916\u89c2\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u4f7f\u7528\u53cc\u5206\u652f\u63a7\u5236\u5668\u534f\u8c03\u4e24\u4e2a\u8026\u5408\u7684\u6269\u6563\u53d8\u6362\u5668\uff08\u89c6\u9891\u548c\u8f68\u8ff9\uff09\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8df3\u8fc7\u51b3\u7b56\u4fdd\u6301\u4e00\u81f4\u6027\uff1b\u6269\u5c55\u9ad8\u9636\u6f5c\u5728\u5dee\u5f02\u8df3\u8fc7\u6d4b\u8bd5\u5230\u81ea\u56de\u5f52\u94fe\u5f0f\u524d\u5411\u673a\u5236\uff0c\u5e76\u901a\u8fc7rollout\u5faa\u73af\u4f20\u64ad\u63a7\u5236\u5668\u7edf\u8ba1\u4fe1\u606f\u4ee5\u5b9e\u73b0\u957f\u65f6\u7a33\u5b9a\u6027\u3002", "result": "\u5728NuPlan\u548cNuScenes\u6570\u636e\u96c6\u4e0a\uff0cDISK\u5b9e\u73b0\u8f68\u8ff9\u6269\u65632\u500d\u52a0\u901f\u548c\u89c6\u9891\u6269\u65631.6\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301L2\u89c4\u5212\u8bef\u5dee\u3001\u89c6\u89c9\u8d28\u91cf\uff08FID/FVD\uff09\u548cNAVSIM PDMS\u5206\u6570\u4e0d\u53d8\u3002", "conclusion": "DISK\u80fd\u591f\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u5b9e\u7528\u7684\u957f\u65f6\u57df\u89c6\u9891\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01031", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01031", "abs": "https://arxiv.org/abs/2602.01031", "authors": ["Dongyang Fan", "Sebastien Delsad", "Nicolas Flammarion", "Maksym Andriushchenko"], "title": "HalluHard: A Hard Multi-Turn Hallucination Benchmark", "comment": null, "summary": "Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\\approx 30\\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HalluHard\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u4e2dLLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5305\u542b950\u4e2a\u79cd\u5b50\u95ee\u9898\u8986\u76d6\u6cd5\u5f8b\u3001\u7814\u7a76\u3001\u533b\u7597\u548c\u7f16\u7a0b\u56db\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u901a\u8fc7\u5f15\u7528\u8981\u6c42\u8861\u91cf\u4e8b\u5b9e\u57fa\u7840\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u7f51\u7edc\u641c\u7d22\u7684\u8bc1\u636e\u68c0\u7d22\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4f1a\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u7f3a\u4e4f\u4e8b\u5b9e\u4f9d\u636e\u7684\u9648\u8ff0\uff0c\u968f\u7740\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\u548c\u65e9\u671f\u9519\u8bef\u7d2f\u79ef\uff0c\u5e7b\u89c9\u95ee\u9898\u4f1a\u52a0\u5267\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u7684\u5e7b\u89c9\u95ee\u9898\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u3002", "method": "1. \u6784\u5efaHalluHard\u57fa\u51c6\uff1a\u5305\u542b950\u4e2a\u79cd\u5b50\u95ee\u9898\uff0c\u8986\u76d6\u6cd5\u5f8b\u6848\u4f8b\u3001\u7814\u7a76\u95ee\u9898\u3001\u533b\u7597\u6307\u5357\u548c\u7f16\u7a0b\u56db\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\uff1b2. \u901a\u8fc7\u8981\u6c42\u5185\u8054\u5f15\u7528\u6765\u64cd\u4f5c\u5316\"\u6709\u4e8b\u5b9e\u57fa\u7840\"\u7684\u6982\u5ff5\uff1b3. \u5f00\u53d1\u8bc4\u4f30\u6d41\u7a0b\uff1a\u901a\u8fc7\u8fed\u4ee3\u7f51\u7edc\u641c\u7d22\u68c0\u7d22\u8bc1\u636e\uff0c\u83b7\u53d6\u3001\u8fc7\u6ee4\u548c\u89e3\u6790\u5168\u6587\u6765\u6e90\uff08\u5305\u62ecPDF\uff09\uff0c\u8bc4\u4f30\u5f15\u7528\u6750\u6599\u662f\u5426\u652f\u6301\u751f\u6210\u5185\u5bb9\u3002", "result": "1. \u5373\u4f7f\u5728\u7f51\u7edc\u641c\u7d22\u652f\u6301\u4e0b\uff0c\u524d\u6cbf\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u7684\u5e7b\u89c9\u7387\u4ecd\u7136\u5f88\u9ad8\uff08\u6700\u5f3a\u914d\u7f6eOpus-4.5\u52a0\u7f51\u7edc\u641c\u7d22\u7ea6\u4e3a30%\uff09\uff1b2. \u5185\u5bb9\u57fa\u7840\u9519\u8bef\u6301\u7eed\u9ad8\u53d1\uff1b3. \u5e7b\u89c9\u884c\u4e3a\u53d7\u6a21\u578b\u80fd\u529b\u3001\u5bf9\u8bdd\u8f6e\u6b21\u4f4d\u7f6e\u3001\u6709\u6548\u63a8\u7406\u548c\u6240\u9700\u77e5\u8bc6\u7c7b\u578b\u7684\u5f71\u54cd\u3002", "conclusion": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u4ecd\u7136\u4e25\u91cd\uff0c\u5373\u4f7f\u7ed3\u5408\u7f51\u7edc\u641c\u7d22\u4e5f\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u673a\u5236\u6765\u786e\u4fddLLM\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u3002\u5e7b\u89c9\u884c\u4e3a\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.00361", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.00361", "abs": "https://arxiv.org/abs/2602.00361", "authors": ["Philipp Altmann", "Maximilian Mansky", "Maximilian Zorn", "Jonas Stein", "Claudia Linnhoff-Popien"], "title": "Quantum Generator Kernels", "comment": "28 pages, 4 figures, 8 tables, under review", "summary": "Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u751f\u6210\u6838(QGK)\uff0c\u901a\u8fc7\u53d8\u5206\u751f\u6210\u7ec4(VGG)\u5c06\u901a\u7528\u751f\u6210\u5668\u5408\u5e76\u4e3a\u53ef\u53c2\u6570\u5316\u7b97\u5b50\uff0c\u5b9e\u73b0\u91cf\u5b50\u7a7a\u95f4\u7684\u89c4\u6a21\u5316\u8986\u76d6\uff0c\u89e3\u51b3NISQ\u8bbe\u5907\u6570\u636e\u5d4c\u5165\u9650\u5236\u95ee\u9898", "motivation": "\u91cf\u5b50\u6838\u65b9\u6cd5\u7406\u8bba\u4e0a\u80fd\u5b9e\u73b0\u7ecf\u5178\u4e0d\u53ef\u5206\u7279\u5f81\u5728\u91cf\u5b50\u7a7a\u95f4\u7684\u53ef\u5206\u6027\uff0c\u4f46\u53d7\u9650\u4e8eNISQ\u786c\u4ef6\u80fd\u529b\uff0c\u9700\u8981\u6709\u6548\u7b56\u7565\u5c06\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u538b\u7f29\u5d4c\u5165\u5230\u53d7\u9650\u7684\u91cf\u5b50\u8bbe\u5907\u4e2d\u3002\u73b0\u6709\u6df7\u5408\u67b6\u6784\u7684\u56fa\u5b9a\u4e2d\u95f4\u5d4c\u5165\u8fc7\u7a0b\u53ef\u80fd\u963b\u788d\u91cf\u5b50\u8ba1\u7b97\u6f5c\u529b\u7684\u5145\u5206\u53d1\u6325", "method": "\u63d0\u51fa\u91cf\u5b50\u751f\u6210\u6838(QGK)\u65b9\u6cd5\uff0c\u5305\u542b\u4e00\u7ec4\u53d8\u5206\u751f\u6210\u7ec4(VGG)\uff0c\u5c06\u901a\u7528\u751f\u6210\u5668\u5408\u5e76\u4e3a\u53ef\u53c2\u6570\u5316\u7b97\u5b50\uff0c\u786e\u4fdd\u91cf\u5b50\u7a7a\u95f4\u7684\u89c4\u6a21\u5316\u8986\u76d6\u3002\u901a\u8fc7\u8bad\u7ec3\u6743\u91cd\u5411\u91cf\u53c2\u6570\u5316VGG\u5728\u5f53\u524d\u6570\u636e\u4e0a\u4e0b\u6587\u4e2d\u7684\u6295\u5f71\uff0c\u4f18\u5316\u6838\u4e0e\u76ee\u6807\u9886\u57df\u7684\u5bf9\u9f50", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQGK\u5728\u6295\u5f71\u548c\u5206\u7c7b\u80fd\u529b\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cf\u5b50\u6838\u548c\u7ecf\u5178\u6838\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u4f5c\u4e3a\u5404\u79cdQML\u5e94\u7528\u901a\u7528\u6846\u67b6\u7684\u6f5c\u529b", "conclusion": "\u91cf\u5b50\u751f\u6210\u6838(QGK)\u901a\u8fc7\u751f\u6210\u5668\u57fa\u7840\u7684\u91cf\u5b50\u6838\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86NISQ\u65f6\u4ee3\u5927\u89c4\u6a21\u6570\u636e\u5d4c\u5165\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u6709\u671b\u6210\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u901a\u7528\u6846\u67b6"}}
{"id": "2602.01063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01063", "abs": "https://arxiv.org/abs/2602.01063", "authors": ["Bin Han", "Deuksin Kwon", "Jonathan Gratch"], "title": "Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u76f8\u540c\u4eba\u683c\u63d0\u793a\u4e0b\uff0c\u4f1a\u6839\u636e\u4e0d\u540c\u5bf9\u8bdd\u573a\u666f\uff08\u7834\u51b0\u3001\u8c08\u5224\u3001\u7fa4\u4f53\u51b3\u7b56\u3001\u5171\u60c5\uff09\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u8bed\u8a00\u3001\u884c\u4e3a\u548c\u60c5\u611f\u7ed3\u679c\uff0c\u8fd9\u79cd\u53d8\u5316\u53cd\u6620\u4e86\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4eba\u683c\u8868\u8fbe\u800c\u975e\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u53ef\u4ee5\u901a\u8fc7\u660e\u786e\u7684\u4eba\u683c\u63d0\u793a\u8fdb\u884c\u6761\u4ef6\u8bbe\u5b9a\uff0c\u4f46\u5176\u884c\u4e3a\u5b9e\u73b0\u5f80\u5f80\u968f\u4e0a\u4e0b\u6587\u53d8\u5316\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u76f8\u540c\u4eba\u683c\u63d0\u793a\u5728\u4e0d\u540c\u5bf9\u8bdd\u573a\u666f\u4e2d\u5982\u4f55\u5bfc\u81f4\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5e76\u63a2\u8ba8\u8fd9\u79cd\u53d8\u5316\u662fLLMs\u7684\u4e0d\u4e00\u81f4\u6027\u8fd8\u662f\u7c7b\u4f3c\u4eba\u7c7b\u884c\u4e3a\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u9002\u5e94\u3002", "method": "\u7814\u7a76\u5728\u56db\u79cd\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u6d4b\u8bd5\u76f8\u540c\u4eba\u683c\u63d0\u793a\uff1a\u7834\u51b0\u4efb\u52a1\u3001\u8c08\u5224\u4efb\u52a1\u3001\u7fa4\u4f53\u51b3\u7b56\u4efb\u52a1\u548c\u5171\u60c5\u4efb\u52a1\uff0c\u5206\u6790LLMs\u5728\u8fd9\u4e9b\u4e0d\u540c\u793e\u4ea4\u548c\u60c5\u611f\u9700\u6c42\u60c5\u5883\u4e0b\u7684\u8bed\u8a00\u3001\u884c\u4e3a\u548c\u60c5\u611f\u7ed3\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7cfb\u7edf\u5730\u5f71\u54cd\u4eba\u683c\u8868\u8fbe\u548c\u60c5\u611f\u8bed\u8c03\uff0c\u76f8\u540c\u7279\u8d28\u5728\u4e0d\u540c\u793e\u4ea4\u548c\u60c5\u611f\u9700\u6c42\u4e0b\u4ee5\u4e0d\u540c\u65b9\u5f0f\u8868\u8fbe\u3002LLMs\u8868\u73b0\u51fa\u4e0a\u4e0b\u6587\u654f\u611f\u800c\u975e\u56fa\u5b9a\u7684\u4eba\u683c\u8868\u8fbe\uff0c\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u793e\u4ea4\u4e92\u52a8\u76ee\u6807\u548c\u60c5\u611f\u6761\u4ef6\u3002", "conclusion": "\u4ece\u6574\u4f53\u7279\u8d28\u7406\u8bba\u89c6\u89d2\u770b\uff0cLLMs\u5c55\u73b0\u51fa\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u4eba\u683c\u8868\u8fbe\uff0c\u8fd9\u79cd\u7075\u6d3b\u6027\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u884c\u4e3a\u9002\u5e94\uff0c\u800c\u975e\u7cfb\u7edf\u4e0d\u4e00\u81f4\u6027\u3002\u8fd9\u5bf9\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00450", "abs": "https://arxiv.org/abs/2602.00450", "authors": ["Ethan Anderson", "Justin Silva", "Kyle Zheng", "Sameer Pusegaonkar", "Yizhou Wang", "Zheng Tang", "Sujit Biswas"], "title": "Model Optimization for Multi-Camera 3D Detection and Tracking", "comment": null, "summary": "Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.", "AI": {"tldr": "Sparse4D\u591a\u76f8\u673a3D\u68c0\u6d4b\u8ddf\u8e2a\u6846\u67b6\u5728\u4f4e\u5e27\u7387\u3001\u91cf\u5316\u538b\u7f29\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u548c\u6df7\u5408\u7cbe\u5ea6\u4f18\u5316\u65b9\u9762\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u8eab\u4efd\u7a33\u5b9a\u6027\u6307\u6807\u3002", "motivation": "\u5ba4\u5185\u591a\u76f8\u673a\u611f\u77e5\u7cfb\u7edf\u9700\u8981\u5904\u7406\u906e\u6321\u548c\u5f02\u6784\u89c6\u89d2\u4e0b\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5e27\u7387\u3001\u8ba1\u7b97\u6548\u7387\u548c\u8de8\u573a\u666f\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u8bc4\u4f30Sparse4D\u6846\u67b6\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff1a\u964d\u4f4e\u8f93\u5165\u5e27\u7387\u3001\u540e\u8bad\u7ec3\u91cf\u5316\uff08INT8/FP8\uff09\u3001\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u5230WILDTRACK\u3001\u4ee5\u53caTransformer Engine\u6df7\u5408\u7cbe\u5ea6\u5fae\u8c03\u3002", "result": "Sparse4D\u5728\u9002\u5ea6\u964d\u4f4e\u5e27\u7387\u65f6\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u4f4e\u4e8e2FPS\u65f6\u8eab\u4efd\u5173\u8054\u5d29\u6e83\uff1b\u9009\u62e9\u6027\u91cf\u5316\u63d0\u4f9b\u6700\u4f73\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\uff1b\u4f4e\u5e27\u7387\u9884\u8bad\u7ec3\u5728WILDTRACK\u4e0a\u5e26\u6765\u663e\u8457\u96f6\u6837\u672c\u63d0\u5347\uff1b\u6df7\u5408\u7cbe\u5ea6\u964d\u4f4e\u5ef6\u8fdf\u4f46\u53ef\u80fd\u7834\u574f\u8eab\u4efd\u4f20\u64ad\u7a33\u5b9a\u6027\u3002", "conclusion": "\u591a\u76f8\u673a\u8ddf\u8e2a\u7cfb\u7edf\u9700\u8981\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u8eab\u4efd\u7a33\u5b9a\u6027\uff0c\u9009\u62e9\u6027\u91cf\u5316\u662f\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4f4e\u5e27\u7387\u9884\u8bad\u7ec3\u6709\u52a9\u4e8e\u8de8\u573a\u666f\u6cdb\u5316\uff0c\u6df7\u5408\u7cbe\u5ea6\u4f18\u5316\u9700\u8981\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2602.01034", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01034", "abs": "https://arxiv.org/abs/2602.01034", "authors": ["Xiangwei Wang", "Wei Wang", "Ken Chen", "Nanduni Nimalsiri", "Saman Halgamuge"], "title": "Discovering Process-Outcome Credit in Multi-Step LLM Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9010\u6b65\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u673a\u5236\u63d0\u4f9b\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408\u89e3\u8026\u63a9\u7801\u7b56\u7565\u548c\u53cc\u95e8\u76d1\u7763\u5fae\u8c03\u76ee\u6807\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\u3001\u6709\u6548\u8fc7\u6ee4\u8bad\u7ec3\u566a\u58f0\u5e76\u5b9e\u73b0\u89e3\u8026\u4fe1\u7528\u5206\u914d\u7684\u6846\u67b6\u3002", "method": "1. \u9010\u6b65\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u673a\u5236\uff1a\u91cf\u5316\u63a8\u7406\u6b65\u9aa4\u76f8\u5bf9\u4e8e\u5355\u8c03\u5386\u53f2\u6c34\u5370\u7684\u5185\u5728\u4ef7\u503c\uff1b2. \u89e3\u8026\u63a9\u7801\u7b56\u7565\uff1a\u8fc7\u7a0b\u5bfc\u5411\u5956\u52b1\u5e94\u7528\u4e8e\u601d\u7ef4\u94fe\uff0c\u7ed3\u679c\u5bfc\u5411\u5956\u52b1\u5e94\u7528\u4e8e\u5b8c\u6574\u5b8c\u6210\uff1b3. \u53cc\u95e8\u76d1\u7763\u5fae\u8c03\u76ee\u6807\uff1a\u5229\u7528\u9ad8\u8d28\u91cf\u7ed3\u6784\u548c\u4e8b\u5b9e\u4fe1\u53f7\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5982MATH\u3001Super-CLEVR\uff09\u6301\u7eed\u4f18\u4e8eGRPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u7387\u4e0a\u90fd\u6709\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5e03\u5916\u9c81\u68d2\u6027\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\u3001\u89e3\u8026\u4fe1\u7528\u5206\u914d\u548c\u7a33\u5b9a\u8bad\u7ec3\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.00372", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00372", "abs": "https://arxiv.org/abs/2602.00372", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation", "comment": "16 pages, 10 tables, 4 figures", "summary": "Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.\n  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.\n  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.", "AI": {"tldr": "SparseKD\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u5316SVD\u526a\u679d\u548c\u81ea\u53c2\u8003\u77e5\u8bc6\u84b8\u998f\uff0c\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6a21\u578b\u5373\u53ef\u5b9e\u73b015-65%\u7684\u53c2\u6570\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u8d28\u91cf\u635f\u5931\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5916\u90e8\u6559\u5e08\u6a21\u578b\u6216\u67b6\u6784\u4fee\u6539\uff0c\u589e\u52a0\u4e86\u90e8\u7f72\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316SVD\u526a\u679d\u548c\u81ea\u53c2\u8003\u77e5\u8bc6\u84b8\u998f\uff1a\u6a21\u578b\u901a\u8fc7\u5339\u914d\u538b\u7f29\u524d\u7684\u81ea\u8eab\u6982\u7387\u5206\u5e03\u6765\"\u81ea\u6211\u6559\u5b66\"\uff0c\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6a21\u578b\u3002\u4f7f\u7528\u56fa\u5b9a\u6821\u51c6\u6570\u636e\u96c6\u548c\u540e\u8bad\u7ec3\u65b9\u5f0f\u3002", "result": "\u81ea\u53c2\u8003\u84b8\u998f\u5355\u72ec\u5e94\u7528\u65f6\uff0c\u76f8\u6bd4\u539f\u59cb\u6536\u655b\u68c0\u67e5\u70b9\u76f8\u5bf9\u63d0\u534739%\u8d28\u91cf\u3002\u7ed3\u5408\u526a\u679d\u53ef\u5b9e\u73b015-65%\u53c2\u6570\u51cf\u5c11\uff0c\u8d28\u91cf\u635f\u5931\u53ef\u63a5\u53d7\u3002\u901f\u5ea6\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u524d\u9988\u5c42\u7684\u5bc6\u96c6\u77e9\u9635\u4e58\u6cd5\u51cf\u5c11\uff0c\u6ce8\u610f\u529b\u5c42\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "SparseKD\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6a21\u578b\u3001\u67b6\u6784\u4fee\u6539\u6216\u5b9a\u5236\u63a8\u7406\u5185\u6838\u7684\u5b9e\u7528\u538b\u7f29\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u5728\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\uff0c\u4e0e\u6ce8\u610f\u529b\u4f18\u5316\u65b9\u6cd5\u4e92\u8865\u3002"}}
{"id": "2602.01064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01064", "abs": "https://arxiv.org/abs/2602.01064", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuo Yang", "Chu Yuan Zhang", "Jianhua Tao"], "title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs", "comment": null, "summary": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \\textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u77e5\u8bc6\u51c0\u5316\"\u6982\u5ff5\uff0c\u5c06\u591a\u4e2a\u6559\u5e08LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u6574\u5408\u4e3a\u5355\u4e00\u63a8\u7406\uff0c\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u548c\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u51c0\u5316\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u5229\u7528\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u65f6\u9762\u4e34\u77e5\u8bc6\u51b2\u7a81\u548c\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6574\u5408\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u51c0\u5316\u6982\u5ff5\uff0c\u5c06\u591a\u4e2a\u6559\u5e08LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u6574\u5408\u4e3a\u5355\u4e00\u63a8\u7406\uff1b\u8bbe\u8ba1\u4e86\u4e94\u79cd\u4e0d\u540c\u89d2\u5ea6\u7684\u51c0\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\u7b49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u84b8\u998f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u6709\u6548\u7f13\u89e3\u4e86\u77e5\u8bc6\u51b2\u7a81\uff1b\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u77e5\u8bc6\u51c0\u5316\u6280\u672f\u80fd\u591f\u4f18\u5316\u591a\u6559\u5e08\u84b8\u998f\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u5f3a\u5927\u4e14\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u521b\u65b0\u51c0\u5316\u6280\u672f\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.00462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00462", "abs": "https://arxiv.org/abs/2602.00462", "authors": ["Benno Krojer", "Shravan Nayak", "Oscar Ma\u00f1as", "Vaibhav Adlakha", "Desmond Elliott", "Siva Reddy", "Marius Mosbach"], "title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs", "comment": null, "summary": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.", "AI": {"tldr": "LatentLens\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u6807\u8bb0\u8868\u793a\u4e0e\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u7684\u4e0a\u4e0b\u6587\u5316\u6587\u672c\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u6807\u8bb0\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u6807\u8bb0\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u4e3a\u4ec0\u4e48\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8f7b\u677e\u5904\u7406\u89c6\u89c9\u6807\u8bb0\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6765\u63ed\u793aLLM\u5904\u7406\u8fc7\u7a0b\u4e2d\u6bcf\u4e00\u5c42\u89c6\u89c9\u6807\u8bb0\u8868\u793a\u6240\u7f16\u7801\u7684\u5185\u5bb9\u3002\u73b0\u6709\u65b9\u6cd5\u5982LogitLens\u4f4e\u4f30\u4e86\u89c6\u89c9\u6807\u8bb0\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "LatentLens\u901a\u8fc7\u7f16\u7801\u5927\u578b\u6587\u672c\u8bed\u6599\u5e93\u5e76\u5b58\u50a8\u6bcf\u4e2a\u6807\u8bb0\u7684\u4e0a\u4e0b\u6587\u5316\u8868\u793a\uff0c\u7136\u540e\u5c06\u89c6\u89c9\u6807\u8bb0\u8868\u793a\u4e0e\u8fd9\u4e9b\u6587\u672c\u8868\u793a\u8fdb\u884c\u6bd4\u8f83\uff0c\u901a\u8fc7top-k\u6700\u8fd1\u90bb\u8868\u793a\u63d0\u4f9b\u89c6\u89c9\u6807\u8bb0\u7684\u63cf\u8ff0\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u7684VLM\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u4e0eLogitLens\u76f8\u6bd4\uff0cLatentLens\u4f7f\u5927\u591a\u6570\u89c6\u89c9\u6807\u8bb0\u5728\u6240\u6709\u7814\u7a76\u6a21\u578b\u548c\u6240\u6709\u5c42\u4e2d\u90fd\u53ef\u89e3\u91ca\uff0c\u4ea7\u751f\u7684\u63cf\u8ff0\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u8bc1\u636e\uff0c\u4e3a\u5206\u6790\u6f5c\u5728\u8868\u793a\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6807\u8bb0\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.01062", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01062", "abs": "https://arxiv.org/abs/2602.01062", "authors": ["Chenyi Li", "Yuan Zhang", "Bo Wang", "Guoqing Ma", "Wei Tang", "Haoyang Huang", "Nan Duan"], "title": "SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u76f8\u4f3c\u5ea6\u7684\u8f68\u8ff9\u7ea7\u591a\u6837\u6027\u76ee\u6807\uff0c\u901a\u8fc7\u7559\u4e00\u6cd5\u8fb9\u9645\u8d21\u732e\u8ba1\u7b97\uff0c\u5c06\u591a\u6837\u6027\u4f5c\u4e3a\u4f18\u52bf\u5851\u5f62\u9879\u878d\u5165\u7b56\u7565\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u65f6\u5bfc\u81f4\u7684\u8f93\u51fa\u591a\u6837\u6027\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5f80\u5f80\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\uff0c\u6a21\u578b\u5c06\u6982\u7387\u8d28\u91cf\u96c6\u4e2d\u5728\u5c11\u6570\u89e3\u51b3\u65b9\u6848\u4e0a\u3002\u8fd9\u8fdd\u80cc\u4e86\u591a\u6837\u6027\u539f\u5219\uff0c\u9700\u8981\u5e73\u8861\u6027\u80fd\u63d0\u5347\u4e0e\u8f93\u51fa\u591a\u6837\u6027\u3002", "method": "1. \u57fa\u4e8e\u6838\u76f8\u4f3c\u5ea6\u5b9a\u4e49\u8f68\u8ff9\u7ea7\u591a\u6837\u6027\u76ee\u6807\uff1b2. \u4f7f\u7528\u7559\u4e00\u6cd5\u8ba1\u7b97\u6bcf\u4e2a\u91c7\u6837\u8f68\u8ff9\u7684\u8fb9\u9645\u8d21\u732e\uff1b3. \u5c06\u591a\u6837\u6027\u76ee\u6807\u4f5c\u4e3a\u53ef\u63d2\u62d4\u7684\u4f18\u52bf\u5851\u5f62\u9879\u878d\u5165\u7b56\u7565\u4f18\u5316\uff1b4. \u901a\u8fc7\u5206\u5e03\u6270\u52a8\u6846\u67b6\u5206\u6790\u5355\u4e2a\u8f68\u8ff9\u5bf9\u8bed\u8a00\u6a21\u578b\u591a\u6837\u6027\u7684\u8d21\u732e\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u7a00\u6709\u8f68\u8ff9\u5177\u6709\u66f4\u9ad8\u8fb9\u9645\u8d21\u732e\u7684\u5355\u8c03\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Pass@1\u548cPass@K\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u4e86\u8f93\u51fa\u591a\u6837\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6838\u76f8\u4f3c\u5ea6\u7684\u591a\u6837\u6027\u76ee\u6807\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u591a\u6837\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7a00\u6709\u8f68\u8ff9\u5bf9\u5168\u5c40\u591a\u6837\u6027\u7684\u91cd\u8981\u8d21\u732e\uff0c\u4e3a\u5e73\u8861LLM\u63a8\u7406\u6027\u80fd\u4e0e\u8f93\u51fa\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00376", "abs": "https://arxiv.org/abs/2602.00376", "authors": ["Delia McGrath", "Curtis Chong", "Rohil Kulkarni", "Gerbrand Ceder", "Adeesh Kolluru"], "title": "MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science", "comment": "17 pages, 9 Figures, submitted", "summary": "Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.", "AI": {"tldr": "MATRIX\u662f\u4e00\u4e2a\u6750\u6599\u79d1\u5b66\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u5b9e\u9a8c\u6570\u636e\u5bf9\u79d1\u5b66\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u8868\u660e\u89c6\u89c9\u76d1\u7763\u80fd\u663e\u8457\u63d0\u5347\u5b9e\u9a8c\u89e3\u91ca\u548c\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u8bc4\u4f30\u89c6\u89c9\u5b9e\u9a8c\u6570\u636e\u5728\u8bad\u7ec3\u540e\u9636\u6bb5\u662f\u5426\u80fd\u591f\u8d85\u8d8a\u7eaf\u6587\u672c\u76d1\u7763\uff0c\u63d0\u5347\u57fa\u4e8e\u673a\u5236\u7684\u89e3\u91ca\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6765\u9694\u79bb\u89c6\u89c9\u57fa\u7840\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165MATRIX\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u7eaf\u6587\u672c\u8bad\u7ec3\u540e\u4e0e\u5305\u542b\u914d\u5bf9\u5b9e\u9a8c\u56fe\u50cf\u7684\u8bad\u7ec3\u540e\uff0c\u9694\u79bb\u89c6\u89c9\u57fa\u7840\u7684\u5f71\u54cd\u3002\u57fa\u51c6\u6db5\u76d6\u57fa\u7840\u7406\u8bba\u3001\u7814\u7a76\u7ea7\u63a8\u7406\u548c\u591a\u79cd\u8868\u5f81\u6a21\u6001\u7684\u5b9e\u9a8c\u5de5\u4ef6\u89e3\u91ca\u3002", "result": "\u89c6\u89c9\u76d1\u7763\u663e\u8457\u6539\u5584\u5b9e\u9a8c\u89e3\u91ca\uff08\u63d0\u534710-25%\uff09\u548c\u7eaf\u6587\u672c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\uff08\u63d0\u53475-16%\uff09\u3002\u6539\u8fdb\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u540e\u9636\u6bb5\u6b63\u786e\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff0c\u4f53\u73b0\u4e86\u8de8\u6a21\u6001\u8868\u5f81\u8fc1\u79fb\u3002\u5728ScienceQA\u548cPubMedQA\u4e0a\u4e5f\u89c2\u5bdf\u5230\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u6a21\u6001\u8bad\u7ec3\u540e\u7684\u76ca\u5904\u4e0d\u4ec5\u9650\u4e8e\u6750\u6599\u79d1\u5b66\uff0c\u80fd\u6269\u5c55\u5230\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u3002MATRIX\u57fa\u51c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.01068", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01068", "abs": "https://arxiv.org/abs/2602.01068", "authors": ["Chaoqun Cui", "Shijing Wang", "Liangbin Huang", "Qingqing Gu", "Zhaolong Huang", "Xiao Zeng", "Wenji Mao"], "title": "From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization", "comment": "Accepted to ICLR 2026", "summary": "The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7ffb\u8bd1\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u5f71\u89c6\u5b57\u5e55\u7ffb\u8bd1\u4e3a\u4f8b\uff0c\u63d0\u51fa\u4e86ALPO\u65b9\u6cd5\u8bad\u7ec3\u8868\u8fbe\u751f\u52a8\u7684\u7ffb\u8bd1\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u4e86\u591a\u8bed\u8a00\u5b57\u5e55\u5e73\u884c\u8bed\u6599\u5e93\u3002", "motivation": "\u968f\u7740\u5e94\u7528\u573a\u666f\u590d\u6742\u5316\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7ffb\u8bd1\u7684\u5c40\u9650\u6027\u9010\u6e10\u663e\u73b0\uff0c\u7279\u522b\u662f\u5728\u5f71\u89c6\u5b57\u5e55\u7ffb\u8bd1\u7b49\u9700\u8981\u8868\u8fbe\u751f\u52a8\u6027\u7684\u9886\u57df\uff0c\u9700\u8981\u6784\u5efa\u6ee1\u8db3\u9886\u57df\u5b9a\u5236\u9700\u6c42\u7684\u7ffb\u8bd1\u6a21\u578b\u3002", "method": "1. \u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u591a\u8bed\u8a00\u5b57\u5e55\u5e73\u884c\u8bed\u6599\u5e93\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u81ea\u9002\u5e94\u5c40\u90e8\u504f\u597d\u4f18\u5316\uff08ALPO\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u7ec6\u7c92\u5ea6\u504f\u597d\u5bf9\u9f50\u95ee\u9898\uff1b3. \u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u7ffb\u8bd1\u5956\u52b1\u6a21\u578b\u548c\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\u3002", "result": "ALPO\u65b9\u6cd5\u5728\u7ffb\u8bd1\u8d28\u91cf\u7684\u591a\u7ef4\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\uff0c\u80fd\u591f\u8bad\u7ec3\u51fa\u8868\u8fbe\u751f\u52a8\u7684\u7ffb\u8bd1\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7ffb\u8bd1\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0cALPO\u65b9\u6cd5\u5728\u5b57\u5e55\u7ffb\u8bd1\u7b49\u9700\u8981\u8868\u8fbe\u751f\u52a8\u6027\u7684\u9886\u57df\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.00463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00463", "abs": "https://arxiv.org/abs/2602.00463", "authors": ["Xin Zhang", "Shen Chen", "Jiale Zhou", "Lei Li"], "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting", "comment": "Accepted to ICASSP2026", "summary": "Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.", "AI": {"tldr": "PSGS\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f3D\u573a\u666f\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u67b6\u6784\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u5168\u666f\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u5168\u666f\u6ed1\u52a8\u673a\u5236\u521d\u59cb\u5316\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u70b9\u4e91\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6709\u9650\u76843D-\u6587\u672c\u914d\u5bf9\u6570\u636e\uff1b2\uff09\u591a\u89c6\u89d2\u62fc\u63a5\u4e0d\u4e00\u81f4\u5bfc\u81f4\u573a\u666f\u8fc7\u4e8e\u7b80\u5355\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u5728VR\u3001AR\u548c\u6e38\u620f\u7b49\u6c89\u6d78\u5f0f\u5e94\u7528\u4e2d\u7684\u9ad8\u8d28\u91cf\u573a\u666f\u751f\u6210\u3002", "method": "PSGS\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u662f\u53cc\u5c42\u4f18\u5316\u67b6\u6784\uff0c\u5305\u62ec\u5e03\u5c40\u63a8\u7406\u5c42\uff08\u5c06\u6587\u672c\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u7a7a\u95f4\u5173\u7cfb\uff09\u548c\u81ea\u4f18\u5316\u5c42\uff08\u901a\u8fc7\u8fed\u4ee3MLLM\u53cd\u9988\u7ec6\u5316\u89c6\u89c9\u7ec6\u8282\uff09\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u5168\u666f\u6ed1\u52a8\u673a\u5236\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u91c7\u6837\u91cd\u53e0\u89c6\u89d2\u521d\u59cb\u5316\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u70b9\u4e91\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u7ed3\u5408\u6df1\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePSGS\u5728\u5168\u666f\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u5438\u5f15\u529b\u76843D\u573a\u666f\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u6c89\u6d78\u5f0f\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PSGS\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5c42\u4f18\u5316\u67b6\u6784\u548c\u5168\u666f\u6ed1\u52a8\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u9a71\u52a83D\u573a\u666f\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u6c89\u6d78\u5f0f\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2602.01075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01075", "abs": "https://arxiv.org/abs/2602.01075", "authors": ["Yepeng Liu", "Yu Huang", "Yu-Xiang Wang", "Yingbin Liang", "Yuheng Bu"], "title": "ConvexBench: Can LLMs Recognize Convex Functions?", "comment": null, "summary": "Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically verifiable benchmark for testing \\textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \\textit{parsing failure} and \\textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u673a\u68b0\u9a8c\u8bc1\u7684\u57fa\u51c6\u6d4b\u8bd5CB\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u51fd\u6570\u590d\u5408\u4e0b\u8bc6\u522b\u7b26\u53f7\u76ee\u6807\u51f8\u6027\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5206\u6cbb\u7684\u4ee3\u7406\u6846\u67b6\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u59cb\u81ea\u52a8\u5316\u7814\u7a76\u7ea7\u6570\u5b66\u548c\u79d1\u5b66\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u7406\u89e3\u548c\u63a8\u7406\u51f8\u6027\u7684\u80fd\u529b\u3002\u51f8\u5206\u6790\u662f\u73b0\u4ee3\u6570\u5b66\u7684\u91cd\u8981\u5206\u652f\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u6d4b\u8bd5LLMs\u5728\u6df1\u5ea6\u51fd\u6570\u590d\u5408\u4e0b\u8bc6\u522b\u51f8\u6027\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86CB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u8bc6\u522b\u7b26\u53f7\u76ee\u6807\u51f8\u6027\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u524d\u6cbfLLMs\u5b58\u5728\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5206\u6cbb\u7684\u4ee3\u7406\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u6784\u5efa\u62bd\u8c61\u8bed\u6cd5\u6811\u6765\u5378\u8f7d\u89e3\u6790\u4efb\u52a1\uff1b2\uff09\u901a\u8fc7\u805a\u7126\u4e0a\u4e0b\u6587\u5bf9\u6bcf\u4e2a\u4e2d\u95f4\u5b50\u8868\u8fbe\u5f0f\u8fdb\u884c\u9012\u5f52\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5b58\u5728\u660e\u663e\u7684\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff1a\u968f\u7740\u6df1\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4ece\u6df1\u5ea62\u65f6\u7684F1\u5206\u65701.0\u964d\u81f3\u6df1\u5ea6100\u65f6\u7684\u7ea60.2\u3002\u63d0\u51fa\u7684\u5206\u6cbb\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u6df1\u5ea6\u590d\u5408\u5931\u8d25\uff0c\u5728\u8f83\u5927\u6df1\u5ea6\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u5982\u6df1\u5ea6100\u65f6F1\u5206\u6570\u8fbe\u52301.0\uff09\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5ea6\u51fd\u6570\u590d\u5408\u7684\u51f8\u6027\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b58\u5728\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff0c\u4e3b\u8981\u8868\u73b0\u4e3a\u89e3\u6790\u5931\u8d25\u548c\u60f0\u6027\u63a8\u7406\u3002\u63d0\u51fa\u7684\u5206\u6cbb\u4ee3\u7406\u6846\u67b6\u901a\u8fc7\u5916\u90e8\u89e3\u6790\u5de5\u5177\u548c\u9012\u5f52\u63a8\u7406\u673a\u5236\uff0c\u80fd\u591f\u53ef\u9760\u5730\u7f13\u89e3\u8fd9\u4e9b\u6df1\u5ea6\u590d\u5408\u5931\u8d25\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u7ec4\u5408\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.00384", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00384", "abs": "https://arxiv.org/abs/2602.00384", "authors": ["Ke Wang", "Nguyen Gia Hien Vu", "Yifan Tang", "Mostafa Rahmani Dehaghani", "G. Gary Wang"], "title": "RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints", "comment": null, "summary": "This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eRePaint\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u5f15\u5bfcDDPM\u6a21\u578b\uff0c\u7528\u4e8e\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u6ee1\u8db3\u6027\u80fd\u548c\u53c2\u6570\u7ea6\u675f\u7684\u8bbe\u8ba1\u751f\u6210\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eDDPM\u7684\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u6027\u80fd\u548c\u53c2\u6570\u7ea6\u675f\uff0c\u4e14\u4e0d\u80fd\u57fa\u4e8e\u90e8\u5206\u53c2\u8003\u8bbe\u8ba1\u751f\u6210\u7f3a\u5931\u7ec4\u4ef6\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u80fd\u9ad8\u6548\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528RePaint\u589e\u5f3a\u6846\u67b6\uff0c\u96c6\u6210\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u5f15\u5bfcDDPM\u6a21\u578b\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e94\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u91cd\u91c7\u6837\uff0c\u5b9e\u73b0\u5bf9\u90e8\u5206\u8bbe\u8ba1\u7684\u53ef\u63a7\u91cd\u7ed8\uff0c\u540c\u65f6\u6ee1\u8db3\u6027\u80fd\u548c\u53c2\u6570\u7ea6\u675f\u3002", "result": "\u5728\u53c2\u6570\u5316\u8239\u4f53\u8bbe\u8ba1\u548c\u7ffc\u578b\u8bbe\u8ba1\u4e24\u4e2a\u4ee3\u8868\u6027\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff0c\u80fd\u591f\u57fa\u4e8e\u90e8\u5206\u53c2\u8003\u8bbe\u8ba1\u751f\u6210\u5177\u6709\u9884\u671f\u6027\u80fd\u7684\u65b0\u9896\u8bbe\u8ba1\uff0c\u7cbe\u5ea6\u8fbe\u5230\u6216\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53c2\u6570\u7ea6\u675f\u611f\u77e5\u751f\u6210\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u901a\u8fc7\u56fa\u5b9a\u90e8\u5206\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u63a7\u521b\u65b0\u3002"}}
{"id": "2602.01070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01070", "abs": "https://arxiv.org/abs/2602.01070", "authors": ["Ahsan Bilal", "Ahmed Mohsin", "Muhammad Umer", "Ali Subhan", "Hassan Rizwan", "Ayesha Mohsin", "Dean Hougen"], "title": "What If We Allocate Test-Time Compute Adaptively?", "comment": null, "summary": "Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.", "AI": {"tldr": "\u63d0\u51fa\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u8f68\u8ff9\u751f\u6210\u4e0e\u9009\u62e9\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u663e\u8457\u4f18\u4e8e\u5747\u5300\u8ba1\u7b97\u5206\u914d\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a\u5747\u5300\u5206\u914d\u63a8\u7406\u8ba1\u7b97\u3001\u4f7f\u7528\u56fa\u5b9a\u91c7\u6837\u7b56\u7565\u3001\u4ec5\u5c06\u9a8c\u8bc1\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u3002\u8fd9\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u65e0\u6cd5\u6839\u636e\u95ee\u9898\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u5f3a\u5ea6\u3002", "method": "\u63d0\u51fa\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u89c6\u4e3a\u8fed\u4ee3\u8f68\u8ff9\u751f\u6210\u4e0e\u9009\u62e9\u8fc7\u7a0b\u3002\u6bcf\u4e2a\u95ee\u9898\u8fd0\u884c\u591a\u6b21\u63a8\u7406\u8fed\u4ee3\uff0c\u6bcf\u8f6e\u8fed\u4ee3\u53ef\u9009\u751f\u6210\u9ad8\u5c42\u8ba1\u5212\u3001\u9009\u62e9\u63a8\u7406\u5de5\u5177\u548c\u8ba1\u7b97\u7b56\u7565\uff0c\u5e76\u751f\u6210\u5019\u9009\u63a8\u7406\u8f68\u8ff9\u3002\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u4f5c\u4e3a\u7edf\u4e00\u63a7\u5236\u4fe1\u53f7\uff1a\u5728\u8fed\u4ee3\u5185\uff0c\u805a\u5408\u6b65\u7ea7PRM\u5206\u6570\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u526a\u679d\u548c\u6269\u5c55\uff1b\u5728\u8fed\u4ee3\u95f4\uff0c\u805a\u5408\u8f68\u8ff9\u5956\u52b1\u7528\u4e8e\u9009\u62e9\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u52a8\u6001PRM\u5f15\u5bfc\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u76f4\u63a5\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u5728MATH-500\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728AIME24\u548cAMO-Bench\u7b49\u66f4\u96be\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6570\u500d\u6539\u8fdb\u3002\u901a\u8fc7\u7406\u8bbaFLOPs\u548c\u8ba1\u7b97\u5f3a\u5ea6\u6307\u6807\u8bc4\u4f30\u6548\u7387\uff0c\u9a8c\u8bc1\u5f15\u5bfc\u7684\u5206\u914d\u5c06\u8ba1\u7b97\u96c6\u4e2d\u5728\u9ad8\u6548\u7528\u63a8\u7406\u8def\u5f84\u4e0a\u3002", "conclusion": "\u9a8c\u8bc1\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8ba1\u7b97\u5206\u914d\u6bd4\u5747\u5300\u8ba1\u7b97\u6269\u5c55\u66f4\u6709\u6548\uff0c\u80fd\u591f\u6839\u636e\u95ee\u9898\u9700\u6c42\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\uff0c\u51cf\u5c11\u6d6a\u8d39\uff0c\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\u3002\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4f5c\u4e3a\u7edf\u4e00\u63a7\u5236\u4fe1\u53f7\uff0c\u5728\u8f68\u8ff9\u751f\u6210\u548c\u9009\u62e9\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.00470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00470", "abs": "https://arxiv.org/abs/2602.00470", "authors": ["Pengyu Chen", "Fangzheng Lyu", "Sicheng Wang", "Cuizhen Wang"], "title": "ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation", "comment": null, "summary": "Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.", "AI": {"tldr": "\u63d0\u51faZS-TreeSeg\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u51a0\u5c42\u8bed\u4e49\u5206\u5272\u548c\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u6280\u672f\uff0c\u5c06\u6811\u51a0\u5efa\u6a21\u4e3a\u661f\u51f8\u5bf9\u8c61\uff0c\u5229\u7528\u62d3\u6251\u6d41\u573a\u5b9e\u73b0\u5bc6\u96c6\u91cd\u53e0\u6811\u51a0\u7684\u81ea\u52a8\u5206\u5272\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u6811\u51a0\u5b9e\u4f8b\u5206\u5272\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u6811\u51a0\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u65b0\u5174\u57fa\u7840\u6a21\u578b\uff08\u5982SAM\uff09\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\uff0c\u5728\u5bc6\u96c6\u6811\u51a0\u4e2d\u5bb9\u6613\u6b20\u5206\u5272\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u80fd\u5904\u7406\u5bc6\u96c6\u91cd\u53e0\u6811\u51a0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faZS-TreeSeg\u96f6\u6837\u672c\u6846\u67b6\uff0c\u5c06\u4e24\u4e2a\u6210\u719f\u4efb\u52a1\u7ed3\u5408\uff1a1)\u51a0\u5c42\u8bed\u4e49\u5206\u5272\uff1b2)\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u3002\u5c06\u6811\u51a0\u5efa\u6a21\u4e3a\u661f\u51f8\u5bf9\u8c61\uff0c\u4f7f\u7528Cellpose-SAM\u5728\u62d3\u6251\u6d41\u573a\u4e2d\u57fa\u4e8e\u5411\u91cf\u6536\u655b\u5b9e\u73b0\u63a5\u89e6\u6811\u51a0\u7684\u6570\u5b66\u5206\u79bb\u3002", "result": "\u5728NEON\u548cBAMFOREST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u548c\u89c6\u89c9\u68c0\u67e5\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7a33\u5065\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u4f20\u611f\u5668\u7c7b\u578b\u548c\u51a0\u5c42\u5bc6\u5ea6\uff0c\u4e3a\u6811\u51a0\u5b9e\u4f8b\u5206\u5272\u548c\u6807\u7b7e\u751f\u6210\u63d0\u4f9b\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ZS-TreeSeg\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u6210\u719f\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5bc6\u96c6\u91cd\u53e0\u6811\u51a0\u5206\u5272\u7684\u96be\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u68ee\u6797\u751f\u7269\u91cf\u4f30\u7b97\u548c\u751f\u6001\u76d1\u6d4b\u3002"}}
{"id": "2602.01078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01078", "abs": "https://arxiv.org/abs/2602.01078", "authors": ["Tong Xia", "Weibin Li", "Gang Liu", "Yong Li"], "title": "AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling", "comment": null, "summary": "LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \\textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \\textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \\textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\\% in prediction performance and 50.2\\% in uncertainty estimation.", "AI": {"tldr": "AutoHealth\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u5efa\u6a21\u5065\u5eb7\u6570\u636e\u5e76\u8bc4\u4f30\u6a21\u578b\u53ef\u9760\u6027\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5065\u5eb7\u6570\u636e\u5e94\u7528\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u96be\u4ee5\u6cdb\u5316\u5230\u5f02\u6784\u7684\u5065\u5eb7\u6570\u636e\u6a21\u6001\uff1b2) \u8fc7\u5ea6\u4f9d\u8d56\u9884\u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u6a21\u677f\uff0c\u5bf9\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u9002\u5e94\u4e0d\u8db3\uff1b3) \u5ffd\u89c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u800c\u8fd9\u5bf9\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u53ef\u9760\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faAutoHealth\u7cfb\u7edf\uff0c\u91c7\u7528\u4e94\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u95ed\u73af\u534f\u8c03\u673a\u5236\uff0c\u6267\u884c\u6570\u636e\u63a2\u7d22\u3001\u4efb\u52a1\u6761\u4ef6\u5316\u6a21\u578b\u6784\u5efa\u3001\u8bad\u7ec3\u548c\u4f18\u5316\uff0c\u540c\u65f6\u4f18\u5148\u8003\u8651\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u7cfb\u7edf\u4e0d\u4ec5\u751f\u6210\u5373\u7528\u6a21\u578b\uff0c\u8fd8\u751f\u6210\u652f\u6301\u53ef\u4fe1\u89e3\u91ca\u548c\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u7684\u7efc\u5408\u62a5\u544a\u3002", "result": "\u5728\u5305\u542b17\u4e2a\u4efb\u52a1\u3001\u6db5\u76d6\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u5b66\u4e60\u8bbe\u7f6e\u7684\u73b0\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoHealth\u5b8c\u6210\u4e86\u6240\u6709\u4efb\u52a1\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u9ad8\u4e8629.2%\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0a\u63d0\u9ad8\u4e8650.2%\u3002", "conclusion": "AutoHealth\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u5065\u5eb7\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u6cdb\u5316\u3001\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u548c\u53ef\u4fe1\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00388", "abs": "https://arxiv.org/abs/2602.00388", "authors": ["Zeyuan He", "Yupeng Chen", "Lang Lin", "Yihan Wang", "Shenxu Chang", "Eric Sommerlade", "Philip Torr", "Junchi Yu", "Adel Bibi", "Jialin Yu"], "title": "A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode", "comment": null, "summary": "Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.", "AI": {"tldr": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08D-LLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u5185\u5728\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb\u6f0f\u6d1e", "motivation": "\u7814\u7a76\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08D-LLMs\uff09\u76f8\u5bf9\u4e8e\u81ea\u56de\u5f52LLMs\uff08AR-LLMs\uff09\u7684\u5b89\u5168\u7279\u6027\uff0c\u7279\u522b\u662f\u5176\u5bf9\u6297\u8d8a\u72f1\u653b\u51fb\u7684\u5185\u5728\u9c81\u68d2\u6027\u673a\u5236", "method": "\u5206\u6790\u6269\u6563\u8f68\u8ff9\u7684\u9010\u6b65\u6291\u5236\u673a\u5236\uff0c\u8bc6\u522b\u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30", "result": "D-LLMs\u5bf9\u4f20\u7edf\u8d8a\u72f1\u653b\u51fb\u5177\u6709\u5185\u5728\u9c81\u68d2\u6027\uff0c\u4f46\u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb\u80fd\u6709\u6548\u7ed5\u8fc7\u5176\u5b89\u5168\u673a\u5236\uff0c\u5728Gemini Diffusion\u7b49\u5546\u4e1a\u6a21\u578b\u4e0a\u5b9e\u73b0\u9996\u6b21\u6210\u529f\u8d8a\u72f1", "conclusion": "D-LLMs\u7684\u5b89\u5168\u4f18\u52bf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e0a\u4e0b\u6587\u5d4c\u5957\u653b\u51fb\u66b4\u9732\u4e86\u5176\u5173\u952e\u6f0f\u6d1e\uff0c\u4e3a\u65e9\u671f\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u91cd\u8981\u53d1\u73b0"}}
{"id": "2602.01116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01116", "abs": "https://arxiv.org/abs/2602.01116", "authors": ["Wenxuan Zhang", "Yuan-Hao Jiang", "Changyong Qi", "Rui Jia", "Yonghe Wu"], "title": "Logic-Oriented Retriever Enhancement via Contrastive Learning", "comment": "accepted by icassp 2026", "summary": "Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.", "AI": {"tldr": "LORE\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u6fc0\u6d3bLLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u6216\u8d44\u6e90\uff0c\u4fdd\u6301\u7d22\u5f15\u517c\u5bb9\u6027", "motivation": "\u4f20\u7edf\u68c0\u7d22\u5668\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u903b\u8f91\u5173\u7cfb\u67e5\u8be2\uff0c\u800cLLM\u672c\u8eab\u5177\u6709\u903b\u8f91\u5206\u6790\u80fd\u529b\u4f46\u672a\u88ab\u5145\u5206\u5229\u7528", "method": "LORE\u91c7\u7528\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5f15\u5bfc\u5d4c\u5165\u5411\u91cf\u5bf9\u9f50\u903b\u8f91\u7ed3\u6784\u800c\u975e\u6d45\u5c42\u76f8\u4f3c\u6027\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3001\u989d\u5916\u8d44\u6e90\u6216\u9884\u68c0\u7d22\u5206\u6790\uff0c\u4fdd\u6301\u7d22\u5f15\u517c\u5bb9\u6027", "result": "LORE\u6301\u7eed\u63d0\u5347\u68c0\u7d22\u6548\u7528\u548c\u4e0b\u6e38\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\uff0c\u5728\u590d\u6742\u903b\u8f91\u5173\u7cfb\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "LORE\u6210\u529f\u6fc0\u6d3b\u4e86LLM\u6f5c\u5728\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u65b9\u6848\uff0c\u4e14\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2602.00484", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00484", "abs": "https://arxiv.org/abs/2602.00484", "authors": ["Rong-Lin Jian", "Ming-Chi Luo", "Chen-Wei Huang", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu"], "title": "GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association", "comment": "Winner Solution of SoccerTrack in ACM Multimedia 2025 Workshop MMSports", "summary": "Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.", "AI": {"tldr": "GTATrack\uff1a\u7528\u4e8e\u9c7c\u773c\u76f8\u673a\u8db3\u7403\u6bd4\u8d5b\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u5206\u5c42\u6846\u67b6\uff0c\u5728SoccerTrack Challenge 2025\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cHOTA\u5f97\u52060.60", "motivation": "\u4f53\u80b2\u573a\u666f\u4e2d\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u9762\u4e34\u7403\u5458\u8fd0\u52a8\u4e0d\u89c4\u5219\u3001\u5916\u89c2\u76f8\u4f3c\u3001\u9891\u7e41\u906e\u6321\u7b49\u6311\u6218\uff0c\u800c\u9759\u6001\u9c7c\u773c\u76f8\u673a\u5f15\u5165\u7684\u51e0\u4f55\u7578\u53d8\u548c\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u56f0\u96be\u3002", "method": "\u63d0\u51faGTATrack\u5206\u5c42\u8ddf\u8e2a\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7528\u4e8e\u8fd0\u52a8\u65e0\u5173\u5728\u7ebf\u5173\u8054\u7684Deep Expansion IoU\uff08Deep-EIoU\uff09\u548c\u7528\u4e8e\u8f68\u8ff9\u7ea7\u7ec6\u5316\u7684\u5168\u5c40\u8f68\u8ff9\u5173\u8054\uff08GTA\uff09\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5b9e\u73b0\u77ed\u671f\u5339\u914d\u548c\u957f\u671f\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528\u4f2a\u6807\u7b7e\u7b56\u7565\u63d0\u5347\u5c0f\u76ee\u6807\u548c\u7578\u53d8\u76ee\u6807\u7684\u68c0\u6d4b\u53ec\u56de\u7387\u3002", "result": "\u5728SoccerTrack Challenge 2025\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0cHOTA\u5f97\u5206\u8fbe\u52300.60\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u62a5\u81f3982\u4e2a\uff0c\u5728\u9c7c\u773c\u76f8\u673a\u8db3\u7403\u8ddf\u8e2a\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "conclusion": "GTATrack\u901a\u8fc7\u5c40\u90e8\u5173\u8054\u548c\u5168\u5c40\u63a8\u7406\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u5207\u6362\u3001\u906e\u6321\u548c\u8ddf\u8e2a\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u9c7c\u773c\u76f8\u673a\u4f53\u80b2\u573a\u666f\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01082", "abs": "https://arxiv.org/abs/2602.01082", "authors": ["Yiliu He", "Tianle Li", "Binghao Ji", "Zhiyuan Liu", "Di Huang"], "title": "EvoOpt-LLM: Evolving industrial optimization models with large language models", "comment": null, "summary": "Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.", "AI": {"tldr": "EvoOpt-LLM\uff1a\u57fa\u4e8eLLM\u7684\u5de5\u4e1a\u4f18\u5316\u5efa\u6a21\u6846\u67b6\uff0c\u652f\u6301\u81ea\u52a8\u6a21\u578b\u6784\u5efa\u3001\u52a8\u6001\u7ea6\u675f\u6ce8\u5165\u548c\u53d8\u91cf\u526a\u679d\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u9ad8\u751f\u6210\u7387\u548c\u53ef\u6267\u884c\u7387\u3002", "motivation": "\u5de5\u4e1a\u89c4\u5212\u548c\u8c03\u5ea6\u4e2d\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5efa\u6a21\u9ad8\u5ea6\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u6a21\u578b\u5e76\u9002\u5e94\u4e1a\u52a1\u89c4\u5219\u53d8\u5316\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u6548\u7387\u4f4e\u3001\u6c42\u89e3\u5668\u6709\u6548\u6027\u6709\u9650\u548c\u5de5\u4e1a\u89c4\u6a21\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e7B\u53c2\u6570LLM\u6784\u5efa\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684LoRA\u5fae\u8c03\u9002\u5e94\uff0c\u652f\u6301\u5b8c\u6574\u7684\u5de5\u4e1a\u4f18\u5316\u5efa\u6a21\u751f\u547d\u5468\u671f\uff1a\u81ea\u52a8\u6a21\u578b\u6784\u5efa\u3001\u52a8\u6001\u4e1a\u52a1\u7ea6\u675f\u6ce8\u5165\u548c\u7aef\u5230\u7aef\u53d8\u91cf\u526a\u679d\u3002", "result": "\u4ec5\u75283,000\u8bad\u7ec3\u6837\u672c\u5b9e\u73b091%\u751f\u6210\u7387\u548c65.9%\u53ef\u6267\u884c\u7387\uff0c\u5173\u952e\u6027\u80fd\u63d0\u5347\u57281,500\u6837\u672c\u5185\u51fa\u73b0\u3002\u7ea6\u675f\u6ce8\u5165\u6a21\u5757\u53ef\u9760\u589e\u5f3a\u73b0\u6709MILP\u6a21\u578b\u5e76\u4fdd\u6301\u539f\u59cb\u76ee\u6807\uff0c\u53d8\u91cf\u526a\u679d\u6a21\u5757\u5728400\u6837\u672c\u4e0b\u5bf9\u4e2d\u578bLP\u6a21\u578b\u8fbe\u5230\u7ea60.56 F1\u5206\u6570\u3002", "conclusion": "EvoOpt-LLM\u5c55\u793a\u4e86\u5de5\u4e1a\u4f18\u5316\u5efa\u6a21\u7684\u5b9e\u7528\u3001\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u4e13\u5bb6\u5e72\u9884\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u6c42\u89e3\u5668\u6548\u7387\u3002"}}
{"id": "2602.00392", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00392", "abs": "https://arxiv.org/abs/2602.00392", "authors": ["Arjun Rao", "Ruth Crasto", "Tessa Ooms", "David Rolnick", "Konstantin Klemmer", "Marc Ru\u00dfwurm"], "title": "Localized, High-resolution Geographic Representations with Slepian Functions", "comment": "23 pages, 12 figures, 6 tables", "summary": "Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7403\u9762Slepian\u51fd\u6570\u7684\u5730\u7406\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u80fd\u5728\u611f\u5174\u8da3\u533a\u57df\u5185\u96c6\u4e2d\u8868\u793a\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7f16\u7801\u4e14\u8ba1\u7b97\u9700\u6c42\u4f4e\uff0c\u540c\u65f6\u63d0\u4f9b\u6df7\u5408Slepian-\u7403\u8c10\u7f16\u7801\u5668\u5e73\u8861\u5c40\u90e8-\u5168\u5c40\u6027\u80fd\u3002", "motivation": "\u5730\u7406\u6570\u636e\u672c\u8d28\u4e0a\u662f\u5c40\u90e8\u7684\uff08\u75be\u75c5\u7206\u53d1\u3001\u751f\u6001\u6a21\u5f0f\u3001\u7ecf\u6d4e\u6d3b\u52a8\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5730\u7406\u4f4d\u7f6e\u7f16\u7801\u5668\u5728\u5168\u7403\u8303\u56f4\u5185\u5747\u5300\u5206\u914d\u8868\u793a\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u5c40\u90e8\u5e94\u7528\u7684\u9ad8\u5206\u8fa8\u7387\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u7403\u9762Slepian\u51fd\u6570\u6784\u5efa\u5730\u7406\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5728\u611f\u5174\u8da3\u533a\u57df\u5185\u96c6\u4e2d\u8868\u793a\u80fd\u529b\uff1b\u540c\u65f6\u63d0\u51fa\u6df7\u5408Slepian-\u7403\u8c10\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5c40\u90e8Slepian\u51fd\u6570\u548c\u5168\u5c40\u7403\u8c10\u51fd\u6570\uff0c\u5e73\u8861\u5c40\u90e8-\u5168\u5c40\u6027\u80fd\u3002", "result": "\u5728\u5206\u7c7b\u3001\u56de\u5f52\u548c\u56fe\u50cf\u589e\u5f3a\u9884\u6d4b\u7b49\u4e94\u4e2a\u4efb\u52a1\u4e2d\uff0cSlepian\u7f16\u7801\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\uff0c\u540c\u65f6\u5177\u5907\u6781\u70b9\u5b89\u5168\u6027\u548c\u7403\u9762\u8ddd\u79bb\u4fdd\u6301\u7b49\u7406\u60f3\u7279\u6027\u3002", "conclusion": "Slepian\u5730\u7406\u4f4d\u7f6e\u7f16\u7801\u5668\u80fd\u6709\u6548\u89e3\u51b3\u5c40\u90e8\u9ad8\u5206\u8fa8\u7387\u5730\u7406\u6570\u636e\u8868\u793a\u95ee\u9898\uff0c\u6df7\u5408\u7f16\u7801\u5668\u5e73\u8861\u4e86\u5c40\u90e8-\u5168\u5c40\u6027\u80fd\uff0c\u4e3a\u5730\u7406\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2602.01119", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01119", "abs": "https://arxiv.org/abs/2602.01119", "authors": ["Konstantin Chernyshev", "Ekaterina Artemova", "Viacheslav Zhukov", "Maksim Nerush", "Mariia Fedorova", "Iryna Repik", "Olga Shapovalova", "Aleksey Sukhorosov", "Vladimir Dobrovolskii", "Natalia Mikhailova", "Sergei Tilga"], "title": "Tendem: A Hybrid AI+Human Platform", "comment": null, "summary": "Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.", "AI": {"tldr": "Tendem\u662f\u4e00\u4e2aAI\u5904\u7406\u7ed3\u6784\u5316\u91cd\u590d\u5de5\u4f5c\u3001\u4eba\u7c7b\u4e13\u5bb6\u4ecb\u5165\u6a21\u578b\u5931\u8d25\u6216\u9a8c\u8bc1\u7ed3\u679c\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u572894\u4e2a\u771f\u5b9e\u4efb\u52a1\u8bc4\u4f30\u4e2d\u6bd4\u7eafAI\u4ee3\u7406\u548c\u7eaf\u4eba\u5de5\u5de5\u4f5c\u6d41\u8868\u73b0\u66f4\u597d", "motivation": "\u89e3\u51b3\u5f53\u524dAI\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u5408AI\u7684\u6548\u7387\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u5224\u65ad\u80fd\u529b\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u3001\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c", "method": "\u6784\u5efa\u6df7\u5408\u7cfb\u7edf\uff1aAI\u5904\u7406\u7ed3\u6784\u5316\u91cd\u590d\u5de5\u4f5c\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728\u6a21\u578b\u5931\u8d25\u65f6\u4ecb\u5165\u5e76\u9a8c\u8bc1\u7ed3\u679c\uff0c\u6240\u6709\u7ed3\u679c\u5728\u4ea4\u4ed8\u524d\u7ecf\u8fc7\u5168\u9762\u8d28\u91cf\u5ba1\u67e5", "result": "\u572894\u4e2a\u771f\u5b9e\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cTendem\u6bd4\u7eafAI\u4ee3\u7406\u548cUpwork\u81ea\u7531\u804c\u4e1a\u8005\u7684\u4eba\u5de5\u5de5\u4f5c\u6d41\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u8f93\u51fa\u548c\u66f4\u5feb\u5468\u8f6c\u65f6\u95f4\uff0c\u8fd0\u8425\u6210\u672c\u4e0e\u7eaf\u4eba\u5de5\u6267\u884c\u76f8\u5f53\uff1b\u5176AI\u4ee3\u7406\u5728\u7b2c\u4e09\u65b9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1SOTA\u6c34\u5e73", "conclusion": "AI\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7ed3\u5408\u7684\u6df7\u5408\u7cfb\u7edf\u5728\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u7eafAI\u6216\u7eaf\u4eba\u5de5\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u6709\u6548\u6027"}}
{"id": "2602.00489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00489", "abs": "https://arxiv.org/abs/2602.00489", "authors": ["Sicong Zang", "Tao Sun", "Cairong Yan"], "title": "Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level", "comment": "Source codes are coming soon", "summary": "Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.", "AI": {"tldr": "SketchMod\u63d0\u51fa\u901a\u8fc7\u53d8\u6362\u6e90\u7b14\u753b\u6765\u5bf9\u9f50\u76ee\u6807\u8349\u56fe\u6a21\u5f0f\uff0c\u5b9e\u73b0\u7b14\u753b\u7ea7\u8349\u56fe\u7f16\u8f91\uff0c\u901a\u8fc7\u5b66\u4e60\u7f29\u653e\u3001\u65b9\u5411\u548c\u4f4d\u7f6e\u4e09\u4e2a\u5173\u952e\u504f\u79fb\u5c5e\u6027\u6765\u8c03\u6574\u6e90\u7b14\u753b\uff0c\u4f7f\u5176\u4e0e\u76ee\u6807\u8349\u56fe\u5728\u7a7a\u95f4\u6bd4\u4f8b\u3001\u5c40\u90e8\u51e0\u4f55\u548c\u8bed\u4e49\u5e03\u5c40\u4e0a\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u91cd\u65b0\u5b9a\u4f4d\u6e90\u7b14\u753b\u5230\u5408\u9002\u753b\u5e03\u4f4d\u7f6e\uff0c\u4f46\u5f53\u6e90\u7b14\u753b\u5728\u5c3a\u5bf8\u548c\u65b9\u5411\u4e0a\u5b58\u5728\u663e\u8457\u53d8\u5316\u65f6\uff0c\u4ec5\u91cd\u65b0\u5b9a\u4f4d\u800c\u4e0d\u8fdb\u4e00\u6b65\u8c03\u6574\u4f1a\u5bfc\u81f4\u7f16\u8f91\u7ed3\u679c\u4e0d\u81ea\u7136\u3002\u4f8b\u5982\uff0c\u8fc7\u5927\u7684\u6e90\u7b14\u753b\u4e0d\u7ecf\u7f29\u653e\u76f4\u63a5\u951a\u5b9a\u5230\u76ee\u6807\u4e0a\u4f1a\u4ea7\u751f\u8bed\u4e49\u4e0d\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "method": "SketchMod\u901a\u8fc7\u5b66\u4e60\u4e09\u4e2a\u5173\u952e\u504f\u79fb\u5c5e\u6027\uff08\u7f29\u653e\u3001\u65b9\u5411\u548c\u4f4d\u7f6e\uff09\u6765\u7ec6\u5316\u6e90\u7b14\u753b\uff1a1\uff09\u901a\u8fc7\u7f29\u653e\u8c03\u6574\u7a7a\u95f4\u6bd4\u4f8b\uff0c2\uff09\u901a\u8fc7\u65cb\u8f6c\u5bf9\u9f50\u5c40\u90e8\u51e0\u4f55\uff0c3\uff09\u901a\u8fc7\u4f4d\u79fb\u6ee1\u8db3\u8bed\u4e49\u5e03\u5c40\u3002\u901a\u8fc7\u66b4\u9732\u6355\u83b7\u7684\u7b14\u753b\u5c5e\u6027\uff0c\u53ef\u4ee5\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u63a7\u5236\u7b14\u753b\u8f6e\u5ed3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSketchMod\u5728\u7b14\u753b\u7ea7\u8349\u56fe\u7f16\u8f91\u4e0a\u5b9e\u73b0\u4e86\u7cbe\u786e\u548c\u7075\u6d3b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u53d8\u6362\u6e90\u7b14\u753b\u6765\u5bf9\u9f50\u76ee\u6807\u8349\u56fe\u6a21\u5f0f\uff0cSketchMod\u80fd\u591f\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u7b14\u753b\u7ea7\u8349\u56fe\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u4ec5\u91cd\u65b0\u5b9a\u4f4d\u7b14\u753b\u5e26\u6765\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01086", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01086", "abs": "https://arxiv.org/abs/2602.01086", "authors": ["Takahito Nakajima"], "title": "MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI", "comment": "19 pages, 5 figures. Code available at https://github.com/medbeads/medbeads", "summary": "Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous \"Clinical Agents\" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a \"Context Mismatch\": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable \"Beads\"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This \"write-once, read-many\" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the \"Context Mismatch\" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for \"Trustworthy Medical AI.\" It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient \"AI-native language.\" We release MedBeads as open-source software to accelerate agent-native data standards.", "AI": {"tldr": "MedBeads\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411AI\u4ee3\u7406\u7684\u539f\u751f\u533b\u7597\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u7528\u4e0d\u53ef\u53d8\u7684Merkle DAG\uff08\u6709\u5411\u65e0\u73af\u56fe\uff09\u8282\u70b9\uff08\u79f0\u4e3a\"Beads\"\uff09\u6765\u5b58\u50a8\u4e34\u5e8a\u4e8b\u4ef6\uff0c\u901a\u8fc7\u5bc6\u7801\u5b66\u94fe\u63a5\u786e\u4fdd\u6570\u636e\u7684\u5b8c\u6574\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u89e3\u51b3\u4f20\u7edfEMR\u7cfb\u7edf\u4e0eAI\u4ee3\u7406\u4e4b\u95f4\u7684\"\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\"\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7535\u5b50\u75c5\u5386\u7cfb\u7edf\uff08EMR\uff09\u548cFHIR\u6807\u51c6\u662f\u4e3a\u4eba\u7c7b\u5ba1\u67e5\u8bbe\u8ba1\u7684\uff0c\u5bfc\u81f4AI\u4ee3\u7406\u63a5\u6536\u788e\u7247\u5316\u6570\u636e\uff0c\u9700\u8981\u4f9d\u8d56\u6982\u7387\u63a8\u7406\uff08\u5982RAG\uff09\u91cd\u5efa\u60a3\u8005\u5386\u53f2\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u5e76\u59a8\u788d\u53ef\u5ba1\u8ba1\u6027\u3002\u8fd9\u79cd\"\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\"\u9650\u5236\u4e86LLM\u4f5c\u4e3a\u81ea\u4e3b\"\u4e34\u5e8a\u4ee3\u7406\"\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faMedBeads\u67b6\u6784\uff1a\u4e34\u5e8a\u4e8b\u4ef6\u4f5c\u4e3a\u4e0d\u53ef\u53d8\u7684\"Beads\"\uff08Merkle DAG\u4e2d\u7684\u8282\u70b9\uff09\uff0c\u901a\u8fc7\u5bc6\u7801\u5b66\u5f15\u7528\u56e0\u679c\u524d\u9a71\u8282\u70b9\u3002\u91c7\u7528\"\u4e00\u6b21\u5199\u5165\uff0c\u591a\u6b21\u8bfb\u53d6\"\u67b6\u6784\uff0c\u4efb\u4f55\u7be1\u6539\u90fd\u53ef\u88ab\u6570\u5b66\u68c0\u6d4b\u3002\u5b9e\u73b0\u5305\u62ecGo\u6838\u5fc3\u5f15\u64ce\u3001Python\u4e2d\u95f4\u4ef6\u7528\u4e8eLLM\u96c6\u6210\uff0c\u4ee5\u53caReact\u53ef\u89c6\u5316\u754c\u9762\u3002", "result": "\u6210\u529f\u4f7f\u7528\u5408\u6210\u6570\u636e\u5b9e\u73b0\u5de5\u4f5c\u6d41\uff1aFHIR\u5230DAG\u8f6c\u6362\u5c06\u6241\u5e73\u8d44\u6e90\u8f6c\u6362\u4e3a\u56e0\u679c\u94fe\u63a5\u56fe\uff1b\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u4e0a\u4e0b\u6587\u68c0\u7d22\u7b97\u6cd5\u4ee5O(V+E)\u590d\u6742\u5ea6\u904d\u5386\u76f8\u5173\u5b50\u56fe\uff0c\u652f\u6301\u5b9e\u65f6\u51b3\u7b56\uff1b\u5bc6\u7801\u5b66\u94fe\u8bbe\u8ba1\u4fdd\u8bc1\u7be1\u6539\u8bc1\u636e\uff1b\u53ef\u89c6\u5316\u754c\u9762\u901a\u8fc7\u663e\u5f0f\u56e0\u679c\u94fe\u63a5\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u3002", "conclusion": "MedBeads\u901a\u8fc7\u4ece\u6982\u7387\u641c\u7d22\u8f6c\u5411\u786e\u5b9a\u6027\u56fe\u904d\u5386\uff0c\u4ece\u53ef\u53d8\u8bb0\u5f55\u8f6c\u5411\u4e0d\u53ef\u53d8\u94fe\uff0c\u89e3\u51b3\u4e86\"\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\"\u95ee\u9898\uff0c\u4e3a\"\u53ef\u4fe1\u533b\u7597AI\"\u63d0\u4f9b\u57fa\u7840\u3002\u5b83\u4fdd\u8bc1AI\u63a5\u6536\u7684\u4e0a\u4e0b\u6587\u662f\u786e\u5b9a\u6027\u548c\u9632\u7be1\u6539\u7684\uff0c\u800cLLM\u8d1f\u8d23\u89e3\u91ca\u3002\u7ed3\u6784\u5316\u7684Bead\u683c\u5f0f\u4f5c\u4e3a\u4ee4\u724c\u9ad8\u6548\u7684\"AI\u539f\u751f\u8bed\u8a00\"\u3002"}}
{"id": "2602.00397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00397", "abs": "https://arxiv.org/abs/2602.00397", "authors": ["Aayush Gautam", "Mukul Gagrani", "Junyoung Park", "Mingu Lee", "Chiris Lott", "Narasimha Reddy"], "title": "Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity", "comment": "10 pages, 7 figures", "summary": "The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.", "AI": {"tldr": "FastForward\uff1a\u4e00\u79cd\u9884\u6d4b\u6027\u7a00\u758f\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u7ea7\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684FFN\u7a00\u758f\u5316\u52a0\u901fLLM\u9884\u586b\u5145\u9636\u6bb5\uff0c\u572850%\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.45\u500d\u8ba1\u7b97\u52a0\u901f\uff0c\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e6%\u3002", "motivation": "LLM\u63a8\u7406\u7684\u9884\u586b\u5145\u9636\u6bb5\u662f\u957f\u4e0a\u4e0b\u6587\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\u3002\u5728\u77ed\u5230\u4e2d\u7b49\u4e0a\u4e0b\u6587\u957f\u5ea6\uff081K-16K token\uff09\u4e0b\uff0c\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u5360\u7528\u4e86\u5927\u90e8\u5206\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u7684FFN\u7a00\u758f\u5316\u65b9\u6cd5\u4e13\u4e3a\u81ea\u56de\u5f52\u89e3\u7801\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5229\u7528\u9884\u586b\u5145\u9636\u6bb5\u7684\u5e76\u884c\u6027\uff0c\u4e14\u5e38\u5e38\u964d\u4f4e\u51c6\u786e\u6027\u3002", "method": "FastForward\u7ed3\u5408\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u6309\u5757\u9009\u62e9\u9ad8\u91cd\u8981\u6027\u795e\u7ecf\u5143\uff1b2\uff09\u8bef\u5dee\u8865\u507f\u7f51\u7edc\uff0c\u7528\u4e8e\u7ea0\u6b63\u7a00\u758f\u5316\u5f15\u8d77\u7684\u8bef\u5dee\uff1b3\uff09\u5c42\u95f4\u7a00\u758f\u5ea6\u8c03\u5ea6\u5668\uff0c\u6839\u636etoken\u6df7\u5408\u91cd\u8981\u6027\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728LLaMA\u548cQwen\u6a21\u578b\uff08\u6700\u59278B\u53c2\u6570\uff09\u4e0a\uff0cFastForward\u572850% FFN\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.45\u500d\u7684\u8ba1\u7b97\u52a0\u901f\uff0c\u5728LongBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u7684\u51c6\u786e\u7387\u635f\u5931\u5c0f\u4e8e6%\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9996token\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u3002", "conclusion": "FastForward\u901a\u8fc7\u9884\u6d4b\u6027\u7a00\u758f\u5316\u6846\u67b6\u6709\u6548\u52a0\u901fLLM\u9884\u586b\u5145\u9636\u6bb5\uff0c\u4e3a\u53d7\u9650\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.01125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01125", "abs": "https://arxiv.org/abs/2602.01125", "authors": ["Jichu Li", "Yilun Zhong", "Zhiting Li", "Feng Zhou", "Quyu Kong"], "title": "Long-range Modeling and Processing of Multimodal Event Sequences", "comment": null, "summary": "Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u5c06\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6269\u5c55\u5230\u89c6\u89c9\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e8f\u5217\u538b\u7f29\u673a\u5236\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6587\u672c\u5206\u6790\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u70b9\u8fc7\u7a0b\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65f6\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5f53\u5e8f\u5217\u957f\u5ea6\u56e0\u591a\u6a21\u6001\u6570\u636e\u800c\u6025\u5267\u589e\u52a0\u65f6\uff0c\u6ce8\u610f\u529b\u673a\u5236\u6a21\u578b\u96be\u4ee5\u751f\u6210\u8fde\u8d2f\u7684\u957f\u6587\u672c\u63cf\u8ff0\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9\u4e8b\u4ef6\u52a8\u6001\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6269\u5c55LLM-based TPPs\u5230\u89c6\u89c9\u6a21\u6001\u7684\u6846\u67b6\uff0c\u5c06\u6587\u672c\u751f\u6210\u4f5c\u4e3a\u6838\u5fc3\u80fd\u529b\u3002\u91c7\u7528\u57fa\u4e8e\u65f6\u95f4\u76f8\u4f3c\u6027\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u538b\u7f29\u673a\u5236\u51cf\u5c11\u5e8f\u5217\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u6a21\u5f0f\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u5148\u5728\u538b\u7f29\u5e8f\u5217\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u5305\u62ec\u6311\u6218\u6027\u7684DanmakuTPP-QA\u57fa\u51c6\u6d4b\u8bd5\u5728\u5185\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u751f\u6210\u6587\u672c\u5206\u6790\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u65f6\u95f4\u70b9\u8fc7\u7a0b\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e8b\u4ef6\u5efa\u6a21\u548c\u6587\u672c\u5206\u6790\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u5904\u7406\u590d\u6742\u5f02\u6b65\u4e8b\u4ef6\u5e8f\u5217\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00490", "abs": "https://arxiv.org/abs/2602.00490", "authors": ["Chia-Ming Lee", "Yu-Hao Ho", "Yu-Fan Lin", "Jen-Wei Lee", "Li-Wei Kang", "Chih-Chung Hsu"], "title": "HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion", "comment": "Accepted by ICASSP 2026", "summary": "Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.", "AI": {"tldr": "\u63d0\u51faHSSDCT\u7f51\u7edc\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff0c\u901a\u8fc7\u5206\u5c42\u5bc6\u96c6\u6b8b\u5deeTransformer\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\uff0c\u4ee5\u53ca\u7a7a\u95f4-\u5149\u8c31\u76f8\u5173\u5c42\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u611f\u53d7\u91ce\u6709\u9650\u3001\u5149\u8c31\u5e26\u5197\u4f59\u3001\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faHSSDCT\u6846\u67b6\uff0c\u5305\u542b\uff1a1)\u5206\u5c42\u5bc6\u96c6\u6b8b\u5deeTransformer\u5757(HDRTB)\uff0c\u901a\u8fc7\u6e10\u8fdb\u6269\u5927\u7a97\u53e3\u548c\u5bc6\u96c6\u6b8b\u5dee\u8fde\u63a5\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\uff1b2)\u7a7a\u95f4-\u5149\u8c31\u76f8\u5173\u5c42(SSCL)\uff0c\u663e\u5f0f\u5206\u89e3\u7a7a\u95f4\u548c\u5149\u8c31\u4f9d\u8d56\uff0c\u5c06\u81ea\u6ce8\u610f\u529b\u964d\u81f3\u7ebf\u6027\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u5149\u8c31\u5197\u4f59\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHSSDCT\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "HSSDCT\u901a\u8fc7\u5206\u5c42\u7a7a\u95f4-\u5149\u8c31\u5bc6\u96c6\u76f8\u5173\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01090", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01090", "abs": "https://arxiv.org/abs/2602.01090", "authors": ["Yang Liu", "Chuan Zhou", "Yancheng Chen", "Shuai Zhang", "Xixun Lin", "Xiaoqing Wang"], "title": "Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization", "comment": "32 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\\% feasibility through three key innovations: (i) \\emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \\emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \\emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.", "AI": {"tldr": "FALCON\u6846\u67b6\u901a\u8fc7\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u3001\u53ef\u884c\u6027\u4fee\u590d\u5c42\u548c\u81ea\u9002\u5e94\u91c7\u6837\u786e\u4fddLLM\u6c42\u89e3\u7ec4\u5408\u4f18\u5316\u95ee\u9898100%\u53ef\u884c\u6027\uff0c\u4f7f\u7528BOPO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e03\u4e2aNP\u96be\u95ee\u9898\u4e0a\u5b9e\u73b0\u5b8c\u7f8e\u53ef\u884c\u6027\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u4fdd\u8bc1\u89e3\u53ef\u884c\u6027\u7684\u673a\u5236\uff0c\u8fd9\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u786e\u4fdd100%\u53ef\u884c\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faFALCON\u6846\u67b6\uff1a1) \u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u786e\u4fdd\u53e5\u6cd5\u6709\u6548\u6027\uff1b2) \u53ef\u884c\u6027\u4fee\u590d\u5c42\u7ea0\u6b63\u8bed\u4e49\u7ea6\u675f\u8fdd\u53cd\uff1b3) \u81ea\u9002\u5e94Best-of-N\u91c7\u6837\u9ad8\u6548\u5206\u914d\u63a8\u7406\u8ba1\u7b97\u3002\u8bad\u7ec3\u65b9\u6cd5BOPO\u57fa\u4e8e\u76ee\u6807\u5dee\u8ddd\u52a0\u6743\u504f\u597d\u5bf9\uff0c\u63d0\u4f9b\u65e0\u4eba\u5de5\u6807\u7b7e\u7684\u5bc6\u96c6\u76d1\u7763\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86BOPO\u7684\u6536\u655b\u6027\u5e76\u7ed9\u51fa\u4e86\u4fee\u590d\u5f15\u8d77\u7684\u8d28\u91cf\u635f\u5931\u754c\u9650\u3002\u5728\u4e03\u4e2aNP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\uff0cFALCON\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u53ef\u884c\u6027\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u548cLLM\u6c42\u89e3\u5668\u7684\u89e3\u8d28\u91cf\u3002", "conclusion": "FALCON\u6846\u67b6\u89e3\u51b3\u4e86LLM\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7f3a\u4e4f\u53ef\u884c\u6027\u4fdd\u8bc1\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89e3\u7801\u3001\u4fee\u590d\u548c\u91c7\u6837\u673a\u5236\u5b9e\u73b0\u4e86100%\u53ef\u884c\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\uff0c\u4e3aLLM\u5728\u5b9e\u9645\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6848\u3002"}}
{"id": "2602.00398", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00398", "abs": "https://arxiv.org/abs/2602.00398", "authors": ["Ajay Jaiswal", "Lauren Hannah", "Han-Byul Kim", "Duc Hoang", "Arnav Kundu", "Mehrdad Farajtabar", "Minsik Cho"], "title": "MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers", "comment": null, "summary": "Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.", "AI": {"tldr": "MemoryLLM\u5c06Transformer\u4e2d\u7684\u524d\u9988\u7f51\u7edc(FFN)\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u8026\uff0c\u5c06\u5176\u89c6\u4e3a\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u4ee4\u724c\u7ea7\u795e\u7ecf\u68c0\u7d22\u8bb0\u5fc6\uff0c\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3FFN\u5b9e\u73b0\u9884\u8ba1\u7b97\u67e5\u627e\u8868\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u7406\u89e3Transformer\u7ec4\u4ef6\u5728LLM\u4e2d\u7684\u8fd0\u4f5c\u673a\u5236\u5bf9AI\u6280\u672f\u8fdb\u6b65\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u524d\u9988\u7f51\u7edc(FFN)\u7684\u53ef\u89e3\u91ca\u6027\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u8026FFN\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u4fbf\u66f4\u6df1\u5165\u5730\u7814\u7a76FFN\u4f5c\u4e3a\u8bb0\u5fc6\u6a21\u5757\u7684\u529f\u80fd\u3002", "method": "\u63d0\u51faMemoryLLM\u65b9\u6cd5\uff1a1) \u5c06FFN\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u8026\uff0c\u5c06FFN\u89c6\u4e3a\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u4ee4\u724c\u7ea7\u795e\u7ecf\u68c0\u7d22\u8bb0\u5fc6\uff1b2) \u4f7f\u7528\u4ee4\u724c\u5d4c\u5165\u72ec\u7acb\u8bad\u7ec3FFN\uff0c\u4f7f\u5176\u6210\u4e3a\u53ef\u9884\u8ba1\u7b97\u7684\u4ee4\u724c\u7ea7\u67e5\u627e\u8868(ToLs)\uff1b3) \u5f15\u5165Flex-MemoryLLM\u4f5c\u4e3a\u4f20\u7edfTransformer\u4e0eMemoryLLM\u4e4b\u95f4\u7684\u6298\u4e2d\u67b6\u6784\u3002", "result": "MemoryLLM\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u65e0\u5173\u7684FFN\uff0c\u53ef\u9884\u8ba1\u7b97\u4e3a\u4ee4\u724c\u7ea7\u67e5\u627e\u8868\uff0c\u652f\u6301\u6309\u9700\u5728VRAM\u548c\u5b58\u50a8\u4e4b\u95f4\u4f20\u8f93\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002\u540c\u65f6\u7814\u7a76\u4e86\u8f93\u5165\u4ee4\u724c\u5982\u4f55\u8bbf\u95eeFFN\u53c2\u6570\u4e2d\u7684\u8bb0\u5fc6\u4f4d\u7f6e\u4ee5\u53caFFN\u8bb0\u5fc6\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MemoryLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684Transformer\u67b6\u6784\u8bbe\u8ba1\uff0c\u901a\u8fc7\u89e3\u8026FFN\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06FFN\u89c6\u4e3a\u53ef\u72ec\u7acb\u7814\u7a76\u7684\u795e\u7ecf\u8bb0\u5fc6\u6a21\u5757\uff0c\u4e0d\u4ec5\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u901a\u8fc7\u9884\u8ba1\u7b97\u67e5\u627e\u8868\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002Flex-MemoryLLM\u5219\u5f25\u5408\u4e86\u4f20\u7edf\u8bbe\u8ba1\u4e0e\u65b0\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2602.01132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01132", "abs": "https://arxiv.org/abs/2602.01132", "authors": ["Abhilekh Borah", "Shubhra Ghosh", "Kedar Joshi", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation", "comment": "19 pages, 6 figures", "summary": "Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLogifus\u903b\u8f91\u6df7\u6dc6\u6846\u67b6\u548cLogiQAte\u8bca\u65ad\u57fa\u51c6\uff0c\u53d1\u73b0LLMs\u5728\u6807\u51c6\u5f62\u5f0f\u4e0b\u80fd\u5904\u7406\u903b\u8f91\u95ee\u9898\uff0c\u4f46\u5728\u903b\u8f91\u7b49\u4ef7\u4f46\u8868\u9762\u5f62\u5f0f\u6df7\u6dc6\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0c\u63ed\u793aLLMs\u7f3a\u4e4f\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u5f62\u5f0f\u4e0b\u80fd\u5f88\u597d\u5904\u7406\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u5728\u903b\u8f91\u7b49\u4ef7\u4f46\u8868\u9762\u5f62\u5f0f\u6df7\u6dc6\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u7814\u7a76\u8fd9\u79cd\u8106\u5f31\u6027\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u8bca\u65ad\u6846\u67b6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u903b\u8f91\u7ed3\u6784\u7684\u771f\u6b63\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faLogifus\u7ed3\u6784\u4fdd\u6301\u7684\u903b\u8f91\u6df7\u6dc6\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u521b\u5efaLogiQAte\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b1,108\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u56db\u4e2a\u63a8\u7406\u4efb\u52a1\uff1a\u4e00\u9636\u903b\u8f91\u8574\u542b\u3001\u8840\u7f18\u5173\u7cfb\u63a8\u7406\u3001\u6570\u5b57\u5e8f\u5217\u6a21\u5f0f\u548c\u65b9\u5411\u611f\u77e5\u63a8\u7406\uff0c\u5747\u5728\u4fdd\u6301\u903b\u8f91\u7b49\u4ef7\u6027\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u8868\u9762\u5f62\u5f0f\u6df7\u6dc6\u3002", "result": "\u8bc4\u4f30\u516d\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u53d1\u73b0\uff0c\u6df7\u6dc6\u4e25\u91cd\u964d\u4f4e\u96f6\u6837\u672c\u6027\u80fd\uff1aGPT-4o\u5e73\u5747\u4e0b\u964d47%\uff0cGPT-5\u4e0b\u964d27%\uff0c\u63a8\u7406\u6a21\u578bo4-mini\u4e0b\u964d22%\u3002\u6240\u6709\u6a21\u578b\u5728\u6df7\u6dc6\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u90fd\u663e\u8457\u5dee\u4e8e\u6807\u51c6\u5f62\u5f0f\u3002", "conclusion": "\u5f53\u524dLLMs\u89e3\u6790\u95ee\u9898\u65f6\u7f3a\u4e4f\u6df1\u5ea6\u7406\u89e3\uff0c\u53ea\u662f\u8868\u9762\u5f62\u5f0f\u5339\u914d\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6784\u5efa\u771f\u6b63\u7406\u89e3\u5e76\u4fdd\u6301\u610f\u4e49\u8d85\u8d8a\u8868\u9762\u5f62\u5f0f\u7684\u6a21\u578b\u7684\u7d27\u8feb\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e25\u91cd\u7f3a\u9677\u3002"}}
{"id": "2602.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00504", "abs": "https://arxiv.org/abs/2602.00504", "authors": ["Jiahe Wu", "Bing Cao", "Qilong Wang", "Qinghua Hu", "Dongdong Li", "Pengfei Zhu"], "title": "RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.", "AI": {"tldr": "RGBX-R1\u6846\u67b6\u901a\u8fc7UAV\u63d0\u793a\u7b56\u7565\u6784\u5efa\u89c6\u89c9\u6a21\u6001\u601d\u7ef4\u94fe\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u589e\u5f3aMLLM\u5bf9\u7ea2\u5916\u3001\u6df1\u5ea6\u7b49X\u6a21\u6001\u7684\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u5728RGBX-Grounding\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf22.71%", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9884\u8bad\u7ec3\u4e8eRGB\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u7ea2\u5916\u3001\u6df1\u5ea6\u3001\u4e8b\u4ef6\u6570\u636e\u7b49\u5176\u4ed6\u89c6\u89c9\u6a21\u6001\u4e0a\u7684\u6027\u80fd\uff0c\u800c\u8fd9\u4e9b\u6a21\u6001\u5bf9\u4e8e\u590d\u6742\u573a\u666f\u81f3\u5173\u91cd\u8981", "method": "1. \u63d0\u51faUnderstand-Associate-Validate (UAV)\u63d0\u793a\u7b56\u7565\u6784\u5efaVisual Modality Chain-of-Thought (VM-CoT)\uff1b2. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1aCold-Start Supervised Fine-Tuning (CS-SFT)\u76d1\u7763\u63a8\u7406\u8fc7\u7a0b\uff0cSpatio-Temporal Reinforcement Fine-Tuning (ST-RFT)\u4f7f\u7528Modality-understanding Spatio-Temporal (MuST)\u5956\u52b1\u5f3a\u5316\u6a21\u6001\u63a8\u7406", "result": "\u6784\u5efa\u9996\u4e2aRGBX-Grounding\u57fa\u51c6\uff0c\u5728\u4e09\u4e2aRGBX grounding\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd522.71%\uff0c\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "conclusion": "RGBX-R1\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86MLLM\u5bf9\u591a\u79cd\u89c6\u89c9\u6a21\u6001\u7684\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01103", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01103", "abs": "https://arxiv.org/abs/2602.01103", "authors": ["Yiming Dong", "Kun Fu", "Haoyu Li", "Xinyuan Zhu", "Yurou Liu", "Lijing Shao", "Jieping Ye", "Zheng Wang"], "title": "Probing RLVR training instability through the lens of objective-level hacking", "comment": null, "summary": "Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u76ee\u6807\u5c42\u9762\u9ed1\u5ba2\u653b\u51fb\u89d2\u5ea6\u89e3\u91caMoE\u6a21\u578b\u5728RLVR\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u5f02\u5e38\u589e\u957f\u7684\u673a\u5236\u3002", "motivation": "MoE\u67b6\u6784\u5728RLVR\u8bad\u7ec3\u4e2d\u5bb9\u6613\u51fa\u73b0\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u5176\u6839\u672c\u539f\u56e0\u548c\u673a\u5236\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76ee\u6807\u5c42\u9762\u9ed1\u5ba2\u653b\u51fb\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u4fe1\u7528\u9519\u4f4d\u5206\u6790\u7cfb\u7edf\u7ea7\u865a\u5047\u4fe1\u53f7\uff0c\u572830B MoE\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u63ed\u793a\u4e86MoE\u6a21\u578b\u4e2d\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u5f02\u5e38\u589e\u957f\u7684\u5173\u952e\u75c5\u7406\u52a8\u6001\u7684\u8d77\u6e90\u548c\u673a\u5236\uff0c\u4e3a\u7406\u89e3RLVR\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u5177\u4f53\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3MoE\u6a21\u578b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89e3\u91ca\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u7684RLVR\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.00403", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00403", "abs": "https://arxiv.org/abs/2602.00403", "authors": ["Hon Tik Tse", "Marlos C. Machado"], "title": "DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning", "comment": null, "summary": "In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.", "AI": {"tldr": "\u63d0\u51fa\u76f4\u63a5\u8fd1\u4f3c\u9ed8\u8ba4\u8868\u793a\u4e3b\u7279\u5f81\u5411\u91cf\u7684\u76ee\u6807\u51fd\u6570\uff0c\u907f\u514d\u5148\u8ba1\u7b97\u77e9\u9635\u518d\u5206\u89e3\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u9ed8\u8ba4\u8868\u793a\u53ca\u5176\u4e3b\u7279\u5f81\u5411\u91cf\u5728\u5956\u52b1\u5851\u9020\u3001\u57fa\u4e8e\u8ba1\u6570\u7684\u63a2\u7d22\u3001\u9009\u9879\u53d1\u73b0\u548c\u8fc1\u79fb\u7b49\u65b9\u9762\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5148\u8fd1\u4f3c\u77e9\u9635\u518d\u8fdb\u884c\u7279\u5f81\u5206\u89e3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u7ef4\u7a7a\u95f4", "method": "\u63a8\u5bfc\u51fa\u76f4\u63a5\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u9ed8\u8ba4\u8868\u793a\u4e3b\u7279\u5f81\u5411\u91cf\u7684\u76ee\u6807\u51fd\u6570\uff0c\u907f\u514d\u4e86\u5148\u8ba1\u7b97\u77e9\u9635\u518d\u5206\u89e3\u7684\u4e24\u6b65\u8fc7\u7a0b", "result": "\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u8be5\u76ee\u6807\u51fd\u6570\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u7279\u5f81\u5411\u91cf\u5e94\u7528\u4e8e\u5956\u52b1\u5851\u9020\u4efb\u52a1", "conclusion": "\u63d0\u51fa\u7684\u76f4\u63a5\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5404\u79cd\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01161", "abs": "https://arxiv.org/abs/2602.01161", "authors": ["Reem I. Masoud", "Chen Feng", "Shunta Asano", "Saied Alshahrani", "Philip Colin Treleaven", "Miguel R. D. Rodrigues"], "title": "Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models", "comment": null, "summary": "The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5fae\u8c03\u6570\u636e\u96c6\u7684\u8bed\u8a00\u7279\u6027\u5982\u4f55\u5f71\u54cdLLM\u7684\u6587\u5316\u5bf9\u9f50\u8868\u73b0\uff0c\u53d1\u73b0\u8bcd\u6c47\u5bfc\u5411\u7279\u6027\u6700\u7a33\u5065\uff0c\u800c\u8bed\u4e49\u6216\u591a\u6837\u6027\u6781\u7aef\u5219\u5e38\u4e2d\u6027\u6216\u6709\u5bb3\u3002", "motivation": "LLM\u5168\u7403\u90e8\u7f72\u5f15\u53d1\u6587\u5316\u9519\u4f4d\u62c5\u5fe7\uff0c\u4f46\u7528\u4e8e\u6587\u5316\u9002\u5e94\u7684\u5fae\u8c03\u6570\u636e\u96c6\u7684\u8bed\u8a00\u7279\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u9700\u8981\u7814\u7a76\u54ea\u4e9b\u8bed\u8a00\u7279\u6027\u4e0e\u6587\u5316\u8868\u73b0\u76f8\u5173\u3002", "method": "\u5bf9\u963f\u62c9\u4f2f\u8bed\u3001\u4e2d\u6587\u548c\u65e5\u8bed\u6570\u636e\u96c6\u8ba1\u7b97\u8f7b\u91cf\u7ea7\u8bed\u8a00\u3001\u8bed\u4e49\u548c\u7ed3\u6784\u6307\u6807\uff0c\u8fdb\u884c\u4e3b\u6210\u5206\u5206\u6790\uff0c\u7136\u540e\u5fae\u8c03\u4e09\u5927LLM\u5bb6\u65cf\u5e76\u8bc4\u4f30\u6587\u5316\u77e5\u8bc6\u3001\u4ef7\u503c\u89c2\u548c\u89c4\u8303\u57fa\u51c6\u3002", "result": "PCA\u6210\u5206\u4e0e\u4e0b\u6e38\u8868\u73b0\u76f8\u5173\u4f46\u5f3a\u70c8\u4f9d\u8d56\u6a21\u578b\uff1b\u8bcd\u6c47\u5bfc\u5411\u6210\u5206\u6700\u7a33\u5065\uff0c\u80fd\u8de8\u6a21\u578b\u548c\u57fa\u51c6\u4ea7\u751f\u66f4\u4e00\u81f4\u8868\u73b0\uff0c\u800c\u5f3a\u8c03\u8bed\u4e49\u6216\u591a\u6837\u6027\u6781\u7aef\u901a\u5e38\u4e2d\u6027\u6216\u6709\u5bb3\u3002", "conclusion": "\u5fae\u8c03\u6570\u636e\u96c6\u7684\u8bed\u8a00\u7279\u6027\u5bf9\u6587\u5316\u5bf9\u9f50\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8bcd\u6c47\u7279\u6027\u6700\u7a33\u5065\uff0c\u4e3a\u6587\u5316\u5bf9\u9f50\u7684\u6570\u636e\u96c6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2602.00505", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00505", "abs": "https://arxiv.org/abs/2602.00505", "authors": ["Jingrui Zhang", "Feng Liang", "Yong Zhang", "Wei Wang", "Runhao Zeng", "Xiping Hu"], "title": "Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models", "comment": null, "summary": "With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.", "AI": {"tldr": "SparseCut\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8de8\u6a21\u6001\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u5feb\u6377\u8fde\u63a5\u5b9e\u73b0\u591a\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u7684\u9ad8\u6548\u96c6\u6210\uff0c\u63d0\u5347\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u800c\u4e0d\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709MLLMs\u5927\u591a\u5173\u6ce8\u6269\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6216\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u5ffd\u89c6\u4e86\u5982\u4f55\u6709\u6548\u5c06\u8de8\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u5230\u8bed\u8a00\u7a7a\u95f4\u4e2d\u3002\u7279\u522b\u662f\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4ec5\u4f7f\u7528\u9ad8\u5c42\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u6a21\u6001\u5bf9\u9f50\u4f1a\u4e22\u5f03\u4e2d\u4f4e\u5c42\u7279\u5f81\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faSparseCut\u67b6\u6784\uff0c\u5728\u8de8\u6a21\u6001\u7f16\u7801\u5668\u548cLLM\u4e4b\u95f4\u5f15\u5165\u7a00\u758f\u5feb\u6377\u8fde\u63a5\uff0c\u5b9e\u73b0\u591a\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\u7684\u9ad8\u6548\u5206\u5c42\u96c6\u6210\u3002\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u9ad8\u6548\u591a\u7c92\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5728\u901a\u8fc7\u5feb\u6377\u8fde\u63a5\u8def\u7531\u524d\u8fdb\u884c\u89c6\u89c9\u7279\u5f81\u878d\u5408\uff0c\u4fdd\u6301\u539f\u59cb\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e14\u4e0d\u589e\u52a0\u8f93\u5165\u957f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSparseCut\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u5404\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\uff0c\u5bf9\u4e0d\u540c\u57fa\u7840LLM\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SparseCut\u901a\u8fc7\u7a00\u758f\u5feb\u6377\u8fde\u63a5\u548c\u591a\u7c92\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u4e2d\u8de8\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.01109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01109", "abs": "https://arxiv.org/abs/2602.01109", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction", "comment": "9 pages, 7 figures", "summary": "Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.", "AI": {"tldr": "BiCarFormer\uff1a\u9996\u4e2a\u878d\u5408DTC\u5e8f\u5217\u548c\u73af\u5883\u6761\u4ef6\u7684\u591a\u6a21\u6001\u591a\u6807\u7b7e\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f66\u8f86\u6545\u969c\u6a21\u5f0f\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8f66\u8f86\u8bca\u65ad\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u8bca\u65ad\u6545\u969c\u7801\u5e8f\u5217\uff0c\u4f46\u5ffd\u7565\u4e86\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u538b\u529b\u7b49\u73af\u5883\u4f20\u611f\u5668\u6570\u636e\u3002\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e13\u5bb6\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u6570\u636e\u590d\u6742\u4e14\u566a\u58f0\u5927\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51faBiCarFormer\u53cc\u5411Transformer\u6a21\u578b\uff0c\u4e13\u95e8\u5904\u7406\u8f66\u8f86\u4e8b\u4ef6\u5e8f\u5217\uff0c\u91c7\u7528\u5d4c\u5165\u878d\u5408\u548c\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u8bca\u65ad\u7801\u4e0e\u73af\u5883\u6570\u636e\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u591a\u6807\u7b7e\u5e8f\u5217\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b22,137\u4e2a\u6545\u969c\u7801\u548c360\u4e2a\u6545\u969c\u6a21\u5f0f\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cBiCarFormer\u76f8\u6bd4\u4ec5\u4f7f\u7528DTC\u5e8f\u5217\u7684\u4f20\u7edf\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u878d\u5408\u73af\u5883\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u8f66\u8f86\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\uff0c\u589e\u5f3a\u6c7d\u8f66\u884c\u4e1a\u81ea\u52a8\u5316\u6d41\u7a0b\u3002"}}
{"id": "2602.00407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00407", "abs": "https://arxiv.org/abs/2602.00407", "authors": ["Suprim Nakarmi", "Junggab Son", "Yue Zhao", "Zuobin Xiong"], "title": "Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks", "comment": "13 pages, 4 figures, and 5 tables", "summary": "Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.", "AI": {"tldr": "Fed-Listing\u662f\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u578b\u6807\u7b7e\u5206\u5e03\u63a8\u65ad\u653b\u51fb\uff0c\u4ec5\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u4ea4\u6362\u7684\u6700\u7ec8\u5c42\u68af\u5ea6\u6765\u63a8\u65ad\u76ee\u6807\u5ba2\u6237\u7aef\u7684\u79c1\u6709\u6807\u7b7e\u7edf\u8ba1\u4fe1\u606f\uff0c\u65e0\u9700\u539f\u59cb\u6570\u636e\u6216\u8282\u70b9\u7279\u5f81\u3002", "motivation": "\u5c3d\u7ba1\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc(FedGNNs)\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u4f46\u7814\u7a76\u8868\u660e\u5171\u4eab\u7684\u6a21\u578b\u66f4\u65b0\uff08\u7279\u522b\u662f\u68af\u5ea6\uff09\u4ecd\u53ef\u80fd\u65e0\u610f\u4e2d\u6cc4\u9732\u672c\u5730\u7528\u6237\u7684\u654f\u611f\u4fe1\u606f\u3002\u76ee\u524d\u9488\u5bf9FedGNNs\u7684\u6807\u7b7e\u5206\u5e03\u63a8\u65ad\u653b\u51fb\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u8fd9\u4e00\u9690\u79c1\u5a01\u80c1\u3002", "method": "Fed-Listing\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u5229\u7528\u8bad\u7ec3\u671f\u95f4\u4ea4\u6362\u7684\u6700\u7ec8\u5c42\u68af\u5ea6\u6765\u63a8\u65ad\u76ee\u6807\u5ba2\u6237\u7aef\u7684\u79c1\u6709\u6807\u7b7e\u7edf\u8ba1\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u8f85\u52a9\u5f71\u5b50\u6570\u636e\u96c6\u751f\u6210\u591a\u6837\u5316\u7684\u6807\u7b7e\u5212\u5206\u7b56\u7565\uff0c\u6a21\u62df\u4e0d\u540c\u7684\u5ba2\u6237\u7aef\u5206\u5e03\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u653b\u51fb\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u79cdGNN\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFed-Listing\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u968f\u673a\u731c\u6d4b\u548cDecaf\uff09\uff0c\u5373\u4f7f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u573a\u666f\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u9632\u5fa1\u673a\u5236\u51e0\u4e4e\u65e0\u6cd5\u964d\u4f4e\u653b\u51fb\u6027\u80fd\uff0c\u9664\u975e\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u6548\u7528\u3002", "conclusion": "Fed-Listing\u63ed\u793a\u4e86\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u5b58\u5728\u7684\u4e25\u91cd\u9690\u79c1\u6f0f\u6d1e\uff0c\u5373\u4f7f\u5728\u4e0d\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u6216\u8282\u70b9\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u5c31\u80fd\u6709\u6548\u63a8\u65ad\u5ba2\u6237\u7aef\u7684\u6807\u7b7e\u5206\u5e03\u3002\u8fd9\u5f3a\u8c03\u4e86\u5728FedGNNs\u4e2d\u5f00\u53d1\u66f4\u5f3a\u5927\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.01162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01162", "abs": "https://arxiv.org/abs/2602.01162", "authors": ["Nipuna Abeykoon", "Ashen Weerathunga", "Pubudu Wijesinghe", "Parameswari Krishnamurthy"], "title": "Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages", "comment": null, "summary": "Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u8bed\u8a00\u7c7b\u578b\u5b66\u6539\u8fdbLLM\u7ffb\u8bd1\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u65e0\u9700\u5e73\u884c\u8bad\u7ec3\u6570\u636e\u6216\u6a21\u578b\u91cd\u8bad\u7ec3\uff0c\u901a\u8fc7\u8bed\u8a00\u6d88\u6b67\u548c\u7c7b\u578b\u5b66\u5408\u89c4\u8bc4\u5206\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u4f18\u52bf\u7c7b\u578b\u6a21\u5f0f\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5bfc\u81f4\u7ffb\u8bd1\u5230\u7c7b\u578b\u5b66\u5dee\u5f02\u5927\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\u51fa\u73b0\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5e73\u884c\u6570\u636e\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a1) \u901a\u7528\u5143\u8bed\u8a00\u6846\u67b6(UMF)\uff0c\u572816\u4e2a\u7c7b\u578b\u5b66\u7ef4\u5ea6\u4e0a\u6784\u5efa\u7ed3\u6784\u5316\u8bed\u8a00\u7279\u5f81\uff0c\u4f7f\u7528\u5dee\u5f02\u52a0\u6743\u8bc4\u5206\uff1b2) \u8ba1\u7b97\u5f15\u64ce\uff0c\u901a\u8fc7\u751f\u6210\u65f6\u7684\u8bed\u8a00\u6d88\u6b67\u548c\u9009\u62e9\u65f6\u7684\u7c7b\u578b\u5b66\u5408\u89c4\u8bc4\u5206\u6765\u8fd0\u4f5c\u3002", "result": "\u57289\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u5e72\u9884\u7387\u4e0e\u82f1\u8bed\u7684\u7c7b\u578b\u5b66\u8ddd\u79bb\u5f3a\u76f8\u5173\u3002\u5728341\u4e2a\u82f1\u8bed\u53e5\u5b50\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u7684\u5e72\u9884\u7cbe\u5ea6\u5206\u522b\u4e3a\uff1a\u4fdd\u5b88\u5904\u7406\u8bed\u8a0048.16%\uff0c\u5f62\u6001\u5bc6\u96c6\u8bed\u8a0028.15%\uff0c\u7ed3\u6784\u7279\u5f81\u5316\u8bed\u8a0086.26%\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u5e73\u884c\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u4e0e\u4efb\u4f55\u80fd\u4ea7\u751f\u591a\u4e2a\u5019\u9009\u8f93\u51fa\u7684LLM\u914d\u5408\u4f7f\u7528\uff0c\u4e3a\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u90e8\u7f72\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u7c7b\u578b\u5b66\u5dee\u5f02\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u7ed3\u6784\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2602.00508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00508", "abs": "https://arxiv.org/abs/2602.00508", "authors": ["Min Shi", "Xiaohui Zeng", "Jiannan Huang", "Yin Cui", "Francesco Ferroni", "Jialuo Li", "Shubham Pachori", "Zhaoshuo Li", "Yogesh Balaji", "Haoxiang Wang", "Tsung-Yi Lin", "Xiao Fu", "Yue Zhao", "Chieh-Yun Chen", "Ming-Yu Liu", "Humphrey Shi"], "title": "DuoGen: Towards General Purpose Interleaved Multimodal Generation", "comment": "Technical Report. Project Page: https://research.nvidia.com/labs/dir/duetgen/", "summary": "Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.", "AI": {"tldr": "DuoGen\u662f\u4e00\u4e2a\u901a\u7528\u4ea4\u9519\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u89e3\u8026\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\u8d28\u91cf\uff0c\u5728\u6587\u672c\u8d28\u91cf\u3001\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf-\u4e0a\u4e0b\u6587\u5bf9\u9f50\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4ea4\u9519\u751f\u6210\u6a21\u578b\u5728\u901a\u7528\u6307\u4ee4\u4e0b\u7684\u8d28\u91cf\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u548c\u57fa\u7840\u6a21\u578b\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u6570\u636e\u3001\u67b6\u6784\u548c\u8bc4\u4f30\u95ee\u9898\u3002", "method": "1) \u6570\u636e\u65b9\u9762\uff1a\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4ece\u7cbe\u9009\u7f51\u7ad9\u91cd\u5199\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u548c\u8986\u76d6\u65e5\u5e38\u573a\u666f\u7684\u591a\u6837\u5316\u5408\u6210\u793a\u4f8b\uff1b2) \u67b6\u6784\u65b9\u9762\uff1a\u5229\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001LLM\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u548c\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210DiT\u7684\u89c6\u89c9\u751f\u6210\u80fd\u529b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u8026\u7b56\u7565\uff1a\u5148\u6307\u4ee4\u8c03\u4f18MLLM\uff0c\u7136\u540e\u7528\u7cbe\u9009\u7684\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u5e8f\u5217\u5bf9\u9f50DiT\u3002", "result": "\u5728\u516c\u5171\u548c\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDuoGen\u5728\u6587\u672c\u8d28\u91cf\u3001\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf-\u4e0a\u4e0b\u6587\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u540c\u65f6\u5728\u7edf\u4e00\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u7f16\u8f91\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DuoGen\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u3001\u67b6\u6784\u548c\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u4e3a\u4ea4\u9519\u591a\u6a21\u6001\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u901a\u7528\u6307\u4ee4\u4e0b\u9ad8\u8d28\u91cf\u4ea4\u9519\u751f\u6210\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.01131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01131", "abs": "https://arxiv.org/abs/2602.01131", "authors": ["Yue Zhong", "Jiawen Kang", "Yongju Tong", "Hong-Ning Dai", "Dong In Kim", "Abbas Jamalipour", "Shengli Xie"], "title": "Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach", "comment": null, "summary": "With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u611f\u77e5-\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\u95ed\u73af\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u7269\u7406\u63a7\u5236\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u663e\u5f0f\u5efa\u6a21\uff0c\u901a\u8fc7Lyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u5c06\u7a33\u5b9a\u6027\u8981\u6c42\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u8d44\u6e90\u8fb9\u754c\uff0c\u5e76\u91c7\u7528Stackelberg\u535a\u5f08\u548c\u8f7b\u91cf\u7ea7PPO\u7b97\u6cd5\u8fdb\u884c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u968f\u7740\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u53d1\u5c55\uff0c\u65e0\u4eba\u673a\u4f5c\u4e3a\u7a7a\u4e2d\u57fa\u7ad9\u9700\u8981\u652f\u6301\u4ece\u5ef6\u8fdf\u654f\u611f\u5173\u952e\u4efb\u52a1\u5230\u5e26\u5bbd\u5bc6\u96c6\u578b\u6570\u636e\u6d41\u7684\u591a\u6837\u5316\u670d\u52a1\u3002\u4f20\u7edf\u4ee5\u541e\u5410\u91cf\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u65e0\u6cd5\u89e3\u51b3\u6709\u9650\u673a\u8f7d\u8d44\u6e90\u4e0e\u4e25\u683c\u7a33\u5b9a\u6027\u8981\u6c42\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "1) \u63d0\u51fa\u611f\u77e5-\u901a\u4fe1-\u8ba1\u7b97-\u63a7\u5236\u95ed\u73af\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a21\u901a\u4fe1\u5ef6\u8fdf\u5bf9\u7269\u7406\u63a7\u5236\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff1b2) \u5229\u7528Lyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u63a8\u5bfc\u63a7\u5236\u7cfb\u7edf\u72b6\u6001\u6f14\u5316\u4e0e\u901a\u4fe1\u7ea6\u675f\u4e4b\u95f4\u7684\u5185\u5728\u6620\u5c04\uff1b3) \u5c06\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3aStackelberg\u535a\u5f08\uff0c\u65e0\u4eba\u673a\u4f5c\u4e3a\u9886\u5bfc\u8005\u52a8\u6001\u5b9a\u4ef7\uff0c\u7528\u6237\u4f5c\u4e3a\u8ddf\u968f\u8005\u4f18\u5316\u8bf7\u6c42\uff1b4) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u526a\u679dPPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u526a\u679d\u673a\u5236\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u52a8\u6001\u4f4e\u7a7a\u73af\u5883\u4e2d\u80fd\u6709\u6548\u4fdd\u969c\u63a7\u5236\u73af\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u6700\u5927\u5316\u7cfb\u7edf\u6548\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u541e\u5410\u91cf\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5c06\u63a7\u5236\u7a33\u5b9a\u6027\u8981\u6c42\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u7684\u901a\u4fe1\u8d44\u6e90\u8fb9\u754c\uff0c\u5e76\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u8f7b\u91cf\u7ea7DRL\u7b97\u6cd5\uff0c\u4e3a\u65e0\u4eba\u673a\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00408", "abs": "https://arxiv.org/abs/2602.00408", "authors": ["Seung Heon Oh", "Jiwon Baek", "Ki Young Cho", "Hee Chang Yoon", "Jong Hun Woo"], "title": "Variational Approach for Job Shop Scheduling", "comment": null, "summary": "This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.", "AI": {"tldr": "VG2S\u6846\u67b6\u9996\u6b21\u5c06\u53d8\u5206\u63a8\u65ad\u5f15\u5165\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u53d8\u5206\u56fe\u7f16\u7801\u5668\u5b66\u4e60\u8c03\u5ea6\u5b9e\u4f8b\u7684\u7ed3\u6784\u8868\u793a\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\u4e2d\u9762\u4e34\u8bad\u7ec3\u975e\u5e73\u7a33\u6027\u548c\u5bf9\u672a\u89c1\u95ee\u9898\u5b9e\u4f8b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u540c\u65f6\u4f18\u5316\u8868\u793a\u5b66\u4e60\u548c\u7b56\u7565\u6267\u884c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u56fe\u5230\u8c03\u5ea6\u5668\uff08VG2S\uff09\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u53d8\u5206\u63a8\u65ad\u5f15\u5165JSSP\u9886\u57df\uff0c\u57fa\u4e8e\u8bc1\u636e\u4e0b\u754c\uff08ELBO\uff09\u548c\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u63a8\u5bfc\u6982\u7387\u76ee\u6807\u3002\u901a\u8fc7\u53d8\u5206\u56fe\u7f16\u7801\u5668\u5b66\u4e60\u8c03\u5ea6\u5b9e\u4f8b\u7684\u9c81\u68d2\u7ed3\u6784\u8868\u793a\uff0c\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\u6570\u5b66\u89e3\u8026\u3002", "result": "VG2S\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5bf9\u8d85\u53c2\u6570\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728DMU\u548cSWV\u7b49\u5927\u89c4\u6a21\u6311\u6218\u6027\u57fa\u51c6\u5b9e\u4f8b\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684DRL\u57fa\u7ebf\u548c\u4f20\u7edf\u8c03\u5ea6\u89c4\u5219\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53d8\u5206\u63a8\u65ad\u5f15\u5165\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\u5e76\u89e3\u8026\u8868\u793a\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\uff0cVG2S\u6846\u67b6\u4e3a\u89e3\u51b3\u4f20\u7edfDRL\u65b9\u6cd5\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5728\u5236\u9020\u8fd0\u8425\u6548\u7387\u63d0\u5347\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01169", "abs": "https://arxiv.org/abs/2602.01169", "authors": ["Shahem Sultan", "Shahem Fadi", "Yousef Melhim", "Ibrahim Alsarraj", "Besher Hassan"], "title": "PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues", "comment": "8 pages, 5 figures", "summary": "This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.", "AI": {"tldr": "PedagoSense\u7cfb\u7edf\u7ed3\u5408\u4e24\u9636\u6bb5\u7b56\u7565\u5206\u7c7b\u5668\u548cLLM\u751f\u6210\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u63a8\u8350\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u4e2d\u7684\u6559\u5b66\u7b56\u7565\uff0c\u63d0\u5347\u6559\u80b2\u6280\u672f\u9002\u5e94\u6027", "motivation": "\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u4e2d\u9700\u8981\u68c0\u6d4b\u548c\u63a8\u8350\u6709\u6548\u7684\u6559\u5b66\u7b56\u7565\u6765\u63d0\u5347\u4ea4\u4e92\u8d28\u91cf\uff0c\u5c06\u6559\u5b66\u7406\u8bba\u4e0e\u5b9e\u9645LLM\u54cd\u5e94\u751f\u6210\u76f8\u7ed3\u5408", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u7b56\u7565\u5206\u7c7b\u5668\uff1a\u5148\u4e8c\u5143\u5206\u7c7b\u68c0\u6d4b\u662f\u5426\u6709\u6559\u5b66\u7b56\u7565\uff0c\u518d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u8bc6\u522b\u5177\u4f53\u7b56\u7565\uff1b\u540c\u65f6\u57fa\u4e8e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u63a8\u8350\u7b56\u7565\u5e76\u7528LLM\u751f\u6210\u76f8\u5e94\u54cd\u5e94", "result": "\u5728\u4eba\u5de5\u6807\u6ce8\u7684\u5e08\u751f\u5bf9\u8bdd\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u6559\u5b66\u7b56\u7565\u68c0\u6d4b\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u589e\u5f3a\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4ecd\u6709\u6311\u6218", "conclusion": "PedagoSense\u6210\u529f\u8fde\u63a5\u6559\u5b66\u7406\u8bba\u4e0e\u5b9e\u9645LLM\u54cd\u5e94\u751f\u6210\uff0c\u4e3a\u81ea\u9002\u5e94\u6559\u80b2\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u6865\u6881"}}
{"id": "2602.00516", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00516", "abs": "https://arxiv.org/abs/2602.00516", "authors": ["Kunal Mahatha", "Jose Dolz", "Christian Desrosiers"], "title": "SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation", "comment": null, "summary": "We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u8bad\u7ec3\u65e0\u5173\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6269\u6563\u8bf1\u5bfc\u4eb2\u548c\u56fe\u4e0a\u7684\u968f\u673a\u6d41\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u4f20\u64ad\u65b9\u6848\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u5272\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u65e0\u5173\u5206\u5272\u65b9\u6cd5\u57fa\u4e8e\u8c31\u56fe\u5206\u5272\u5047\u8bbe\uff0c\u5b58\u5728\u591a\u4e2a\u6839\u672c\u7f3a\u9677\uff1a\u9700\u8981\u9884\u8bbe\u7c07\u6570\u91cf\u3001\u8fb9\u754c\u8fc7\u5ea6\u5e73\u6ed1\u3001\u5bf9\u566a\u58f0\u548c\u591a\u6a21\u6001\u4eb2\u548c\u5206\u5e03\u654f\u611f\uff0c\u4e14\u5ffd\u89c6\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002", "method": "\u5c06\u8bad\u7ec3\u65e0\u5173\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6269\u6563\u8bf1\u5bfc\u4eb2\u548c\u56fe\u4e0a\u7684\u968f\u673a\u6d41\u5e73\u8861\u95ee\u9898\uff0c\u5f15\u5165\u9a6c\u5c14\u53ef\u592b\u4f20\u64ad\u65b9\u6848\uff0c\u6267\u884c\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u6807\u7b7e\u6269\u6563\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u526a\u679d\u7b56\u7565\u6291\u5236\u4e0d\u53ef\u9760\u8f6c\u79fb\u5e76\u589e\u5f3a\u7f6e\u4fe1\u4eb2\u548c\u8def\u5f84\u3002", "result": "\u5728\u4e03\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4ea7\u751f\u66f4\u9510\u5229\u7684\u8fb9\u754c\u3001\u66f4\u8fde\u8d2f\u7684\u533a\u57df\uff0c\u76f8\u6bd4\u57fa\u4e8e\u8c31\u805a\u7c7b\u7684\u65b9\u6cd5\u83b7\u5f97\u663e\u8457\u66f4\u7a33\u5b9a\u7684\u63a9\u7801\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u968f\u673a\u6d41\u5e73\u8861\u95ee\u9898\u5e76\u6574\u5408\u5168\u5c40\u6269\u6563\u6ce8\u610f\u529b\u4e0e\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8c31\u56fe\u5206\u5272\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u96f6\u6837\u672c\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2602.01146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01146", "abs": "https://arxiv.org/abs/2602.01146", "authors": ["Sidharth Pulipaka", "Oliver Chen", "Manas Sharma", "Taaha S Bajwa", "Vyas Raina", "Ivaxi Sheth"], "title": "PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?", "comment": "70 pages, 26 figures, under review", "summary": "Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPersistBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u957f\u671f\u8bb0\u5fc6\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8de8\u57df\u6cc4\u6f0f\u548c\u8bb0\u5fc6\u8bf1\u5bfc\u8c04\u5a9a\u65b9\u9762\u5b58\u5728\u9ad8\u5931\u8d25\u7387\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u52a9\u624b\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u957f\u671f\u8bb0\u5fc6\u4e0eLLM\u96c6\u6210\uff0c\u8bb0\u5fc6\u6301\u4e45\u5316\u867d\u7136\u80fd\u589e\u5f3a\u4e2a\u6027\u5316\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u88ab\u5ffd\u89c6\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u6d4b\u91cf\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u63d0\u51faPersistBench\u57fa\u51c6\uff0c\u8bc6\u522b\u4e24\u79cd\u957f\u671f\u8bb0\u5fc6\u7279\u6709\u98ce\u9669\uff1a\u8de8\u57df\u6cc4\u6f0f\uff08LLM\u4e0d\u9002\u5f53\u5730\u4ece\u957f\u671f\u8bb0\u5fc6\u4e2d\u6ce8\u5165\u4e0a\u4e0b\u6587\uff09\u548c\u8bb0\u5fc6\u8bf1\u5bfc\u8c04\u5a9a\uff08\u5b58\u50a8\u7684\u957f\u671f\u8bb0\u5fc6\u6697\u4e2d\u5f3a\u5316\u7528\u6237\u504f\u89c1\uff09\u3002\u572818\u4e2a\u524d\u6cbf\u548c\u5f00\u6e90LLM\u4e0a\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u60ca\u4eba\u7684\u9ad8\u5931\u8d25\u7387\uff1a\u8de8\u57df\u6837\u672c\u4e2d\u4f4d\u5931\u8d25\u738753%\uff0c\u8c04\u5a9a\u6837\u672c\u4e2d\u4f4d\u5931\u8d25\u738797%\u3002\u8868\u660e\u73b0\u6709LLM\u5728\u957f\u671f\u8bb0\u5fc6\u5b89\u5168\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002", "conclusion": "PersistBench\u57fa\u51c6\u9f13\u52b1\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684\u957f\u671f\u8bb0\u5fc6\u4f7f\u7528\u65b9\u5f0f\uff0c\u4ee5\u5e94\u5bf9\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u957f\u671f\u8bb0\u5fc6\u5e26\u6765\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2602.00412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00412", "abs": "https://arxiv.org/abs/2602.00412", "authors": ["Marcos L. P. Bueno", "Joaquin Vanschoren"], "title": "Robustness of AutoML on Dirty Categorical Data", "comment": null, "summary": "The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u500b\u5c07\u985e\u5225\u8cc7\u6599\u8f49\u63db\u70ba\u6578\u503c\u8cc7\u6599\u7684\u7ba1\u9053\uff0c\u8b93AutoML\u65b9\u6cd5\u80fd\u5920\u8655\u7406\u7d93\u904e\u9032\u968e\u7de8\u78bc\u7684\u9ad2\u985e\u5225\u8cc7\u6599\uff0c\u4e26\u6bd4\u8f03\u73fe\u6709AutoML\u65b9\u6cd5\u8207\u65b0\u7ba1\u9053\u7684\u8868\u73fe\u5dee\u7570\u3002", "motivation": "\u73fe\u6709AutoML\u65b9\u6cd5\u5728\u8655\u7406\u5206\u985e\u554f\u984c\u6642\u80fd\u61c9\u5c0d\u6578\u64da\u7f3a\u9677\uff0c\u4f46\u5c0d\u65bc\u9ad2\u985e\u5225\u8cc7\u6599\uff08\u5982\u9ad8\u57fa\u6578\u985e\u5225\u7279\u5fb5\uff09\u7684\u884c\u70ba\u4e86\u89e3\u4e0d\u8db3\u3002\u96d6\u7136\u5f62\u614b\u7de8\u78bc\u5668\u80fd\u63d0\u5347ML\u6a21\u578b\u5728\u9ad2\u985e\u5225\u8cc7\u6599\u4e0a\u7684\u8868\u73fe\uff0c\u4f46\u9019\u4e9b\u7de8\u78bc\u5668\u5728AutoML\u4e2d\u7684\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51fa\u4e00\u500b\u7ba1\u9053\uff0c\u5c07\u985e\u5225\u8cc7\u6599\u8f49\u63db\u70ba\u6578\u503c\u8cc7\u6599\uff0c\u4f7fAutoML\u80fd\u5920\u8655\u7406\u7d93\u904e\u9032\u968e\u7de8\u78bc\u65b9\u6848\u8f49\u63db\u7684\u985e\u5225\u8cc7\u6599\u3002\u5728\u9ad2\u8cc7\u6599\u96c6\u4e0a\u5c0d\u73fe\u6709AutoML\u65b9\u6cd5\u7684\u7a69\u5065\u6027\u9032\u884c\u57fa\u6e96\u6e2c\u8a66\uff0c\u4e26\u8207\u63d0\u51fa\u7684\u7ba1\u9053\u9032\u884c\u6bd4\u8f03\u3002", "result": "\u901a\u904e\u6bd4\u8f03\u73fe\u6709AutoML\u65b9\u6cd5\u8207\u65b0\u7ba1\u9053\u5728\u9ad2\u8cc7\u6599\u96c6\u4e0a\u7684\u8868\u73fe\uff0c\u7372\u5f97\u9810\u6e2c\u6027\u80fd\u5dee\u7570\u7684\u6d1e\u5bdf\u3002\u540c\u6642\u5206\u6790AutoML\u69cb\u5efa\u7684ML\u7ba1\u9053\uff0c\u8d85\u8d8a\u50c5\u95dc\u6ce8\u6700\u4f73\u6a21\u578b\u7684\u50b3\u7d71\u8a55\u4f30\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5c0dAutoML\u65b9\u6cd5\u5728\u9ad2\u985e\u5225\u8cc7\u6599\u4e0a\u8868\u73fe\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4e26\u5c55\u793a\u4e86\u9032\u968e\u7de8\u78bc\u65b9\u6848\u5728AutoML\u6d41\u7a0b\u4e2d\u7684\u6f5b\u5728\u50f9\u503c\uff0c\u6709\u52a9\u65bc\u63d0\u5347AutoML\u8655\u7406\u73fe\u5be6\u4e16\u754c\u9ad2\u8cc7\u6599\u7684\u80fd\u529b\u3002"}}
{"id": "2602.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01170", "abs": "https://arxiv.org/abs/2602.01170", "authors": ["Besher Hassan", "Ibrahim Alsarraj", "Musaab Hasan", "Yousef Melhim", "Shahem Fadi", "Shahem Sultan"], "title": "EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech", "comment": "10 pages, 3 figures", "summary": "This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.", "AI": {"tldr": "EmoAra\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u60c5\u611f\u4fdd\u6301\u8de8\u8bed\u8a00\u8bed\u97f3\u901a\u4fe1\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3a\u94f6\u884c\u5ba2\u670d\u573a\u666f\u8bbe\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u82f1\u8bed\u8bed\u97f3\u5e76\u751f\u6210\u4fdd\u7559\u60c5\u611f\u8272\u5f69\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u8f93\u51fa\u3002", "motivation": "\u94f6\u884c\u5ba2\u670d\u573a\u666f\u4e2d\u60c5\u611f\u8bed\u5883\u5bf9\u670d\u52a1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8de8\u8bed\u8a00\u4ea4\u6d41\u65f6\u4fdd\u6301\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\uff0c\u786e\u4fdd\u670d\u52a1\u8d28\u91cf\u4e0d\u53d7\u8bed\u8a00\u969c\u788d\u5f71\u54cd\u3002", "method": "\u96c6\u6210\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u6587\u672c\u8f6c\u8bed\u97f3\u56db\u4e2a\u6a21\u5757\uff1aCNN\u60c5\u611f\u5206\u7c7b\u5668\u3001Whisper\u82f1\u8bed\u8f6c\u5f55\u3001\u5fae\u8c03MarianMT\u82f1\u963f\u7ffb\u8bd1\u6a21\u578b\u3001MMS-TTS-Ara\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u5408\u6210\u3002", "result": "\u60c5\u611f\u5206\u7c7bF1\u5206\u657094%\uff0c\u7ffb\u8bd1\u6027\u80fdBLEU 56\u5206\u548cBERTScore F1 88.7%\uff0c\u94f6\u884c\u9886\u57df\u7ffb\u8bd1\u4eba\u5de5\u8bc4\u4f30\u5e73\u5747\u5f97\u520681%\u3002", "conclusion": "EmoAra\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u8bed\u8a00\u8bed\u97f3\u901a\u4fe1\u4e2d\u7684\u60c5\u611f\u4fdd\u6301\uff0c\u5728\u94f6\u884c\u5ba2\u670d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u5173\u5b9e\u73b0\u548c\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00522", "abs": "https://arxiv.org/abs/2602.00522", "authors": ["Chaoran Xu", "Chengkan Lv", "Qiyu Chen", "Feng Zhang", "Zhengtao Zhang"], "title": "MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval", "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.", "AI": {"tldr": "MRAD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u68c0\u7d22\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e24\u7ea7\u8bb0\u5fc6\u5e93\u76f4\u63a5\u68c0\u7d22\u76f8\u4f3c\u6027\u6765\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u65e0\u9700\u590d\u6742\u5efa\u6a21\uff0c\u572816\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u63d0\u793a\u5b66\u4e60\u6216\u590d\u6742\u5efa\u6a21\u6765\u62df\u5408\u6570\u636e\u5206\u5e03\uff0c\u5bfc\u81f4\u8bad\u7ec3/\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u8de8\u57df\u7a33\u5b9a\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMRAD\u6846\u67b6\uff1a\u51bb\u7ed3CLIP\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u6784\u5efa\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u4e24\u7ea7\u8bb0\u5fc6\u5e93\u5b58\u50a8\u7279\u5f81-\u6807\u7b7e\u5bf9\u4f5c\u4e3a\u952e\u503c\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u76f8\u4f3c\u6027\u68c0\u7d22\u76f4\u63a5\u83b7\u5f97\u5f02\u5e38\u5206\u6570\u3002\u8fd8\u63d0\u51fa\u4e86\u4e24\u4e2a\u8f7b\u91cf\u53d8\u4f53\uff1aMRAD-FT\u5fae\u8c03\u68c0\u7d22\u5ea6\u91cf\uff0cMRAD-CLIP\u5c06\u6b63\u5e38/\u5f02\u5e38\u533a\u57df\u5148\u9a8c\u6ce8\u5165CLIP\u6587\u672c\u63d0\u793a\u3002", "result": "\u572816\u4e2a\u5de5\u4e1a\u548c\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0cMRAD\u6846\u67b6\u5728\u5f02\u5e38\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u65e0\u8bba\u662f\u514d\u8bad\u7ec3\u8fd8\u662f\u57fa\u4e8e\u8bad\u7ec3\u7684\u8bbe\u7f6e\u4e0b\u90fd\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u5145\u5206\u5229\u7528\u539f\u59cb\u6570\u636e\u7684\u7ecf\u9a8c\u5206\u5e03\u800c\u975e\u4ec5\u4f9d\u8d56\u6a21\u578b\u62df\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002\u8bb0\u5fc6\u68c0\u7d22\u65b9\u6cd5\u6bd4\u53c2\u6570\u5316\u62df\u5408\u66f4\u6709\u6548\u3002"}}
{"id": "2602.01148", "categories": ["cs.AI", "cs.IT", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01148", "abs": "https://arxiv.org/abs/2602.01148", "authors": ["Jiaxuan Zou", "Yaozhong Xiong", "Yong Liu"], "title": "Capabilities and Fundamental Limits of Latent Chain-of-Thought", "comment": null, "summary": "Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6f5c\u5728\u601d\u7ef4\u94fe\u6a21\u578b\u7684\u63a2\u7d22-\u6267\u884c\u6743\u8861\u7531\u51b3\u7b56\u786e\u5b9a\u6027\u63a7\u5236\uff0c\u63d0\u51fa\u4e86\u7b26\u53f7\u6307\u6570\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u6f5c\u5728\u601d\u7ef4\u94fe\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u56f0\u60d1\u7684\u6027\u80fd\u4e0d\u4e00\u81f4\u6027\uff1a\u5728\u63a2\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff08ProsQA: 97.0%\uff09\uff0c\u4f46\u5728\u8ba1\u7b97\u4efb\u52a1\u4e0a\u5931\u8d25\uff08GSM8K: 34.1%\uff09\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u6743\u8861\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "1) \u7406\u8bba\u8868\u5f81\u63a2\u7d22-\u6267\u884c\u6743\u8861\uff0c\u8bc1\u660e\u9ad8\u786e\u5b9a\u6027\u652f\u6301\u7cbe\u786e\u6267\u884c\u4f46\u6291\u5236\u63a2\u7d22\uff0c\u4f4e\u786e\u5b9a\u6027\u4fc3\u8fdb\u641c\u7d22\u4f46\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\uff1b2) \u5f15\u5165\u7b26\u53f7\u6307\u6570\u4f5c\u4e3a\u91cf\u5316\u51b3\u7b56\u627f\u8bfa\u7684\u6838\u5fc3\u673a\u5236\uff1b3) \u8bc1\u660e\u8bfe\u7a0b\u5b66\u4e60\u7684\u7406\u8bba\u5fc5\u8981\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u51b3\u7b56\u786e\u5b9a\u6027\u4e0e\u6027\u80fd\u6743\u8861\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u7b26\u53f7\u6307\u6570\u4f5c\u4e3a\u63a7\u5236\u63a2\u7d22-\u6267\u884c\u6743\u8861\u7684\u6838\u5fc3\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u8bad\u7ec3\u4f1a\u56e0\u5206\u5e03\u4e0d\u5339\u914d\u800c\u5931\u8d25\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u8bbe\u8ba1\u8303\u5f0f\u4ece\u4e8c\u5143\u67b6\u6784\u9009\u62e9\u8f6c\u5411\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u8282\u51b3\u7b56\u786e\u5b9a\u6027\uff0c\u4e3a\u6f5c\u5728\u601d\u7ef4\u94fe\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.00423", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00423", "abs": "https://arxiv.org/abs/2602.00423", "authors": ["Quang-Huy Nguyen", "Zongliang Yue", "Hao Chen", "Wei-Shinn Ku", "Jiaqi Wang"], "title": "Federated-inspired Single-cell Batch Integration in Latent Space", "comment": null, "summary": "Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.", "AI": {"tldr": "scBatchProx\uff1a\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u539f\u7406\u7684\u540e\u5904\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u4e2d\u7684\u6279\u6b21\u6548\u5e94\u6821\u6b63\uff0c\u65e0\u9700\u539f\u59cb\u8868\u8fbe\u6570\u636e\u6216\u96c6\u4e2d\u5f0f\u4f18\u5316", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u4ea7\u751f\u5927\u91cf\u9ad8\u7ef4\u6570\u636e\uff0c\u4f46\u8de8\u5b9e\u9a8c\u7684\u6570\u636e\u79ef\u7d2f\u4f1a\u5f15\u5165\u6279\u6b21\u6548\u5e94\uff0c\u63a9\u76d6\u771f\u5b9e\u7684\u751f\u7269\u4fe1\u53f7\u3002\u73b0\u6709\u6279\u6b21\u6821\u6b63\u65b9\u6cd5\u8981\u4e48\u6821\u6b63\u4e0d\u8db3\uff0c\u8981\u4e48\u9700\u8981\u5728\u5b8c\u6574\u6570\u636e\u96c6\u4e0a\u96c6\u4e2d\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5206\u5e03\u5f0f\u548c\u6301\u7eed\u6f14\u5316\u7684\u5355\u7ec6\u80de\u6570\u636e\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "scBatchProx\u662f\u4e00\u79cd\u53d7\u8054\u90a6\u5b66\u4e60\u542f\u53d1\u7684\u540e\u5904\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u6279\u6b21\u89c6\u4e3a\u5ba2\u6237\u7aef\uff0c\u5728\u8fd1\u7aef\u6b63\u5219\u5316\u4e0b\u5b66\u4e60\u6279\u6b21\u6761\u4ef6\u9002\u914d\u5668\uff0c\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6821\u6b63\u6279\u6b21\u7ed3\u6784\uff0c\u65e0\u9700\u539f\u59cb\u8868\u8fbe\u6570\u636e\u6216\u96c6\u4e2d\u5f0f\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u8f7b\u91cf\u7ea7\u4e14\u53ef\u90e8\u7f72\uff0c\u4ec5\u4f18\u5316\u6279\u6b21\u7279\u5b9a\u7684\u9002\u914d\u5668\u53c2\u6570\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cscBatchProx\u5728\u6574\u4f53\u5d4c\u5165\u8d28\u91cf\u4e0a\u6301\u7eed\u5e26\u6765\u7ea63-8%\u7684\u76f8\u5bf9\u589e\u76ca\uff0c\u572890%\u7684\u6570\u636e-\u65b9\u6cd5\u5bf9\u4e2d\u6539\u5584\u4e86\u6279\u6b21\u6821\u6b63\uff0c\u572885%\u7684\u6570\u636e-\u65b9\u6cd5\u5bf9\u4e2d\u6539\u5584\u4e86\u751f\u7269\u4fdd\u5b88\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u671d\u7740\u5728\u52a8\u6001\u5355\u7ec6\u80de\u6570\u636e\u7cfb\u7edf\u4e2d\u5b9e\u9645\u6539\u8fdb\u5b66\u4e60\u8868\u793a\u8fc8\u51fa\u4e86\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u90e8\u7f72\u7684\u6279\u6b21\u6821\u6b63\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u548c\u6301\u7eed\u6f14\u5316\u7684\u6570\u636e\u73af\u5883\u3002"}}
{"id": "2602.01193", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01193", "abs": "https://arxiv.org/abs/2602.01193", "authors": ["Shashini Nilukshi", "Deshan Sumanathilaka"], "title": "Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation", "comment": "2 figures, 2 Tables, Accepted at IEEE TIC 2026", "summary": "This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9\u8bcd\u4e49\u6d88\u6b67\uff08VWSD\uff09\u8fdb\u884c\u4e86\u5c0f\u578b\u7efc\u8ff0\uff0cVWSD\u662f\u4f20\u7edf\u8bcd\u4e49\u6d88\u6b67\u7684\u591a\u6a21\u6001\u6269\u5c55\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u89e3\u51b3\u8bcd\u6c47\u6b67\u4e49\u95ee\u9898\u3002\u7efc\u8ff0\u6db5\u76d6\u4e86\u4ece\u65e9\u671f\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5230\u57fa\u4e8eCLIP\u3001\u6269\u6563\u751f\u6210\u548cLLM\u589e\u5f3a\u7684\u65b0\u6846\u67b6\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86VWSD\u5728\u6027\u80fd\u4e0a\u7684\u63d0\u5347\uff08MRR\u63d0\u9ad86-8%\uff09\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4e0a\u4e0b\u6587\u9650\u5236\u3001\u6a21\u578b\u504f\u89c1\u3001\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7f3a\u4e4f\u7b49\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u8bcd\u4e49\u6d88\u6b67\uff08WSD\uff09\u4ec5\u4f9d\u8d56\u6587\u672c\u548c\u8bcd\u6c47\u8d44\u6e90\uff0c\u800c\u89c6\u89c9\u8bcd\u4e49\u6d88\u6b67\uff08VWSD\uff09\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u7ebf\u7d22\u6765\u89e3\u51b3\u8bcd\u6c47\u6b67\u4e49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u3002\u968f\u7740\u591a\u6a21\u6001\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u56de\u987eVWSD\u4ece\u65e9\u671f\u65b9\u6cd5\u5230\u73b0\u4ee3\u6846\u67b6\u7684\u6f14\u8fdb\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u672c\u6587\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u67902016-2025\u5e74\u95f4VWSD\u76f8\u5173\u7814\u7a76\u3002\u6db5\u76d6\u7279\u5f81\u878d\u5408\u3001\u56fe\u6a21\u578b\u3001\u5bf9\u6bd4\u5d4c\u5165\u7b49\u6280\u672f\u53d1\u5c55\uff0c\u7279\u522b\u5173\u6ce8\u57fa\u4e8eCLIP\u7684\u5fae\u8c03\u3001\u6269\u6563\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001LLM\u589e\u5f3a\u7b49\u65b9\u6cd5\uff0c\u4ee5\u53ca\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u8bed\u8a00\u9002\u5e94\u7b49\u7b56\u7565\u3002", "result": "\u5b9a\u91cf\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eCLIP\u7684\u5fae\u8c03\u6a21\u578b\u548cLLM\u589e\u5f3a\u7684VWSD\u7cfb\u7edf\u5728Mean Reciprocal Rank\uff08MRR\uff09\u6307\u6807\u4e0a\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u53476-8%\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "VWSD\u9886\u57df\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0cCLIP\u5bf9\u9f50\u3001\u6269\u6563\u751f\u6210\u548cLLM\u63a8\u7406\u7684\u878d\u5408\u4ee3\u8868\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002\u4f46\u4ecd\u9700\u89e3\u51b3\u4e0a\u4e0b\u6587\u9650\u5236\u3001\u6a21\u578b\u504f\u89c1\u3001\u591a\u8bed\u8a00\u6570\u636e\u96c6\u7f3a\u4e4f\u548c\u8bc4\u4f30\u6846\u67b6\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u591a\u8bed\u8a00\u7684\u6d88\u6b67\u7cfb\u7edf\u3002"}}
{"id": "2602.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00523", "abs": "https://arxiv.org/abs/2602.00523", "authors": ["Yujia Tong", "Tian Zhang", "Yunyang Wan", "Kaiwei Lin", "Jingling Yuan", "Chuang Hu"], "title": "SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding", "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\\times$ for Qwen2.5-VL-72B.", "AI": {"tldr": "SAGE\uff1a\u57fa\u4e8e\u5b9e\u65f6\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u52a8\u6001\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u6d4b\u6811\u7ed3\u6784\u6765\u52a0\u901f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u6811\u7ed3\u6784\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u751f\u6210\u6b65\u9aa4\u4e2d\u9884\u6d4b\u96be\u5ea6\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u63a5\u53d7\u957f\u5ea6\u4e0d\u7406\u60f3\u548c\u52a0\u901f\u6548\u679c\u6709\u9650", "method": "\u57fa\u4e8e\u8f93\u51fa\u71b5\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u6811\u7ed3\u6784\uff1a\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u6784\u5efa\u66f4\u6df1\u66f4\u7a84\u7684\u6811\u4ee5\u6700\u5927\u5316\u63a8\u6d4b\u6df1\u5ea6\uff0c\u4e0d\u786e\u5b9a\u65f6\u6784\u5efa\u66f4\u6d45\u66f4\u5bbd\u7684\u6811\u4ee5\u591a\u6837\u5316\u63a2\u7d22", "result": "SAGE\u663e\u8457\u63d0\u5347\u63a5\u53d7\u957f\u5ea6\u548c\u52a0\u901f\u6548\u679c\uff0c\u5728LLaVA-OneVision-72B\u4e0a\u8fbe\u52303.36\u500d\u89e3\u7801\u52a0\u901f\uff0cQwen2.5-VL-72B\u4e0a\u8fbe\u52303.18\u500d\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u65e0\u635f", "conclusion": "\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u6811\u7ed3\u6784\u80fd\u6709\u6548\u9002\u5e94\u4e0d\u540c\u751f\u6210\u6b65\u9aa4\u7684\u9884\u6d4b\u96be\u5ea6\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387"}}
{"id": "2602.01155", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01155", "abs": "https://arxiv.org/abs/2602.01155", "authors": ["Hugo Math", "Julian Lorentz", "Stefan Oelsner", "Rainer Lienhart"], "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles", "comment": "7 pages, 3 figures", "summary": "Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.", "AI": {"tldr": "CAREP\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u8f66\u8f86\u8bca\u65ad\u6545\u969c\u7801(DTCs)\u4e2d\u81ea\u52a8\u751f\u6210\u9519\u8bef\u6a21\u5f0f(EP)\u89c4\u5219\uff0c\u66ff\u4ee3\u4f20\u7edf\u624b\u5de5\u5236\u5b9a\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u4ea7\u751f\u6570\u5343\u79cd\u4e0d\u540c\u7684\u8bca\u65ad\u6545\u969c\u7801\uff0c\u6c7d\u8f66\u5236\u9020\u5546\u4f7f\u7528\u8fd9\u4e9b\u4ee3\u7801\u7684\u5e03\u5c14\u7ec4\u5408\uff08\u9519\u8bef\u6a21\u5f0f\uff09\u6765\u8868\u5f81\u7cfb\u7edf\u6545\u969c\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89c4\u5219\u76ee\u524d\u4ecd\u7531\u9886\u57df\u4e13\u5bb6\u624b\u5de5\u5236\u5b9a\uff0c\u968f\u7740\u8f66\u8f86\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u65e2\u6602\u8d35\u53c8\u5bb9\u6613\u51fa\u9519\u3002", "method": "CAREP\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a1\uff09\u56e0\u679c\u53d1\u73b0\u667a\u80fd\u4f53\u8bc6\u522bDTC-EP\u6f5c\u5728\u5173\u7cfb\uff1b2\uff09\u4e0a\u4e0b\u6587\u4fe1\u606f\u667a\u80fd\u4f53\u6574\u5408\u5143\u6570\u636e\u548c\u63cf\u8ff0\uff1b3\uff09\u7f16\u6392\u5668\u667a\u80fd\u4f53\u5408\u6210\u5019\u9009\u5e03\u5c14\u89c4\u5219\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u72ec\u7279DTCs\u548c474\u4e2a\u9519\u8bef\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAREP\u80fd\u591f\u81ea\u52a8\u51c6\u786e\u5730\u53d1\u73b0\u672a\u77e5\u7684EP\u89c4\u5219\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u7684\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "CAREP\u7ed3\u5408\u5b9e\u7528\u56e0\u679c\u53d1\u73b0\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u63a8\u7406\uff0c\u4ee3\u8868\u4e86\u5411\u5168\u81ea\u52a8\u6545\u969c\u8bca\u65ad\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8f66\u8f86\u7ef4\u62a4\u3002"}}
{"id": "2602.00424", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.00424", "abs": "https://arxiv.org/abs/2602.00424", "authors": ["Philipp Hoellmer", "Stefano Martiniani"], "title": "Open Materials Generation with Inference-Time Reinforcement Learning", "comment": "16 pages, 8 figures, 1 table", "summary": "Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.", "AI": {"tldr": "OMatG-IRL\uff1a\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u5728\u5b66\u4e60\u7684\u901f\u5ea6\u573a\u4e0a\u64cd\u4f5c\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u5f97\u5206\uff0c\u7528\u4e8e\u6676\u4f53\u6750\u6599\u751f\u6210\u548c\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u3002", "motivation": "\u8fde\u7eed\u65f6\u95f4\u751f\u6210\u6a21\u578b\u53ef\u7528\u4e8e\u6676\u4f53\u6750\u6599\u8bbe\u8ba1\uff0c\u4f46\u96be\u4ee5\u5c06\u76ee\u6807\u5c5e\u6027\u660e\u786e\u7eb3\u5165\u751f\u6210\u8fc7\u7a0b\u3002\u73b0\u6709\u7684\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u8bbf\u95ee\u5f97\u5206\u51fd\u6570\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5728\u4ec5\u5b66\u4e60\u901f\u5ea6\u573a\u7684\u6d41\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faOMatG-IRL\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u5b66\u4e60\u7684\u901f\u5ea6\u573a\u4e0a\u8fdb\u884c\u7b56\u7565\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8\u751f\u6210\u52a8\u529b\u5b66\u6765\u4fdd\u6301\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u540c\u65f6\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u63a2\u7d22\u548c\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u3002", "result": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u7ec4\u5408\u6761\u4ef6\u591a\u6837\u6027\u7684\u540c\u65f6\u6709\u6548\u5f3a\u5316\u57fa\u4e8e\u80fd\u91cf\u7684\u76ee\u6807\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u5f97\u5206\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u80fd\u5b66\u4e60\u65f6\u95f4\u4f9d\u8d56\u7684\u901f\u5ea6\u9000\u706b\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u3002", "conclusion": "OMatG-IRL\u4e3a\u6d41\u5f0f\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5f97\u5206\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u6676\u4f53\u6750\u6599\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c5e\u6027\u5bfc\u5411\u8bbe\u8ba1\u548c\u91c7\u6837\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.01203", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01203", "abs": "https://arxiv.org/abs/2602.01203", "authors": ["Zizhuo Fu", "Wenxuan Zeng", "Runsheng Wang", "Meng Li"], "title": "Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse", "comment": null, "summary": "Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6ce8\u610f\u529b\u6c47\uff08attention sink\uff09\u73b0\u8c61\u4e0e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7ed3\u6784\u7684\u5185\u5728\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u6ce8\u610f\u529b\u5934\u5d29\u6e83\u95ee\u9898\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ecf\u5e38\u5bf9\u7b2c\u4e00\u4e2atoken\u5206\u914d\u4e0d\u6210\u6bd4\u4f8b\u7684\u6ce8\u610f\u529b\uff08\u6ce8\u610f\u529b\u6c47\u73b0\u8c61\uff09\u3002\u867d\u7136\u5df2\u6709\u4e00\u4e9b\u65b9\u6cd5\uff08\u5982GPT-OSS\u4e2d\u7684Sink Attention\u548cQwen3-Next\u4e2d\u7684Gated Attention\uff09\u8bd5\u56fe\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u6ce8\u610f\u529b\u673a\u5236\u4e4b\u95f4\u7684\u5173\u7cfb\u7f3a\u4e4f\u5168\u9762\u5206\u6790\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc1\u636e\u8bc1\u660eVanilla Attention\u548cSink Attention\u4e2d\u7684\u6ce8\u610f\u529b\u6c47\u81ea\u7136\u6784\u5efa\u4e86\u6ce8\u610f\u529b\u5c42\u5185\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u673a\u5236\u3002\u4e3a\u7f13\u89e3\u6ce8\u610f\u529b\u5934\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5177\u6709\u8f85\u52a9\u8d1f\u8f7d\u5747\u8861\u635f\u5931\u7684\u6c47\u611f\u77e5\u8bad\u7ec3\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Vanilla Attention\u3001Sink Attention\u548cGated Attention\u4e2d\u90fd\u80fd\u6709\u6548\u5b9e\u73b0\u6ce8\u610f\u529b\u5934\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22\u6ce8\u610f\u529b\u5c42\u5185\u56fa\u6709\u7684MoE\u7ed3\u6784\u3002"}}
{"id": "2602.00531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00531", "abs": "https://arxiv.org/abs/2602.00531", "authors": ["Tianyi Zhang", "Antoine Simoulin", "Kai Li", "Sana Lakdawala", "Shiqing Yu", "Arpit Mittal", "Hongyu Fu", "Yu Lin"], "title": "Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment", "comment": null, "summary": "Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.", "AI": {"tldr": "VLDet\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7279\u5f81\u91d1\u5b57\u5854\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u5728COCO\u548cLVIS\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u7cfb\u7edf\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u867d\u7136\u80fd\u8bc6\u522b\u8bad\u7ec3\u96c6\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u7c7b\u522b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5c06CLIP\u7684\u5355\u5c3a\u5ea6\u56fe\u50cf\u4e3b\u5e72\u7f51\u7edc\u9002\u914d\u5230\u68c0\u6d4b\u6846\u67b6\u6216\u786e\u4fdd\u9c81\u68d2\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faVLDet\u6846\u67b6\uff0c\u5305\u542bVL-PUB\u6a21\u5757\u6765\u5229\u7528CLIP\u7684\u89c6\u89c9-\u8bed\u8a00\u77e5\u8bc6\u5e76\u901a\u8fc7\u7279\u5f81\u91d1\u5b57\u5854\u9002\u914d\u4e3b\u5e72\u7f51\u7edc\uff0c\u4ee5\u53caSigRPN\u5757\u5f15\u5165\u57fa\u4e8esigmoid\u7684\u951a\u70b9-\u6587\u672c\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\u6765\u6539\u8fdb\u65b0\u7c7b\u522b\u68c0\u6d4b\u3002", "result": "\u5728COCO2017\u6570\u636e\u96c6\u4e0a\u8fbe\u523058.7 AP\uff08\u65b0\u7c7b\u522b\uff09\uff0c\u5728LVIS\u6570\u636e\u96c6\u4e0a\u8fbe\u523024.8 AP\uff0c\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534727.6%\u548c6.9%\u3002\u540c\u65f6\u5728\u95ed\u96c6\u76ee\u6807\u68c0\u6d4b\u4e0a\u4e5f\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "VLDet\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u7279\u5f81\u91d1\u5b57\u5854\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u76ee\u6807\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01167", "abs": "https://arxiv.org/abs/2602.01167", "authors": ["Zhiming Liu", "Yujie Wei", "Lei Feng", "Xiu Su", "Xiaobo Xia", "Weili Guan", "Zeke Xie", "Shuo Yang"], "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models", "comment": null, "summary": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u5c42\uff0c\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u5c42\u5254\u9664\u65b9\u6cd5TaLo\uff0c\u53ef\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9ed8\u8ba4\u4f7f\u7528\u6240\u6709\u5c42\u8fdb\u884c\u9884\u6d4b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5c42\u53cd\u800c\u4f1a\u635f\u5bb3\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9700\u8981\u8bc6\u522b\u5e76\u7ed5\u8fc7\u8fd9\u4e9b\u5e72\u6270\u5c42", "method": "\u901a\u8fc7\u5c42\u5e72\u9884\u5206\u6790\u5404\u5c42\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4efb\u52a1-\u5c42\u4ea4\u4e92\u5411\u91cf\u91cf\u5316\u5c42\u5e72\u9884\u6548\u679c\uff0c\u5e76\u8bbe\u8ba1TaLo\u65b9\u6cd5\u52a8\u6001\u8bc6\u522b\u548c\u7ed5\u8fc7\u6700\u5e72\u6270\u5c42", "result": "TaLo\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u63d0\u5347\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u6027\u80fd\uff0c\u5982\u5728ScienceQA\u7684Maps\u4efb\u52a1\u4e0a\u5c06Qwen-VL\u51c6\u786e\u7387\u63d0\u534716.6%", "conclusion": "\u63ed\u793a\u4e86\u9884\u8bad\u7ec3VLM\u4e2d\u610f\u5916\u7684\u6a21\u5757\u5316\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u4f18\u5316\u673a\u5236\uff0c\u53ef\u89e3\u9501\u6a21\u578b\u7684\u9690\u85cf\u80fd\u529b"}}
{"id": "2602.00426", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00426", "abs": "https://arxiv.org/abs/2602.00426", "authors": ["Vikram Krishnamurthy"], "title": "LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference", "comment": "27 pages, 12 figures. Mathematical survey framing LLMs as high-dimensional nonlinear autoregressive models with attention, covering training, alignment, and inference, with nanoGPT/nanochat-style code examples. Feedback welcome", "summary": "Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53d8\u6362\u5668\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u5177\u6709\u6ce8\u610f\u529b\u4f9d\u8d56\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u6db5\u76d6\u4e86\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\u65b9\u6cd5\u548c\u63a8\u7406\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u67b6\u6784\u7ec4\u4ef6\u548c\u8bad\u7ec3\u8fc7\u7a0b\u6765\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u3002\u672c\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u7b80\u6d01\u7684\u6570\u5b66\u53c2\u8003\uff0c\u63d0\u4f9b\u65b9\u7a0b\u7ea7\u522b\u7684LLM\u8bad\u7ec3\u3001\u5bf9\u9f50\u548c\u751f\u6210\u63cf\u8ff0\u3002", "method": "\u5c06LLMs\u5f62\u5f0f\u5316\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u81ea\u6ce8\u610f\u529b\u88ab\u8868\u8ff0\u4e3a\u91cd\u590d\u7684\u53cc\u7ebf\u6027-softmax-\u7ebf\u6027\u7ec4\u5408\u3002\u6846\u67b6\u5305\u62ec\uff1a1\uff09\u901a\u8fc7\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b2\uff09\u5bf9\u9f50\u65b9\u6cd5\u5982RLHF\u3001DPO\u3001RSFT\u3001RLVR\uff1b3\uff09\u63a8\u7406\u65f6\u7684\u81ea\u56de\u5f52\u751f\u6210\u3002", "result": "\u8be5\u6570\u5b66\u6846\u67b6\u80fd\u591f\u5bf9\u5bf9\u9f50\u8bf1\u5bfc\u884c\u4e3a\uff08\u5305\u62ec\u5949\u627f\uff09\u3001\u63a8\u7406\u65f6\u73b0\u8c61\uff08\u5982\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u4ee5\u53ca\u6301\u7eed\u5b66\u4e60\u7b49\u6269\u5c55\u8fdb\u884c\u539f\u7406\u6027\u5206\u6790\u3002", "conclusion": "\u8be5\u516c\u5f0f\u5316\u65b9\u6cd5\u4e3aLLM\u7684\u89e3\u91ca\u548c\u8fdb\u4e00\u6b65\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u53c2\u8003\u6846\u67b6\uff0c\u4f7f\u81ea\u6ce8\u610f\u529b\u81ea\u7136\u5730\u4f5c\u4e3a\u91cd\u590d\u7684\u53cc\u7ebf\u6027-softmax-\u7ebf\u6027\u7ec4\u5408\u51fa\u73b0\uff0c\u4ea7\u751f\u9ad8\u5ea6\u8868\u8fbe\u6027\u7684\u5e8f\u5217\u6a21\u578b\u3002"}}
{"id": "2602.01204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01204", "abs": "https://arxiv.org/abs/2602.01204", "authors": ["Xuqin Zhang", "Quan He", "Zhenrui Zheng", "Zongzhang Zhang", "Xu He", "Dong Li"], "title": "ASTER: Agentic Scaling with Tool-integrated Extended Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.", "AI": {"tldr": "ASTER\u6846\u67b6\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u51b7\u542f\u52a8\u7b56\u7565\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u7684\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u4f7f4B\u53c2\u6570\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728LLMs\u4e2d\u7528\u4e8e\u957f\u7a0b\u63a8\u7406\u65f6\u5b58\u5728\"\u4ea4\u4e92\u5d29\u6e83\"\u95ee\u9898\uff1a\u6a21\u578b\u65e0\u6cd5\u7ef4\u6301\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\uff0c\u9000\u5316\u4e3a\u5927\u91cf\u5185\u90e8\u63a8\u7406\u548c\u7b80\u5355\u7684\u540e\u9a8c\u4ee3\u7801\u9a8c\u8bc1", "method": "\u63d0\u51faASTER\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u51b7\u542f\u52a8\u7b56\u7565\u4f18\u5148\u9009\u62e9\u4ea4\u4e92\u5bc6\u96c6\u7684\u8f68\u8ff9\uff0c\u4ec5\u97004K\u4e2a\u4ea4\u4e92\u5bc6\u96c6\u8f68\u8ff9\u5c31\u80fd\u5efa\u7acb\u5f3a\u5927\u7684\u5148\u9a8c\uff0c\u652f\u6301\u6269\u5c55\u7684RL\u8bad\u7ec3", "result": "ASTER-4B\u5728\u7ade\u4e89\u6027\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5728AIME 2025\u4e0a\u8fbe\u523090.0%\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecDeepSeek-V3.2-Exp\u5728\u5185\u7684\u9886\u5148\u5f00\u6e90\u6a21\u578b", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u51b7\u542f\u52a8\u7b56\u7565\u53ef\u4ee5\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u7684\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u5c0f\u89c4\u6a21\u7684\u4e13\u5bb6\u51b7\u542f\u52a8\u96c6\u80fd\u591f\u5efa\u7acb\u5f3a\u5927\u7684\u884c\u4e3a\u5148\u9a8c\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u63a2\u7d22\u548c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.00536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00536", "abs": "https://arxiv.org/abs/2602.00536", "authors": ["Yifan Zhang", "Qian Chen", "Yi Liu", "Wengen Li", "Jihong Guan"], "title": "SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal", "comment": null, "summary": "Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.", "AI": {"tldr": "SADER\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u65f6\u76f8\u9065\u611f\u5f71\u50cf\u53bb\u4e91\u7684\u7ed3\u6784\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65f6\u76f8\u6761\u4ef6\u6269\u6563\u7f51\u7edc\u3001\u4e91\u611f\u77e5\u6ce8\u610f\u529b\u635f\u5931\u548c\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u4e91\u6548\u679c\u548c\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u4e91\u6c61\u67d3\u4e25\u91cd\u964d\u4f4e\u4e86\u9065\u611f\u5f71\u50cf\u7684\u53ef\u7528\u6027\uff0c\u5bf9\u4e0b\u6e38\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u6784\u6210\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u91c7\u6837\u6548\u7387\u6709\u9650\u3001\u5728\u591a\u65f6\u76f8\u9065\u611f\u573a\u666f\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u7ed3\u6784\u548c\u65f6\u95f4\u5148\u9a8c\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u53ef\u6269\u5c55\u7684\u591a\u65f6\u76f8\u6761\u4ef6\u6269\u6563\u7f51\u7edc\uff08MTCDN\uff09\uff0c\u901a\u8fc7\u65f6\u95f4\u878d\u5408\u548c\u6df7\u5408\u6ce8\u610f\u529b\u5145\u5206\u6355\u6349\u591a\u65f6\u76f8\u548c\u591a\u6a21\u6001\u76f8\u5173\u6027\uff1b2. \u5f15\u5165\u4e91\u611f\u77e5\u6ce8\u610f\u529b\u635f\u5931\uff0c\u901a\u8fc7\u8003\u8651\u4e91\u539a\u5ea6\u548c\u4eae\u5ea6\u5dee\u5f02\u6765\u5f3a\u8c03\u4e91\u4e3b\u5bfc\u533a\u57df\uff1b3. \u8bbe\u8ba1\u786e\u5b9a\u6027\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u5728\u56fa\u5b9a\u91c7\u6837\u6b65\u6570\u4e0b\u901a\u8fc7\u5f15\u5bfc\u6821\u6b63\u66ff\u6362\u5f02\u5e38\u503c\u6765\u8fed\u4ee3\u4f18\u5316\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u591a\u65f6\u76f8\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSADER\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53bb\u4e91\u65b9\u6cd5\u3002", "conclusion": "SADER\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\u3001\u4e91\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9ad8\u6548\u7684\u91c7\u6837\u7b56\u7565\uff0c\u4e3a\u591a\u65f6\u76f8\u9065\u611f\u5f71\u50cf\u53bb\u4e91\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01171", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.01171", "abs": "https://arxiv.org/abs/2602.01171", "authors": ["Stefan Szeider"], "title": "ASP-Bench: From Natural Language to Logic Programs", "comment": null, "summary": "Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.\n  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.\n  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.", "AI": {"tldr": "ASP-Bench\u662f\u4e00\u4e2a\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u7b54\u6848\u96c6\u7a0b\u5e8f\uff08ASP\uff09\u7684\u7cfb\u7edf\uff0c\u8986\u76d6\u4e86ASP\u7684\u5404\u79cd\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5c55\u793a\u4e86\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u903b\u8f91\u7a0b\u5e8f\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u795e\u7ecf\u7b26\u53f7\u5de5\u7a0b\u7684\u53d1\u5c55\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u7c7b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5efa\u6a21\u7684\u96be\u5ea6\u56e0\u7d20\u3002", "method": "\u521b\u5efaASP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5b9e\u4f8b\uff0864\u4e2a\u57fa\u7840\u95ee\u9898\u7684\u7b80\u5355\u548c\u56f0\u96be\u53d8\u4f53\uff09\uff0c\u8986\u76d6ASP\u7684\u5404\u79cd\u7279\u6027\u5982\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u548c\u4f18\u5316\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u5305\u542b\u53c2\u8003\u9a8c\u8bc1\u5668\u6765\u68c0\u67e5\u89e3\u51b3\u65b9\u6848\u662f\u5426\u7b26\u5408\u89c4\u8303\u3002\u4f7f\u7528\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u6765\u5efa\u6a21\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5bf9ASP\u7279\u6027\u7684\u7cfb\u7edf\u8986\u76d6\uff0c\u5e76\u6cbf\u4e03\u4e2a\u72ec\u7acb\u7684\u63a8\u7406\u65b9\u9762\uff08\u4f18\u5316\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u9ed8\u8ba4\u903b\u8f91\u3001\u8d44\u6e90\u5206\u914d\u3001\u9012\u5f52\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u91cf\u5316\u590d\u6742\u6027\uff09\u5bf9\u95ee\u9898\u8fdb\u884c\u7279\u5f81\u5316\u3002\u57fa\u4e8eReAct\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u9971\u548c\uff0c\u8868\u660e\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u662f\u5efa\u6a21\u81ea\u7136\u8bed\u8a00ASP\u95ee\u9898\u7684\u53ef\u9760\u4e14\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "conclusion": "ASP-Bench\u4e3a\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5230ASP\u7684\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u7684\u96be\u5ea6\u5206\u6790\u5e2e\u52a9\u7406\u89e3\u95ee\u9898\u5efa\u6a21\u7684\u590d\u6742\u6027\u3002\u57fa\u4e8e\u53cd\u9988\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5c55\u793a\u4e86\u5904\u7406\u8fd9\u7c7b\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u5de5\u7a0b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2602.00446", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00446", "abs": "https://arxiv.org/abs/2602.00446", "authors": ["Ziyao Wang", "Nizhang Li", "Pingzhi Li", "Guoheng Sun", "Tianlong Chen", "Ang Li"], "title": "Towards Building Non-Fine-Tunable Foundation Models", "comment": null, "summary": "Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.", "AI": {"tldr": "\u63d0\u51faPrivate Mask Pre-Training (PMP)\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u65f6\u5c06\u8868\u5f81\u5b66\u4e60\u96c6\u4e2d\u5728\u7a00\u758f\u5b50\u7f51\u7edc\u4e2d\uff0c\u5e76\u4fdd\u5bc6\u8be5\u5b50\u7f51\u7edc\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u4f7f\u672a\u7ecf\u6388\u6743\u7684\u4e0b\u6e38\u5fae\u8c03\u96be\u4ee5\u6709\u6548\u9002\u5e94\uff0c\u4ece\u800c\u4fdd\u62a4\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u7684\u7ecf\u6d4e\u548c\u5b89\u5168\u5229\u76ca\u3002", "motivation": "\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u867d\u7136\u4fc3\u8fdb\u4e86\u5e7f\u6cdb\u91cd\u7528\uff0c\u4f46\u4e5f\u4f7f\u6a21\u578b\u8bad\u7ec3\u8005\u9762\u4e34\u672a\u7ecf\u63a7\u5236\u7684\u4e0b\u6e38\u5fae\u8c03\u5e26\u6765\u7684\u7ecf\u6d4e\u548c\u5b89\u5168\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u6a21\u578b\u5728\u53d1\u5e03\u540e\u4fdd\u6301\u5e7f\u6cdb\u53ef\u7528\u6027\uff0c\u540c\u65f6\u9650\u5236\u672a\u7ecf\u6388\u6743\u7684\u4efb\u52a1\u65e0\u5173\u5fae\u8c03\u5e26\u6765\u7684\u9002\u5e94\u589e\u76ca\u3002", "method": "\u63d0\u51faPrivate Mask Pre-Training (PMP)\u9884\u8bad\u7ec3\u6846\u67b6\uff1a1) \u5728\u8bad\u7ec3\u65e9\u671f\u8bc6\u522b\u7a00\u758f\u5b50\u7f51\u7edc\uff1b2) \u5c06\u8868\u5f81\u5b66\u4e60\u96c6\u4e2d\u5728\u8be5\u5b50\u7f51\u7edc\u4e2d\uff1b3) \u4fdd\u5bc6\u5b9a\u4e49\u8be5\u5b50\u7f51\u7edc\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u4ec5\u53d1\u5e03\u6700\u7ec8\u7684\u5bc6\u96c6\u6743\u91cd\u3002\u8fd9\u6837\u672a\u7ecf\u6388\u6743\u7684\u5fae\u8c03\u7531\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u63a9\u7801\uff0c\u4f1a\u66f4\u65b0\u4e0e\u9884\u8bad\u7ec3\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u7684\u53c2\u6570\uff0c\u5bfc\u81f4\u5fae\u8c03\u76ee\u6807\u4e0e\u9884\u8bad\u7ec3\u51e0\u4f55\u7ed3\u6784\u4e0d\u5339\u914d\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8fd9\u79cd\u4e0d\u5339\u914d\u4f1a\u7834\u574f\u57fa\u4e8e\u68af\u5ea6\u7684\u9002\u5e94\u8fc7\u7a0b\u5e76\u9650\u5236\u5fae\u8c03\u589e\u76ca\u3002\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0cPMP\u80fd\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\u6301\u7eed\u964d\u4f4e\u672a\u7ecf\u6388\u6743\u5fae\u8c03\u7684\u6548\u679c\uff0c\u4e14\u975e\u5fae\u8c03\u6027\u7684\u5f3a\u5ea6\u53ef\u901a\u8fc7\u63a9\u7801\u6bd4\u4f8b\u63a7\u5236\u3002", "conclusion": "PMP\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u975e\u5fae\u8c03\u6027\u57fa\u7840\u6a21\u578b\uff0c\u65e2\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u5e7f\u6cdb\u53ef\u7528\u6027\uff0c\u53c8\u4fdd\u62a4\u4e86\u6a21\u578b\u8bad\u7ec3\u8005\u7684\u5229\u76ca\uff0c\u901a\u8fc7\u63a7\u5236\u63a9\u7801\u6bd4\u4f8b\u53ef\u4ee5\u8c03\u8282\u4fdd\u62a4\u5f3a\u5ea6\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01208", "abs": "https://arxiv.org/abs/2602.01208", "authors": ["Kai Zhang", "Jiayi Liao", "Chengpeng Li", "Ziyuan Xie", "Sihang Li", "Xiang Wang"], "title": "Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling", "comment": null, "summary": "Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \\textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\\% over Pass@1 and 22.70\\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.", "AI": {"tldr": "Chronos\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u65f6\u5e8f\u63a8\u7406\u8bc4\u5206\u5668\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u5b66\u4e60token\u6982\u7387\u7279\u5f81\u5206\u914d\u8d28\u91cf\u5206\u6570\u5e76\u8fdb\u884c\u52a0\u6743\u6295\u7968\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff08\u5982\u591a\u6570\u6295\u7968\u548c\u542f\u53d1\u5f0ftoken\u7ea7\u8bc4\u5206\uff09\u5e73\u7b49\u5bf9\u5f85\u63a8\u7406\u8f68\u8ff9\u6216token\uff0c\u5bb9\u6613\u53d7\u5230\u8f68\u8ff9\u8d28\u91cf\u5927\u5e45\u6ce2\u52a8\u548c\u5c40\u90e8\u903b\u8f91\u5931\u8d25\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8f68\u8ff9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "Chronos\u5c06\u6bcf\u4e2a\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u5b66\u4e60\u6355\u83b7token\u6982\u7387\u7279\u5f81\uff0c\u4e3a\u8f68\u8ff9\u5206\u914d\u8d28\u91cf\u5206\u6570\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u6295\u7968\u673a\u5236\u3002\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u65f6\u5e8f\u63a8\u7406\u8bc4\u5206\u5668\u3002", "result": "\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChronos\u5728\u5404\u79cd\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002Chronos@128\u5728HMMT25\u4e0a\u76f8\u5bf9Pass@1\u63d0\u534734.21%\uff0c\u76f8\u5bf9Maj@128\u63d0\u534722.70%\uff08\u4f7f\u7528Qwen3-4B-Thinking-2507\uff09\u3002", "conclusion": "Chronos\u901a\u8fc7\u5c06\u63a8\u7406\u8f68\u8ff9\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\u5e76\u8fdb\u884c\u7cbe\u7ec6\u8d28\u91cf\u8bc4\u4f30\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u5373\u63d2\u5373\u7528\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.00542", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00542", "abs": "https://arxiv.org/abs/2602.00542", "authors": ["Mohammad Saeid", "Amir Salarpour", "Pedram MohajerAnsari", "Mert D. Pes\u00e9"], "title": "NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation", "comment": "Accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV 2026)", "summary": "We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods", "AI": {"tldr": "NPNet\u662f\u4e00\u79cd\u5b8c\u5168\u975e\u53c2\u6570\u5316\u76843D\u70b9\u4e91\u5206\u7c7b\u548c\u90e8\u4ef6\u5206\u5272\u65b9\u6cd5\uff0c\u4e0d\u4f7f\u7528\u5b66\u4e60\u6743\u91cd\uff0c\u4ec5\u4f9d\u8d56\u786e\u5b9a\u6027\u7b97\u5b50\u6784\u5efa\u70b9\u7279\u5f81\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9ad8\u65af-\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\u9002\u5e94\u4e0d\u540c\u5c3a\u5ea6\u548c\u91c7\u6837\u5bc6\u5ea6\u3002", "motivation": "\u73b0\u67093D\u70b9\u4e91\u5904\u7406\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5b66\u4e60\u6743\u91cd\uff0c\u4f46\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5b8c\u5168\u975e\u53c2\u6570\u5316\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u786e\u5b9a\u6027\u7b97\u5b50\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u5728\u4e0d\u540c\u5c3a\u5ea6\u548c\u91c7\u6837\u5bc6\u5ea6\u4e0b\u8868\u73b0\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "NPNet\u91c7\u7528\u5b8c\u5168\u975e\u53c2\u6570\u5316\u8bbe\u8ba1\uff0c\u4f7f\u7528\u6700\u8fdc\u70b9\u91c7\u6837\u3001k\u8fd1\u90bb\u548c\u6c60\u5316\u7b49\u786e\u5b9a\u6027\u7b97\u5b50\u6784\u5efa\u70b9\u7279\u5f81\u3002\u6838\u5fc3\u521b\u65b0\u662f\u81ea\u9002\u5e94\u9ad8\u65af-\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\uff0c\u5176\u5e26\u5bbd\u548c\u9ad8\u65af-\u4f59\u5f26\u6df7\u5408\u53c2\u6570\u6839\u636e\u8f93\u5165\u51e0\u4f55\u81ea\u52a8\u9009\u62e9\u3002\u5bf9\u4e8e\u5206\u5272\u4efb\u52a1\uff0c\u8fd8\u52a0\u5165\u56fa\u5b9a\u9891\u7387\u5085\u91cc\u53f6\u7279\u5f81\u63d0\u4f9b\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728ModelNet40/ModelNet-R\u3001ScanObjectNN\u548cShapeNetPart\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNPNet\u5728\u975e\u53c2\u6570\u5316\u57fa\u7ebf\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728ModelNet40\u7684\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e2d\u6548\u679c\u663e\u8457\u3002\u76f8\u6bd4\u4e4b\u524d\u7684\u975e\u53c2\u6570\u5316\u65b9\u6cd5\uff0cNPNet\u5728\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u4e5f\u6709\u4f18\u52bf\u3002", "conclusion": "NPNet\u8bc1\u660e\u4e86\u5b8c\u5168\u975e\u53c2\u6570\u5316\u65b9\u6cd5\u57283D\u70b9\u4e91\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u5c3a\u5ea6\u548c\u91c7\u6837\u5bc6\u5ea6\u4e0b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u70b9\u4e91\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.01198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01198", "abs": "https://arxiv.org/abs/2602.01198", "authors": ["Liang Zhang", "Yu Zhao", "Longyue Wang", "Tianqi Shi", "Weihua Luo", "Kaifu Zhang", "Jinsong Su"], "title": "A State-Transition Framework for Efficient LLM Reasoning", "comment": "ICLR 2026", "summary": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u5c06LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u4f30\u8ba1\u63a8\u7406\u72b6\u6001\uff0c\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u4e3a\u7ebf\u6027\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u867d\u7136\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u751f\u6210\u957fCoT\u5e8f\u5217\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u5f88\u9ad8\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u901a\u8fc7\u538b\u7f29CoT\u5e8f\u5217\u6765\u63d0\u5347\u6548\u7387\uff0c\u4f46\u8fd9\u4e0e\u6d4b\u8bd5\u65f6\u6269\u5c55\u76f8\u51b2\u7a81\uff0c\u9650\u5236\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\u8fc7\u7a0b\uff1b2. \u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u4f30\u8ba1\u63a8\u7406\u72b6\u6001\uff0c\u8bb0\u5f55\u5386\u53f2\u63a8\u7406\u4fe1\u606f\uff1b3. \u57fa\u4e8e\u67e5\u8be2\u63d0\u793a\u548c\u63a8\u7406\u72b6\u6001\uff0cLLM\u9ad8\u6548\u6267\u884c\u5f53\u524d\u63a8\u7406\u6b65\u9aa4\u5e76\u66f4\u65b0\u72b6\u6001\uff1b4. \u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7684\u63a8\u7406\u7b56\u7565\u7f13\u89e3\u566a\u58f0\u63a8\u7406\u6b65\u9aa4\u5bfc\u81f4\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86LLM\u7684\u63a8\u7406\u6548\u7387\uff0c\u8fd8\u589e\u5f3a\u4e86\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\uff0c\u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u957fCoT\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2602.00451", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00451", "abs": "https://arxiv.org/abs/2602.00451", "authors": ["Xiaoyu Wang", "Xiaotian Li", "Zhixiang Zhou", "Chen Li", "Yong Liu"], "title": "Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA", "comment": "17 Pages", "summary": "Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \\texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \\texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \\texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.", "AI": {"tldr": "TAD-LoRA\uff1a\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u7684\u53bb\u4e2d\u5fc3\u5316\u4f4e\u79e9\u9002\u5e94\u6846\u67b6\uff0c\u89e3\u51b3DFL\u4e2dLoRA\u66f4\u65b0\u7684\u62d3\u6251\u4f9d\u8d56\u4ea4\u53c9\u9879\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u901a\u4fe1\u62d3\u6251\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u4f5c\u4e3a\u65e0\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u53d8\u4f53\uff0c\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u9762\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u5206\u89e3\u7ed3\u6784\u5bfc\u81f4\u53bb\u4e2d\u5fc3\u5316\u805a\u5408\u65f6\u4ea7\u751f\u62d3\u6251\u4f9d\u8d56\u7684\u4ea4\u53c9\u9879\uff0c\u8fd9\u4e9b\u9879\u5728\u52a8\u6001\u901a\u4fe1\u56fe\u4e0b\u53ef\u80fd\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faTAD-LoRA\u6846\u67b6\uff0c\u534f\u8c03LoRA\u56e0\u5b50\u7684\u66f4\u65b0\u548c\u6df7\u5408\u4ee5\u63a7\u5236\u5ba2\u6237\u7aef\u95f4\u9519\u4f4d\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u5728\u975e\u51f8\u76ee\u6807\u4e0b\u7684\u6536\u655b\u6027\uff0c\u660e\u786e\u8868\u5f81\u62d3\u6251\u8bf1\u5bfc\u4ea4\u53c9\u9879\u8bef\u5dee\u4e0e\u5757\u5750\u6807\u8868\u793a\u504f\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5728\u4e0d\u540c\u901a\u4fe1\u6761\u4ef6\u4e0b\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u6790\uff1aTAD-LoRA\u5728\u5404\u79cd\u901a\u4fe1\u573a\u666f\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\uff0c\u5728\u5f3a\u8fde\u63a5\u62d3\u6251\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5728\u4e2d\u5ea6\u548c\u5f31\u8fde\u63a5\u62d3\u6251\u4e0b\u83b7\u5f97\u660e\u663e\u589e\u76ca\uff0c\u5728MNLI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "TAD-LoRA\u6709\u6548\u89e3\u51b3\u4e86DFL\u4e2dLoRA\u66f4\u65b0\u7684\u62d3\u6251\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u8c03\u66f4\u65b0\u548c\u6df7\u5408\u7b56\u7565\u63a7\u5236\u5ba2\u6237\u7aef\u95f4\u9519\u4f4d\uff0c\u5728\u4e0d\u540c\u901a\u4fe1\u62d3\u6251\u4e0b\u90fd\u80fd\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u548c\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2602.01227", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01227", "abs": "https://arxiv.org/abs/2602.01227", "authors": ["Zhanming Shen", "Zeyu Qin", "Jiaqi Hu", "Wentao Ye", "Hao Chen", "Xiaomeng Hu", "Haokai Xu", "Gang Chen", "Yi R. Fung", "Haobo Wang"], "title": "Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority", "comment": null, "summary": "The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"Token Priority\"\u4f5c\u4e3a\u89e3\u51b3\u7ec6\u7c92\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u4e0e\u7c97\u7c92\u5ea6\u76d1\u7763\u4fe1\u53f7\u4e4b\u95f4\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u7684\u5173\u952e\u6865\u6881\uff0c\u5c06SFT\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7cbe\u786e\u7684\u5206\u5e03\u91cd\u5851\u8fc7\u7a0b\uff0c\u800c\u975e\u7b80\u5355\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u4ece\u62df\u5408\u7ecf\u9a8c\u6570\u636e\u5230\u5b9e\u73b0\u771f\u6b63\u4eba\u7c7b\u6548\u7528\u7684\u8f6c\u53d8\u53d7\u5230\u7c92\u5ea6\u4e0d\u5339\u914d\u7684\u6839\u672c\u7ea6\u675f\uff0c\u7ec6\u7c92\u5ea6\u7684\u81ea\u56de\u5f52\u751f\u6210\u5f80\u5f80\u7531\u7c97\u7c92\u5ea6\u6216\u5747\u5300\u7684\u4fe1\u53f7\u76d1\u7763\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u597d\u7684\u5bf9\u9f50\u673a\u5236\u3002", "method": "\u63d0\u51faToken Priority\u6846\u67b6\uff0c\u5c06SFT\u5f62\u5f0f\u5316\u4e3a\u7cbe\u786e\u7684\u5206\u5e03\u91cd\u5851\u8fc7\u7a0b\uff0c\u5c06\u539f\u59cb\u6570\u636e\u4e0e\u7406\u60f3\u5bf9\u9f50\u6d41\u5f62\u5bf9\u9f50\u3002\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u7c7b\u4e3a\u4e24\u79cd\u673a\u5236\uff1a\u7528\u4e8e\u566a\u58f0\u8fc7\u6ee4\u7684Positive Priority\u548c\u7528\u4e8e\u6bd2\u6027\u6a21\u5f0f\u9057\u5fd8\u7684Signed Priority\u3002", "result": "\u901a\u8fc7\u7edf\u4e00\u89c6\u89d2\u5206\u6790\u8fd1\u671f\u7a81\u7834\uff0c\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u8fdb\u5c55\u548c\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u5efa\u8bae\u3002", "conclusion": "Token Priority\u662f\u89e3\u51b3\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u7684\u5173\u952e\u6865\u6881\uff0c\u4e3aSFT\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u5fae\u8c03\u4ece\u7b80\u5355\u4f18\u5316\u63d0\u5347\u4e3a\u7cbe\u786e\u7684\u5206\u5e03\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u4e3a\u672a\u6765\u5bf9\u9f50\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.00559", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00559", "abs": "https://arxiv.org/abs/2602.00559", "authors": ["Wenbin Xing", "Quanxing Zha", "Lizheng Zu", "Mengran Li", "Ming Li", "Junchi Yan"], "title": "Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models", "comment": null, "summary": "Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., \"All are correct\" and \"None of the above\") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OmniVCHall\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5b64\u7acb\u548c\u7ec4\u5408\u5e7b\u89c9\uff0c\u5e76\u5f00\u53d1\u4e86TriCD\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\u6765\u7f13\u89e3\u7ec4\u5408\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5e7b\u89c9\u7f13\u89e3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u9519\u8bef\u7c7b\u578b\uff0c\u800c\u7531\u591a\u4e2a\u4ea4\u4e92\u65f6\u7a7a\u56e0\u7d20\u9519\u8bef\u63a8\u7406\u4ea7\u751f\u7684\u7ec4\u5408\u5e7b\u89c9\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efaOmniVCHall\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u6837\u89c6\u9891\u9886\u57df\uff0c\u5f15\u5165\u65b0\u578b\u76f8\u673a\u5e7b\u89c9\u7c7b\u578b\uff0c\u5b9a\u4e49\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6cd5\uff0c\u5305\u542b\u5bf9\u6297\u6027\u7b54\u6848\u9009\u9879\uff1b2) \u63d0\u51faTriCD\u5bf9\u6bd4\u89e3\u7801\u6846\u67b6\uff0c\u5305\u542b\u4e09\u8def\u5f84\u6821\u51c6\u673a\u5236\uff1a\u81ea\u9002\u5e94\u6270\u52a8\u63a7\u5236\u5668\u6784\u5efa\u8d1f\u6837\u672c\u89c6\u9891\u53d8\u4f53\uff0c\u663e\u8457\u6027\u5f15\u5bfc\u589e\u5f3a\u6a21\u5757\u5f3a\u5316\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u8bc4\u4f3039\u4e2a\u4ee3\u8868\u6027VLLM\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\uff08\u5982Qwen3-VL\u548cGPT-5\uff09\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002TriCD\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u9aa8\u5e72\u6a21\u578b\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "\u7ec4\u5408\u5e7b\u89c9\u662f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6311\u6218\uff0cOmniVCHall\u57fa\u51c6\u80fd\u7cfb\u7edf\u8bc4\u4f30\u8be5\u95ee\u9898\uff0cTriCD\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u7ec4\u5408\u5e7b\u89c9\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01202", "abs": "https://arxiv.org/abs/2602.01202", "authors": ["Mingze Kong", "Zikun Qu", "Zhongquan Zhou", "Pengyu Liang", "Xiang Li", "Zhiwei Shang", "Zhi Hong", "Kaiyu Huang", "Zhiyong Wang", "Zhongxiang Dai"], "title": "Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction", "comment": null, "summary": "The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.", "AI": {"tldr": "Workflow-R1\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8f6e\u81ea\u7136\u8bed\u8a00\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7GSsPO\u7b97\u6cd5\u89e3\u51b3\u4f18\u5316\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5c06\u5de5\u4f5c\u6d41\u5408\u6210\u89c6\u4e3a\u9759\u6001\u3001\u4e00\u6b21\u6027\u7684\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u8fd9\u8fc7\u5ea6\u7ea6\u675f\u4e86\u6a21\u578b\u7684\u7f16\u7801\u80fd\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u95ee\u9898\u89e3\u51b3\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faWorkflow-R1\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8f6e\u81ea\u7136\u8bed\u8a00\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u3002\u5f15\u5165Group Sub-sequence Policy Optimization (GSsPO)\u7b97\u6cd5\uff0c\u5c06\u4f18\u5316\u5355\u5143\u91cd\u65b0\u6821\u51c6\u4e3a\u590d\u5408\u5b50\u5e8f\u5217\uff08\u7279\u522b\u662f\u539f\u5b50Think-Action\u5faa\u73af\uff09\uff0c\u4f7f\u68af\u5ea6\u66f4\u65b0\u4e0e\u4ea4\u4e92\u8bed\u4e49\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWorkflow-R1\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86GSsPO\u4f5c\u4e3a\u987a\u5e8f\u63a8\u7406\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "Workflow-R1\u4e3a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0cGSsPO\u7b97\u6cd5\u53ef\u63a8\u5e7f\u5230\u5e7f\u6cdb\u7684\u591a\u8f6e\u667a\u80fd\u4f53\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u3002"}}
{"id": "2602.00453", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00453", "abs": "https://arxiv.org/abs/2602.00453", "authors": ["Ziyao Wang", "Daeun Jung", "Yexiao He", "Guoheng Sun", "Zheyu Shen", "Myungjin Lee", "Ang Li"], "title": "FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.", "AI": {"tldr": "FedMOA\uff1a\u4e00\u4e2a\u8054\u90a6GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u548c\u4efb\u52a1\u611f\u77e5\u805a\u5408\uff0c\u5728\u5f02\u6784\u5956\u52b1\u4e0b\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u540c\u65f6\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edfRL\u5bf9\u9f50\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5185\u5b58\u6d88\u8017\u5927\uff0cGRPO\u7684\u65e0critic\u67b6\u6784\u9002\u5408\u8bbe\u5907\u7aef\u8bad\u7ec3\uff0c\u4f46\u8054\u90a6\u8bbe\u7f6e\u9762\u4e34\u5f02\u6784\u5956\u52b1\u3001\u591a\u76ee\u6807\u4f18\u5316\u4e0d\u5e73\u8861\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faFedMOA\u6846\u67b6\uff1a1\uff09\u672c\u5730\u8bad\u7ec3\u91c7\u7528\u57fa\u4e8e\u8d85\u68af\u5ea6\u4e0b\u964d\u7684\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u5728\u4e3b\u63a8\u7406\u76ee\u6807\u9971\u548c\u65f6\u4f18\u5148\u5904\u7406\u8f85\u52a9\u76ee\u6807\uff1b2\uff09\u670d\u52a1\u5668\u7aef\u4f7f\u7528\u4efb\u52a1\u548c\u51c6\u786e\u7387\u611f\u77e5\u7684\u805a\u5408\u7b56\u7565\uff0c\u4f18\u5148\u9009\u62e9\u9ad8\u8d28\u91cf\u66f4\u65b0\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedMOA\u59cb\u7ec8\u4f18\u4e8e\u8054\u90a6\u5e73\u5747\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe2.2%\uff0c\u540c\u65f6\u6539\u5584\u4e86\u5168\u5c40\u6027\u80fd\u3001\u4e2a\u6027\u5316\u80fd\u529b\u548c\u591a\u76ee\u6807\u5e73\u8861\u3002", "conclusion": "FedMOA\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6GRPO\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5f02\u6784\u5956\u52b1\u4e0b\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2602.01239", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01239", "abs": "https://arxiv.org/abs/2602.01239", "authors": ["Jamshid Mozafari", "Hamed Zamani", "Guido Zuccon", "Adam Jatowt"], "title": "Inferential Question Answering", "comment": "Proceedings of the ACM Web Conference 2026 (WWW 2026)", "summary": "Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u63a8\u7406\u95ee\u7b54\"\u65b0\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u4ece\u4ec5\u63d0\u4f9b\u7ebf\u7d22\u7684\u652f\u6301\u6027\u6bb5\u843d\u4e2d\u63a8\u65ad\u7b54\u6848\uff0c\u800c\u975e\u76f4\u63a5\u63d0\u53d6\u7b54\u6848\u3002\u6784\u5efa\u4e86QUIT\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u73b0\u6709QA\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u7cfb\u7edf\u5927\u591a\u5173\u6ce8\u7b54\u6848\u76f4\u63a5\u63d0\u53d6\uff0c\u4f46\u8bb8\u591a\u95ee\u9898\u9700\u8981\u63a8\u7406\u80fd\u529b\u2014\u2014\u4ece\u5df2\u6709\u4fe1\u606f\u4e2d\u63a8\u65ad\u51fa\u672a\u660e\u786e\u9648\u8ff0\u7684\u7b54\u6848\u3002\u5f53\u524d\u65b9\u6cd5\u5728\u9700\u8981\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u65b0\u7684\u4efb\u52a1\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1) \u63d0\u51fa\"\u63a8\u7406\u95ee\u7b54\"\u65b0\u4efb\u52a1\uff1b2) \u6784\u5efaQUIT\u6570\u636e\u96c6\uff087,401\u4e2a\u95ee\u9898\uff0c240\u4e07\u6bb5\u843d\uff09\uff0c\u57fa\u4e8e\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u7684\u9ad8\u6536\u655b\u63d0\u793a\u6784\u5efa\uff0c\u4f7f\u7528LLM\u53ef\u56de\u7b54\u6027\u548c\u4eba\u5de5\u9a8c\u8bc1\u8fdb\u884c\u4e09\u7ea7\u76f8\u5173\u6027\u6807\u6ce8\uff1b3) \u5bf9\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u548cLLM\u9605\u8bfb\u5668\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u4f20\u7edfQA\u65b9\u6cd5\u5728\u63a8\u7406\u95ee\u7b54\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1a\u68c0\u7d22\u5668\u6027\u80fd\u4f4e\u4e0b\uff0c\u91cd\u6392\u5e8f\u5668\u63d0\u5347\u6709\u9650\uff0c\u5fae\u8c03\u6539\u8fdb\u4e0d\u4e00\u81f4\u3002\u5373\u4f7f\u662f\u9762\u5411\u63a8\u7406\u7684LLM\u4e5f\u65e0\u6cd5\u8d85\u8d8a\u8f83\u5c0f\u7684\u901a\u7528\u6a21\u578b\u3002\u5f53\u524dQA\u6d41\u7a0b\u5c1a\u672a\u51c6\u5907\u597d\u5904\u7406\u57fa\u4e8e\u63a8\u7406\u7684\u4efb\u52a1\u3002", "conclusion": "\u63a8\u7406\u95ee\u7b54\u5efa\u7acb\u4e86\u4e00\u7c7b\u65b0\u7684QA\u4efb\u52a1\uff0c\u63a8\u52a8\u4ece\u95f4\u63a5\u6587\u672c\u8bc1\u636e\u4e2d\u7406\u89e3\u548c\u63a8\u7406\u7684\u80fd\u529b\u53d1\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u4ee5\u5e94\u5bf9\u63a8\u7406\u6311\u6218\uff0c\u8fd9\u4e3aQA\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.00570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00570", "abs": "https://arxiv.org/abs/2602.00570", "authors": ["Xingyu Luo", "Yidong Cai", "Jie Liu", "Jie Tang", "Gangshan Wu", "Limin Wang"], "title": "GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates", "comment": null, "summary": "Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD", "AI": {"tldr": "GLAD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5f0f\u8bed\u8a00\u8f85\u52a9\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u591a\u6a21\u6001\u878d\u5408\u589e\u5f3a\u6587\u672c\u63cf\u8ff0\u4e0e\u6a21\u677f\u56fe\u50cf\u7684\u517c\u5bb9\u6027\uff0c\u63d0\u5347\u4f4e\u8bed\u4e49\u56fe\u50cf\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u65b9\u6cd5\u5728\u5904\u7406\u4f4e\u8bed\u4e49\u56fe\u50cf\uff08\u5982\u6a21\u7cca\u3001\u4f4e\u5206\u8fa8\u7387\uff09\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u62fc\u63a5\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u6548\u679c\u6709\u9650\uff0c\u56e0\u4e3a\u5b58\u5728\u6a21\u6001\u95f4\u7279\u5f81\u5dee\u8ddd\u3002", "method": "\u63d0\u51faGLAD\u6a21\u578b\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u591a\u6a21\u6001\u878d\u5408\uff0c\u5c06\u6587\u672c\u63cf\u8ff0\u4e0e\u6a21\u677f\u56fe\u50cf\u878d\u5408\u4ee5\u589e\u5f3a\u8bed\u8a00\u4e0e\u56fe\u50cf\u7684\u517c\u5bb9\u6027\uff0c\u63d0\u5347\u6a21\u677f\u56fe\u50cf\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u63a8\u7406\u901f\u5ea6\u3002\u6a21\u7cca\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u6a21\u677f\u56fe\u50cf\u5728\u751f\u6210\u5f0f\u878d\u5408\u8303\u5f0f\u4e0b\u5f97\u5230\u6062\u590d\u3002", "conclusion": "GLAD\u901a\u8fc7\u751f\u6210\u5f0f\u8bed\u8a00\u8f85\u52a9\u878d\u5408\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4f4e\u8bed\u4e49\u56fe\u50cf\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u6a21\u6001\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01206", "abs": "https://arxiv.org/abs/2602.01206", "authors": ["Zeinab Dehghani"], "title": "Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)", "comment": null, "summary": "The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.", "AI": {"tldr": "gSMILE\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u6270\u52a8\u3001Wasserstein\u8ddd\u79bb\u548c\u52a0\u6743\u4ee3\u7406\u5efa\u6a21\u6765\u91cf\u5316\u63d0\u793a\u7ec4\u4ef6\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u4e3aLLMs\u63d0\u4f9b\u7ec6\u7c92\u5ea6token\u5f52\u56e0\uff0c\u4e3a\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5206\u6790\u6307\u4ee4\u4fee\u6539\u7684\u5f71\u54cd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u867d\u7136\u80fd\u4ea7\u751f\u590d\u6742\u7684\u6587\u672c\u548c\u89c6\u89c9\u8f93\u51fa\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u4fe1\u4efb\u548c\u95ee\u8d23\u3002\u9700\u8981\u5f00\u53d1\u89e3\u91ca\u751f\u6210\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u6269\u5c55SMILE\u65b9\u6cd5\u5230\u751f\u6210\u5f0f\u8bbe\u7f6e\uff0c\u4f7f\u7528\u6587\u672c\u8f93\u5165\u7684\u53d7\u63a7\u6270\u52a8\u3001Wasserstein\u8ddd\u79bb\u5ea6\u91cf\u548c\u52a0\u6743\u4ee3\u7406\u5efa\u6a21\u6765\u91cf\u5316\u548c\u53ef\u89c6\u5316\u63d0\u793a\u7ec4\u4ef6\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u3002\u7ed3\u5408\u57fa\u4e8e\u573a\u666f\u7684\u8bc4\u4f30\u7b56\u7565\u548c\u64cd\u4f5c\u8bbe\u8ba1\u57df\u6846\u67b6\u3002", "result": "gSMILE\u4e3aLLMs\u63d0\u4f9b\u7ec6\u7c92\u5ea6token\u7ea7\u5f52\u56e0\u548c\u76f4\u89c2\u7684\u70ed\u529b\u56fe\uff0c\u7a81\u51fa\u663e\u793a\u6709\u5f71\u54cd\u7684token\u548c\u63a8\u7406\u8def\u5f84\uff1b\u5728\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e2d\u5206\u6790\u6307\u4ee4\u4fee\u6539\u5bf9\u7ed3\u679c\u56fe\u50cf\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660egSMILE\u4ea7\u751f\u7a33\u5065\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u5f52\u56e0\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u3002", "conclusion": "gSMILE\u6709\u6f5c\u529b\u63a8\u8fdb\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u900f\u660e\u3001\u53ef\u9760\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\uff0c\u901a\u8fc7\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u89e3\u91ca\u6846\u67b6\u6765\u589e\u5f3a\u5bf9\u751f\u6210\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002"}}
{"id": "2602.00458", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00458", "abs": "https://arxiv.org/abs/2602.00458", "authors": ["Omer Haq"], "title": "LatentTrack: Sequential Weight Generation via Latent Filtering", "comment": null, "summary": "We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.\n  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.", "AI": {"tldr": "LatentTrack (LT) \u662f\u4e00\u79cd\u7528\u4e8e\u975e\u5e73\u7a33\u52a8\u6001\u4e0b\u5728\u7ebf\u6982\u7387\u9884\u6d4b\u7684\u5e8f\u5217\u795e\u7ecf\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u56e0\u679c\u8d1d\u53f6\u65af\u6ee4\u6ce2\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u751f\u6210\u9884\u6d4b\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u6052\u5b9a\u65f6\u95f4\u5728\u7ebf\u9002\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u975e\u5e73\u7a33\u52a8\u6001\u7684\u5728\u7ebf\u9884\u6d4b\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9002\u5e94\u5206\u5e03\u6f02\u79fb\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6bcf\u6b65\u68af\u5ea6\u66f4\u65b0\uff08\u8ba1\u7b97\u6210\u672c\u9ad8\uff09\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u5904\u7406\u975e\u5e73\u7a33\u6027\u3002LT\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u6bcf\u6b65\u68af\u5ea6\u66f4\u65b0\u5c31\u80fd\u5728\u7ebf\u9002\u5e94\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\u3002", "method": "LT\u5728\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u6267\u884c\u56e0\u679c\u8d1d\u53f6\u65af\u6ee4\u6ce2\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u9884\u6d4b\u6a21\u578b\u53c2\u6570\u3002\u91c7\u7528\u9884\u6d4b-\u751f\u6210-\u66f4\u65b0\u7684\u6ee4\u6ce2\u6846\u67b6\uff1a\u5b66\u4e60\u6f5c\u5728\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6f5c\u5728\u5206\u5e03\uff0c\u901a\u8fc7\u644a\u9500\u63a8\u7406\u4f7f\u7528\u65b0\u89c2\u6d4b\u66f4\u65b0\uff0c\u652f\u6301\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6f5c\u5728\u52a8\u6001\uff0c\u901a\u8fc7\u6f5c\u5728\u8f68\u8ff9\u7684\u8499\u7279\u5361\u6d1b\u63a8\u7406\u4ea7\u751f\u6821\u51c6\u7684\u9884\u6d4b\u6df7\u5408\u3002", "result": "\u5728Jena Climate\u57fa\u51c6\u6d4b\u8bd5\u7684\u957f\u671f\u5728\u7ebf\u56de\u5f52\u8bc4\u4f30\u4e2d\uff0cLT\u59cb\u7ec8\u6bd4\u6709\u72b6\u6001\u5e8f\u5217\u548c\u9759\u6001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u57fa\u7ebf\u83b7\u5f97\u66f4\u4f4e\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u548c\u5747\u65b9\u8bef\u5dee\uff0c\u5177\u6709\u7ade\u4e89\u529b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u8868\u660e\u6f5c\u5728\u6761\u4ef6\u51fd\u6570\u6f14\u5316\u662f\u4f20\u7edf\u6f5c\u5728\u72b6\u6001\u5efa\u6a21\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "LT\u901a\u8fc7\u6f5c\u5728\u6761\u4ef6\u51fd\u6570\u6f14\u5316\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5728\u7ebf\u6982\u7387\u9884\u6d4b\uff0c\u65e0\u9700\u6bcf\u6b65\u68af\u5ea6\u66f4\u65b0\u5c31\u80fd\u9002\u5e94\u975e\u5e73\u7a33\u52a8\u6001\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01240", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01240", "abs": "https://arxiv.org/abs/2602.01240", "authors": ["Ke Sun", "Guangsheng Bao", "Han Cui", "Yue Zhang"], "title": "Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection", "comment": null, "summary": "Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.", "AI": {"tldr": "\u63d0\u51faDetectRouter\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u7531\u9009\u62e9\u6700\u5339\u914d\u7684\u4ee3\u7406\u6a21\u578b\u6765\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\uff0c\u76f8\u6bd4\u56fa\u5b9a\u4ee3\u7406\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u4ee3\u7406\u6a21\u578b\uff0c\u4f46\u68c0\u6d4b\u6027\u80fd\u968f\u4ee3\u7406-\u6e90\u6a21\u578b\u5bf9\u9f50\u7a0b\u5ea6\u53d8\u5316\u5f88\u5927\u3002\u7814\u7a76\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u4ee3\u7406\u80fd\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u8fbe\u5230\u6700\u4f18\uff0c\u4f46\u591a\u6837\u5316\u7684\u4ee3\u7406\u6c60\u4e2d\u901a\u5e38\u5b58\u5728\u5339\u914d\u7684\u4ee3\u7406", "method": "\u63d0\u51faDetectRouter\u539f\u578b\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u767d\u76d2\u6a21\u578b\u6784\u5efa\u5224\u522b\u6027\u539f\u578b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u51e0\u4f55\u8ddd\u79bb\u4e0e\u68c0\u6d4b\u5206\u6570\u5bf9\u9f50\uff0c\u6cdb\u5316\u5230\u9ed1\u76d2\u6e90\u6a21\u578b", "result": "\u5728EvoBench\u548cMAGE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDetectRouter\u5728\u591a\u4e2a\u68c0\u6d4b\u6807\u51c6\u548c\u6a21\u578b\u5bb6\u65cf\u4e0a\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb", "conclusion": "\u5c06\u9c81\u68d2\u68c0\u6d4b\u8f6c\u5316\u4e3a\u8def\u7531\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u5408\u9002\u7684\u4ee3\u7406\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2602.00579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00579", "abs": "https://arxiv.org/abs/2602.00579", "authors": ["JiaKui Hu", "Zhengjian Yao", "Lujia Jin", "Yanye Lu"], "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration", "comment": "Accepted by ICLR 2026", "summary": "Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.", "AI": {"tldr": "BDG\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9000\u5316\u5224\u522b\u4e0e\u751f\u6210\uff0c\u5728\u901a\u7528\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4fdd\u6301\u7eb9\u7406\u6062\u590d\u80fd\u529b\u7684\u540c\u65f6\u589e\u5f3a\u5bf9\u591a\u4efb\u52a1\u591a\u9000\u5316\u573a\u666f\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u56fe\u50cf\u6062\u590d\u9700\u8981\u6a21\u578b\u4ece\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e2d\u53bb\u9664\u5404\u79cd\u9000\u5316\uff0c\u751f\u6210\u5177\u6709\u4e30\u5bcc\u7ec6\u8282\u7684\u5e72\u51c0\u56fe\u50cf\u3002\u6311\u6218\u5728\u4e8e\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u5206\u5e03\u91c7\u6837\u4ee5\u53ca\u57fa\u4e8e\u9000\u5316\u8c03\u6574\u8f93\u51fa\u3002", "method": "\u63d0\u51faBDG\u65b9\u6cd5\uff1a1) \u63d0\u51faMAS-GLCM\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9000\u5316\u7c7b\u578b\u548c\u7ea7\u522b\u5224\u522b\uff1b2) \u5c06\u6269\u6563\u8bad\u7ec3\u5206\u4e3a\u751f\u6210\u3001\u6865\u63a5\u548c\u6062\u590d\u4e09\u4e2a\u9636\u6bb5\uff0c\u4fdd\u6301\u6269\u6563\u6a21\u578b\u7eb9\u7406\u6062\u590d\u80fd\u529b\u7684\u540c\u65f6\u5c06MAS-GLCM\u5224\u522b\u4fe1\u606f\u6574\u5408\u5230\u6062\u590d\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0cBDG\u5728\u5168\u80fd\u6062\u590d\u548c\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4fdd\u771f\u5ea6\u5927\u5e45\u6539\u5584\u800c\u4e0d\u635f\u5bb3\u611f\u77e5\u8d28\u91cf\u3002", "conclusion": "BDG\u65b9\u6cd5\u901a\u8fc7\u6865\u63a5\u9000\u5316\u5224\u522b\u4e0e\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6062\u590d\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2602.01207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01207", "abs": "https://arxiv.org/abs/2602.01207", "authors": ["Hui Wu", "Hengyi Cai", "Jinman Zhao", "Xinran Chen", "Ziheng Li", "Zhejun Zhao", "Shuaiqiang Wang", "Yuchen Li", "Dawei Yin"], "title": "Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models", "comment": null, "summary": "Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u52a8\u6001\u504f\u597d\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u7b56\u7565\u66f4\u65b0\u7684\u4fe1\u566a\u6bd4\u6765\u63d0\u5347\u5bf9\u9f50\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u9759\u6001\u65b9\u6cd5\u80fd\u52a0\u901f\u6536\u655b\u5e76\u53d6\u5f97\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u901a\u5e38\u5c06\u6240\u6709\u504f\u597d\u5bf9\u540c\u7b49\u5bf9\u5f85\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u5b9e\u4f8b\u7684\u6f14\u5316\u6548\u7528\u3002\u8fd9\u79cd\u9759\u6001\u65b9\u6cd5\u5bfc\u81f4\u4f4e\u6548\u6216\u4e0d\u7a33\u5b9a\u7684\u4f18\u5316\uff1a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u5728\u68af\u5ea6\u53ef\u5ffd\u7565\u7684\u5e73\u51e1\u5bf9\u4e0a\uff0c\u5e76\u53d7\u5230\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u6837\u672c\u566a\u58f0\u7684\u5f71\u54cd\u3002", "method": "SAGE\u6846\u67b6\u6574\u5408\u4e86\u7c97\u7c92\u5ea6\u8bfe\u7a0b\u673a\u5236\uff08\u6839\u636e\u6a21\u578b\u80fd\u529b\u5237\u65b0\u5019\u9009\u6c60\uff09\u548c\u7ec6\u7c92\u5ea6\u7a33\u5b9a\u6027\u611f\u77e5\u8bc4\u5206\u51fd\u6570\uff08\u4f18\u5148\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u3001\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u9519\u8bef\uff0c\u540c\u65f6\u8fc7\u6ee4\u4e0d\u7a33\u5b9a\u6837\u672c\uff09\uff0c\u4ee5\u6700\u5927\u5316\u7b56\u7565\u66f4\u65b0\u7684\u4fe1\u566a\u6bd4\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAGE\u663e\u8457\u52a0\u901f\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u63a8\u7406\u5bf9\u9f50\u4e2d\u91c7\u7528\u7b56\u7565\u611f\u77e5\u3001\u7a33\u5b9a\u6027\u610f\u8bc6\u7684\u6570\u636e\u9009\u62e9\u7684\u5173\u952e\u4f5c\u7528\uff0c\u52a8\u6001\u6846\u67b6\u76f8\u6bd4\u9759\u6001\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u8fdb\u884c\u504f\u597d\u5bf9\u9f50\u3002"}}
{"id": "2602.00460", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00460", "abs": "https://arxiv.org/abs/2602.00460", "authors": ["Georgios Sotirchos", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "Search Inspired Exploration in Reinforcement Learning", "comment": null, "summary": "Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \\textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.", "AI": {"tldr": "SIERL\u662f\u4e00\u79cd\u53d7\u641c\u7d22\u542f\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5df2\u77e5\u72b6\u6001\u7a7a\u95f4\u8fb9\u754c\u9009\u62e9\u5b50\u76ee\u6807\u6765\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u8bfe\u7a0b\u5b66\u4e60\u548cGo-Explore\u4f9d\u8d56\u624b\u5de5\u542f\u53d1\u5f0f\uff0c\u800c\u597d\u5947\u5fc3\u9a71\u52a8\u65b9\u6cd5\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u7b56\u7565\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "method": "SIERL\u5728\u6bcf\u8f6e\u5f00\u59cb\u65f6\u4ece\u5df2\u77e5\u72b6\u6001\u7a7a\u95f4\u7684\u8fb9\u754c\uff08\u524d\u6cbf\uff09\u9009\u62e9\u5b50\u76ee\u6807\uff0c\u7136\u540e\u4ee3\u7406\u7ee7\u7eed\u5411\u4e3b\u8981\u4efb\u52a1\u76ee\u6807\u63a2\u7d22\u3002\u5b50\u76ee\u6807\u9009\u62e9\u673a\u5236\u57fa\u4e8e\u6210\u672c\u4f30\u8ba1\uff08\u5230\u8fbe\u6210\u672c\u548c\u524d\u5f80\u6210\u672c\uff09\u4f18\u5148\u8003\u8651\u524d\u6cbf\u72b6\u6001\uff0c\u786e\u4fdd\u9009\u62e9\u65e2\u4e0d\u8fc7\u4e8e\u719f\u6089\u4e5f\u4e0d\u5b8c\u5168\u65b0\u9896\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0cSIERL\u5728\u5b9e\u73b0\u4e3b\u8981\u4efb\u52a1\u76ee\u6807\u548c\u6cdb\u5316\u5230\u73af\u5883\u4e2d\u4efb\u610f\u72b6\u6001\u65b9\u9762\u90fd\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SIERL\u901a\u8fc7\u53d7\u641c\u7d22\u542f\u53d1\u7684\u5b50\u76ee\u6807\u9009\u62e9\u673a\u5236\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u5df2\u77e5\u72b6\u6001\u7a7a\u95f4\u8fb9\u754c\uff0c\u6709\u6548\u5f15\u5bfc\u63a2\u7d22\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01244", "abs": "https://arxiv.org/abs/2602.01244", "authors": ["Siwei Wu", "Yizhi Li", "Yuyang Song", "Wei Zhang", "Yang Wang", "Riza Batista-Navarro", "Xian Yang", "Mingjie Tang", "Bryan Dai", "Jian Yang", "Chenghua Lin"], "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments", "comment": "Agentic Trajectory, Agentic Model, Terminal, Code Agent", "summary": "Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\textbf{\\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \\textbf{\\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \\textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, \\textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.", "AI": {"tldr": "TerminalTraj\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7Docker\u5316\u73af\u5883\u89e3\u51b3\u4e86\u7ec8\u7aef\u4efb\u52a1\u6570\u636e\u7684\u53ef\u6267\u884c\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u6311\u6218\uff0c\u751f\u6210\u4e865\u4e07\u591a\u4e2a\u5df2\u9a8c\u8bc1\u7684\u7ec8\u7aef\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7ec8\u7aef\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u7ec8\u7aef\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u6a21\u578b\u9700\u8981\u9ad8\u8d28\u91cf\u3001\u957f\u89c6\u91ce\u7684\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\uff0c\u4f46\u5927\u89c4\u6a21\u6784\u5efa\u8fd9\u6837\u7684\u6570\u636e\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u53ef\u6267\u884c\u6027\uff08\u9700\u8981\u5408\u9002\u7684Docker\u73af\u5883\uff09\u548c\u53ef\u9a8c\u8bc1\u6027\uff08\u5f02\u6784\u4efb\u52a1\u8f93\u51fa\u96be\u4ee5\u7edf\u4e00\u9a8c\u8bc1\uff09\u3002", "method": "\u63d0\u51faTerminalTraj\u7ba1\u9053\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a(1)\u7b5b\u9009\u9ad8\u8d28\u91cf\u4ed3\u5e93\u6784\u5efaDocker\u5316\u6267\u884c\u73af\u5883\uff1b(2)\u751f\u6210\u4e0eDocker\u5bf9\u9f50\u7684\u4efb\u52a1\u5b9e\u4f8b\uff1b(3)\u5408\u6210\u5e26\u6709\u53ef\u6267\u884c\u9a8c\u8bc1\u4ee3\u7801\u7684\u667a\u80fd\u4f53\u8f68\u8ff9\u3002", "result": "\u6784\u5efa\u4e8632K\u4e2aDocker\u955c\u50cf\u548c50,733\u4e2a\u5df2\u9a8c\u8bc1\u7684\u7ec8\u7aef\u8f68\u8ff9\uff0c\u8986\u76d68\u4e2a\u9886\u57df\u3002\u57fa\u4e8eQwen2.5-Coder\u8bad\u7ec3\u7684\u6a21\u578b\u5728TerminalBench\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff1aTB 1.0\u63d0\u534720%\uff0cTB 2.0\u63d0\u534710%\u3002TerminalTraj-32B\u5728\u5c11\u4e8e100B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TerminalTraj\u6709\u6548\u89e3\u51b3\u4e86\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\u751f\u6210\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u4e3a\u8bad\u7ec3\u7ec8\u7aef\u4efb\u52a1\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u884c\u4e3a\u3002"}}
{"id": "2602.00583", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00583", "abs": "https://arxiv.org/abs/2602.00583", "authors": ["Xiangdong Li", "Ye Lou", "Ao Gao", "Wei Zhang", "Siyang Song"], "title": "MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation", "comment": null, "summary": "The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.", "AI": {"tldr": "MAUGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u903c\u771f\u7684\u4eba\u8138\u8868\u60c5\u548c\u5bf9\u5e94\u7684\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u52a8\u4f5c\u5355\u5143\u6807\u7b7e\uff08\u5305\u62ec\u53d1\u751f\u548c\u5f3a\u5ea6\uff09\uff0c\u5e76\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6MIFA\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u4e14\u5177\u6709\u7cbe\u786e\u52a8\u4f5c\u5355\u5143\u6807\u6ce8\u7684\u4eba\u8138\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6cdb\u5316\u7684AU\u8bc6\u522b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u4e86MAUGen\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1\uff09\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6a21\u5757\uff0c\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6355\u6349\u6587\u672c\u63cf\u8ff0\u3001\u4eba\u8138\u8eab\u4efd\u3001\u8868\u60c5\u56fe\u50cf\u548cAU\u6fc0\u6d3b\u4e4b\u95f4\u7684\u5173\u7cfb\uff1b2\uff09\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u6807\u7b7e\u751f\u6210\u5668\uff0c\u5c06\u8054\u5408\u8868\u793a\u89e3\u7801\u4e3a\u5bf9\u9f50\u7684\u4eba\u8138\u56fe\u50cf-\u6807\u7b7e\u5bf9\u3002", "result": "\u521b\u5efa\u4e86MIFA\u6570\u636e\u96c6\uff0c\u5305\u542b\u5168\u9762\u7684AU\u6807\u6ce8\u548c\u8eab\u4efd\u53d8\u5316\u3002\u5b9e\u9a8c\u8868\u660eMAUGen\u5728\u5408\u6210\u903c\u771f\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u591a\u6837\u7684\u4eba\u8138\u56fe\u50cf\u548c\u8bed\u4e49\u5bf9\u9f50\u7684AU\u6807\u7b7e\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAUGen\u901a\u8fc7\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u89e3\u51b3\u4e86AU\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684AU\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2602.01222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01222", "abs": "https://arxiv.org/abs/2602.01222", "authors": ["Shaoxiong Yang", "Junting Li", "Mengyuan Zhang", "Chao Li", "Wei Liu", "Jian Luan"], "title": "FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation", "comment": "Accepted by ICLR 2026", "summary": "Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.", "AI": {"tldr": "FutureMind\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\uff0c\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6218\u7565\u601d\u7ef4\u6a21\u5f0f\u5148\u9a8c\uff0c\u63d0\u5347\u5176\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6210\u672c\u654f\u611f\u548c\u8d44\u6e90\u6709\u9650\u573a\u666f\u4e2d\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5728\u9700\u8981\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6709\u6548\u68c0\u7d22\u7684\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u5c0f\u8bed\u8a00\u6a21\u578b\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\uff0c\u53c8\u80fd\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "method": "\u63d0\u51faFutureMind\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u95ee\u9898\u5206\u6790\u3001\u903b\u8f91\u63a8\u7406\u3001\u7b56\u7565\u89c4\u5212\u548c\u68c0\u7d22\u6307\u5bfc\u3002\u91c7\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u68c0\u7d22\u8303\u5f0f\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u7684\u5b50\u95ee\u9898\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u5411\u5c0f\u8bed\u8a00\u6a21\u578b\u4f20\u8f93\u6218\u7565\u601d\u7ef4\u6a21\u5f0f\u3002", "result": "\u57282WikiMultihopQA\u3001MuSiQue\u3001Bamboogle\u548cFrames\u7b49\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFutureMind\u59cb\u7ec8\u4f18\u4e8eSearch-o1\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u4e0d\u540cSLM\u67b6\u6784\u548c\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "FutureMind\u6210\u529f\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u53d1\u73b0\u601d\u7ef4\u6a21\u5f0f\u84b8\u998f\u8fc7\u7a0b\u53d7\u5230\u5e08\u751f\u6a21\u578b\u95f4\u8ba4\u77e5\u504f\u5dee\u74f6\u9888\u7684\u9650\u5236\uff0c\u8fd9\u4e3a\u63a8\u7406\u6280\u80fd\u7684\u53ef\u8fc1\u79fb\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u4e3a\u5f00\u53d1\u517c\u5177\u6548\u7387\u548c\u771f\u6b63\u8ba4\u77e5\u80fd\u529b\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.00465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00465", "abs": "https://arxiv.org/abs/2602.00465", "authors": ["Jiaqi Yin", "Baiming Chen", "Jia Fei", "Mingjun Yang"], "title": "PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction", "comment": "Preprint. Under review. During the preprint stage, inquiries and feedback can be directed to Jiaqi Yin (yjqhit@gmail.com)", "summary": "Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \\emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \\textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPAIR-Former\u65b9\u6cd5\u89e3\u51b3miRNA-mRNA\u9776\u5411\u9884\u6d4b\u95ee\u9898\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u5173\u7cfb\u591a\u793a\u4f8b\u5b66\u4e60\uff0c\u901a\u8fc7\u5ec9\u4ef7\u5168\u6c60\u626b\u63cf\u548c\u591a\u6837\u5316\u5b9e\u4f8b\u9009\u62e9\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "miRNA-mRNA\u9776\u5411\u9884\u6d4b\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u9884\u6d4b\u95ee\u9898\uff1a\u6bcf\u4e2a\u8f6c\u5f55\u672c\u4ea7\u751f\u5927\u91cf\u5019\u9009\u9776\u4f4d\u70b9\uff0c\u4f46\u53ea\u6709\u914d\u5bf9\u7ea7\u6807\u7b7e\u53ef\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u5728\u6709\u9650\u9884\u7b97\u4e0b\u6709\u6548\u5904\u7406\u5927\u91cf\u5019\u9009\u5b9e\u4f8b\u3002", "method": "\u63d0\u51faPAIR-Former\uff08Pool-Aware Instance-Relational Transformer\uff09\uff0c\u91c7\u7528BR-MIL\u6846\u67b6\uff1a1\uff09\u5ec9\u4ef7\u5168\u6c60\u626b\u63cf\uff1b2\uff09\u5728CPU\u4e0a\u9009\u62e9\u6700\u591aK\u4e2a\u591a\u6837\u5316\u7684\u5019\u9009\u9776\u4f4d\u70b9\uff1b3\uff09\u4f7f\u7528\u7f6e\u6362\u4e0d\u53d8\u7684Set Transformer\u805a\u5408\u5668\u5904\u7406\u9009\u5b9a\u6807\u8bb0\u3002", "result": "\u5728miRAW\u6570\u636e\u96c6\u4e0a\uff0cPAIR-Former\u5728\u5b9e\u7528\u64cd\u4f5c\u9884\u7b97\uff08K*=64\uff09\u4e0b\u4f18\u4e8e\u5f3a\u6c60\u5316\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u63a7\u7684\u51c6\u786e\u7387-\u8ba1\u7b97\u6743\u8861\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u9884\u7b97\u9009\u62e9\u4e0e\u8fd1\u4f3c\u8bef\u5dee\uff08\u968fK\u51cf\u5c0f\uff09\u548c\u6cdb\u5316\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "PAIR-Former\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21miRNA-mRNA\u9776\u5411\u9884\u6d4b\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u901a\u8fc7\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u591a\u6837\u5316\u5b9e\u4f8b\u9009\u62e9\u548c\u5173\u7cfb\u5904\u7406\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.01246", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01246", "abs": "https://arxiv.org/abs/2602.01246", "authors": ["Jamshid Mozafari", "Seyed Parsa Mousavinasab", "Adam Jatowt"], "title": "PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian", "comment": "Submitted to SIGIR 2026", "summary": "Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.", "AI": {"tldr": "PARSE\u662f\u9996\u4e2a\u6ce2\u65af\u8bed\u5f00\u653e\u9886\u57df\u63a8\u7406\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b10,800\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u5e03\u5c14\u3001\u591a\u9879\u9009\u62e9\u548c\u4e8b\u5b9e\u578b\u683c\u5f0f\uff0c\u901a\u8fc7LLM\u751f\u6210\u6d41\u7a0b\u6784\u5efa\u5e76\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\uff0c\u7528\u4e8e\u8bc4\u4f30\u6ce2\u65af\u8bed\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6ce2\u65af\u8bed\u4f5c\u4e3a\u62e5\u6709\u7ea61.3\u4ebf\u4f7f\u7528\u8005\u7684\u8bed\u8a00\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5f00\u653e\u9886\u57df\u63a8\u7406\u95ee\u7b54\u57fa\u51c6\u6765\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u7684QA\u7cfb\u7edf\uff0c\u8fd9\u963b\u788d\u4e86\u6ce2\u65af\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u53d7\u63a7\u7684LLM\u751f\u6210\u6d41\u7a0b\u6784\u5efa\u57fa\u51c6\uff0c\u5305\u542b\u591a\u9636\u6bb5\u8fc7\u6ee4\u3001\u6807\u6ce8\u548c\u4e00\u81f4\u6027\u68c0\u67e5\u4ee5\u786e\u4fdd\u8bed\u8a00\u548c\u4e8b\u5b9e\u8d28\u91cf\uff0c\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u5e76\u6d4b\u8bd5\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u6ce2\u65af\u8bed\u63d0\u793a\u548c\u7ed3\u6784\u5316\u63d0\u793a\uff08\u5e03\u5c14/\u591a\u9879\u9009\u62e9\u7528\u601d\u7ef4\u94fe\uff0c\u4e8b\u5b9e\u578b\u7528\u5c11\u6837\u672c\uff09\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u5fae\u8c03\u7279\u522b\u662f\u6ce2\u65af\u8bed\u4e13\u7528\u6a21\u578b\u80fd\u8fdb\u4e00\u6b65\u6539\u5584\u7ed3\u679c\uff0cPARSE\u652f\u6301\u516c\u5e73\u6bd4\u8f83\u548c\u5b9e\u9645\u6a21\u578b\u9002\u914d\u3002", "conclusion": "PARSE\u586b\u8865\u4e86\u6ce2\u65af\u8bedQA\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5f00\u53d1\u548c\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u7684LLMs\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u652f\u6301\u6ce2\u65af\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00593", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00593", "abs": "https://arxiv.org/abs/2602.00593", "authors": ["Yifan Jiang", "Cong Zhang", "Bofei Zhang", "Yifan Yang", "Bingzhang Wang", "Yew-Soon Ong"], "title": "From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking", "comment": null, "summary": "Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.", "AI": {"tldr": "Pix2Fact\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u4e13\u5bb6\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u4ec5\u8fbe\u523024%\u51c6\u786e\u7387\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b56%\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u8be6\u7ec6\u89c6\u89c9\u5b9a\u4f4d\u548c\u77e5\u8bc6\u63a8\u7406\u534f\u540c\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5206\u522b\u8bc4\u4f30\u8fd9\u4e9b\u6280\u80fd\uff0c\u65e0\u6cd5\u6355\u6349\u4e24\u8005\u7684\u534f\u540c\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b1,000\u5f20\u9ad8\u5206\u8fa8\u7387\uff084K+\uff09\u56fe\u50cf\u7684Pix2Fact\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d68\u4e2a\u65e5\u5e38\u751f\u6d3b\u573a\u666f\uff0c\u7531\u5168\u7403\u9876\u5c16\u5927\u5b66\u7684\u535a\u58eb\u4e0e\u4e13\u4e1a\u6570\u636e\u6807\u6ce8\u516c\u53f8\u5408\u4f5c\u7cbe\u5fc3\u8bbe\u8ba1\u95ee\u9898\u548c\u7b54\u6848\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u9700\u8981\u8be6\u7ec6\u89c6\u89c9\u5b9a\u4f4d\u3001\u591a\u8df3\u63a8\u7406\u548c\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u3002", "result": "\u8bc4\u4f309\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGemini-3-Pro\u548cGPT-5\u7b49\u4e13\u6709\u6a21\u578b\uff09\uff0c\u6700\u5148\u8fdb\u6a21\u578b\u4ec5\u8fbe\u523024.0%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u8fbe\u523056%\uff0c\u663e\u793a\u51fa\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "Pix2Fact\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u5236\u4eba\u7c7b\u7ea7\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5c06\u6210\u4e3a\u63a8\u52a8\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u9700\u8981\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u7a33\u5065\u7684\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.01237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01237", "abs": "https://arxiv.org/abs/2602.01237", "authors": ["Katrina Brown", "Aneesh Muppidi", "Rana Shahout"], "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models", "comment": "ICML ES-FoMo 2025", "summary": "Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPredictive Scheduling\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\u9884\u4f30\u67e5\u8be2\u96be\u5ea6\uff0c\u52a8\u6001\u5206\u914d\u56fa\u5b9atoken\u9884\u7b97\u4ee5\u6700\u5927\u5316\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5728GSM8K\u57fa\u51c6\u4e0a\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u4f7f\u7528\u56fa\u5b9atoken\u9884\u7b97\u8fdb\u884c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u5bfc\u81f4\u7b80\u5355\u95ee\u9898\u8ba1\u7b97\u8fc7\u5ea6\u800c\u56f0\u96be\u95ee\u9898\u8ba1\u7b97\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u9884\u7b97\u5206\u914d\u673a\u5236\u6765\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51faPredictive Scheduling\u6846\u67b6\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\uff08\u57fa\u4e8e\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u7684MLP\u6216\u57fa\u4e8e\u95ee\u9898\u6587\u672c\u7684LoRA\u5fae\u8c03\u5206\u7c7b\u5668\uff09\u5728\u5b8c\u6574\u751f\u6210\u524d\u9884\u4f30\u67e5\u8be2\u7684\u63a8\u7406\u957f\u5ea6\u6216\u96be\u5ea6\uff1b2) \u8bbe\u8ba1\u8d2a\u5fc3\u6279\u91cf\u5206\u914d\u5668\uff0c\u52a8\u6001\u5206\u914d\u56fa\u5b9a\u603btoken\u9884\u7b97\u4ee5\u6700\u5927\u5316\u9884\u671f\u51c6\u786e\u6027\u3002", "result": "\u5728GSM8K\u7b97\u672f\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u5747\u5300\u9884\u7b97\u5206\u914d\uff0c\u9884\u6d4b\u8c03\u5ea6\u5728\u76f8\u540ctoken\u6210\u672c\u4e0b\u83b7\u5f97\u9ad8\u8fbe7.9\u4e2a\u767e\u5206\u70b9\u7684\u7edd\u5bf9\u51c6\u786e\u6027\u63d0\u5347\uff0c\u7f29\u5c0f\u4e86\u8d85\u8fc750%\u4e0e\u5b8c\u7f8e\u9884\u77e5oracle\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u7cfb\u7edf\u5c42\u95f4\u5206\u6790\u663e\u793atransformer\u4e2d\u95f4\u5c42\uff0812-17\u5c42\uff09\u643a\u5e26\u6700\u4e30\u5bcc\u7684\u89c4\u6a21\u4f30\u8ba1\u4fe1\u53f7\u3002", "conclusion": "\u9884\u8fd0\u884c\u9884\u7b97\u9884\u6d4b\u80fd\u591f\u5b9e\u73b0\u5bf9\u8ba1\u7b97-\u51c6\u786e\u6027\u6743\u8861\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u3001\u6210\u672c\u9ad8\u6548\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u9884\u7b97\u5206\u914d\u5728\u4f18\u5316\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.00475", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00475", "abs": "https://arxiv.org/abs/2602.00475", "authors": ["Michael Psenka", "Michael Rabbat", "Aditi Krishnapriyan", "Yann LeCun", "Amir Bar"], "title": "Parallel Stochastic Gradient-Based Planning for World Models", "comment": "23 pages, 7 figures", "summary": "World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables (\"virtual states\") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.", "AI": {"tldr": "GRASP\u662f\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4e16\u754c\u6a21\u578b\u7684\u5e76\u884c\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u865a\u62df\u72b6\u6001\u4f18\u5316\u548c\u968f\u673a\u6027\u5f15\u5165\u89e3\u51b3\u89c6\u89c9\u8f93\u5165\u7684\u957f\u65f6\u57df\u63a7\u5236\u4efb\u52a1", "motivation": "\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4ece\u539f\u59cb\u611f\u5b98\u8f93\u5165\uff08\u5982\u89c6\u9891\uff09\u6a21\u62df\u73af\u5883\u52a8\u6001\uff0c\u4f46\u7528\u4e8e\u89c4\u5212\u65f6\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\u4e14\u975e\u7ed3\u6784\u5316\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u65f6\u57df\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u4e2d\u6548\u7387\u8f83\u4f4e", "method": "\u63d0\u51faGRASP\u89c4\u5212\u5668\uff1a1) \u5c06\u72b6\u6001\u89c6\u4e3a\u4f18\u5316\u53d8\u91cf\uff08\u865a\u62df\u72b6\u6001\uff09\uff0c\u65bd\u52a0\u8f6f\u52a8\u529b\u5b66\u7ea6\u675f\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\uff1b2) \u5f15\u5165\u72b6\u6001\u968f\u673a\u6027\u4fc3\u8fdb\u63a2\u7d22\u907f\u514d\u5c40\u90e8\u6700\u4f18\uff1b3) \u4fee\u6539\u68af\u5ea6\u7ed3\u6784\uff0c\u4ec5\u9700\u52a8\u4f5c\u8f93\u5165\u68af\u5ea6\uff0c\u964d\u4f4e\u9ad8\u7ef4\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u7684\u68af\u5ea6\u654f\u611f\u6027", "result": "\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u4e16\u754c\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cGRASP\u5728\u957f\u65f6\u57df\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ea4\u53c9\u71b5\u65b9\u6cd5(CEM)\u548c\u666e\u901a\u68af\u5ea6\u4f18\u5316(GD)\uff0c\u6210\u529f\u7387\u548c\u6536\u655b\u65f6\u95f4\u5747\u6709\u63d0\u5347", "conclusion": "GRASP\u4f5c\u4e3a\u4e00\u79cd\u968f\u673a\u975e\u51dd\u805a\u6216\u914d\u70b9\u6700\u4f18\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4e16\u754c\u6a21\u578b\u7684\u68af\u5ea6\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8f93\u5165\u7684\u957f\u65f6\u57df\u89c4\u5212\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97"}}
{"id": "2602.01274", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01274", "abs": "https://arxiv.org/abs/2602.01274", "authors": ["Situo Zhang", "Yifan Zhang", "Zichen Zhu", "Hankun Wang", "Da Ma", "Danyang Zhang", "Lu Chen", "Kai Yu"], "title": "PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length", "comment": null, "summary": "Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.", "AI": {"tldr": "Pacer\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u63a7\u5236\u8349\u7a3f\u957f\u5ea6\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9884\u9a8c\u8bc1\u5c42\u5b9e\u73b0\u5757\u7ea7\u9884\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u4f7f\u7528\u56fa\u5b9a\u8349\u7a3f\u957f\u5ea6\uff0c\u4f46\u5b9e\u9a8c\u53d1\u73b0\u6700\u4f18\u8349\u7a3f\u957f\u5ea6\u5728\u4e0d\u540c\u89e3\u7801\u6b65\u9aa4\u4e2d\u5dee\u5f02\u5f88\u5927\uff0c\u8fd9\u9650\u5236\u4e86\u8fdb\u4e00\u6b65\u52a0\u901f\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faPacer\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9884\u9a8c\u8bc1\u5c42\u5bf9\u8349\u7a3f\u4ee4\u724c\u8fdb\u884c\u5757\u7ea7\u9884\u9a8c\u8bc1\uff0c\u5728\u9884\u9a8c\u8bc1\u5931\u8d25\u65f6\u505c\u6b62\u4ee4\u724c\u751f\u6210\uff0c\u5b9e\u73b0\u52a8\u6001\u8349\u7a3f\u957f\u5ea6\u63a7\u5236\u3002", "result": "Pacer\u5728\u591a\u4e2aSD\u6a21\u578b\u5bf9\u4e0a\u5b9e\u73b0\u6700\u9ad82.66\u500d\u52a0\u901f\uff08\u76f8\u6bd4\u81ea\u56de\u5f52\u89e3\u7801\uff09\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\uff1b\u4e0eOuroboros\u96c6\u6210\u540e\u8fbe\u5230\u6700\u9ad83.09\u500d\u52a0\u901f\u3002", "conclusion": "Pacer\u901a\u8fc7\u52a8\u6001\u63a7\u5236\u8349\u7a3f\u957f\u5ea6\u6709\u6548\u89e3\u51b3\u4e86\u56fa\u5b9a\u8349\u7a3f\u957f\u5ea6\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u52a0\u901f\u6548\u679c\uff0c\u4e3aLLM\u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00618", "abs": "https://arxiv.org/abs/2602.00618", "authors": ["Yian Zhao", "Rushi Ye", "Ruochong Zheng", "Zesen Cheng", "Chaoran Feng", "Jiashu Yang", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting", "comment": "ICCV 2025", "summary": "3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \\textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.", "AI": {"tldr": "\u63d0\u51faTune-Your-Style\u65b9\u6cd5\uff0c\u5b9e\u73b0\u53ef\u8c03\u8282\u98ce\u683c\u5f3a\u5ea6\u76843D\u98ce\u683c\u8fc1\u79fb\uff0c\u7528\u6237\u53ef\u7075\u6d3b\u8c03\u6574\u5185\u5bb9\u4e0e\u98ce\u683c\u5e73\u8861", "motivation": "\u73b0\u67093D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u8f93\u51fa\u8303\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7528\u6237\u5bf9\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u9700\u8981\u589e\u5f3a\u53ef\u5b9a\u5236\u6027", "method": "\u5f15\u5165\u9ad8\u65af\u795e\u7ecf\u5143\u663e\u5f0f\u5efa\u6a21\u98ce\u683c\u5f3a\u5ea6\uff0c\u53c2\u6570\u5316\u53ef\u5b66\u4e60\u98ce\u683c\u8c03\u8282\u5668\uff1b\u63d0\u51fa\u53ef\u8c03\u8282\u98ce\u683c\u5316\u5f15\u5bfc\uff0c\u901a\u8fc7\u8de8\u89c6\u56fe\u98ce\u683c\u5bf9\u9f50\u4ece\u6269\u6563\u6a21\u578b\u83b7\u5f97\u591a\u89c6\u56fe\u4e00\u81f4\u98ce\u683c\u5316\u89c6\u56fe\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u8c03\u5236\u5168\u98ce\u683c\u5f15\u5bfc\u548c\u96f6\u98ce\u683c\u5f15\u5bfc\u7684\u5e73\u8861", "result": "\u65b9\u6cd5\u4e0d\u4ec5\u4ea7\u751f\u89c6\u89c9\u5438\u5f15\u4eba\u7684\u7ed3\u679c\uff0c\u8fd8\u5c55\u73b0\u51fa3D\u98ce\u683c\u8fc1\u79fb\u7684\u7075\u6d3b\u53ef\u5b9a\u5236\u6027", "conclusion": "\u63d0\u51fa\u7684Tune-Your-Style\u8303\u5f0f\u6210\u529f\u5b9e\u73b0\u4e86\u5f3a\u5ea6\u53ef\u8c03\u76843D\u98ce\u683c\u8fc1\u79fb\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u5bf9\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u7684\u63a7\u5236\u80fd\u529b"}}
{"id": "2602.01276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01276", "abs": "https://arxiv.org/abs/2602.01276", "authors": ["Abdulsobur Oyewale", "Tommaso Soru"], "title": "LLM-Driven Ontology Construction for Enterprise Knowledge Graphs", "comment": "20th International Conference on Semantic Computing (ICSC 2026)", "summary": "Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.", "AI": {"tldr": "OntoEKG\uff1a\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u4f01\u4e1a\u6570\u636e\u81ea\u52a8\u751f\u6210\u9886\u57df\u7279\u5b9a\u672c\u4f53\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u8574\u542b\u4e24\u4e2a\u6a21\u5757\u5b9e\u73b0\uff0c\u5728Data\u9886\u57df\u8fbe\u52300.724\u7684\u6a21\u7cca\u5339\u914dF1\u5206\u6570\u3002", "motivation": "\u4f01\u4e1a\u77e5\u8bc6\u56fe\u8c31\u5bf9\u4e8e\u7edf\u4e00\u5f02\u6784\u6570\u636e\u548c\u5b9e\u65bd\u8bed\u4e49\u6cbb\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5e95\u5c42\u672c\u4f53\u7684\u6784\u5efa\u4ecd\u7136\u662f\u8d44\u6e90\u5bc6\u96c6\u3001\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u7684\u4eba\u5de5\u8fc7\u7a0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faOntoEKG\u6d41\u6c34\u7ebf\uff0c\u5c06\u5efa\u6a21\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u63d0\u53d6\u6a21\u5757\u8bc6\u522b\u6838\u5fc3\u7c7b\u548c\u5c5e\u6027\uff1b2\uff09\u8574\u542b\u6a21\u5757\u5c06\u8fd9\u4e9b\u5143\u7d20\u903b\u8f91\u7ed3\u6784\u5316\u5230\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u7136\u540e\u5e8f\u5217\u5316\u4e3a\u6807\u51c6RDF\u683c\u5f0f\u3002", "result": "\u5728Data\u3001Finance\u548cLogistics\u9886\u57df\u6587\u6863\u6784\u5efa\u7684\u65b0\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5728Data\u9886\u57df\u8fbe\u5230\u6a21\u7cca\u5339\u914dF1\u5206\u65700.724\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8303\u56f4\u5b9a\u4e49\u548c\u5c42\u6b21\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "OntoEKG\u5c55\u793a\u4e86LLM\u9a71\u52a8\u672c\u4f53\u751f\u6210\u7684\u6f5c\u529b\uff0c\u4f46\u8fd8\u9700\u8981\u6539\u8fdb\u8303\u56f4\u5b9a\u4e49\u548c\u5c42\u6b21\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u4f01\u4e1a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e2d\u7684\u8d44\u6e90\u5bc6\u96c6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.00476", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00476", "abs": "https://arxiv.org/abs/2602.00476", "authors": ["Hengchang Liu", "Zhao Yang", "Bing Su"], "title": "Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly", "comment": null, "summary": "Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \\textit{Oracle Peak} that emerges near the ground-truth length and a systematic \\textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \\textbf{CAL} (\\textbf{C}alibrated \\textbf{A}daptive \\textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\\% over fixed-length baselines and 40.5\\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\\% and 9.9\\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.", "AI": {"tldr": "CAL\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6821\u51c6\u81ea\u9002\u5e94\u957f\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u7b2c\u4e00\u6b65\u53bb\u566a\u4e2d\u7684\u7edf\u8ba1\u4fe1\u53f7\u6765\u53d1\u73b0\u6b63\u786e\u7684\u586b\u5145\u957f\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u548c\u6587\u672c\u586b\u5145\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5929\u751f\u9002\u5408\u586b\u5145\u4efb\u52a1\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u9884\u8bbe\u7684\u586b\u5145\u957f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u586b\u5145\u957f\u5ea6\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u73b0\u5b9e\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u586b\u5145\u957f\u5ea6\u901a\u5e38\u662f\u672a\u77e5\u7684\u3002", "method": "CAL\u65b9\u6cd5\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u7b2c\u4e00\u6b65\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u7684\u4e24\u4e2a\u5173\u952e\u7edf\u8ba1\u73b0\u8c61\uff1a\u5c40\u90e8\"Oracle Peak\"\uff08\u5728\u771f\u5b9e\u957f\u5ea6\u9644\u8fd1\u51fa\u73b0\uff09\u548c\u7cfb\u7edf\u6027\u7684\"Length Bias\"\u3002\u901a\u8fc7\u6821\u51c6\u8fd9\u79cd\u504f\u5dee\u5e76\u5229\u7528\u4fe1\u53f7\uff0cCAL\u5728\u6b63\u5f0f\u89e3\u7801\u524d\u901a\u8fc7\u9ad8\u6548\u641c\u7d22\u6765\u8fd1\u4f3c\u6700\u4f18\u586b\u5145\u957f\u5ea6\u3002", "result": "CAL\u5728\u4ee3\u7801\u586b\u5145\u4efb\u52a1\u4e2d\uff0cPass@1\u6bd4\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe47.7%\uff0c\u6bd4\u57fa\u4e8e\u804a\u5929\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u63d0\u534740.5%\uff1b\u5728\u6587\u672c\u586b\u5145\u4efb\u52a1\u4e2d\uff0cBLEU-2\u548cROUGE-L\u5206\u522b\u63d0\u5347\u9ad8\u8fbe8.5%\u548c9.9%\u3002", "conclusion": "CAL\u5c55\u793a\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u53d1\u73b0\u6b63\u786e\u586b\u5145\u957f\u5ea6\u7684\u5185\u5728\u80fd\u529b\uff0c\u901a\u8fc7\u5229\u7528\u7b2c\u4e00\u6b65\u53bb\u566a\u4e2d\u7684\u7edf\u8ba1\u4fe1\u53f7\u5e76\u6821\u51c6\u504f\u5dee\uff0c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u7684\u586b\u5145\uff0c\u4e3aDLM\u586b\u5145\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01313", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01313", "abs": "https://arxiv.org/abs/2602.01313", "authors": ["Chuanrui Hu", "Tong Li", "Xingze Gao", "Hongda Chen", "Dannong Xu", "Yi Bai", "Tianwei Lin", "Xinda Zhao", "Xiaohong Li", "Jiaqi An", "Yunyun Han", "Jian Pei", "Yafeng Deng"], "title": "EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models", "comment": "10 pages, 2 figures, 4 tables", "summary": "Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.", "AI": {"tldr": "EverMemBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5bf9\u8bdd\u8bb0\u5fc6\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b\u591a\u53c2\u4e0e\u8005\u3001\u591a\u8bdd\u9898\u7684\u957f\u5bf9\u8bdd\uff08\u8d85100\u4e07tokens\uff09\uff0c\u901a\u8fc71000+QA\u5bf9\u6d4b\u8bd5\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7ec6\u7c92\u5ea6\u53ec\u56de\u3001\u8bb0\u5fc6\u610f\u8bc6\u548c\u7528\u6237\u753b\u50cf\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u53cc\u4eba\u5355\u8bdd\u9898\u5bf9\u8bdd\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u7684\u590d\u6742\u6027\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u66f4\u771f\u5b9e\u3001\u590d\u6742\u573a\u666f\u4e0b\u7684\u957f\u671f\u5bf9\u8bdd\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "\u6784\u5efaEverMemBench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u53c2\u4e0e\u8005\u3001\u591a\u7fa4\u7ec4\u5bf9\u8bdd\uff0c\u65f6\u95f4\u8de8\u5ea6\u957f\uff08\u8d85100\u4e07tokens\uff09\uff0c\u5177\u6709\u65f6\u95f4\u6f14\u5316\u4fe1\u606f\u3001\u8de8\u8bdd\u9898\u4ea4\u7ec7\u548c\u89d2\u8272\u7279\u5b9a\u4eba\u8bbe\u3002\u901a\u8fc71000+\u4e2aQA\u5bf9\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u8bb0\u5fc6\u7cfb\u7edf\uff1a\u7ec6\u7c92\u5ea6\u53ec\u56de\u3001\u8bb0\u5fc6\u610f\u8bc6\u548c\u7528\u6237\u753b\u50cf\u7406\u89e3\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u5173\u952e\u9650\u5236\uff1a1\uff09\u591a\u8df3\u63a8\u7406\u5728\u591a\u53c2\u4e0e\u8005\u573a\u666f\u4e2d\u5d29\u6e83\uff0c\u5373\u4f7foracle\u6a21\u578b\u4e5f\u53ea\u670926%\u51c6\u786e\u7387\uff1b2\uff09\u65f6\u95f4\u63a8\u7406\u4ecd\u672a\u89e3\u51b3\uff0c\u9700\u8981\u8d85\u8d8a\u65f6\u95f4\u6233\u5339\u914d\u7684\u7248\u672c\u8bed\u4e49\uff1b3\uff09\u8bb0\u5fc6\u610f\u8bc6\u53d7\u68c0\u7d22\u74f6\u9888\uff0c\u5f53\u524d\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\u65e0\u6cd5\u5f25\u5408\u67e5\u8be2\u4e0e\u9690\u542b\u76f8\u5173\u8bb0\u5fc6\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002", "conclusion": "EverMemBench\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u8bb0\u5fc6\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2602.00621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00621", "abs": "https://arxiv.org/abs/2602.00621", "authors": ["Guangtao Lyu", "Xinyi Cheng", "Qi Liu", "Chenghao Xu", "Jiexi Yan", "Muli Yang", "Fen Fang", "Cheng Deng"], "title": "Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering", "comment": null, "summary": "LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790LVLM\u5185\u90e8\u8868\u793a\uff0c\u53d1\u73b0\u5e7b\u89c9\u6e90\u4e8e\u56fe\u50cf\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u5f02\u5e38\u6fc0\u6d3b\uff0c\u63d0\u51fa\u5bf9\u6bd4\u795e\u7ecf\u5143\u5f15\u5bfc\u65b9\u6cd5\u5728\u9884\u586b\u5145\u9636\u6bb5\u589e\u5f3a\u4fe1\u606f\u6027\u795e\u7ecf\u5143\u3001\u6291\u5236\u6270\u52a8\u6fc0\u6d3b\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u7f13\u89e3LVLM\u5e7b\u89c9\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u7ea7\u8c03\u6574\uff0c\u5bf9\u4ea7\u751f\u5e7b\u89c9\u7684\u5185\u90e8\u673a\u5236\u63a2\u7d22\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u4ece\u8868\u793a\u5c42\u9762\u6df1\u5165\u7406\u89e3\u5e7b\u89c9\u4ea7\u751f\u673a\u5236\uff0c\u4e3a\u66f4\u6709\u6548\u7684\u5e72\u9884\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u5f15\u5165\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3\u89c6\u89c9\u5d4c\u5165\u4e3a\u7a00\u758f\u53ef\u89e3\u91ca\u795e\u7ecf\u5143\uff1b\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u8bc6\u522b\u4e0d\u540c\u7c7b\u578b\u795e\u7ecf\u5143\uff1b\u63d0\u51fa\u5bf9\u6bd4\u795e\u7ecf\u5143\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5e72\u51c0\u548c\u566a\u58f0\u8f93\u5165\u8bc6\u522b\u56fe\u50cf\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u4fe1\u606f\u6027\u795e\u7ecf\u5143\u3001\u6291\u5236\u6270\u52a8\u6fc0\u6d3b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5e7b\u89c9\u5e38\u6e90\u4e8e\u56fe\u50cf\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u7834\u574f\u6216\u865a\u5047\u6fc0\u6d3b\uff1bCNS\u65b9\u6cd5\u5728\u5e7b\u89c9\u8bc4\u4f30\u548c\u901a\u7528\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u80fd\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u4e14\u4e0e\u73b0\u6709\u89e3\u7801\u9636\u6bb5\u65b9\u6cd5\u5b8c\u5168\u517c\u5bb9\u3002", "conclusion": "\u4ece\u8868\u793a\u5c42\u9762\u5206\u6790LVLM\u5e7b\u89c9\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0cCNS\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u5e72\u9884\u5728\u9884\u586b\u5145\u9636\u6bb5\u6539\u5584\u89c6\u89c9\u57fa\u7840\u8868\u793a\uff0c\u4e3a\u7f13\u89e3\u5e7b\u89c9\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01297", "abs": "https://arxiv.org/abs/2602.01297", "authors": ["Shaowei Shen", "Xiaohong Yang", "Jie Yang", "Lianfen Huang", "Yongcai Zhang", "Yang Zou", "Seyyedali Hosseinalipour"], "title": "RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis", "comment": "9 pages, 4 figures", "summary": "Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.", "AI": {"tldr": "RE-MCDF\u662f\u4e00\u4e2a\u5173\u7cfb\u589e\u5f3a\u7684\u591a\u4e13\u5bb6\u4e34\u5e8a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1-\u4fee\u8ba2\u95ed\u73af\u67b6\u6784\uff0c\u6574\u5408\u591a\u4e2a\u4e13\u5bb6\u7ec4\u4ef6\u6765\u63d0\u5347\u795e\u7ecf\u79d1\u7535\u5b50\u75c5\u5386\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u5b50\u75c5\u5386\uff08\u7279\u522b\u662f\u795e\u7ecf\u79d1\uff09\u5177\u6709\u5f02\u6784\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\uff0c\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5bb9\u6613\u4ea7\u751f\u81ea\u6211\u5f3a\u5316\u7684\u9519\u8bef\uff0c\u73b0\u6709\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4ea4\u4e92\u6d45\u5c42\u4e14\u7f3a\u4e4f\u5bf9\u75be\u75c5\u95f4\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u65e0\u6cd5\u6392\u9664\u4e34\u5e8a\u4e0d\u53ef\u884c\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51faRE-MCDF\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1\uff09\u751f\u6210\u5019\u9009\u8bca\u65ad\u548c\u8bc1\u636e\u7684\u4e3b\u4e13\u5bb6\uff1b2\uff09\u52a8\u6001\u4f18\u5148\u5904\u7406\u5f02\u6784\u4e34\u5e8a\u6307\u6807\u7684\u5b9e\u9a8c\u5ba4\u4e13\u5bb6\uff1b3\uff09\u5f3a\u5236\u6267\u884c\u75be\u75c5\u95f4\u903b\u8f91\u7ea6\u675f\u7684\u591a\u5173\u7cfb\u611f\u77e5\u4e0e\u8bc4\u4f30\u4e13\u5bb6\u7ec4\u3002\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u524d\u4e24\u4e2a\u4e13\u5bb6\u81ea\u9002\u5e94\u91cd\u52a0\u6743EMR\u8bc1\u636e\uff0c\u4e13\u5bb6\u7ec4\u9a8c\u8bc1\u548c\u4fee\u6b63\u5019\u9009\u8bca\u65ad\u4ee5\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728CMEMR\u7684\u795e\u7ecf\u79d1\u5b50\u96c6\uff08NEEMRs\uff09\u548c\u81ea\u5efa\u6570\u636e\u96c6\uff08XMEMRs\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRE-MCDF\u5728\u590d\u6742\u8bca\u65ad\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RE-MCDF\u901a\u8fc7\u6574\u5408\u591a\u4e13\u5bb6\u534f\u4f5c\u548c\u663e\u5f0f\u5efa\u6a21\u75be\u75c5\u95f4\u903b\u8f91\u5173\u7cfb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u79d1\u7535\u5b50\u75c5\u5386\u8bca\u65ad\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2602.00478", "categories": ["cs.LG", "cs.AI", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00478", "abs": "https://arxiv.org/abs/2602.00478", "authors": ["Xi Lin", "Ping Guo", "Yilu Liu", "Qingfu Zhang", "Jianyong Sun"], "title": "Quality-Diversity Optimization as Multi-Objective Optimization", "comment": null, "summary": "The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5177\u6709\u5927\u91cf\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u73b0\u6709MOO\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8eQD\u95ee\u9898", "motivation": "\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u5728\u673a\u5668\u4eba\u63a7\u5236\u3001\u521b\u610f\u8bbe\u8ba1\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709QD\u7b97\u6cd5\u5404\u6709\u4e0d\u540c\u8bbe\u8ba1\u539f\u5219\u3002\u672c\u7814\u7a76\u4e0d\u63d0\u51fa\u65b0\u7b97\u6cd5\uff0c\u800c\u662f\u5efa\u7acbQD\u4e0eMOO\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4f7f\u6210\u719f\u7684MOO\u65b9\u6cd5\u80fd\u76f4\u63a5\u5e94\u7528\u4e8eQD\u95ee\u9898", "method": "\u5c06QD\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u5177\u6709\u5927\u91cf\u4f18\u5316\u76ee\u6807\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u6210\u719f\u7684MOO\u65b9\u6cd5\uff08\u7279\u522b\u662f\u57fa\u4e8e\u96c6\u5408\u7684\u6807\u91cf\u5316\u6280\u672f\uff09\u901a\u8fc7\u534f\u4f5c\u641c\u7d22\u8fc7\u7a0b\u89e3\u51b3QD\u95ee\u9898", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u7ee7\u627f\u4e86MOO\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u540c\u65f6\u4e3aQD\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u60f3\u7279\u6027\u3002\u591a\u4e2aQD\u5e94\u7528\u7684\u5b9e\u9a8c\u7814\u7a76\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684QD\u7b97\u6cd5\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u5c06QD\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3aMOO\u95ee\u9898\uff0c\u6210\u529f\u5efa\u7acb\u4e86\u4e24\u79cd\u4f18\u5316\u8303\u5f0f\u4e4b\u95f4\u7684\u7406\u8bba\u6865\u6881\uff0c\u4f7fMOO\u65b9\u6cd5\u80fd\u76f4\u63a5\u5e94\u7528\u4e8eQD\u9886\u57df\uff0c\u4e3aQD\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177"}}
{"id": "2602.01326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01326", "abs": "https://arxiv.org/abs/2602.01326", "authors": ["Zirui Wu", "Lin Zheng", "Zhihui Xie", "Jiacheng Ye", "Jiahui Gao", "Shansan Gong", "Yansong Feng", "Zhenguo Li", "Wei Bi", "Guorui Zhou", "Lingpeng Kong"], "title": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "comment": "ICLR 2026", "summary": "Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.", "AI": {"tldr": "DreamOn\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u52a8\u6001\u53ef\u53d8\u957f\u5ea6\u7684\u4ee3\u7801\u586b\u5145\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfDLMs\u9700\u8981\u56fa\u5b9a\u957f\u5ea6\u63a9\u7801\u5e8f\u5217\u7684\u9650\u5236\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b(DLMs)\u867d\u7136\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u975e\u81ea\u56de\u5f52\u586b\u5145\u80fd\u529b\uff0c\u4f46\u9700\u8981\u56fa\u5b9a\u957f\u5ea6\u7684\u63a9\u7801\u5e8f\u5217\uff0c\u5f53\u9884\u5b9a\u4e49\u63a9\u7801\u5927\u5c0f\u4e0e\u7406\u60f3\u5b8c\u6210\u957f\u5ea6\u4e0d\u5339\u914d\u65f6\uff0c\u4ee3\u7801\u586b\u5145\u6027\u80fd\u4f1a\u4e25\u91cd\u4e0b\u964d\u3002", "method": "DreamOn\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u957f\u5ea6\u63a7\u5236\u72b6\u6001\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u81ea\u8eab\u9884\u6d4b\u81ea\u4e3b\u6269\u5c55\u6216\u6536\u7f29\u8f93\u51fa\u957f\u5ea6\uff0c\u53ea\u9700\u5bf9\u8bad\u7ec3\u76ee\u6807\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\u4e14\u65e0\u9700\u67b6\u6784\u66f4\u6539\u3002", "result": "\u5728Dream-Coder-7B\u548cDiffuCoder-7B\u57fa\u7840\u4e0a\u6784\u5efa\u7684DreamOn\uff0c\u5728HumanEval-Infilling\u548cSantaCoder-FIM\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5339\u914d\u4e86\u4f7f\u7528\u771f\u5b9e\u957f\u5ea6\u83b7\u5f97\u7684oracle\u6027\u80fd\u3002", "conclusion": "DreamOn\u6d88\u9664\u4e86DLMs\u5b9e\u9645\u90e8\u7f72\u7684\u57fa\u672c\u969c\u788d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u7075\u6d3b\u6027\u548c\u53ef\u53d8\u957f\u5ea6\u751f\u6210\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2602.00627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00627", "abs": "https://arxiv.org/abs/2602.00627", "authors": ["Benxiang Zhai", "Yifang Xu", "Guofeng Zhang", "Yang Li", "Sidan Du"], "title": "FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization", "comment": "Accept by ICANN 2025", "summary": "Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.", "AI": {"tldr": "FaceSnap\uff1a\u57fa\u4e8eStable Diffusion\u7684\u5355\u53c2\u8003\u56fe\u50cf\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u5355\u6b21\u63a8\u7406\u5373\u53ef\u751f\u6210\u9ad8\u4fdd\u771f\u9762\u90e8\u7ec6\u8282", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8017\u65f6\u5fae\u8c03\u4e14\u6cdb\u5316\u6027\u5dee\uff0c\u8981\u4e48\u65e0\u6cd5\u5b9e\u73b0\u9762\u90e8\u7ec6\u8282\u7684\u9ad8\u4fdd\u771f\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u53c8\u65e0\u9700\u5fae\u8c03\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eStable Diffusion\uff0c\u8bbe\u8ba1\u9762\u90e8\u5c5e\u6027\u6df7\u5408\u5668\u63d0\u53d6\u591a\u5c42\u6b21\u7279\u5f81\uff0c\u5f15\u5165\u5730\u6807\u9884\u6d4b\u5668\u4fdd\u6301\u4e0d\u540c\u59ff\u6001\u4e0b\u7684\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4f7f\u7528ID\u4fdd\u6301\u6a21\u5757\u6ce8\u5165UNet\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u548c\u5b9a\u5236\u8096\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u8be5\u9886\u57df\u7684\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "FaceSnap\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u53c2\u8003\u56fe\u50cf\u4e2a\u6027\u5316\u8096\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u4fdd\u771f\u9762\u90e8\u7ec6\u8282\uff0c\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.01346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01346", "abs": "https://arxiv.org/abs/2602.01346", "authors": ["Wei Yang", "Hong Xie", "Tao Tan", "Xin Li", "Defu Lian", "Enhong Chen"], "title": "Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance", "comment": "Preprint. Under review", "summary": "While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u7f16\u7801\u5668\u5185\u90e8\u529f\u80fd\u52a8\u6001\u7684VLM\u9009\u62e9\u6846\u67b6\uff0c\u4f7f\u7528\u65b9\u5411\u6027\u7535\u5bfc\u6563\u5ea6(DCD)\u5ea6\u91cf\u4efb\u52a1\u95f4\u529f\u80fd\u5757\u8986\u76d6\u5ea6\uff0c\u65e0\u9700\u76f4\u63a5\u63a8\u7406\u5373\u53ef\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u6392\u540d\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u4f17\u591a\uff0c\u4f46\u4e3a\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u9009\u62e9\u6700\u4f18\u9884\u8bad\u7ec3\u6a21\u578b\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u9009\u62e9\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6570\u636e\u5bc6\u96c6\u578b\u4ee3\u7406\uff0c\u8981\u4e48\u4f7f\u7528\u5bf9\u79f0\u6587\u672c\u63cf\u8ff0\u7b26\uff0c\u5ffd\u7565\u4e86\u8fc1\u79fb\u80fd\u529b\u7684\u56fa\u6709\u65b9\u5411\u6027\u548c\u6a21\u578b\u7279\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u5c42\u95f4\u7535\u5bfc\u8868\u793a\u6bcf\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u5bf9\u9f50\u63a8\u5bfc\u76ee\u6807\u6761\u4ef6\u5757\u91cd\u8981\u6027\u5206\u5e03\u3002\u63d0\u51fa\u65b9\u5411\u6027\u7535\u5bfc\u6563\u5ea6(DCD)\uff0c\u8fd9\u662f\u4e00\u4e2a\u975e\u5bf9\u79f0\u5ea6\u91cf\uff0c\u91cf\u5316\u6e90\u4efb\u52a1\u5982\u4f55\u6709\u6548\u8986\u76d6\u76ee\u6807\u4efb\u52a1\u7684\u663e\u8457\u529f\u80fd\u5757\uff0c\u4ece\u800c\u65e0\u9700\u76f4\u63a5\u63a8\u7406\u5373\u53ef\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u6392\u540d\u3002", "result": "\u572848\u4e2aVLM\u548c21\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\uff0c\u5728NDCG@5\u4e0a\u6bd4SWAB\u63d0\u9ad8\u4e8614.7%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u9009\u62e9\u5efa\u7acb\u5728\u89c6\u89c9\u7f16\u7801\u5668\u5185\u90e8\u529f\u80fd\u52a8\u6001\u4e0a\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u9884\u6d4bVLM\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u548c\u65b9\u5411\u6027\u8003\u8651\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2602.00482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00482", "abs": "https://arxiv.org/abs/2602.00482", "authors": ["Jiarui Zhang", "Yuchen Yang", "Ran Yan", "Zhiyu Mei", "Liyuan Zhang", "Daifeng Li", "Wei Fu", "Jiaxuan Gao", "Shusheng Xu", "Yi Wu", "Binhang Yuan"], "title": "AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\\times$ in $\u03c4^2$-bench higher training throughput.", "AI": {"tldr": "AREAL-DTA\uff1a\u4e00\u79cd\u7528\u4e8eRL\u540e\u8bad\u7ec3\u7684\u9ad8\u6548\u524d\u7f00\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7DFS\u52a8\u6001\u904d\u5386\u524d\u7f00\u6811\uff0c\u5b9e\u73b0\u8ba1\u7b97\u548c\u5185\u5b58\u4f18\u5316", "motivation": "\u73b0\u6709RL\u6846\u67b6\u5728\u5904\u7406\u5171\u4eab\u957f\u524d\u7f00\u7684rollout\u5e8f\u5217\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u524d\u7f00\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u6d6a\u8d39\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "\u91c7\u7528DFS\u6267\u884c\u7b56\u7565\u52a8\u6001\u904d\u5386rollout\u524d\u7f00\u6811\uff0c\u6bcf\u6b21\u53ea\u5b9e\u4f8b\u5316\u5355\u6761\u6839\u5230\u53f6\u8def\u5f84\uff1b\u7ed3\u5408\u8d1f\u8f7d\u5747\u8861\u5206\u5e03\u5f0f\u6279\u5904\u7406\u673a\u5236\u5728\u591aGPU\u4e0a\u52a8\u6001\u6784\u5efa\u548c\u5904\u7406\u524d\u7f00\u6811", "result": "\u5728RL\u540e\u8bad\u7ec3\u4efb\u52a1\u4e2d\uff0cAREAL-DTA\u5b9e\u73b0\u4e86\u9ad8\u8fbe8.31\u500d\u7684\u03c4\u00b2-bench\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347", "conclusion": "AREAL-DTA\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u524d\u7f00\u5171\u4eab\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u8bad\u7ec3\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u7684RL\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01348", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01348", "abs": "https://arxiv.org/abs/2602.01348", "authors": ["Yu Liu", "Wenxiao Zhang", "Cong Cao", "Fangfang Yuan", "Weizhuo Chen", "Cheng Hu", "Pin Xu", "Yuling Yang", "Kun Peng", "Diandian Guo", "Qiang Sun", "Yanbing Liu", "Jin B. Hong", "Zhiyuan Ma"], "title": "CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering", "comment": null, "summary": "Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u673a\u5236\u4f18\u5316\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u89e3\u51b3\u63a8\u7406\u5d29\u6e83\u3001\u63a8\u7406-\u7b54\u6848\u4e0d\u4e00\u81f4\u548c\u683c\u5f0f\u63a7\u5236\u4e22\u5931\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1) \u63a8\u7406\u5d29\u6e83 - \u591a\u8df3\u7ec4\u5408\u548c\u566a\u58f0\u68c0\u7d22\u4f7f\u63a8\u7406\u4e0d\u7a33\u5b9a\uff1b2) \u63a8\u7406-\u7b54\u6848\u4e0d\u4e00\u81f4 - LLM\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u8bc1\u636e-\u5e72\u6270\u7269\u6df7\u5408\u5bfc\u81f4\u7b54\u6848\u6b63\u786e\u4f46\u63a8\u7406\u4e0d\u5fe0\u5b9e\uff1b3) \u683c\u5f0f\u63a7\u5236\u4e22\u5931 - \u4f20\u7edf\u601d\u7ef4\u94fe\u751f\u6210\u5e38\u504f\u79bb\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u3002", "method": "\u63d0\u51faCRAFT\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cc\u5956\u52b1\u673a\u5236\uff1a\u786e\u5b9a\u6027\u5956\u52b1\u786e\u4fdd\u7ed3\u6784\u6b63\u786e\u6027\uff0c\u57fa\u4e8e\u8bc4\u5224\u8005\u7684\u5956\u52b1\u9a8c\u8bc1\u8bed\u4e49\u5fe0\u5b9e\u6027\u3002\u652f\u6301\u53ef\u63a7\u7684\u63a8\u7406\u8f68\u8ff9\u53d8\u4f53\uff0c\u7cfb\u7edf\u5206\u6790\u7ed3\u6784\u548c\u89c4\u6a21\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAFT\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u5fe0\u5b9e\u6027\u3002CRAFT 7B\u6a21\u578b\u5728\u591a\u4e2a\u63a8\u7406\u8f68\u8ff9\u8bbe\u7f6e\u4e0b\u4e0e\u95ed\u6e90LLMs\u7ade\u4e89\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8de8\u6a21\u578b\u89c4\u6a21\u7684\u826f\u597d\u8868\u73b0\u3002", "conclusion": "CRAFT\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u53cc\u5956\u52b1\u673a\u5236\u548c\u53ef\u63a7\u63a8\u7406\u8f68\u8ff9\u8bbe\u8ba1\u4e3a\u53ef\u9760\u63a8\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.00635", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00635", "abs": "https://arxiv.org/abs/2602.00635", "authors": ["Lingsong Wang", "Mancheng Meng", "Ziyan Wu", "Terrence Chen", "Fan Yang", "Dinggang Shen"], "title": "S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning", "comment": null, "summary": "Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.", "AI": {"tldr": "S\u00b3POT\uff1a\u4e00\u79cd\u7ed3\u5408\u4eba\u8138\u751f\u6210\u4e0e\u81ea\u76d1\u7763\u7a7a\u95f4\u63d0\u793a\u7684\u5bf9\u6bd4\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4eba\u8138\u89e3\u6790\u4e2d\u906e\u6321\u5206\u5272\u95ee\u9898\uff0c\u65e0\u9700\u906e\u6321\u6807\u6ce8\u6570\u636e", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u89e3\u6790\u65b9\u6cd5\u901a\u5e38\u5c06\u906e\u6321\u9519\u8bef\u5206\u7c7b\u4e3a\u9762\u90e8\u7ec4\u4ef6\uff0c\u56e0\u4e3a\u906e\u6321\u662f\u9ad8\u5c42\u6982\u5ff5\u800c\u975e\u5177\u4f53\u7269\u4f53\u7c7b\u522b\uff0c\u6784\u5efa\u8986\u76d6\u6240\u6709\u906e\u6321\u7c7b\u522b\u7684\u771f\u5b9e\u6570\u636e\u96c6\u51e0\u4e4e\u4e0d\u53ef\u80fd\uff0c\u4e14\u7cbe\u786e\u63a9\u7801\u6807\u6ce8\u6210\u672c\u9ad8\u6602", "method": "\u63d0\u51faS\u00b3POT\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1) \u53c2\u8003\u751f\u6210(RF)\uff1a\u5229\u7528\u89e3\u6790\u63a9\u7801\u7684\u7ed3\u6784\u6307\u5bfc\u751f\u6210\u65e0\u906e\u6321\u53c2\u8003\u56fe\u50cf\uff1b2) \u7279\u5f81\u589e\u5f3a(FE)\uff1a\u5bf9\u6bd4\u539f\u59cb\u4e0e\u53c2\u8003\u56fe\u50cf\u7684token\u83b7\u5f97\u521d\u59cb\u63d0\u793a\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u4fee\u6539\u56fe\u50cf\u7279\u5f81\uff1b3) \u63d0\u793a\u9009\u62e9(PS)\uff1a\u57fa\u4e8e\u589e\u5f3a\u7279\u5f81\u6784\u5efa\u6b63\u8d1f\u63d0\u793a\u96c6\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u7f51\u7edc\u7b5b\u9009\u4f9b\u63a9\u7801\u89e3\u7801\u5668\u4f7f\u7528", "result": "\u5728\u4e13\u95e8\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eS\u00b3POT\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u4e14\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u6548", "conclusion": "S\u00b3POT\u901a\u8fc7\u7ed3\u5408\u4eba\u8138\u751f\u6210\u5668\u7684\u906e\u6321\u91cd\u5efa\u80fd\u529b\u548c\u57fa\u7840\u5206\u5272\u6a21\u578b\u7684\u63d0\u793a\u5206\u5272\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u906e\u6321\u6807\u6ce8\u7684\u906e\u6321\u5206\u5272\uff0c\u4e3a\u89e3\u51b3\u4eba\u8138\u89e3\u6790\u4e2d\u7684\u906e\u6321\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.01355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01355", "abs": "https://arxiv.org/abs/2602.01355", "authors": ["Haojia Zhu", "Qinyuan Xu", "Haoyu Li", "Yuxi Liu", "Hanchen Qiu", "Jiaoyan Chen", "Jiahui Jin"], "title": "Aggregation Queries over Unstructured Text: Benchmark and Agentic Method", "comment": null, "summary": "Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to \"find all,\" not merely \"find one.\" Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5b8c\u6574\u6027\u7684\u5b9e\u4f53\u7ea7\u805a\u5408\u67e5\u8be2\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86AGGBench\u57fa\u51c6\u6d4b\u8bd5\u548cDFA\u6a21\u5757\u5316\u4ee3\u7406\u57fa\u7ebf\u6765\u89e3\u51b3\u6587\u672c\u805a\u5408\u67e5\u8be2\u4e2d\u7684\"\u627e\u5168\"\u800c\u975e\"\u627e\u4e00\u4e2a\"\u7684\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u4e0a\u7684\u805a\u5408\u67e5\u8be2\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002\u4e0e\u666e\u901a\u95ee\u7b54\u4e0d\u540c\uff0c\u805a\u5408\u67e5\u8be2\u9700\u8981\u6536\u96c6\u8be6\u5c3d\u7684\u8bc1\u636e\uff0c\u7cfb\u7edf\u9700\u8981\"\u627e\u5168\"\u800c\u4e0d\u4ec5\u4ec5\u662f\"\u627e\u4e00\u4e2a\"\u3002\u73b0\u6709\u7684Text-to-SQL\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8303\u5f0f\u65e0\u6cd5\u5b9e\u73b0\u8fd9\u79cd\u5b8c\u6574\u6027\u3002", "method": "\u8bba\u6587\u5728\u8bed\u6599\u5e93\u6709\u754c\u8bbe\u7f6e\u4e0b\u5f62\u5f0f\u5316\u4e86\u5177\u6709\u4e25\u683c\u5b8c\u6574\u6027\u8981\u6c42\u7684\u5b9e\u4f53\u7ea7\u805a\u5408\u67e5\u8be2\u3002\u63d0\u51fa\u4e86AGGBench\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5927\u89c4\u6a21\u8bed\u6599\u4e0b\u7684\u5b8c\u6574\u6027\u5bfc\u5411\u805a\u5408\u3002\u540c\u65f6\u63d0\u51fa\u4e86DFA\uff08\u6d88\u6b67-\u8fc7\u6ee4-\u805a\u5408\uff09\u6a21\u5757\u5316\u4ee3\u7406\u57fa\u7ebf\uff0c\u5c06\u805a\u5408\u67e5\u8be2\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u9636\u6bb5\uff0c\u5e76\u66b4\u9732\u4e0e\u6b67\u4e49\u3001\u8fc7\u6ee4\u548c\u805a\u5408\u76f8\u5173\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDFA\u5728\u805a\u5408\u8bc1\u636e\u8986\u76d6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684RAG\u548c\u4ee3\u7406\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5b8c\u6574\u6027\u5bfc\u5411\u7684\u805a\u5408\u67e5\u8be2\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6846\u67b6\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u6709\u6548\u7684\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\"\u627e\u5168\"\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.00488", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00488", "abs": "https://arxiv.org/abs/2602.00488", "authors": ["Dongbin Jiao", "Zisheng Chen", "Xianyi Wang", "Jintao Shi", "Shengcai Liu", "Shi Yan"], "title": "OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing", "comment": null, "summary": "Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.", "AI": {"tldr": "OD-DEAL\uff1a\u4e00\u79cd\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u9057\u4f20\u641c\u7d22\u548c\u5728\u7ebf\u91cd\u5fc3\u805a\u7c7b\u5206\u89e3\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u4e13\u5bb6\u542f\u53d1\u5f0f\u884c\u4e3a\u8f6c\u79fb\u5230\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21CVRP\u95ee\u9898\u7684\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6c42\u89e3\u3002", "motivation": "\u5927\u89c4\u6a21\u5e26\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\u7684\u6c42\u89e3\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u590d\u6742\u5ea6\u9ad8\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u542f\u53d1\u5f0f\u7b97\u6cd5\u8d28\u91cf\uff0c\u53c8\u80fd\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOD-DEAL\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\uff0c\u7d27\u5bc6\u96c6\u6210\u6df7\u5408\u9057\u4f20\u641c\u7d22\uff08HGS\uff09\u548c\u5728\u7ebf\u91cd\u5fc3\u805a\u7c7b\uff08BCC\uff09\u5206\u89e3\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u77e5\u8bc6\u84b8\u998f\u5c06\u4e13\u5bb6\u542f\u53d1\u5f0f\u884c\u4e3a\u8f6c\u79fb\u5230\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u751f\u6210\u7b56\u7565\u3002\u6846\u67b6\u901a\u8fc7\u6781\u5c0f\u6781\u5927\u535a\u5f08\u8bad\u7ec3\uff0c\u5c06\u5206\u6cbb\u7b56\u7565\u84b8\u998f\u4e3a\u5bc6\u96c6\u4ee3\u7406\u5956\u52b1\uff0c\u5b9e\u73b0\u65e0\u9700\u805a\u7c7b\u7684\u5927\u89c4\u6a21\u5b9e\u4f8b\u63a8\u7406\u3002", "result": "OD-DEAL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5b9e\u65f6CVRP\u6027\u80fd\uff0c\u80fd\u591f\u6c42\u89e310000\u8282\u70b9\u5b9e\u4f8b\uff0c\u5e76\u8868\u73b0\u51fa\u63a5\u8fd1\u6052\u5b9a\u7684\u795e\u7ecf\u7f29\u653e\u7279\u6027\u3002\u8fd9\u5b9e\u73b0\u4e86\u4e9a\u79d2\u7ea7\u3001\u542f\u53d1\u5f0f\u8d28\u91cf\u7684\u63a8\u7406\uff0c\u6ee1\u8db3\u52a8\u6001\u5927\u89c4\u6a21\u90e8\u7f72\u9700\u6c42\u3002", "conclusion": "OD-DEAL\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21CVRP\u6c42\u89e3\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\u8d28\u91cf\u4e0e\u795e\u7ecf\u7f51\u7edc\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u7684\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01362", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01362", "abs": "https://arxiv.org/abs/2602.01362", "authors": ["Yue Liu", "Yuzhong Zhao", "Zheyong Xie", "Qixiang Ye", "Jianbin Jiao", "Yao Hu", "Shaosheng Cao", "Yunfan Liu"], "title": "Balancing Understanding and Generation in Discrete Diffusion Models", "comment": "32 pages, Code is available at https://github.com/MzeroMiko/XDLM", "summary": "In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM", "AI": {"tldr": "XDLM\u7edf\u4e00\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b(MDLM)\u548c\u5747\u5300\u566a\u58f0\u6269\u6563\u8bed\u8a00\u6a21\u578b(UDLM)\uff0c\u901a\u8fc7\u5e73\u7a33\u566a\u58f0\u6838\u6865\u63a5\u4e24\u79cd\u8303\u5f0f\uff0c\u5728\u8bed\u4e49\u7406\u89e3\u548c\u5c11\u6b65\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u79bb\u6563\u751f\u6210\u5efa\u6a21\u4e2d\uff0cMDLM\u64c5\u957f\u8bed\u4e49\u7406\u89e3\u548c\u96f6\u6837\u672c\u6cdb\u5316\uff0cUDLM\u5728\u5c11\u6b65\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e24\u8005\u90fd\u65e0\u6cd5\u5728\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e0a\u53d6\u5f97\u5e73\u8861\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u517c\u987e\u4e24\u79cd\u80fd\u529b\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51faXDLM\uff0c\u901a\u8fc7\u5e73\u7a33\u566a\u58f0\u6838\u6865\u63a5MDLM\u548cUDLM\u4e24\u79cd\u8303\u5f0f\u3002\u5173\u952e\u8d21\u732e\u5305\u62ec\uff1a(1) \u7406\u8bba\u7edf\u4e00MDLM\u548cUDLM\uff0c\u5c06\u4e24\u8005\u4f5c\u4e3a\u7279\u4f8b\u6062\u590d\uff1b(2) \u901a\u8fc7\u540e\u9a8c\u6982\u7387\u7684\u4ee3\u6570\u7b80\u5316\u7f13\u89e3\u5185\u5b58\u74f6\u9888\u3002", "result": "XDLM\u5728\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u63a8\u8fdb\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\uff1a\u96f6\u6837\u672c\u6587\u672c\u57fa\u51c6\u4e0a\u8d85\u8d8aUDLM 5.4\u5206\uff1b\u5c11\u6b65\u56fe\u50cf\u751f\u6210FID 54.1 vs MDLM\u768480.8\uff1b\u8c03\u4f188B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u4ec5\u752832\u6b65\u8fbe\u523015.0 MBPP\uff0c\u6027\u80fd\u7ffb\u500d\u3002", "conclusion": "XDLM\u6210\u529f\u7edf\u4e00\u4e86MDLM\u548cUDLM\u4e24\u79cd\u8303\u5f0f\uff0c\u5728\u8bed\u4e49\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u663e\u793aXDLM\u5177\u6709\u4f18\u8d8a\u7684\u957f\u7a0b\u6269\u5c55\u6f5c\u529b\uff0c\u4e3a\u79bb\u6563\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2602.00637", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00637", "abs": "https://arxiv.org/abs/2602.00637", "authors": ["Vivek Madhavaram", "Vartika Sengar", "Arkadipta De", "Charu Sharma"], "title": "VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning", "comment": "WACV 2026, Project page: https://vivekmadhavaram.github.io/vizor/", "summary": "Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like \"left/right\", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.", "AI": {"tldr": "VIZOR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb3D\u573a\u666f\u6784\u5efa\u5bc6\u96c6\u3001\u89c6\u89d2\u4e0d\u53d8\u76843D\u573a\u666f\u56fe\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u4f53\u6b63\u9762\u65b9\u5411\u5b9a\u4e49\u7a7a\u95f4\u5173\u7cfb\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u89c6\u89d2\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u901a\u5e38\u4ece\u7279\u5b9a\u53c2\u8003\u89c6\u89d2\u521b\u5efa\u573a\u666f\u56fe\uff0c\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u7a7a\u95f4\u5173\u7cfb\uff08\u5982\"\u5de6/\u53f3\"\uff09\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89d2\u4f9d\u8d56\u6027\u548c\u6807\u6ce8\u6570\u636e\u9700\u6c42\u3002", "method": "\u63d0\u51faVIZOR\u6846\u67b6\uff1a1\uff09\u76f4\u63a5\u4ece\u539f\u59cb3D\u573a\u666f\u6784\u5efa\u573a\u666f\u56fe\uff1b2\uff09\u57fa\u4e8e\u6bcf\u4e2a\u7269\u4f53\u6b63\u9762\u65b9\u5411\u5b9a\u4e49\u7a7a\u95f4\u5173\u7cfb\uff0c\u786e\u4fdd\u89c6\u89d2\u4e0d\u53d8\u6027\uff1b3\uff09\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u63a8\u65ad\u5f00\u653e\u8bcd\u6c47\u7684\u7a7a\u95f4\u548c\u90bb\u8fd1\u5173\u7cfb\uff1b4\uff09\u7aef\u5230\u7aef\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u5728\u573a\u666f\u56fe\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u67e5\u8be2\u5f0f\u7269\u4f53\u5b9a\u4f4d\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728Replica\u548cNr3D\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b022%\u548c4.81%\u7684\u96f6\u6837\u672c\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "VIZOR\u901a\u8fc7\u89c6\u89d2\u4e0d\u53d8\u7684\u7a7a\u95f4\u5173\u7cfb\u5b9a\u4e49\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u56fe\u751f\u6210\u7684\u89c6\u89d2\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.01425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01425", "abs": "https://arxiv.org/abs/2602.01425", "authors": ["Vikram Natarajan", "Devina Jain", "Shivam Arora", "Satvik Golechha", "Joseph Bloom"], "title": "Building Better Deception Probes Using Targeted Instruction Pairs", "comment": null, "summary": "Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.", "AI": {"tldr": "\u7ebf\u6027\u63a2\u9488\u5728\u68c0\u6d4bAI\u6b3a\u9a97\u884c\u4e3a\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u548c\u8bef\u62a5\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u5bf9\u9009\u62e9\u662f\u5173\u952e\u56e0\u7d20\uff0c\u9488\u5bf9\u7279\u5b9a\u6b3a\u9a97\u7c7b\u578b\u8bbe\u8ba1\u63a2\u9488\u6bd4\u901a\u7528\u68c0\u6d4b\u5668\u66f4\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u63a2\u9488\u65b9\u6cd5\u5728\u68c0\u6d4bAI\u7cfb\u7edf\u6b3a\u9a97\u884c\u4e3a\u65f6\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u5305\u62ec\u865a\u5047\u76f8\u5173\u6027\u548c\u5bf9\u975e\u6b3a\u9a97\u6027\u54cd\u5e94\u7684\u8bef\u62a5\u3002\u9700\u8981\u7406\u89e3\u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u63a2\u9488\u4f1a\u5931\u8d25\uff0c\u5e76\u627e\u5230\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u6307\u4ee4\u5bf9\u7684\u91cd\u8981\u6027\uff0c\u5e76\u57fa\u4e8e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6b3a\u9a97\u5206\u7c7b\u5b66\u6765\u9488\u5bf9\u7279\u5b9a\u6b3a\u9a97\u884c\u4e3a\u8bbe\u8ba1\u63a2\u9488\u3002\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u5bf9\u6355\u6349\u7684\u662f\u6b3a\u9a97\u610f\u56fe\u800c\u975e\u5185\u5bb9\u7279\u5b9a\u6a21\u5f0f\u3002", "result": "\u6307\u4ee4\u9009\u62e9\u5bf9\u63a2\u9488\u6027\u80fd\u5f71\u54cd\u5de8\u5927\uff08\u536070.6%\u7684\u65b9\u5dee\uff09\u3002\u9488\u5bf9\u7279\u5b9a\u6b3a\u9a97\u7c7b\u578b\u8bbe\u8ba1\u7684\u63a2\u9488\u5728\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\u3002\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6b3a\u9a97\u7c7b\u578b\u5177\u6709\u5f02\u8d28\u6027\u3002", "conclusion": "\u7ec4\u7ec7\u5e94\u8be5\u8bbe\u8ba1\u9488\u5bf9\u5176\u7279\u5b9a\u5a01\u80c1\u6a21\u578b\u7684\u4e13\u95e8\u5316\u63a2\u9488\uff0c\u800c\u4e0d\u662f\u5bfb\u6c42\u901a\u7528\u7684\u6b3a\u9a97\u68c0\u6d4b\u5668\u3002\u6307\u4ee4\u5bf9\u9009\u62e9\u662f\u5173\u952e\u56e0\u7d20\uff0c\u56e0\u4e3a\u5b83\u6355\u6349\u7684\u662f\u6b3a\u9a97\u610f\u56fe\u800c\u975e\u5185\u5bb9\u6a21\u5f0f\u3002"}}
{"id": "2602.00511", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00511", "abs": "https://arxiv.org/abs/2602.00511", "authors": ["Akram Aldroubi"], "title": "Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions", "comment": null, "summary": "Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.\n  PUNN constructs $k$ nonnegative functions $h_1, \\ldots, h_k$ satisfying $\\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\\text{class } i \\mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.\n  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).\n  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.", "AI": {"tldr": "\u63d0\u51faPUNN\u67b6\u6784\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u5355\u4f4d\u5206\u89e3\u76f4\u63a5\u751f\u6210\u7c7b\u522b\u6982\u7387\uff0c\u65e0\u9700softmax\u5c42\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u5668\u8bbe\u8ba1", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u96be\u4ee5\u89e3\u91ca\uff0csoftmax\u6a21\u578b\u4e2d\u7684\u7c7b\u522b\u533a\u57df\u901a\u8fc7logits\u7684\u4e0d\u7b49\u5f0f\u7cfb\u7edf\u9690\u5f0f\u5b9a\u4e49\uff0c\u96be\u4ee5\u63d0\u53d6\u548c\u53ef\u89c6\u5316", "method": "\u5f15\u5165PUNN\u67b6\u6784\uff0c\u5b66\u4e60k\u4e2a\u975e\u8d1f\u51fd\u6570h\u2081,...,h\u2096\u6ee1\u8db3\u2211h\u1d62(x)=1\uff0c\u6bcf\u4e2ah\u1d62(x)\u76f4\u63a5\u8868\u793aP(class i|x)\u3002\u95e8\u51fd\u6570g\u1d62\u53ef\u4f7f\u7528\u591a\u79cd\u6fc0\u6d3b\u51fd\u6570\u548c\u53c2\u6570\u5316\u8bbe\u8ba1", "result": "\u5728\u5408\u6210\u6570\u636e\u3001UCI\u57fa\u51c6\u548cMNIST\u4e0a\uff0c\u57fa\u4e8eMLP\u95e8\u51fd\u6570\u7684PUNN\u51c6\u786e\u7387\u6bd4\u6807\u51c6\u591a\u5c42\u611f\u77e5\u673a\u4f4e0.3-0.6%\u3002\u5f53\u51e0\u4f55\u5148\u9a8c\u5339\u914d\u6570\u636e\u7ed3\u6784\u65f6\uff0c\u5f62\u72b6\u611f\u77e5\u95e8\u51fd\u6570\u7528\u5c11300\u500d\u7684\u53c2\u6570\u5b9e\u73b0\u76f8\u5f53\u51c6\u786e\u7387", "conclusion": "PUNN\u8bc1\u660e\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u67b6\u6784\u53ef\u4ee5\u4e0e\u9ed1\u76d2\u6a21\u578b\u7ade\u4e89\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u7684\u7c7b\u522b\u6982\u7387\u5206\u914d\uff0c\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2602.01378", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01378", "abs": "https://arxiv.org/abs/2602.01378", "authors": ["Poushali Sengupta", "Shashi Raj Pandey", "Sabita Maharjan", "Frank Eliassen"], "title": "Context Dependence and Reliability in Autoregressive Language Models", "comment": null, "summary": "Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.", "AI": {"tldr": "\u63d0\u51faRISE\u65b9\u6cd5\u89e3\u51b3LLM\u89e3\u91ca\u4e2d\u5197\u4f59\u4fe1\u606f\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u7684\u72ec\u7279\u5f71\u54cd\u529b\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\u3002", "motivation": "LLM\u751f\u6210\u8f93\u51fa\u65f6\u4f7f\u7528\u5927\u91cf\u4e0a\u4e0b\u6587\uff0c\u5305\u542b\u63d0\u793a\u3001\u68c0\u7d22\u6bb5\u843d\u548c\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002\u6807\u51c6\u89e3\u91ca\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u91cd\u53e0\u4e0a\u4e0b\u6587\uff0c\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u4f1a\u5bfc\u81f4\u5f52\u56e0\u5206\u6570\u4e0d\u53ef\u9884\u6d4b\u7684\u53d8\u5316\uff0c\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\u5e76\u5e26\u6765\u63d0\u793a\u6ce8\u5165\u7b49\u98ce\u9669\u3002\u9700\u8981\u533a\u5206\u771f\u6b63\u5f71\u54cd\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u5143\u7d20\u4e0e\u76f8\u5173\u5143\u7d20\u3002", "method": "\u63d0\u51faRISE\uff08Redundancy-Insensitive Scoring of Explanation\uff09\u65b9\u6cd5\uff0c\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u76f8\u5bf9\u4e8e\u5176\u4ed6\u8f93\u5165\u7684\u72ec\u7279\u5f71\u54cd\u529b\uff0c\u6700\u5c0f\u5316\u5197\u4f59\u5f71\u54cd\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u7a33\u5b9a\u7684\u5f52\u56e0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRISE\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u89e3\u91ca\uff0c\u5f3a\u8c03\u6761\u4ef6\u4fe1\u606f\u5bf9\u4e8e\u53ef\u4fe1LLM\u89e3\u91ca\u548c\u76d1\u63a7\u7684\u91cd\u8981\u6027\u3002", "conclusion": "RISE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u89e3\u91ca\u4e2d\u7684\u5197\u4f59\u654f\u611f\u6027\u95ee\u9898\uff0c\u4e3a\u5173\u952e\u5e94\u7528\u4e2d\u8bc6\u522b\u771f\u6b63\u5f71\u54cd\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u5143\u7d20\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u76d1\u63a7\u3002"}}
{"id": "2602.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00639", "abs": "https://arxiv.org/abs/2602.00639", "authors": ["Yifang Xu", "Benxiang Zhai", "Chenyu Zhang", "Ming Li", "Yang Li", "Sidan Du"], "title": "Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization", "comment": "Accepted by Information Fusion 2025", "summary": "Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.", "AI": {"tldr": "Diff-PC\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u8096\u50cf\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u8fc73D\u4eba\u8138\u5148\u9a8c\u3001ID\u7f16\u7801\u5668\u3001ID\u63a7\u5236\u5668\u548cID\u6ce8\u5165\u5668\u5b9e\u73b0\u9ad8\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u7cbe\u786e\u9762\u90e8\u63a7\u5236\u548c\u591a\u6837\u5316\u80cc\u666f\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u8096\u50cf\u5b9a\u5236\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u9762\u90e8\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u3001\u63a7\u5236\u9762\u90e8\u5c5e\u6027\u5e76\u751f\u6210\u591a\u6837\u5316\u80cc\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u75283D\u4eba\u8138\u9884\u6d4b\u5668\u91cd\u5efa\u5305\u542b\u53c2\u8003\u8eab\u4efd\u3001\u76ee\u6807\u8868\u60c5\u548c\u59ff\u6001\u76843D\u611f\u77e5\u4eba\u8138\u5148\u9a8c\uff1b2. \u8bbe\u8ba1ID\u7f16\u7801\u5668\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u9762\u90e8\u7279\u5f81\uff1b3. \u5f00\u53d1ID\u63a7\u5236\u5668\u5229\u75283D\u4eba\u8138\u5f15\u5bfc\u8eab\u4efd\u7279\u5f81\u5bf9\u9f50\uff1b4. \u5f15\u5165ID\u6ce8\u5165\u5668\u589e\u5f3a\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u9762\u90e8\u53ef\u63a7\u6027\uff1b5. \u5728\u6536\u96c6\u7684\u8eab\u4efd\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiff-PC\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u9762\u90e8\u63a7\u5236\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u517c\u5bb9\u591a\u98ce\u683c\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "Diff-PC\u4e3a\u96f6\u6837\u672c\u8096\u50cf\u5b9a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u7cbe\u786e\u9762\u90e8\u63a7\u5236\u548c\u591a\u6837\u5316\u80cc\u666f\u751f\u6210\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01443", "abs": "https://arxiv.org/abs/2602.01443", "authors": ["Alberto Castelo", "Zahra Zanjani Foumani", "Ailin Fan", "Keat Yang Koay", "Vibhor Malik", "Yuanzheng Zhu", "Han Li", "Meysam Feghhi", "Ronie Uliana", "Shuang Xie", "Zhaoyu Zhang", "Angelo Ocana Martins", "Mingyu Zhao", "Francis Pelland", "Jonathan Faerman", "Nikolas LeBlanc", "Aaron Glazer", "Andrew McNamara", "Lingyun Wang", "Zhong Wu"], "title": "SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce", "comment": null, "summary": "A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.", "AI": {"tldr": "SimGym\uff1a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u79bb\u7ebfA/B\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4e70\u5bb6\u884c\u4e3a\u66ff\u4ee3\u771f\u5b9e\u6d41\u91cf\u6d4b\u8bd5\uff0c\u5c06\u5b9e\u9a8c\u5468\u671f\u4ece\u6570\u5468\u7f29\u77ed\u81f31\u5c0f\u65f6\u5185", "motivation": "\u4f20\u7edfA/B\u6d4b\u8bd5\u9700\u8981\u5206\u6d41\u771f\u5b9e\u6d41\u91cf\u3001\u8017\u65f6\u6570\u5468\u624d\u80fd\u83b7\u5f97\u663e\u8457\u7ed3\u679c\uff0c\u4e14\u53ef\u80fd\u635f\u5bb3\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u4e00\u79cd\u5feb\u901f\u3001\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4ece\u751f\u4ea7\u4ea4\u4e92\u6570\u636e\u4e2d\u63d0\u53d6\u4e70\u5bb6\u753b\u50cf\u548c\u610f\u56fe\uff0c\u8bc6\u522b\u884c\u4e3a\u539f\u578b\uff0c\u4f7f\u7528LLM\u4ee3\u7406\u5728\u5b9e\u65f6\u6d4f\u89c8\u5668\u4e2d\u6a21\u62df\u63a7\u5236\u7ec4\u548c\u5b9e\u9a8c\u7ec4\u7684\u52a0\u6743\u4f1a\u8bdd", "result": "\u5728\u4e3b\u8981\u7535\u5546\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u5373\u4f7f\u672a\u7ecf\u5bf9\u9f50\u8bad\u7ec3\uff0cSimGym\u4ee3\u7406\u4e5f\u80fd\u4e0e\u89c2\u5bdf\u5230\u7684\u7ed3\u679c\u53d8\u5316\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5c06\u5b9e\u9a8c\u5468\u671f\u4ece\u6570\u5468\u7f29\u77ed\u81f31\u5c0f\u65f6\u4ee5\u5185", "conclusion": "SimGym\u5b9e\u73b0\u4e86\u65e0\u9700\u66b4\u9732\u771f\u5b9e\u4e70\u5bb6\u7684\u5feb\u901f\u5b9e\u9a8c\uff0c\u4e3a\u7535\u5546UI\u53d8\u66f4\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u79bb\u7ebfA/B\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00513", "abs": "https://arxiv.org/abs/2602.00513", "authors": ["Md Tanvirul Alam", "Aritran Piplai", "Ionut Cardei", "Nidhi Rastogi", "Peter J Worth"], "title": "Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs", "comment": null, "summary": "Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \\textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.", "AI": {"tldr": "\u63d0\u51faMinerva\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6539\u8fdb\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9a8c\u8bc1\u5668\u548c\u81ea\u8bad\u7ec3\u673a\u5236\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\u5206\u6790\u5e08\u9700\u8981\u5c06\u5608\u6742\u7684\u975e\u7ed3\u6784\u5316\u5b89\u5168\u6570\u636e\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u7684\u81ea\u52a8\u5316\u5c31\u7eea\u8868\u793a\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7ed3\u6784\u5316CTI\u8f93\u51fa\u65f6\u4ecd\u7136\u8106\u5f31\uff0c\u4e14\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u3002CTI\u6807\u51c6\u548c\u793e\u533a\u7ef4\u62a4\u8d44\u6e90\u5b9a\u4e49\u4e86\u89c4\u8303\u7684\u6807\u8bc6\u7b26\u548c\u6a21\u5f0f\uff0c\u80fd\u591f\u5bf9\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u786e\u5b9a\u6027\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faMinerva\u6846\u67b6\uff0c\u5305\u542b\u7edf\u4e00\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6db5\u76d6\u591a\u4e2aCTI\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u914d\u6709\u7279\u5b9a\u9a8c\u8bc1\u5668\u6765\u8bc4\u5206\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u6807\u8bc6\u7b26\u9884\u6d4b\u3002\u4e3a\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u81ea\u8bad\u7ec3\u673a\u5236\uff0c\u751f\u6210\u989d\u5916\u7684\u5df2\u9a8c\u8bc1\u8f68\u8ff9\u5e76\u5c06\u5176\u84b8\u998f\u56de\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u5229\u7528CTI\u6807\u51c6\u4e2d\u56fa\u6709\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01381", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01381", "abs": "https://arxiv.org/abs/2602.01381", "authors": ["Youheng Zhu", "Yiping Lu"], "title": "On the Power of (Approximate) Reward Models for Inference-Time Scaling", "comment": null, "summary": "Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.\n  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684Bellman\u8bef\u5dee\u662f\u5f71\u54cdSMC\u63a8\u7406\u65f6\u7f29\u653e\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5f53\u8bef\u5dee\u4e3aO(1/T)\u65f6\uff0c\u53ef\u5c06\u63a8\u7406\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u591a\u9879\u5f0f\u7ea7\u3002", "motivation": "\u5b9e\u9645\u90e8\u7f72\u7684\u7cfb\u7edf\u4e2d\u53ea\u6709\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u53ef\u7528\uff0c\u800c\u975e\u771f\u5b9e\u5956\u52b1\u6a21\u578b\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u6839\u672c\u95ee\u9898\uff1a\u4e3a\u4ec0\u4e48\u4ee5\u53ca\u4f55\u65f6\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u8db3\u4ee5\u652f\u6301\u6709\u6548\u7684\u63a8\u7406\u65f6\u7f29\u653e\uff1f\u672c\u6587\u65e8\u5728\u4ece\u7406\u8bba\u4e0a\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc6\u522b\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684Bellman\u8bef\u5dee\u4f5c\u4e3a\u5173\u952e\u6307\u6807\u3002\u8bc1\u660e\u5f53Bellman\u8bef\u5dee\u6709\u754c\u4e8eO(1/T)\u65f6\uff0c\u7ed3\u5408SMC\u65b9\u6cd5\u53ef\u4ee5\u5c06\u63a8\u7406\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7(T)\u964d\u4f4e\u5230\u591a\u9879\u5f0f\u7ea7(T)\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u7528\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\uff0c\u53ea\u8981\u5176Bellman\u8bef\u5dee\u63a7\u5236\u5728O(1/T)\u8303\u56f4\u5185\uff0cSMC\u63a8\u7406\u65f6\u7f29\u653e\u5c31\u80fd\u5b9e\u73b0\u6307\u6570\u7ea7\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u591a\u9879\u5f0f\u7ea7\u3002", "conclusion": "\u8fd1\u4f3c\u5956\u52b1\u6a21\u578b\u7684Bellman\u8bef\u5dee\u662f\u51b3\u5b9aSMC\u63a8\u7406\u65f6\u7f29\u653e\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5f53\u8bef\u5dee\u8db3\u591f\u5c0f\u65f6\uff0c\u5373\u4f7f\u4f7f\u7528\u8fd1\u4f3c\u5956\u52b1\u4e5f\u80fd\u83b7\u5f97\u6307\u6570\u7ea7\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\uff0c\u8fd9\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.00650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00650", "abs": "https://arxiv.org/abs/2602.00650", "authors": ["Mohammadreza Gholipour Shahraki", "Mehdi Rezaeian", "Mohammad Ghasemzadeh"], "title": "A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation", "comment": null, "summary": "Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.", "AI": {"tldr": "\u63d0\u51faMamba-SAM\uff0c\u4e00\u79cd\u7ed3\u5408\u51bb\u7ed3SAM\u7f16\u7801\u5668\u548cMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9ad8\u6548\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u89e3\u51b3\u9886\u57df\u504f\u79fb\u30012D\u8bbe\u8ba1\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5982SAM\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5b58\u5728\u9886\u57df\u504f\u79fb\u30012D\u8bbe\u8ba1\u9650\u5236\u548c\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u80fd\u5904\u74063D\u4e0a\u4e0b\u6587\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u7b56\u7565\uff1a1) \u53cc\u5206\u652f\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u51bb\u7ed3SAM\u7f16\u7801\u5668\u7279\u5f81\u548c\u53ef\u8bad\u7ec3VMamba\u7f16\u7801\u5668\u7684\u9886\u57df\u7279\u5b9a\u8868\u793a\uff1b2) \u9002\u914d\u5668\u65b9\u6cd5\uff0c\u5728\u51bb\u7ed3SAM ViT\u7f16\u7801\u5668\u4e2d\u6ce8\u5165\u8f7b\u91cf\u7ea73D\u611f\u77e5\u7684Tri-Plane Mamba\u6a21\u5757\u3002\u5f15\u5165\u591a\u9891\u95e8\u63a7\u5377\u79ef\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728ACDC\u5fc3\u810fMRI\u6570\u636e\u96c6\u4e0a\uff0c\u53cc\u5206\u652fMamba-SAM-Base\u6a21\u578b\u8fbe\u52300.906\u5e73\u5747Dice\u5206\u6570\uff0c\u4e0eUNet++\u76f8\u5f53\uff0c\u5728\u5fc3\u808c\u548c\u5de6\u5fc3\u5ba4\u5206\u5272\u4e0a\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002\u9002\u914d\u5668TP MFGC\u53d8\u4f53\u63d0\u4f9b4.77 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u7cbe\u5ea6\u4e3a0.880 Dice\u3002", "conclusion": "\u5c06\u57fa\u7840\u6a21\u578b\u4e0e\u9ad8\u6548\u7684SSM\u67b6\u6784\u6df7\u5408\uff0c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2602.01465", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01465", "abs": "https://arxiv.org/abs/2602.01465", "authors": ["Nikita Benkovich", "Vitalii Valkov"], "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering", "comment": null, "summary": "Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u5efa\u6a21\u4e3a\u7ec4\u7ec7\u5316\u6d41\u7a0b\uff0c\u6a21\u62df\u5de5\u7a0b\u56e2\u961f\u7ed3\u6784\uff0c\u5728SWE-bench 500\u4e0a\u5b9e\u73b072.4%\u7684\u4efb\u52a1\u89e3\u51b3\u7387\uff0c\u8d85\u8d8a\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u81ea\u4e3b\u7cfb\u7edf\u5c06\u95ee\u9898\u89e3\u51b3\u89c6\u4e3a\u5355\u4e00\u6216\u6d41\u6c34\u7ebf\u8fc7\u7a0b\uff0c\u800c\u73b0\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u662f\u56e2\u961f\u534f\u4f5c\u6d3b\u52a8\uff0c\u5177\u6709\u660e\u786e\u7684\u89d2\u8272\u5206\u79bb\u3001\u6c9f\u901a\u548c\u5ba1\u67e5\u3002\u9700\u8981\u6a21\u62df\u771f\u5b9e\u5de5\u7a0b\u56e2\u961f\u7684\u7ec4\u7ec7\u7ed3\u6784\u6765\u63d0\u5347\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\u3002", "method": "\u57fa\u4e8eagyn\u5f00\u6e90\u5e73\u53f0\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u914d\u534f\u8c03\u3001\u7814\u7a76\u3001\u5b9e\u73b0\u3001\u5ba1\u67e5\u7b49\u4e13\u95e8\u89d2\u8272\uff0c\u63d0\u4f9b\u9694\u79bb\u6c99\u7bb1\u8fdb\u884c\u5b9e\u9a8c\uff0c\u652f\u6301\u7ed3\u6784\u5316\u901a\u4fe1\u3002\u7cfb\u7edf\u9075\u5faa\u5b9a\u4e49\u597d\u7684\u5f00\u53d1\u65b9\u6cd5\u8bba\uff0c\u5305\u62ec\u5206\u6790\u3001\u4efb\u52a1\u89c4\u8303\u3001\u62c9\u53d6\u8bf7\u6c42\u521b\u5efa\u548c\u8fed\u4ee3\u5ba1\u67e5\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5728SWE-bench 500\u4e0a\u5b9e\u73b072.4%\u7684\u4efb\u52a1\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u53ef\u6bd4\u8bed\u8a00\u6a21\u578b\u7684\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002\u7cfb\u7edf\u8bbe\u8ba1\u7528\u4e8e\u5b9e\u9645\u751f\u4ea7\u4f7f\u7528\uff0c\u800c\u975e\u9488\u5bf9SWE-bench\u8fdb\u884c\u8c03\u4f18\u3002", "conclusion": "\u590d\u5236\u56e2\u961f\u7ed3\u6784\u3001\u65b9\u6cd5\u8bba\u548c\u6c9f\u901a\u662f\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u672a\u6765\u8fdb\u5c55\u53ef\u80fd\u540c\u6837\u4f9d\u8d56\u4e8e\u7ec4\u7ec7\u8bbe\u8ba1\u548c\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u6539\u8fdb\u3002"}}
{"id": "2602.00515", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00515", "abs": "https://arxiv.org/abs/2602.00515", "authors": ["Lin Liu", "Rita Machacy", "Simi Kuniyilh"], "title": "Contrastive Learning for Privacy Enhancements in Industrial Internet of Things", "comment": null, "summary": "The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u4e2d\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u5de5\u4e1a\u6570\u636e\u7684\u72ec\u7279\u7279\u6027\u3001\u7cfb\u7edf\u67b6\u6784\u548c\u5e94\u7528\u573a\u666f\uff0c\u5e76\u8ba8\u8bba\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3001\u5f00\u653e\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u5728\u5b9e\u73b0\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u8de8\u7ad9\u70b9\u4f18\u5316\u7684\u540c\u65f6\uff0c\u7531\u4e8e\u8fd0\u8425\u6570\u636e\u7684\u654f\u611f\u6027\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u9690\u79c1\u548c\u673a\u5bc6\u6027\u98ce\u9669\u3002\u5bf9\u6bd4\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9\u6807\u8bb0\u6570\u636e\u548c\u539f\u59cb\u6570\u636e\u5171\u4eab\u7684\u4f9d\u8d56\uff0c\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u5206\u6790\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u56de\u987e\u548c\u5206\u6790\u4e86\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u9886\u57df\u4e2d\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u3002\u91cd\u70b9\u5173\u6ce8\u5de5\u4e1a\u6570\u636e\u7684\u72ec\u7279\u7279\u6027\u3001\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u4ee5\u53ca\u5404\u79cd\u5e94\u7528\u573a\u666f\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5de5\u4e1a\u7269\u8054\u7f51\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u5168\u9762\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u6280\u672f\u6311\u6218\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u5728\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u9700\u8981\u9488\u5bf9\u5de5\u4e1a\u6570\u636e\u7684\u72ec\u7279\u7279\u6027\u3001\u7cfb\u7edf\u67b6\u6784\u9700\u6c42\u548c\u5e94\u7528\u573a\u666f\uff0c\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01395", "abs": "https://arxiv.org/abs/2602.01395", "authors": ["Almog Tavor", "Itay Ebenspanger", "Neil Cnaan", "Mor Geva"], "title": "Rethinking Selective Knowledge Distillation", "comment": null, "summary": "Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9009\u62e9\u6027\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u751f\u71b5\u7684\u4f4d\u7f6e\u9009\u62e9\u65b9\u6cd5(SE-KD)\uff0c\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6548\u7387\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5bc6\u96c6\u76d1\u7763\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8f6c\u5411\u9009\u62e9\u6027\u84b8\u998f\uff0c\u53ea\u5bf9\u90e8\u5206token\u4f4d\u7f6e\u3001\u8bcd\u6c47\u7c7b\u522b\u6216\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u76d1\u7763\u3002\u7136\u800c\uff0c\u54ea\u4e9b\u91cd\u8981\u6027\u4fe1\u53f7\u3001\u9009\u62e9\u7b56\u7565\u4ee5\u53ca\u5b83\u4eec\u7684\u7ec4\u5408\u6700\u6709\u6548\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "1. \u89e3\u6784\u9009\u62e9\u6027\u77e5\u8bc6\u84b8\u998f\u7684\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u4f4d\u7f6e\u3001\u7c7b\u522b\u548c\u6837\u672c\uff1b2. \u7cfb\u7edf\u6bd4\u8f83\u91cd\u8981\u6027\u4fe1\u53f7\u548c\u9009\u62e9\u7b56\u7565\uff1b3. \u63d0\u51fa\u5b66\u751f\u71b5\u5f15\u5bfc\u7684\u4f4d\u7f6e\u9009\u62e9\u65b9\u6cd5(SE-KD)\uff1b4. \u5c06\u65b9\u6cd5\u6269\u5c55\u5230\u7c7b\u522b\u548c\u6837\u672c\u7ef4\u5ea6(SE-KD 3X)\u3002", "result": "SE-KD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u5e38\u4f18\u4e8e\u5bc6\u96c6\u84b8\u998f\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u4e0b\u6e38\u4efb\u52a1\u9075\u4ece\u6027\u548c\u5185\u5b58\u6548\u7387\u3002SE-KD 3X\u8fdb\u4e00\u6b65\u5e26\u6765\u4e92\u8865\u7684\u6548\u7387\u63d0\u5347\uff0c\u4f7f\u79bb\u7ebf\u6559\u5e08\u7f13\u5b58\u53d8\u5f97\u53ef\u884c\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u51cf\u5c11\u4e8670%\u7684\u5899\u949f\u65f6\u95f4\u300118%\u7684\u5cf0\u503c\u5185\u5b58\u548c80%\u7684\u5b58\u50a8\u4f7f\u7528\uff0c\u4e14\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u9009\u62e9\u6027\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u5b66\u751f\u71b5\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00653", "abs": "https://arxiv.org/abs/2602.00653", "authors": ["Lukas Kuhn", "Giuseppe Serra", "Florian Buettner"], "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment", "comment": null, "summary": "Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.", "AI": {"tldr": "NOVA\u662f\u4e00\u79cd\u975e\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u548c\u5206\u5e03\u6b63\u5219\u5316\uff0c\u65e0\u9700\u8d1f\u91c7\u6837\u3001\u52a8\u91cf\u7f16\u7801\u5668\u6216\u505c\u6b62\u68af\u5ea6\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08\u5982CLIP\uff09\u9700\u8981\u5927\u6279\u91cf\u3001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d1f\u91c7\u6837\u548c\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u590d\u6742\u4e14\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5b9a\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "NOVA\u4f7f\u7528\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u56fe\u50cf\u89c6\u56fe\u9884\u6d4b\u6587\u672c\u5d4c\u5165\uff0c\u540c\u65f6\u901a\u8fc7Sketched Isotropic Gaussian Regularization (SIGReg)\u5f3a\u5236\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ed3\u6784\u3002\u4f7f\u7528\u51bb\u7ed3\u7684\u9886\u57df\u7279\u5b9a\u6587\u672c\u7f16\u7801\u5668\uff08ClinicalBERT\uff09\u548c\u4ece\u5934\u8bad\u7ec3\u7684\u89c6\u89c9Transformer\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u80f8\u90e8X\u5149\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cNOVA\u4f18\u4e8e\u591a\u4e2a\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u8bad\u7ec3\u8fd0\u884c\u66f4\u52a0\u7a33\u5b9a\u4e00\u81f4\u3002", "conclusion": "\u975e\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u4e3a\u5bf9\u6bd4\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u7b49\u4e13\u4e1a\u9886\u57df\u3002"}}
{"id": "2602.01474", "categories": ["cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.01474", "abs": "https://arxiv.org/abs/2602.01474", "authors": ["Gillian K. Hadfield"], "title": "Legal Infrastructure for Transformative AI Governance", "comment": null, "summary": "Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u6cbb\u7406\u4e0d\u4ec5\u9700\u8981\u5b9e\u8d28\u89c4\u5219\uff0c\u66f4\u9700\u8981\u5efa\u7acb\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u5177\u4f53\u6846\u67b6\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dAI\u6cbb\u7406\u4e3b\u8981\u5173\u6ce8\u5b9e\u8d28\u89c4\u5219\uff08\u5982\u9650\u5236\u548c\u68c0\u67e5\uff09\uff0c\u4f46\u5ffd\u89c6\u4e86\u5efa\u7acb\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\u7684\u91cd\u8981\u6027\u3002AI\u7684\u53d8\u9769\u6027\u7279\u5f81\u7279\u522b\u9700\u8981\u5173\u6ce8\u6784\u5efa\u6cd5\u5f8b\u548c\u76d1\u7ba1\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u89c4\u5219\u7684\u6709\u6548\u751f\u6210\u548c\u5b9e\u65bd\u3002", "method": "\u4f5c\u8005\u56de\u987e\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u5177\u4f53\u6846\u67b6\u8bbe\u8ba1\uff1a1\uff09\u524d\u6cbf\u6a21\u578b\u7684\u6ce8\u518c\u5236\u5ea6\uff1b2\uff09\u81ea\u4e3b\u4ee3\u7406\u7684\u6ce8\u518c\u548c\u8bc6\u522b\u5236\u5ea6\uff1b3\uff09\u76d1\u7ba1\u5e02\u573a\u7684\u8bbe\u8ba1\uff0c\u4ee5\u4fc3\u8fdb\u79c1\u8425\u516c\u53f8\u5728AI\u76d1\u7ba1\u670d\u52a1\u65b9\u9762\u7684\u521b\u65b0\u548c\u63d0\u4f9b\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u79cd\u5177\u4f53\u7684\u6cd5\u5f8b\u548c\u76d1\u7ba1\u57fa\u7840\u8bbe\u65bd\u65b9\u6848\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u4e86\u8d85\u8d8a\u5b9e\u8d28\u89c4\u5219\u7684\u7cfb\u7edf\u6027\u6846\u67b6\u8bbe\u8ba1\u601d\u8def\u3002", "conclusion": "AI\u6cbb\u7406\u4e0d\u4ec5\u9700\u8981\u5173\u6ce8\"\u4ec0\u4e48\u89c4\u5219\"\uff0c\u66f4\u9700\u8981\u5173\u6ce8\"\u5982\u4f55\u5efa\u7acb\u89c4\u5219\u751f\u6210\u548c\u5b9e\u65bd\u7684\u57fa\u7840\u8bbe\u65bd\"\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u4e09\u79cd\u6846\u67b6\u8bbe\u8ba1\u4e3a\u89e3\u51b3AI\u6cbb\u7406\u4e2d\u7684\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002"}}
{"id": "2602.00520", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00520", "abs": "https://arxiv.org/abs/2602.00520", "authors": ["Minghui Sun", "Haoyu Gong", "Xingyu You", "Jillian Hurst", "Benjamin Goldstein", "Matthew Engelhard"], "title": "NEST: Nested Event Stream Transformer for Sequences of Multisets", "comment": "11 pages", "summary": "Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.", "AI": {"tldr": "NEST\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u4e8b\u4ef6\u6d41\u7684\u5206\u5c42Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u4fdd\u7559\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u6d41\u57fa\u7840\u6a21\u578b\u5c06\u5c42\u6b21\u7ed3\u6784\u6241\u5e73\u5316\u4e3a\u5355\u7ef4\u5e8f\u5217\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff08\u5bc6\u96c6\u6ce8\u610f\u529b\uff09\u548c\u5b66\u4e60\u865a\u5047\u7684\u96c6\u5408\u5185\u5173\u7cfb\uff0c\u540c\u65f6\u542f\u53d1\u5f0f\u540e\u8bad\u7ec3\u6c60\u5316\u5bfc\u81f4\u96c6\u5408\u7ea7\u8868\u793a\u8d28\u91cf\u4e0b\u964d", "method": "\u63d0\u51faNEST\uff08\u5d4c\u5957\u4e8b\u4ef6\u6d41Transformer\uff09\uff0c\u4fdd\u7559\u4e8b\u4ef6\u6d41\u7684\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\uff08\u5e8f\u5217\u7684\u591a\u91cd\u96c6\uff09\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u96c6\u5408\u5efa\u6a21\uff08MSM\uff09\u8303\u5f0f\u6765\u63d0\u5347\u96c6\u5408\u7ea7\u8868\u793a\u5b66\u4e60", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u91cd\u96c6\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNEST\u80fd\u591f\u6355\u6349\u771f\u5b9e\u4e16\u754c\u52a8\u6001\uff0c\u540c\u65f6\u63d0\u9ad8\u9884\u8bad\u7ec3\u6548\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "conclusion": "\u5728\u57fa\u7840\u6a21\u578b\u67b6\u6784\u4e2d\u4fdd\u7559\u539f\u59cb\u5c42\u6b21\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u65e2\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u53c8\u80fd\u63d0\u5347\u8868\u793a\u8d28\u91cf"}}
{"id": "2602.01401", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01401", "abs": "https://arxiv.org/abs/2602.01401", "authors": ["Niansong Zhang", "Sunwoo Kim", "Shreesha Srinath", "Zhiru Zhang"], "title": "From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis", "comment": null, "summary": "The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u5728AI\u4ee3\u7406\u65f6\u4ee3\uff0c\u9ad8\u5c42\u6b21\u7efc\u5408\uff08HLS\uff09\u4ecd\u7136\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u4f5c\u4e3a\u4ee3\u7406\u4f18\u5316\u7684\u81ea\u7136\u5c42\uff0c\u63d0\u4f9b\u5feb\u901f\u8fed\u4ee3\u3001\u53ef\u79fb\u690d\u6027\u548c\u8bbe\u8ba1\u53ef\u7f6e\u6362\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0cAI\u9a71\u52a8\u7684\u786c\u4ef6\u8bbe\u8ba1\u53d7\u5230\u5173\u6ce8\uff0c\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5728\u4ee3\u7406\u65f6\u4ee3\uff0c\u9ad8\u5c42\u6b21\u7efc\u5408\uff08HLS\uff09\u662f\u5426\u4ecd\u7136\u91cd\u8981\uff1f\u4f5c\u8005\u8ba4\u4e3aHLS\u4ecd\u7136\u5fc5\u4e0d\u53ef\u5c11\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u652f\u6301\u4ee3\u7406\u4f18\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8fd9\u662f\u4e00\u7bc7\u7acb\u573a\u8bba\u6587\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\u8fdb\u884c\u5206\u6790\uff1a1) \u89e3\u91caHLS\u4f5c\u4e3a\u5b9e\u7528\u62bd\u8c61\u5c42\u548c\u9ec4\u91d1\u53c2\u8003\u7684\u4f5c\u7528\uff1b2) \u8bc6\u522b\u5f53\u524dHLS\u5de5\u5177\u7684\u5173\u952e\u9650\u5236\uff1b3) \u63d0\u51fa\u4ee3\u7406HLS\u5171\u751f\u6f14\u5316\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u8bba\u6587\u660e\u786e\u4e86HLS\u5728\u4ee3\u7406\u65f6\u4ee3\u7684\u91cd\u8981\u6027\uff0c\u8bc6\u522b\u4e86\u5f53\u524dHLS\u5de5\u5177\u7684\u4e09\u5927\u9650\u5236\uff08\u6027\u80fd\u53cd\u9988\u4e0d\u8db3\u3001\u63a5\u53e3\u50f5\u5316\u3001\u53ef\u8c03\u8bd5\u6027\u6709\u9650\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4ece\u534f\u540c\u8bbe\u8ba1\u5230\u81ea\u4e3b\u8bbe\u8ba1\u4f19\u4f34\u7684\u6f14\u5316\u8def\u5f84\u5206\u7c7b\u6cd5\u3002", "conclusion": "HLS\u5728AI\u4ee3\u7406\u65f6\u4ee3\u4ecd\u7136\u81f3\u5173\u91cd\u8981\uff0c\u5b83\u63d0\u4f9b\u4e86\u4ee3\u7406\u4f18\u5316\u7684\u81ea\u7136\u62bd\u8c61\u5c42\u3002\u867d\u7136\u6210\u719f\u7684\u4ee3\u7406\u786c\u4ef6\u7cfb\u7edf\u5c06\u540c\u65f6\u5229\u7528HLS\u548cRTL\uff0c\u4f46HLS\u7684\u5feb\u901f\u8fed\u4ee3\u3001\u53ef\u79fb\u690d\u6027\u548c\u8bbe\u8ba1\u53ef\u7f6e\u6362\u6027\u4f7f\u5176\u6210\u4e3a\u4ee3\u7406\u4f18\u5316\u7684\u7406\u60f3\u9009\u62e9\u3002AI\u4ee3\u7406\u7279\u522b\u9002\u5408\u89e3\u51b3\u5f53\u524dHLS\u5de5\u5177\u7684\u9650\u5236\u3002"}}
{"id": "2602.00661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00661", "abs": "https://arxiv.org/abs/2602.00661", "authors": ["Ahsan Raza Siyal", "Markus Haltmeier", "Ruth Steiger", "Elke Ruth Gizewski", "Astrid Ellen Grams"], "title": "Schr\u00f6dinger-Inspired Time-Evolution for 4D Deformation Forecasting", "comment": null, "summary": "Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schr\u00f6dinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $\u03c8= A e^{i\u03c6}$, which is evolved forward in time using a differentiable, unrolled Schr\u00f6dinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schr\u00f6dinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u859b\u5b9a\u8c14\u65b9\u7a0b\u542f\u53d1\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u67b6\u6784\uff0c\u7528\u4e8e4D\uff083D+\u65f6\u95f4\uff09\u65f6\u7a7a\u9884\u6d4b\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u859b\u5b9a\u8c14\u65f6\u95f4\u6b65\u8fdb\u5668\u6f14\u5316\u590d\u503c\u6ce2\u51fd\u6570\uff0c\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u957f\u671f\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4e09\u7ef4\u73b0\u8c61\uff08\u5982\u533b\u5b66\u5f71\u50cf\u3001\u6d41\u4f53\u52a8\u529b\u5b66\uff09\u7684\u65f6\u7a7a\u9884\u6d4b\u95ee\u9898\u3002\u4f20\u7edf\u65e0\u7ea6\u675f\u795e\u7ecf\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u6f02\u79fb\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u6765\u63d0\u9ad8\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u89e3\u5256\u5b66\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u859b\u5b9a\u8c14\u542f\u53d1\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u67b6\u6784\uff0c\u5728\u6df1\u5ea6\u5377\u79ef\u6846\u67b6\u4e2d\u5d4c\u5165\u663e\u5f0f\u65f6\u95f4\u6f14\u5316\u7b97\u5b50\u3002\u6a21\u578b\u4ece\u89c2\u6d4b\u7684\u4f53\u5e8f\u5217\u4e2d\u5b66\u4e60\u4f53\u7d20\u7ea7\u7684\u632f\u5e45\u3001\u76f8\u4f4d\u548c\u52bf\u573a\uff0c\u5b9a\u4e49\u590d\u503c\u6ce2\u51fd\u6570\u03c8=Ae^{i\u03c6}\uff0c\u7136\u540e\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u859b\u5b9a\u8c14\u65f6\u95f4\u6b65\u8fdb\u5668\u5411\u524d\u6f14\u5316\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7269\u7406\u7ed3\u6784\u6f14\u5316\u7b97\u5b50\u548c\u6df1\u5ea6\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u771f\u5b9e\u5f62\u72b6\u53d8\u5f62\u548c\u62d3\u6251\u53d8\u5316\u7684\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c55\u793a\u4e86\u51c6\u786e\u7a33\u5b9a\u7684\u672a\u67654D\u72b6\u6001\u9884\u6d4b\uff0c\u5305\u62ec\u4f53\u7d20\u5f3a\u5ea6\u548c\u53d8\u5f62\u573a\u3002\u5b9e\u73b0\u4e86\u957f\u671f\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u8868\u793a\u4ee5\u53ca\u4e0e\u53d8\u5f62\u5408\u6210\u81ea\u7136\u517c\u5bb9\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u859b\u5b9a\u8c14\u578b\u6f14\u5316\u7b97\u5b50\u6574\u5408\u5230\u7aef\u5230\u7aef4D\u795e\u7ecf\u9884\u6d4b\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\uff0c\u4e3a\u53ef\u89e3\u91ca\u3001\u7a33\u5b9a\u4e14\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u65f6\u7a7a\u9884\u6d4b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\uff0c\u7ed3\u5408\u4e86\u6df1\u5ea6\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u57fa\u4e8e\u7269\u7406\u5efa\u6a21\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.01475", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01475", "abs": "https://arxiv.org/abs/2602.01475", "authors": ["Brij Malhotra", "Shivvrat Arya", "Tahrima Rahman", "Vibhav Giridhar Gogate"], "title": "Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models", "comment": null, "summary": "Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u644a\u9500\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u6ce8\u610f\u529b\u7f51\u7edc\u9884\u6d4b\u5c40\u90e8\u79fb\u52a8\u51cf\u5c11\u6c49\u660e\u8ddd\u79bb\u7684\u80fd\u529b\uff0c\u6765\u6539\u8fdb\u6982\u7387\u56fe\u6a21\u578b\u4e2d\u91cd\u590d\u67e5\u8be2\u573a\u666f\u4e0b\u7684\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u3002", "motivation": "\u5728\u6982\u7387\u56fe\u6a21\u578b\u7684MPE\u63a8\u7406\u4e2d\uff0c\u5f53\u56fe\u5f62\u6a21\u578b\u56fa\u5b9a\u4f46\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u8bc1\u636e\u6a21\u5f0f\u91cd\u590d\u6267\u884c\u63a8\u7406\u65f6\uff0c\u73b0\u6709\u7684\u968f\u673a\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4e14\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6307\u5bfc\u65e0\u6cd5\u5728\u76f8\u540c\u6a21\u578b\u7684\u591a\u4e2a\u63a8\u7406\u67e5\u8be2\u4e2d\u6709\u6548\u590d\u7528\u3002", "method": "\u5229\u7528\u56fa\u5b9a\u7684\u56fe\u7ed3\u6784\uff0c\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7f51\u7edc\u6765\u8bc4\u4f30\u5c40\u90e8\u79fb\u52a8\uff0c\u9884\u6d4b\u5176\u51cf\u5c11\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\u6c49\u660e\u8ddd\u79bb\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u4e0e\u73b0\u6709\u5c40\u90e8\u641c\u7d22\u8fc7\u7a0b\u65e0\u7f1d\u96c6\u6210\uff0c\u5728\u90bb\u5c45\u9009\u62e9\u65f6\u5e73\u8861\u77ed\u671f\u4f3c\u7136\u589e\u76ca\u548c\u957f\u671f\u6f5c\u529b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8ddd\u79bb\u51cf\u5c11\u7684\u79fb\u52a8\u9009\u62e9\u80fd\u6539\u5584\u6536\u655b\u884c\u4e3a\uff0c\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5728\u644a\u9500\u63a8\u7406\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u9ad8\u6811\u5bbd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6301\u7eed\u4f18\u4e8eSLS\u548cGLS+\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u644a\u9500\u6846\u67b6\u6709\u6548\u6539\u8fdb\u4e86\u91cd\u590d\u67e5\u8be2\u573a\u666f\u4e0b\u7684\u5c40\u90e8\u641c\u7d22\u6027\u80fd\uff0c\u901a\u8fc7\u9884\u6d4b\u8ddd\u79bb\u51cf\u5c11\u7684\u79fb\u52a8\u6765\u5e73\u8861\u77ed\u671f\u548c\u957f\u671f\u641c\u7d22\u76ee\u6807\uff0c\u5728\u56fa\u5b9a\u56fe\u7ed3\u6784\u7684\u6982\u7387\u56fe\u6a21\u578b\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.00526", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00526", "abs": "https://arxiv.org/abs/2602.00526", "authors": ["Kaiwen Zha", "Chao Li", "Hao He", "Peng Cao", "Tianhong Li", "Ali Mirzazadeh", "Ellen Zhang", "Jong Woo Lee", "Yoon Kim", "Dina Katabi"], "title": "Physiology as Language: Translating Respiration to Sleep EEG", "comment": "Tech report", "summary": "This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u547c\u5438\u4fe1\u53f7\u5408\u6210\u7761\u7720\u8111\u7535\u56fe\u7684\u8de8\u751f\u7406\u5b66\u7ffb\u8bd1\u4efb\u52a1\uff0c\u901a\u8fc7\u6ce2\u5f62\u6761\u4ef6\u751f\u6210\u6846\u67b6\u5b9e\u73b0\uff0c\u572828,000\u4eba\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u5e74\u9f84\u4f30\u8ba1\u3001\u6027\u522b\u68c0\u6d4b\u548c\u7761\u7720\u5206\u671f\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u771f\u5b9e\u8111\u7535\u56fe\u3002", "motivation": "\u63a2\u7d22\u4ece\u547c\u5438\u4fe1\u53f7\u5408\u6210\u7761\u7720\u8111\u7535\u56fe\u7684\u53ef\u80fd\u6027\uff0c\u89e3\u51b3\u4e24\u79cd\u751f\u7406\u4fe1\u53f7\u4e4b\u95f4\u7684\u663e\u8457\u590d\u6742\u6027\u5dee\u8ddd\uff0c\u5b9e\u73b0\u975e\u63a5\u89e6\u5f0f\u8fdc\u7a0b\u795e\u7ecf\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u6ce2\u5f62\u6761\u4ef6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u6807\u8bb0\u5316\u7ea6\u675f\u8111\u7535\u56fe\u76ee\u6807\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u547c\u5438\u52a8\u6001\u7279\u5f81\u3002", "result": "\u8111\u7535\u56fe\u9891\u8c31\u56fe\u91cd\u5efa\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a7%\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u771f\u5b9e\u8111\u7535\u56fe\uff1a\u5e74\u9f84\u4f30\u8ba1\uff08MAE 5.0 vs 5.1\u5e74\uff09\u3001\u6027\u522b\u68c0\u6d4b\uff08AUROC 0.81 vs 0.82\uff09\u3001\u7761\u7720\u5206\u671f\uff08\u51c6\u786e\u73870.84 vs 0.88\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u547c\u5438\u4fe1\u53f7\u5230\u8111\u7535\u56fe\u7684\u8de8\u751f\u7406\u5b66\u7ffb\u8bd1\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u65e0\u7ebf\u5c04\u9891\u53cd\u5c04\u7684\u975e\u63a5\u89e6\u5f0f\u4f20\u611f\uff0c\u4e3a\u8fdc\u7a0b\u7761\u7720\u795e\u7ecf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2602.01447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01447", "abs": "https://arxiv.org/abs/2602.01447", "authors": ["Hieu Minh Duong", "Rupa Ghosh", "Cong Hoan Nguyen", "Eugene Levin", "Todd Gary", "Long Nguyen"], "title": "SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction", "comment": null, "summary": "Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.", "AI": {"tldr": "SentiFuse\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5c42\u548c\u591a\u79cd\u878d\u5408\u7b56\u7565\u96c6\u6210\u5f02\u6784\u60c5\u611f\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u548c\u7b80\u5355\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u5206\u6790\u6a21\u578b\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6709\u6548\u6574\u5408\u8fd9\u4e9b\u4e92\u8865\u7684\u6a21\u578b\u80fd\u529b\uff0c\u65e0\u6cd5\u7cfb\u7edf\u6027\u5730\u5229\u7528\u6a21\u578b\u95f4\u7684\u4e92\u8865\u6027\u3002", "method": "\u63d0\u51faSentiFuse\u6846\u67b6\uff0c\u5305\u542b\u6807\u51c6\u5316\u5c42\u548c\u4e09\u79cd\u878d\u5408\u7b56\u7565\uff1a\u51b3\u7b56\u7ea7\u878d\u5408\u3001\u7279\u5f81\u7ea7\u878d\u5408\u548c\u81ea\u9002\u5e94\u878d\u5408\uff0c\u652f\u6301\u5f02\u6784\u60c5\u611f\u6a21\u578b\u7684\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u5728Crowdflower\u3001GoEmotions\u548cSentiment140\u4e09\u4e2a\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\uff0cSentiFuse\u59cb\u7ec8\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u548c\u7b80\u5355\u96c6\u6210\u3002\u7279\u5f81\u7ea7\u878d\u5408\u6548\u679c\u6700\u597d\uff0cF1\u5206\u6570\u6bd4\u6700\u4f73\u5355\u4e2a\u6a21\u578b\u548c\u7b80\u5355\u5e73\u5747\u63d0\u5347\u9ad8\u8fbe4%\uff0c\u81ea\u9002\u5e94\u878d\u5408\u5728\u5904\u7406\u5426\u5b9a\u3001\u6df7\u5408\u60c5\u611f\u7b49\u590d\u6742\u60c5\u51b5\u65f6\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5229\u7528\u6a21\u578b\u4e92\u8865\u6027\uff0cSentiFuse\u80fd\u591f\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u60c5\u611f\u5206\u6790\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u6587\u672c\u7c7b\u578b\u3002"}}
{"id": "2602.00669", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.00669", "abs": "https://arxiv.org/abs/2602.00669", "authors": ["Marina Crespo Aguirre", "Jonathan Williams-Ramirez", "Dina Zemlyanker", "Xiaoling Hu", "Lucas J. Deden-Binder", "Rogeny Herisse", "Mark Montine", "Theresa R. Connors", "Christopher Mount", "Christine L. MacDonald", "C. Dirk Keene", "Caitlin S. Latimer", "Derek H. Oakley", "Bradley T. Hyman", "Ana Lawry Aguila", "Juan Eugenio Iglesias"], "title": "Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation", "comment": "12 pages of main content, 5 pages of supplement", "summary": "Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4ece\u89e3\u5256\u7167\u7247\u7684\u5404\u5411\u5f02\u60273D\u91cd\u5efa\u4e2d\u751f\u6210\u89e3\u5256\u4e00\u81f4\u7684\u5404\u5411\u540c\u6027\u4f53\u79ef\uff0c\u63d0\u5347\u795e\u7ecf\u75c5\u7406\u5b66\u5206\u6790\u7684\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u4ece2D\u89e3\u5256\u7167\u7247\u91cd\u5efa3D\u8111\u4f53\u79ef\u7684\u65b9\u6cd5\u6709\u65f6\u4f1a\u4ea7\u751f\u7c97\u7cd9\u3001\u8fc7\u5ea6\u5e73\u6ed1\u7684\u7ed3\u6784\u91cd\u5efa\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5404\u5411\u5f02\u6027\uff08\u539a\u5207\u7247\uff09\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u63d0\u9ad8\u91cd\u5efa\u7684\u5206\u8fa8\u7387\u548c\u89e3\u5256\u4fdd\u771f\u5ea6", "method": "\u5f15\u5165\u8ba1\u7b97\u9ad8\u6548\u7684\u8d85\u5206\u8fa8\u7387\u6b65\u9aa4\uff0c\u901a\u8fc7\u8bad\u7ec3\u5728\u9886\u57df\u968f\u673a\u5316\u7684\u5408\u6210\u6570\u636e\u4e0a\uff0c\u4ece\u5404\u5411\u5f02\u60273D\u91cd\u5efa\u4e2d\u63d2\u8865\u5207\u7247\uff0c\u751f\u6210\u89e3\u5256\u4e00\u81f4\u7684\u5404\u5411\u540c\u6027\u4f53\u79ef", "result": "\u63d2\u8865\u540e\u7684\u4f53\u79ef\u6539\u5584\u4e86\u81ea\u52a8\u5206\u5272\u6548\u679c\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684Dice\u5206\u6570\uff08\u7279\u522b\u662f\u5728\u76ae\u8d28\u548c\u767d\u8d28\u533a\u57df\uff09\uff0c\u5728\u8868\u9762\u91cd\u5efa\u548c\u56fe\u8c31\u914d\u51c6\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u66f4\u51c6\u786e\u7684\u76ae\u8d28\u8868\u9762\u548cMRI\u914d\u51c6", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u57fa\u4e8e\u7167\u7247\u91cd\u5efa\u7684\u5206\u8fa8\u7387\u548c\u89e3\u5256\u4fdd\u771f\u5ea6\uff0c\u8be5\u65b9\u6cd5\u52a0\u5f3a\u4e86\u795e\u7ecf\u75c5\u7406\u5b66\u548c\u795e\u7ecf\u5f71\u50cf\u5b66\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u65b9\u6cd5\u5df2\u516c\u5f00\u53ef\u7528"}}
{"id": "2602.01518", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01518", "abs": "https://arxiv.org/abs/2602.01518", "authors": ["Jongseok Park", "Sunga Kim", "Alvin Cheung", "Ion Stoica"], "title": "Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection", "comment": null, "summary": "Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.", "AI": {"tldr": "Qrita\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67a2\u8f74\u9009\u62e9\u7684\u9ad8\u6548Top-k\u548cTop-p\u7b97\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6392\u5e8f\u65b9\u6cd5\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u4e862\u500d\u541e\u5410\u91cf\u548c\u4e00\u534a\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u786e\u5b9a\u6027\u8f93\u51fa\u3002", "motivation": "Top-k\u548cTop-p\u662f\u5927\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u7684\u4e3b\u8981\u622a\u65ad\u7b97\u5b50\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u8bcd\u6c47\u8868\u4e0a\u9ad8\u6548\u5b9e\u73b0\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6392\u5e8f\uff08\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff09\uff0c\u8981\u4e48\u4f7f\u7528\u968f\u673a\u65b9\u6cd5\uff08\u6539\u53d8\u7b97\u6cd5\u8f93\u51fa\uff09\u3002", "method": "\u57fa\u4e8eRTop-k\u7684\u67a2\u8f74\u9009\u62e9\u7b56\u7565\uff0c\u6269\u5c55\u4e3aTop-k\u548cTop-p\u7b97\u6cd5\uff0c\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684sigma\u622a\u65ad\uff0c\u5927\u5e45\u51cf\u5c11\u76ee\u6807\u5143\u7d20\u641c\u7d22\u7a7a\u95f4\uff1b2) \u56db\u5143\u67a2\u8f74\u641c\u7d22\u4e0e\u91cd\u590d\u5904\u7406\uff0c\u5c06\u67a2\u8f74\u641c\u7d22\u8fed\u4ee3\u51cf\u534a\u5e76\u4fdd\u8bc1\u786e\u5b9a\u6027\u8f93\u51fa\u3002\u4f7f\u7528Triton GPU\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\u3002", "result": "\u4e0evLLM\u3001SGLang\u548cFlashinfer\u7b49\u9ad8\u6027\u80fdLLM\u6267\u884c\u5f15\u64ce\u7684Top-k\u548cTop-p\u5185\u6838\u76f8\u6bd4\uff0cQrita\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u541e\u5410\u91cf\u548c\u4e00\u534a\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u4f9b\u4e0e\u6392\u5e8f\u7b97\u6cd5\u76f8\u540c\u7684\u8f93\u51fa\u3002", "conclusion": "Qrita\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u786e\u5b9a\u6027\u7684Top-k\u548cTop-p\u7b97\u6cd5\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728GPU\u4e0a\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2602.00533", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00533", "abs": "https://arxiv.org/abs/2602.00533", "authors": ["Core Francisco Park"], "title": "Convergent World Representations and Divergent Tasks", "comment": null, "summary": "While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.", "AI": {"tldr": "\u591a\u4efb\u52a1\u8bad\u7ec3\u80fd\u4ea7\u751f\u6536\u655b\u7684\u4e16\u754c\u8868\u793a\uff0c\u4f46\u67d0\u4e9b\"\u53d1\u6563\"\u4efb\u52a1\u4f1a\u635f\u5bb3\u65b0\u5b9e\u4f53\u7684\u8868\u793a\u6574\u5408\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u7814\u7a76\u795e\u7ecf\u8868\u793a\u7684\u51e0\u4f55\u7279\u6027\u53ca\u5176\u5728\u4e0b\u6e38\u9002\u5e94\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u76ee\u524d\u5bf9\u8fd9\u4e9b\u6761\u4ef6\u7406\u89e3\u4e0d\u8db3", "method": "\u6784\u5efa\u5206\u79bb\u4e16\u754c\u3001\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u548c\u6a21\u578b\u8868\u793a\u7684\u6846\u67b6\uff0c\u4f7f\u75285,075\u4e2a\u57ce\u5e02\u5750\u6807\u5b9a\u4e49\u4e16\u754c\uff0c7\u4e2a\u51e0\u4f55\u4efb\u52a1\u751f\u6210\u81ea\u56de\u5f52\u8bad\u7ec3\u6570\u636e\uff0c\u7814\u7a76\u591a\u4efb\u52a1\u8bad\u7ec3\u548c\u5fae\u8c03", "result": "\u4e0d\u540c\u4efb\u52a1\u4ea7\u751f\u4e0d\u540c\u7684\u4e16\u754c\u8868\u793a\u51e0\u4f55\uff0c\u4f46\u591a\u4efb\u52a1\u8bad\u7ec3\u4f7f\u8868\u793a\u6536\u655b\u5bf9\u9f50\uff1b\u67d0\u4e9b\u53d1\u6563\u4efb\u52a1\u4f1a\u635f\u5bb3\u65b0\u5b9e\u4f53\u7684\u8868\u793a\u6574\u5408\u548c\u6cdb\u5316", "conclusion": "\u591a\u4efb\u52a1\u5173\u7cfb\u8bad\u7ec3\u80fd\u53ef\u9760\u4ea7\u751f\u6536\u655b\u7684\u4e16\u754c\u8868\u793a\uff0c\u4f46\u6f5c\u4f0f\u7684\u53d1\u6563\u4efb\u52a1\u53ef\u80fd\u901a\u8fc7\u5fae\u8c03\u707e\u96be\u6027\u5730\u635f\u5bb3\u65b0\u5b9e\u4f53\u6574\u5408"}}
{"id": "2602.01451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01451", "abs": "https://arxiv.org/abs/2602.01451", "authors": ["Umme Abira Azmary", "MD Ikramul Kayes", "Swakkhar Shatabda", "Farig Yousuf Sadeque"], "title": "Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language", "comment": null, "summary": "Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u95ee\u7b54\u6a21\u578b\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u53cd\u4e8b\u5b9e\u95ee\u7b54\u6570\u636e\u96c6BanglaCQA\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53d1\u73b0\u601d\u7ef4\u94fe\u63d0\u793a\u80fd\u6709\u6548\u63d0\u53d6\u53cd\u4e8b\u5b9e\u573a\u666f\u4e2d\u7684\u53c2\u6570\u77e5\u8bc6\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u7684\u95ee\u7b54\u6a21\u578b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u8bed\u8a00\u590d\u6742\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u5b5f\u52a0\u62c9\u8bedQA\u6570\u636e\u96c6\u7f3a\u4e4f\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u53c2\u6570\u77e5\u8bc6\u8fd8\u662f\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u7ed3\u6784\uff0c\u9700\u8981\u6784\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u7814\u7a76\u6a21\u578b\u5728\u4e8b\u5b9e\u548c\u53cd\u4e8b\u5b9e\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u6765\u6e90\u3002", "method": "1) \u6784\u5efaBanglaCQA\u6570\u636e\u96c6\uff1a\u6269\u5c55\u73b0\u6709\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\uff0c\u96c6\u6210\u53cd\u4e8b\u5b9e\u6bb5\u843d\u548c\u53ef\u56de\u7b54\u6027\u6807\u6ce8\uff1b2) \u63d0\u51fa\u5fae\u8c03\u7ba1\u9053\uff1a\u9488\u5bf9\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8bed\u8a00\u7279\u5b9a\u6a21\u578b\u548c\u591a\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u7ba1\u9053\uff1a\u9488\u5bf9\u4ec5\u89e3\u7801\u5668LLM\uff1b4) \u5e94\u7528\u57fa\u4e8eLLM\u548c\u4eba\u5de5\u7684\u8bc4\u4f30\u6280\u672f\uff0c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8861\u91cf\u7b54\u6848\u8d28\u91cf\u3002", "result": "1) \u521b\u5efa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u53cd\u4e8b\u5b9eQA\u6570\u636e\u96c6BanglaCQA\uff1b2) \u53d1\u73b0\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u53cd\u4e8b\u5b9e\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\uff0c\u80fd\u6709\u6548\u63d0\u53d6\u4ec5\u89e3\u7801\u5668LLM\u4e2d\u7684\u53c2\u6570\u77e5\u8bc6\uff1b3) \u63d0\u4f9b\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0d\u540cQA\u8bbe\u7f6e\u4e0b\u6a21\u578b\u6027\u80fd\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u4e3a\u5206\u6790\u5b5f\u52a0\u62c9\u8bedQA\u4e2d\u7684\u77e5\u8bc6\u6765\u6e90\u5f15\u5165\u4e86\u65b0\u6846\u67b6\uff0c\u8fd8\u63ed\u793a\u4e86\u5173\u952e\u53d1\u73b0\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u5f00\u8f9f\u4e86\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.00671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00671", "abs": "https://arxiv.org/abs/2602.00671", "authors": ["Yangzhi Ma", "Bojun Liu", "Wenting Liao", "Dong Liu", "Zhu Li", "Li Li"], "title": "HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression", "comment": null, "summary": "While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.", "AI": {"tldr": "HPC\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d41\u5f0f\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u7684\u65b0\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u70b9\u57fa\u6f5c\u5728\u8868\u793a\u548c\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u5728\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u5360\u7528\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6d41\u5f0f\u4f20\u8f93\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u5197\u4f59\u6216\u5c40\u90e8\u76f8\u5173\u6027\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHPC\u6846\u67b6\uff1a1) \u91c7\u7528\u5206\u5c42\u70b9\u57fa\u6f5c\u5728\u8868\u793a\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u9ad8\u65af\u64cd\u4f5c\u4ee5\u907f\u514d\u672a\u5360\u7528\u7a7a\u95f4\u7684\u53c2\u6570\u5197\u4f59\uff1b2) \u901a\u8fc7\u5b9a\u5236\u805a\u5408\u65b9\u6848\u5b9e\u73b0\u9ad8\u7d27\u51d1\u6027\u548c\u4f4e\u7a7a\u95f4\u5197\u4f59\uff1b3) \u9996\u6b21\u7814\u7a76\u901a\u8fc7\u6316\u6398\u548c\u5229\u7528\u53c2\u6570\u95f4\u7684\u5e27\u95f4\u76f8\u5173\u6027\u6765\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u538b\u7f29\u6846\u67b6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHPC\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e8667%\u7684\u5b58\u50a8\u51cf\u5c11\u3002", "conclusion": "HPC\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5c42\u70b9\u57fa\u6f5c\u5728\u8868\u793a\u548c\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u6d41\u5f0f\u4f20\u8f93\u4e2d\u7684\u5b58\u50a8\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u81ea\u7531\u89c6\u70b9\u89c6\u9891\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.01532", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01532", "abs": "https://arxiv.org/abs/2602.01532", "authors": ["Yuxuan Fu", "Xiaoyu Tan", "Teqi Hao", "Chen Zhan", "Xihe Qiu"], "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents", "comment": null, "summary": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: \"make haste slowly\"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.", "AI": {"tldr": "PRISM\u6846\u67b6\u901a\u8fc7\u51b3\u7b56\u7406\u8bba\u95e8\u63a7\u548c\u53cc\u8fc7\u7a0b\u63a8\u7406\u67b6\u6784\uff0c\u5b9e\u73b0\u6210\u672c\u654f\u611f\u7684\u9009\u62e9\u6027\u5e72\u9884\uff0c\u5728\u4e3b\u52a8\u4ee3\u7406\u4e2d\u5e73\u8861\u5e2e\u52a9\u6536\u76ca\u4e0e\u6253\u6270\u8d1f\u62c5\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u4e3b\u52a8\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u8106\u5f31\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u4e0d\u5206\u9752\u7ea2\u7682\u767d\u7684\u957f\u63a8\u7406\uff0c\u5bf9\u6536\u76ca-\u8d1f\u62c5\u6743\u8861\u7f3a\u4e4f\u63a7\u5236\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u3001\u53ef\u8c03\u7684\u63a7\u5236\u673a\u5236\u3002", "method": "\u91c7\u7528\u6210\u672c\u654f\u611f\u9009\u62e9\u6027\u5e72\u9884\u6846\u67b6\uff0c\u7ed3\u5408\u51b3\u7b56\u7406\u8bba\u95e8\u63a7\u548c\u53cc\u8fc7\u7a0b\u63a8\u7406\u67b6\u6784\u3002\u95e8\u63a7\u57fa\u4e8e\u6821\u51c6\u7684\u7528\u6237\u63a5\u53d7\u6982\u7387\u548c\u4e0d\u5bf9\u79f0\u6210\u672c\u9608\u503c\uff0c\u4ec5\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u8c03\u7528\u8d44\u6e90\u5bc6\u96c6\u7684\u6162\u6a21\u5f0f\u8fdb\u884c\u53cd\u4e8b\u5b9e\u68c0\u67e5\u3002\u8bad\u7ec3\u4f7f\u7528\u95e8\u5bf9\u9f50\u3001\u6a21\u5f0f\u9501\u5b9a\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u5728ProactiveBench\u4e0a\uff0cPRISM\u5c06\u8bef\u62a5\u7387\u964d\u4f4e22.78%\uff0cF1\u5206\u6570\u63d0\u9ad820.14%\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u51b3\u7b56\u7406\u8bba\u7684\u95e8\u63a7\u3001\u9009\u62e9\u6027\u6162\u63a8\u7406\u548c\u5bf9\u9f50\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ea7\u751f\u7cbe\u786e\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u4e3b\u52a8\u4ee3\u7406\uff0c\u4e3a\u4e3b\u52a8\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u8c03\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2602.00534", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00534", "abs": "https://arxiv.org/abs/2602.00534", "authors": ["Apurba Prasad Padhy", "Fernando Camacho", "Saibal Mukhopadhyay"], "title": "AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models", "comment": null, "summary": "State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.", "AI": {"tldr": "\u63d0\u51faAIRE-Prune\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fd1\u8109\u51b2\u54cd\u5e94\u80fd\u91cf\u8bc4\u5206\u5bf9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u526a\u679d\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSMs)\u901a\u5e38\u9700\u8981\u5728\u5bb9\u91cf\u3001\u641c\u7d22\u7a7a\u95f4\u6216\u7a33\u5b9a\u6027\u65b9\u9762\u505a\u51fa\u727a\u7272\u6765\u62b5\u6d88\u5927\u72b6\u6001\u7ef4\u5ea6\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAIRE-Prune\u65b9\u6cd5\uff1a\u4e3a\u6bcf\u4e2a\u72b6\u6001\u5206\u914d\u57fa\u4e8e\u6e10\u8fd1\u8109\u51b2\u54cd\u5e94\u80fd\u91cf\u7684\u5c01\u95ed\u5f62\u5f0f\u8bc4\u5206\uff08\u5373\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u5185\u8d21\u732e\u7684\u603b\u8109\u51b2\u54cd\u5e94\u80fd\u91cf\uff09\uff0c\u5e76\u901a\u8fc7\u5c42\u95f4\u5f52\u4e00\u5316\u5b9e\u73b0\u8de8\u5c42\u5168\u5c40\u6bd4\u8f83\u548c\u9009\u62e9\u3002\u8fd9\u6269\u5c55\u4e86\u6a21\u6001\u622a\u65ad\u65b9\u6cd5\u4ece\u5355\u7cfb\u7edf\u5230\u6df1\u5ea6\u5806\u6808\uff0c\u5e76\u4f7f\u526a\u679d\u4e0e\u6e10\u8fd1\u54cd\u5e94\u80fd\u91cf\u800c\u975e\u6700\u574f\u60c5\u51b5\u589e\u76ca\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAIRE-Prune\u63ed\u793a\u4e86SISO\u548cMIMO SSMs\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\uff0c\u5e73\u5747\u526a\u679d\u7387\u8fbe\u523060.8%\uff0c\u5e73\u5747\u7cbe\u5ea6\u4e0b\u964d\u4ec5\u4e3a0.29%\uff08\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff09\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "AIRE-Prune\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7ed3\u6784\u5316\u540e\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3aSSMs\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01472", "abs": "https://arxiv.org/abs/2602.01472", "authors": ["Jie Deng", "Shining Liang", "Jun Li", "Hongzhi Li", "Yutao Xie"], "title": "ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure", "comment": null, "summary": "Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.", "AI": {"tldr": "ConPress\u901a\u8fc7\u591a\u95ee\u9898\u4e0a\u4e0b\u6587\u538b\u529b\u8bf1\u5bfc\u6a21\u578b\u81ea\u6211\u538b\u7f29\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u51cf\u5c11\u63a8\u7406token\u4f7f\u7528\uff0c\u4fdd\u6301\u51c6\u786e\u7387", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u751f\u6210\u957f\u94fe\u5f0f\u601d\u7ef4\u8f68\u8ff9\u89e3\u51b3\u63a8\u7406\u4efb\u52a1\uff0c\u5bfc\u81f4\u5927\u91cf\u63a8\u7406\u5f00\u9500\u3002\u7814\u7a76\u53d1\u73b0\u591a\u95ee\u9898\u63d0\u793a\u4e0b\u6a21\u578b\u4f1a\u81ea\u53d1\u4ea7\u751f\u66f4\u77ed\u7684\u63a8\u7406\u8f68\u8ff9\uff08\u81ea\u6211\u538b\u7f29\u73b0\u8c61\uff09\uff0c\u8fd9\u4e3a\u51cf\u5c11\u63a8\u7406\u6210\u672c\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faConPress\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u591a\u95ee\u9898\u63d0\u793a\u8bf1\u5bfc\u81ea\u6211\u538b\u7f29\u73b0\u8c61\uff1b2\uff09\u91c7\u6837\u6a21\u578b\u8f93\u51fa\uff1b3\uff09\u89e3\u6790\u548c\u8fc7\u6ee4\u6bcf\u4e2a\u95ee\u9898\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u83b7\u5f97\u7b80\u6d01\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\uff1b4\uff09\u4f7f\u7528\u8fd9\u4e9b\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06\u538b\u7f29\u63a8\u7406\u884c\u4e3a\u5185\u5316\u5230\u5355\u95ee\u9898\u8bbe\u7f6e\u4e2d\u3002", "result": "\u4ec5\u4f7f\u75288k\u5fae\u8c03\u6837\u672c\uff0c\u5728MATH500\u4e0a\u51cf\u5c1159%\u63a8\u7406token\u4f7f\u7528\uff0c\u5728AIME25\u4e0a\u51cf\u5c1133%\u63a8\u7406token\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "ConPress\u901a\u8fc7\u5229\u7528\u591a\u95ee\u9898\u4e0a\u4e0b\u6587\u538b\u529b\u8bf1\u5bfc\u7684\u81ea\u6211\u538b\u7f29\u73b0\u8c61\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u7684\u81ea\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u6a21\u578b\u7684token\u4f7f\u7528\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00683", "abs": "https://arxiv.org/abs/2602.00683", "authors": ["Thong Thanh Nguyen"], "title": "Video Understanding: Through A Temporal Lens", "comment": "PhD Thesis, NUS, 2025", "summary": "This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using \"recurrent adapters\" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new \"temporal-oriented recipe\" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e94\u79cd\u65b9\u6cd5\u6539\u8fdb\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u65f6\u5e8f\u5173\u7cfb\u5efa\u6a21\uff1a\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u3001\u72b6\u6001\u7a7a\u95f4\u5c42\u96c6\u6210\u3001\u7ec6\u7c92\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4ee5\u53ca\u5bf9\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u9762\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5728\u5229\u7528\u89c6\u9891\u5143\u7d20\u95f4\u7684\u65f6\u5e8f\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65f6\u5e8f\u5efa\u6a21\u65b9\u6cd5\u6765\u6355\u6349\u89c6\u9891\u5185\u5bb9\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "1) \u4f7f\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u566a\u58f0\u9c81\u68d2\u5bf9\u6bd4\u5b66\u4e60\u7684\u81ea\u52a8\u6807\u6ce8\u6846\u67b6\uff1b2) \u53c2\u6570\u9ad8\u6548\u7684\"\u5faa\u73af\u9002\u914d\u5668\"\u5fae\u8c03\u7b56\u7565\uff1b3) \u96c6\u6210\u72b6\u6001\u7a7a\u95f4\u5c42\u8fdb\u884c\u957f\u89c6\u9891\u5efa\u6a21\uff1b4) \u5efa\u6a21\u8fd0\u52a8\u4e0e\u89c6\u9891\u7247\u6bb5\u7ec6\u7c92\u5ea6\u5173\u7cfb\u7684\u65b0\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff1b5) \u5bf9\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u9762\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u4e94\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u7684\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\uff08\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u548c\u957f\u7247\u5185\u5bb9\uff09\uff0c\u8bc1\u660e\u663e\u5f0f\u65f6\u5e8f\u5efa\u6a21\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u89c6\u9891\u5185\u5bb9\u7684\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u663e\u5f0f\u65f6\u5e8f\u5efa\u6a21\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u9891\u52a8\u6001\u5185\u5bb9\u7684\u8868\u793a\u548c\u63a8\u7406\u80fd\u529b\uff0c\u89c6\u89c9-\u8bed\u8a00\u63a5\u53e3\u662f\u65f6\u5e8f\u63a8\u7406\u7684\u74f6\u9888\uff0c\u9700\u8981\"\u65f6\u5e8f\u5bfc\u5411\"\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.01539", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01539", "abs": "https://arxiv.org/abs/2602.01539", "authors": ["Xiaoyu Wen", "Zhida He", "Han Qi", "Ziyu Wan", "Zhongtian Ma", "Ying Wen", "Tianhang Zheng", "Xingcheng Xu", "Chaochao Lu", "Qiaosheng Zhang"], "title": "MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety", "comment": null, "summary": "Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \\textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \\textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \\textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \\textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.", "AI": {"tldr": "MAGIC\u662f\u4e00\u4e2a\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u4e4b\u95f4\u7684\u5bf9\u6297\u6027\u535a\u5f08\u5b9e\u73b0LLM\u5b89\u5168\u5bf9\u9f50\uff0c\u65e0\u9700\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u9884\u6536\u96c6\u6570\u636e\u5206\u5e03\uff0c\u65e0\u6cd5\u8ddf\u4e0a\u4e0d\u65ad\u6f14\u5316\u7684\u5bf9\u6297\u653b\u51fb\u3002\u9700\u8981\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u5c06LLM\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5bf9\u6297\u6027\u975e\u5bf9\u79f0\u535a\u5f08\uff1a\u653b\u51fb\u8005\u667a\u80fd\u4f53\u5b66\u4e60\u8fed\u4ee3\u91cd\u5199\u67e5\u8be2\u4e3a\u6b3a\u9a97\u6027\u63d0\u793a\uff0c\u9632\u5fa1\u8005\u667a\u80fd\u4f53\u540c\u65f6\u4f18\u5316\u7b56\u7565\u8bc6\u522b\u5e76\u62d2\u7edd\u6b64\u7c7b\u8f93\u5165\uff0c\u5f62\u6210\u534f\u540c\u6f14\u5316\u8fc7\u7a0b\u3002", "result": "\u653b\u51fb\u8005\u6f14\u5316\u51fa\u65b0\u9896\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u9632\u5fa1\u8005\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u653b\u51fb\u6a21\u5f0f\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u5e2e\u52a9\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u7684\u9632\u5fa1\u6210\u529f\u7387\u3002", "conclusion": "MAGIC\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\u52a8\u6001\u5b89\u5168\u5bf9\u9f50\uff0c\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5206\u5e03\u3002"}}
{"id": "2602.00535", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00535", "abs": "https://arxiv.org/abs/2602.00535", "authors": ["Liyu Zerihun", "Alexandr Plashchinsky"], "title": "Invertible Memory Flow Networks", "comment": null, "summary": "Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of \"sweeper\" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.", "AI": {"tldr": "IMFN\u901a\u8fc7\u4e8c\u53c9\u6811\u5206\u89e3\u89e3\u51b3\u957f\u5e8f\u5217\u538b\u7f29\u95ee\u9898\uff0c\u907f\u514d\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u56f0\u96be\uff0c\u5b9e\u73b0O(log N)\u6df1\u5ea6\u548c\u4e9a\u7ebf\u6027\u8bef\u5dee\u79ef\u7d2f", "motivation": "\u957f\u5e8f\u5217\u795e\u7ecf\u8bb0\u5fc6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff1aRNN\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0cTransformer\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u800c\u5c06\u957f\u5e8f\u5217\u538b\u7f29\u4e3a\u56fa\u5b9a\u8868\u793a\u7531\u4e8e\u4f18\u5316\u56f0\u96be\u800c\u96be\u4ee5\u89e3\u51b3", "method": "\u4f7f\u7528\u53ef\u9006\u8bb0\u5fc6\u6d41\u7f51\u7edc\uff08IMFN\uff09\uff0c\u901a\u8fc7\u4e8c\u53c9\u6811\u5206\u89e3\u5c06\u957f\u5e8f\u5217\u538b\u7f29\u95ee\u9898\u5206\u89e3\u4e3a\u6210\u5bf9\u5408\u5e76\u4efb\u52a1\uff0c\u6bcf\u4e2a\"sweeper\"\u6a21\u5757\u5b66\u4e60\u7b80\u5355\u76842\u5bf91\u538b\u7f29\uff0c\u5b9e\u73b0O(log N)\u6df1\u5ea6", "result": "\u5728\u957fMNIST\u5e8f\u5217\u548cUCF-101\u89c6\u9891\u4e0a\u9a8c\u8bc1\u4e86IMFN\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u538b\u7f29\u9ad8\u7ef4\u6570\u636e\u7684\u957f\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u5b9e\u73b0O(1)\u987a\u5e8f\u63a8\u7406\u6b65\u9aa4", "conclusion": "IMFN\u901a\u8fc7\u5206\u89e3\u7b56\u7565\u4f7f\u957f\u5e8f\u5217\u538b\u7f29\u53d8\u5f97\u53ef\u884c\uff0c\u907f\u514d\u4e86\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u56f0\u96be\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01479", "abs": "https://arxiv.org/abs/2602.01479", "authors": ["Xueqing Peng", "Ruoyu Xiang", "Fan Zhang", "Mingzi Song", "Mingyang Jiang", "Yan Wang", "Lingfei Qian", "Taiki Hara", "Yuqing Guo", "Jimin Huang", "Junichi Tsujii", "Sophia Ananiadou"], "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance", "comment": null, "summary": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.", "AI": {"tldr": "Ebisu\u662f\u4e00\u4e2a\u9488\u5bf9\u65e5\u8bed\u91d1\u878d\u8bed\u8a00\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u4efb\u52a1\uff1aJF-ICR\uff08\u8bc4\u4f30\u6295\u8d44\u8005\u95ee\u7b54\u4e2d\u7684\u9690\u542b\u627f\u8bfa\u4e0e\u62d2\u7edd\u8bc6\u522b\uff09\u548cJF-TE\uff08\u8bc4\u4f30\u4e13\u4e1a\u62ab\u9732\u4e2d\u5d4c\u5957\u91d1\u878d\u672f\u8bed\u7684\u5206\u5c42\u63d0\u53d6\u4e0e\u6392\u5e8f\uff09\u3002", "motivation": "\u65e5\u8bed\u91d1\u878d\u8bed\u8a00\u7ed3\u5408\u4e86\u7c98\u7740\u8bed\u3001\u5934\u5c3e\u7ed3\u6784\u3001\u6df7\u5408\u4e66\u5199\u7cfb\u7edf\u4ee5\u53ca\u4f9d\u8d56\u95f4\u63a5\u8868\u8fbe\u548c\u9690\u542b\u627f\u8bfa\u7684\u9ad8\u8bed\u5883\u6c9f\u901a\u89c4\u8303\uff0c\u8fd9\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u548c\u5904\u7406\u8fd9\u79cd\u590d\u6742\u7684\u8bed\u8a00\u6587\u5316\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u521b\u5efa\u4e86Ebisu\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1a1) JF-ICR\uff1a\u8bc4\u4f30\u6295\u8d44\u8005\u95ee\u7b54\u4e2d\u7684\u9690\u542b\u627f\u8bfa\u4e0e\u62d2\u7edd\u8bc6\u522b\uff1b2) JF-TE\uff1a\u8bc4\u4f30\u4e13\u4e1a\u62ab\u9732\u4e2d\u5d4c\u5957\u91d1\u878d\u672f\u8bed\u7684\u5206\u5c42\u63d0\u53d6\u4e0e\u6392\u5e8f\u3002\u5bf9\u5f00\u6e90\u548c\u4e13\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u901a\u7528\u6a21\u578b\u3001\u65e5\u8bed\u9002\u5e94\u6a21\u578b\u548c\u91d1\u878d\u4e13\u7528\u6a21\u578b\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002\u867d\u7136\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u5e26\u6765\u6709\u9650\u7684\u6539\u8fdb\uff0c\u4f46\u8bed\u8a00\u548c\u9886\u57df\u7279\u5b9a\u7684\u9002\u5e94\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u63d0\u9ad8\u6027\u80fd\uff0c\u4ecd\u7136\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "Ebisu\u4e3a\u63a8\u8fdb\u57fa\u4e8e\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u7684\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u805a\u7126\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u811a\u672c\u90fd\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.00687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00687", "abs": "https://arxiv.org/abs/2602.00687", "authors": ["Yuankun Zeng", "Shaohui Li", "Zhi Li", "Shulan Ruan", "Yu Liu", "You He"], "title": "V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication", "comment": null, "summary": "Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.", "AI": {"tldr": "V2X-DSC\uff1a\u57fa\u4e8e\u5206\u5e03\u5f0f\u4fe1\u6e90\u7f16\u7801\u7684V2X\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u7f16\u89e3\u7801\u5668\u5728\u5e26\u5bbd\u53d7\u9650\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u878d\u5408", "motivation": "\u534f\u540c\u611f\u77e5\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7279\u5f81\u878d\u5408\u63d0\u53473D\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4e2d\u95f4\u7279\u5f81\u5171\u4eab\u9762\u4e34\u4e25\u683c\u5e26\u5bbd\u9650\u5236\uff0c\u5bc6\u96c6BEV\u7279\u5f81\u4f1a\u9971\u548cV2X\u94fe\u8def\u3002\u89c2\u5bdf\u5230\u534f\u4f5c\u65b9\u89c2\u6d4b\u540c\u4e00\u7269\u7406\u4e16\u754c\uff0c\u5176\u7279\u5f81\u9ad8\u5ea6\u76f8\u5173\uff0c\u63a5\u6536\u65b9\u53ea\u9700\u83b7\u53d6\u8d85\u51fa\u672c\u5730\u4e0a\u4e0b\u6587\u7684\u65b0\u4fe1\u606f\u3002", "method": "\u4ece\u5206\u5e03\u5f0f\u4fe1\u6e90\u7f16\u7801\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u51faV2X-DSC\u6846\u67b6\uff0c\u5305\u542b\u6761\u4ef6\u7f16\u89e3\u7801\u5668(DCC)\u3002\u53d1\u9001\u65b9\u5c06BEV\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u7f16\u7801\uff0c\u63a5\u6536\u65b9\u4ee5\u672c\u5730\u7279\u5f81\u4f5c\u4e3a\u8fb9\u4fe1\u606f\u8fdb\u884c\u6761\u4ef6\u91cd\u5efa\uff0c\u5c06\u6bd4\u7279\u5206\u914d\u7ed9\u4e92\u8865\u7ebf\u7d22\u800c\u975e\u5197\u4f59\u5185\u5bb9\u3002\u8fd9\u79cd\u6761\u4ef6\u7ed3\u6784\u6b63\u5219\u5316\u5b66\u4e60\uff0c\u9f13\u52b1\u589e\u91cf\u8868\u793a\u5e76\u4ea7\u751f\u4f4e\u566a\u58f0\u7279\u5f81\u3002", "result": "\u5728DAIR-V2X\u3001OPV2V\u548cV2X-Real\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u5728KB\u7ea7\u901a\u4fe1\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6-\u5e26\u5bbd\u6743\u8861\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u901a\u4fe1\u5c42\u6cdb\u5316\u5230\u591a\u79cd\u878d\u5408\u9aa8\u5e72\u7f51\u7edc\u4e2d\u3002", "conclusion": "V2X-DSC\u901a\u8fc7\u6761\u4ef6\u7f16\u89e3\u7801\u5668\u6709\u6548\u89e3\u51b3\u4e86V2X\u534f\u540c\u611f\u77e5\u4e2d\u7684\u5e26\u5bbd\u9650\u5236\u95ee\u9898\uff0c\u5229\u7528\u7279\u5f81\u76f8\u5173\u6027\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2602.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01550", "abs": "https://arxiv.org/abs/2602.01550", "authors": ["S1-NexusAgent Team"], "title": "S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research", "comment": "In progress", "summary": "Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.", "AI": {"tldr": "S1-NexusAgent\uff1a\u9762\u5411\u591a\u5b66\u79d1\u79d1\u5b66\u7814\u7a76\u7684\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212-\u4ee3\u7801\u6267\u884c\u8303\u5f0f\u3001MCP\u534f\u8bae\u96c6\u6210\u3001\u7a00\u758f\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u6280\u80fd\u84b8\u998f\u673a\u5236\uff0c\u5728\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u548c\u5de5\u5177\u578b\u667a\u80fd\u4f53\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u548c\u4e13\u7528\u5de5\u5177\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u5305\u62ec\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u4e0d\u8db3\u3001\u76ee\u6807\u7ef4\u62a4\u4e0d\u9c81\u68d2\u3001\u7f3a\u4e4f\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u79d1\u5b66\u7814\u7a76\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5c42Plan-and-CodeAct\u6267\u884c\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u67b6\u6784\u89e3\u8026\u5168\u5c40\u79d1\u5b66\u89c4\u5212\u548c\u5b50\u4efb\u52a1\u5de5\u5177\u6267\u884c\uff1b\u96c6\u6210Model Context Protocol\u652f\u6301\u6570\u5343\u8de8\u5b66\u79d1\u79d1\u5b66\u5de5\u5177\uff1b\u5f15\u5165\u57fa\u4e8e\u5bf9\u8c61\u5f15\u7528\u7684\u7a00\u758f\u4e0a\u4e0b\u6587\u7ba1\u7406\uff1b\u901a\u8fc7Critic Agent\u8bc4\u4f30\u6267\u884c\u8f68\u8ff9\u5e76\u84b8\u998f\u9ad8\u8d28\u91cf\u7814\u7a76\u8def\u5f84\u4e3a\u53ef\u91cd\u7528Scientific Skills\u3002", "result": "\u5728\u751f\u7269\u3001\u5316\u5b66\u3001\u6750\u6599\u79d1\u5b66\u7b49\u6743\u5a01\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08biomini-eval\u3001ChemBench\u3001MatSciBench\uff09\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5728\u590d\u6742\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "S1-NexusAgent\u901a\u8fc7\u81ea\u8fdb\u5316\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u79d1\u5b66\u7814\u7a76\u7684\u590d\u6742\u6311\u6218\uff0c\u4e3a\u53ef\u6301\u7eed\u3001\u957f\u65f6\u7a0b\u7684\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8de8\u5b66\u79d1\u79d1\u5b66\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u89c4\u5212\u3001\u6267\u884c\u548c\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2602.00539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00539", "abs": "https://arxiv.org/abs/2602.00539", "authors": ["Xinmo Jin", "Bowen Fan", "Xunkai Li", "Henan Sun", "YuXin Zeng", "Zekai Chen", "Yuxuan Sun", "Jia Li", "Qiangqiang Dai", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang"], "title": "OpenDDI: A Comprehensive Benchmark for DDI Prediction", "comment": null, "summary": "Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI", "AI": {"tldr": "OpenDDI\u662f\u4e00\u4e2a\u7528\u4e8e\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7edf\u4e00\u4e86\u591a\u4e2a\u6570\u636e\u96c6\u3001\u836f\u7269\u8868\u793a\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u63d0\u4f9b\u4e8620\u4e2aSOTA\u6a21\u578b\u57fa\u51c6\u548c10\u4e2a\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\uff08\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5355\u6a21\u6001\u836f\u7269\u8868\u793a\uff09\uff1b2\uff09\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\uff08\u4e0d\u4e00\u81f4\u7684\u573a\u666f\u3001\u6307\u6807\u548c\u57fa\u51c6\uff09\u3002\u8fd9\u4e9b\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u63d0\u51faOpenDDI\u57fa\u51c6\uff0c\u4ece\u4e24\u4e2a\u89d2\u5ea6\u89e3\u51b3\u95ee\u9898\uff1a1\uff09\u6570\u636e\u89d2\u5ea6\uff1a\u7edf\u4e006\u4e2a\u5e38\u7528DDI\u6570\u636e\u96c6\u548c2\u79cd\u73b0\u6709\u836f\u7269\u8868\u793a\uff0c\u65b0\u589e3\u4e2a\u5927\u89c4\u6a21LLM\u589e\u5f3a\u6570\u636e\u96c6\u548c\u8986\u76d65\u79cd\u6a21\u6001\u7684\u591a\u6a21\u6001\u836f\u7269\u8868\u793a\uff1b2\uff09\u8bc4\u4f30\u89d2\u5ea6\uff1a\u7edf\u4e0020\u4e2aSOTA\u6a21\u578b\u57fa\u51c6\uff0c\u6db5\u76d63\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u63d0\u4f9b\u6570\u636e\u8d28\u91cf\u3001\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u7684\u6807\u51c6\u5316\u534f\u8bae\u3002", "result": "\u57fa\u4e8eOpenDDI\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5f97\u51fa\u4e8610\u4e2a\u6709\u4ef7\u503c\u7684DDI\u9884\u6d4b\u89c1\u89e3\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u9650\u5236\uff0c\u4e3a\u8fd9\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u3002", "conclusion": "OpenDDI\u4e3aDDI\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u6807\u51c6\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.01511", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01511", "abs": "https://arxiv.org/abs/2602.01511", "authors": ["Ran Xu", "Tianci Liu", "Zihan Dong", "Tony You", "Ilgee Hong", "Carl Yang", "Linjun Zhang", "Tao Zhao", "Haoyu Wang"], "title": "Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training", "comment": "The first two authors contributed equally", "summary": "Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.", "AI": {"tldr": "Rubric-ARM\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u8bc4\u5206\u6807\u51c6\u751f\u6210\u5668\u548c\u8bc4\u5224\u5668\uff0c\u89e3\u51b3\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5728\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982\u521b\u610f\u5199\u4f5c\uff09\u4e2d\u5355\u4e00\u6807\u91cf\u8bc4\u5206\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u4f7f\u7528\u6807\u91cf\u5206\u6570\u9884\u6d4b\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982\u521b\u610f\u5199\u4f5c\u3001\u5f00\u653e\u5f0f\u6307\u4ee4\u9075\u5faa\uff09\u4e2d\u54cd\u5e94\u8d28\u91cf\u7684\u591a\u7ef4\u5ea6\u7279\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8bc4\u5206\u6807\u51c6\u6216\u5206\u79bb\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faRubric-ARM\u6846\u67b6\uff0c\u5c06\u8bc4\u5206\u6807\u51c6\u751f\u6210\u89c6\u4e3a\u6f5c\u5728\u52a8\u4f5c\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u504f\u597d\u53cd\u9988\u4e2d\u8054\u5408\u4f18\u5316\u8bc4\u5206\u6807\u51c6\u751f\u6210\u5668\u548c\u8bc4\u5224\u5668\u3002\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b56\u7565\u7f13\u89e3\u540c\u65f6\u66f4\u65b0\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8be5\u7b56\u7565\u80fd\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7684\u4e0b\u6e38\u7b56\u7565\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "Rubric-ARM\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bc4\u5206\u6807\u51c6\u751f\u6210\u548c\u8bc4\u5224\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\u4e2d\u54cd\u5e94\u8d28\u91cf\u8bc4\u4f30\u7684\u591a\u7ef4\u5ea6\u95ee\u9898\uff0c\u4e3a\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00702", "abs": "https://arxiv.org/abs/2602.00702", "authors": ["Ruikui Wang", "Jinheng Feng", "Lang Tian", "Huaishao Luo", "Chaochao Li", "Liangbo Zhou", "Huan Zhang", "Youzheng Wu", "Xiaodong He"], "title": "JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning", "comment": null, "summary": "Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.", "AI": {"tldr": "JoyAvatar\u662f\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u957f\u65f6\u95f4\u3001\u6587\u672c\u53ef\u63a7\u7684\u865a\u62df\u4eba\u89c6\u9891\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6559\u5e08\u589e\u5f3a\u8bad\u7ec3\u7b97\u6cd5\u548c\u591a\u6a21\u6001\u6761\u4ef6\u52a8\u6001\u8c03\u5236\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u52a8\u4f5c\u3001\u76f8\u673a\u8f68\u8ff9\u548c\u80cc\u666f\u8f6c\u6362\u7684\u6587\u672c\u5bf9\u9f50\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u4eba\u6a21\u578b\u5728\u8bf4\u8bdd\u3001\u516c\u5f00\u6f14\u8bb2\u548c\u5531\u6b4c\u7b49\u573a\u666f\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6587\u672c\u6307\u4ee4\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u6d89\u53ca\u590d\u6742\u5143\u7d20\u5982\u5927\u5e45\u5168\u8eab\u8fd0\u52a8\u3001\u52a8\u6001\u76f8\u673a\u8f68\u8ff9\u3001\u80cc\u666f\u8f6c\u6362\u6216\u4eba\u673a\u4ea4\u4e92\u65f6\u3002\u9700\u8981\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faJoyAvatar\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u53cc\u6559\u5e08\u589e\u5f3a\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4ece\u57fa\u7840\u6a21\u578b\u8fc1\u79fb\u6587\u672c\u53ef\u63a7\u6027\u540c\u65f6\u5b66\u4e60\u89c6\u542c\u540c\u6b65\uff1b2) \u8bad\u7ec3\u65f6\u57fa\u4e8e\u4e0d\u540c\u53bb\u566a\u65f6\u95f4\u6b65\u52a8\u6001\u8c03\u5236\u591a\u6a21\u6001\u6761\u4ef6\uff08\u5982\u97f3\u9891\u548c\u6587\u672c\uff09\u7684\u5f3a\u5ea6\uff0c\u7f13\u89e3\u5f02\u8d28\u6761\u4ef6\u4fe1\u53f7\u95f4\u7684\u51b2\u7a81\u3002", "result": "GSB\u8bc4\u4f30\u663e\u793aJoyAvatar\u4f18\u4e8eOmnihuman-1.5\u548cKlingAvatar 2.0\u7b49\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u81ea\u7136\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u5168\u8eab\u8fd0\u52a8\u548c\u52a8\u6001\u76f8\u673a\u79fb\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7684\u5507\u5f62\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u652f\u6301\u591a\u4eba\u5bf9\u8bdd\u548c\u975e\u4eba\u7c7b\u89d2\u8272\u626e\u6f14\u7b49\u590d\u6742\u5e94\u7528\u3002", "conclusion": "JoyAvatar\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b97\u6cd5\u548c\u6761\u4ef6\u8c03\u5236\u673a\u5236\uff0c\u663e\u8457\u6269\u5c55\u4e86\u865a\u62df\u4eba\u6a21\u578b\u751f\u6210\u590d\u6742\u6587\u672c\u5bf9\u9f50\u89c6\u9891\u7684\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u57fa\u672c\u865a\u62df\u4eba\u529f\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u573a\u666f\u53d8\u5316\u3002"}}
{"id": "2602.01556", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01556", "abs": "https://arxiv.org/abs/2602.01556", "authors": ["Hong Su"], "title": "Autonomous Question Formation for Large Language Model-Driven AI Systems", "comment": null, "summary": "Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u6a21\u62df\u7684AI\u6846\u67b6\uff0c\u8ba9AI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u5e76\u8bbe\u5b9a\u4efb\u52a1\uff0c\u901a\u8fc7\u63a8\u7406\u5185\u90e8\u72b6\u6001\u3001\u73af\u5883\u89c2\u5bdf\u548c\u4e0e\u5176\u4ed6AI\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u4efb\u52a1\u9009\u62e9\u548c\u6267\u884c\u7684\u5148\u51b3\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u9884\u5b9a\u4e49\u4efb\u52a1\u548c\u56fa\u5b9a\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73af\u5883\u53d8\u5316\u65f6\u81ea\u4e3b\u8bc6\u522b\u5e94\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002\u9700\u8981\u8ba9AI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u548c\u8bbe\u5b9a\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4eba\u7c7b\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u9996\u8981\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6574\u5408\u5185\u90e8\u9a71\u52a8\u3001\u73af\u5883\u611f\u77e5\u548c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u7684\u63d0\u793a\u8303\u56f4\uff0c\u9010\u6b65\u6269\u5c55\u8ba4\u77e5\u8986\u76d6\uff0c\u5e76\u652f\u6301\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u95ee\u9898\u5f62\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u73af\u5883\u611f\u77e5\u63d0\u793a\u76f8\u6bd4\u5185\u90e8\u9a71\u52a8\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u65e0\u8fdb\u98df\u4e8b\u4ef6\uff0c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u63d0\u793a\u572820\u5929\u6a21\u62df\u4e2d\u8fdb\u4e00\u6b65\u51cf\u5c11\u7d2f\u8ba1\u65e0\u8fdb\u98df\u4e8b\u4ef6\u8d85\u8fc760%\uff0c\u7edf\u8ba1\u663e\u8457\u6539\u8fdb(p<0.05)\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7fAI\u7cfb\u7edf\u80fd\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u548c\u8bbe\u5b9a\u4efb\u52a1\uff0c\u63d0\u9ad8\u5728\u52a8\u6001\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u8d28\u91cf\uff0c\u901a\u8fc7\u591a\u7ea7\u63d0\u793a\u8303\u56f4\u6709\u6548\u6539\u5584\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.00541", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00541", "abs": "https://arxiv.org/abs/2602.00541", "authors": ["Zilin Jing", "Vincent Jeanselme", "Yuta Kobayashi", "Simon A. Lee", "Chao Pang", "Aparajita Kashyap", "Yanwei Li", "Xinzhuo Jiang", "Shalmali Joshi"], "title": "One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models", "comment": null, "summary": "Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability", "AI": {"tldr": "ORA\uff1a\u4e00\u79cd\u65b0\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u57fa\u7840\u6a21\u578b\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u65f6\u95f4\u548c\u76f8\u5173\u6d4b\u91cf\u503c\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u8868\u793a", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e34\u5e8a\u4e8b\u4ef6\u662f\u4e0d\u89c4\u5219\u91c7\u6837\u7684\uff0c\u5305\u542b\u79bb\u6563\u4e8b\u4ef6\u548c\u6570\u503c\u6d4b\u91cf\u503c\u7684\u6df7\u5408\u3002\u867d\u7136EHR\u5177\u6709\u5e8f\u5217\u6027\u8d28\uff0c\u7c7b\u4f3c\u4e8e\u81ea\u7136\u8bed\u8a00\uff0c\u4f46\u4f20\u7edf\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u8bad\u7ec3\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349EHR\u7684\u5b8c\u6574\u7ed3\u6784\uff0c\u7279\u522b\u662f\u8fde\u7eed\u6d4b\u91cf\u503c\u548c\u4e8b\u4ef6\u65f6\u95f4\u4fe1\u606f", "method": "\u63d0\u51faORA\uff08marked time-to-event\uff09\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u65f6\u95f4\u548c\u76f8\u5173\u6d4b\u91cf\u503c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8003\u8651\u4e8b\u4ef6\u53d1\u751f\u7684\u65f6\u95f4\uff0c\u8fd8\u8003\u8651\u4e0e\u4e8b\u4ef6\u76f8\u5173\u7684\u8fde\u7eed\u6d4b\u91cf\u503c\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349EHR\u7684\u5b8c\u6574\u7ed3\u6784", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u4e0b\u6e38\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u4e0a\uff0cORA\u76ee\u6807\u76f8\u6bd4\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u548c\u5ffd\u7565\u8fde\u7eed\u6d4b\u91cf\u503c\u7684\u9884\u8bad\u7ec3\u635f\u5931\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u8868\u793a\u3002\u91cd\u8981\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6539\u5584\u4e86\u4f20\u7edf\u5206\u7c7b\u8bc4\u4f30\uff0c\u8fd8\u5728\u56de\u5f52\u548c\u65f6\u95f4\u5230\u4e8b\u4ef6\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u597d", "conclusion": "ORA\u4e0d\u4ec5\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u66f4\u91cd\u8981\u7684\u662f\u8868\u660e\uff1a\u8003\u8651EHR\u7ed3\u6784\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u5bf9\u4e8e\u6269\u5c55\u4e0b\u6e38\u80fd\u529b\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u4e3aEHR\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411"}}
{"id": "2602.01560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01560", "abs": "https://arxiv.org/abs/2602.01560", "authors": ["Keito Inoshita", "Michiaki Omura", "Tsukasa Yamanaka", "Go Maeda", "Kentaro Tsuji"], "title": "Argument Rarity-based Originality Assessment for AI-Assisted Writing", "comment": null, "summary": "As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAROA\u6846\u67b6\uff0c\u57fa\u4e8e\u8bba\u8bc1\u7a00\u6709\u6027\u81ea\u52a8\u8bc4\u4f30\u5b66\u751f\u8bae\u8bba\u6587\u7684\u539f\u521b\u6027\uff0c\u5c06\u539f\u521b\u6027\u5b9a\u4e49\u4e3a\u53c2\u8003\u8bed\u6599\u5e93\u4e2d\u7684\u7a00\u6709\u7a0b\u5ea6\uff0c\u5305\u542b\u7ed3\u6784\u3001\u8bba\u70b9\u3001\u8bc1\u636e\u548c\u8ba4\u77e5\u6df1\u5ea6\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u53d1\u73b0\u8d28\u91cf\u4e0e\u539f\u521b\u6027\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff0cAI\u8bba\u6587\u5728\u7ed3\u6784\u590d\u6742\u5ea6\u4e0a\u53ef\u4e0e\u4eba\u7c7b\u5ab2\u7f8e\u4f46\u8bba\u70b9\u539f\u521b\u6027\u8f83\u4f4e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u8f7b\u677e\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\uff0c\u4f20\u7edf\u7684\u8d28\u91cf\u5bfc\u5411\u5199\u4f5c\u8bc4\u4f30\u6b63\u5728\u5931\u53bb\u610f\u4e49\u3002\u5982\u679c\u6559\u80b2\u7684\u6838\u5fc3\u76ee\u6807\u662f\u57f9\u517b\u6279\u5224\u6027\u601d\u7ef4\u548c\u539f\u521b\u89c2\u70b9\uff0c\u8bc4\u4f30\u8303\u5f0f\u5fc5\u987b\u4ece\u8d28\u91cf\u8f6c\u5411\u539f\u521b\u6027\u3002", "method": "\u63d0\u51fa\u8bba\u8bc1\u7a00\u6709\u6027\u539f\u521b\u6027\u8bc4\u4f30\uff08AROA\uff09\u6846\u67b6\uff0c\u5c06\u539f\u521b\u6027\u5b9a\u4e49\u4e3a\u53c2\u8003\u8bed\u6599\u5e93\u4e2d\u7684\u7a00\u6709\u7a0b\u5ea6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e92\u8865\u7ec4\u4ef6\u8bc4\u4f30\uff1a\u7ed3\u6784\u7a00\u6709\u6027\u3001\u8bba\u70b9\u7a00\u6709\u6027\u3001\u8bc1\u636e\u7a00\u6709\u6027\u548c\u8ba4\u77e5\u6df1\u5ea6\u3002\u4f7f\u7528\u5bc6\u5ea6\u4f30\u8ba1\u91cf\u5316\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u7a00\u6709\u6027\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u8c03\u6574\u673a\u5236\u6574\u5408\uff0c\u5c06\u8d28\u91cf\u548c\u539f\u521b\u6027\u89c6\u4e3a\u72ec\u7acb\u8bc4\u4f30\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u8d28\u91cf\u4e0e\u8bba\u70b9\u7a00\u6709\u6027\u5b58\u5728\u5f3a\u8d1f\u76f8\u5173\uff0c\u8868\u660e\u8d28\u91cf\u4e0e\u539f\u521b\u6027\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff1a\u66f4\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u503e\u5411\u4e8e\u4f9d\u8d56\u5178\u578b\u7684\u8bba\u70b9\u6a21\u5f0f\u3002AI\u8bba\u6587\u5728\u7ed3\u6784\u590d\u6742\u5ea6\u4e0a\u53ef\u4e0e\u4eba\u7c7b\u8bba\u6587\u5ab2\u7f8e\uff0c\u4f46\u5176\u8bba\u70b9\u7a00\u6709\u6027\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u8868\u660eLLM\u80fd\u590d\u73b0\u8bba\u8bc1\u5f62\u5f0f\u4f46\u5728\u5185\u5bb9\u539f\u521b\u6027\u4e0a\u6709\u9650\u5236\u3002", "conclusion": "AROA\u6846\u67b6\u4e3a\u81ea\u52a8\u8bc4\u4f30\u8bae\u8bba\u6587\u539f\u521b\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u8d28\u91cf\u4e0e\u539f\u521b\u6027\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u663e\u793a\u5f53\u524dLLM\u5728\u8bba\u8bc1\u5f62\u5f0f\u6a21\u4eff\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u5728\u5185\u5bb9\u539f\u521b\u6027\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u4e3a\u6559\u80b2\u8bc4\u4f30\u8303\u5f0f\u8f6c\u53d8\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2602.00703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00703", "abs": "https://arxiv.org/abs/2602.00703", "authors": ["Zhongtian Huang", "Zhi Chen", "Zi Huang", "Xin Yu", "Daniel Smith", "Chaitanya Purushothama", "Erik Van Oosterom", "Alex Wu", "William Salter", "Yan Li", "Scott Chapman"], "title": "StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components", "comment": null, "summary": "Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $\u03bc$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\u7528\u4e8e\u9ad8\u7cb1\u6c14\u5b54\u5206\u6790\uff0c\u7ed3\u5408\u8865\u4e01\u9884\u5904\u7406\u548c\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5fae\u5c0f\u6c14\u5b54\u7ed3\u6784\u5206\u5272\u6027\u80fd", "motivation": "\u9ad8\u7cb1\u4f5c\u4e3a\u8010\u65f1\u4f5c\u7269\u5bf9\u6c14\u5019\u9002\u5e94\u6027\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6c14\u5b54\u81ea\u52a8\u5206\u6790\u56f0\u96be\uff0c\u56e0\u4e3a\u6c14\u5b54\u5fae\u5c0f\uff08<40\u03bcm\uff09\u4e14\u5f62\u72b6\u591a\u53d8\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5d4c\u5957\u5c0f\u7ed3\u6784\u548c\u6807\u6ce8\u74f6\u9888\u7684\u6311\u6218", "method": "\u6536\u96c6\u5e76\u6807\u6ce811,060\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u7cb1\u53f6\u7247\u56fe\u50cf\u8865\u4e01\uff0c\u8986\u76d6\u591a\u79cd\u57fa\u56e0\u578b\u548c\u53f6\u8868\u9762\u7684\u4e09\u4e2a\u6c14\u5b54\u7ec4\u4ef6\uff1b\u5c06\u9ad8\u5206\u8fa8\u7387\u663e\u5fae\u56fe\u50cf\u5206\u5272\u4e3a\u91cd\u53e0\u5c0f\u8865\u4e01\uff1b\u5e94\u7528\u4f2a\u6807\u7b7e\u7b56\u7565\u751f\u621056,428\u4e2a\u4f2a\u6807\u6ce8\u8865\u4e01\uff1b\u5efa\u7acb\u534a\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u6846\u67b6", "result": "\u8bed\u4e49\u5206\u5272\u6a21\u578b\u6700\u4f73mIoU\u4ece65.93%\u63d0\u5347\u81f370.35%\uff0c\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u6700\u4f73AP\u4ece28.30%\u63d0\u5347\u81f346.10%\uff0c\u8bc1\u660e\u8865\u4e01\u9884\u5904\u7406\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u663e\u8457\u6539\u5584\u7cbe\u7ec6\u6c14\u5b54\u7ed3\u6784\u5206\u5272", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u652f\u6301\u53ef\u6269\u5c55\u7684\u6c14\u5b54\u7279\u5f81\u63d0\u53d6\uff0c\u4fc3\u8fdbAI\u9a71\u52a8\u8868\u578b\u5206\u6790\u5728\u4f5c\u7269\u79d1\u5b66\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u9ad8\u901a\u91cf\u6c14\u5b54\u8868\u578b\u5206\u6790\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01608", "abs": "https://arxiv.org/abs/2602.01608", "authors": ["Mu Yuan", "Liekang Zeng", "Guoliang Xing", "Lan Zhang", "Yunhao Liu"], "title": "Reasoning with Autoregressive-Diffusion Collaborative Thoughts", "comment": null, "summary": "Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.", "AI": {"tldr": "\u63d0\u51faCollaborative Thoughts\u6846\u67b6\uff0c\u8ba9\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u901a\u8fc7\u95ed\u73af\u4ea4\u4e92\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u8fdb\u884c\u63a8\u7406\u548c\u751f\u6210", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u64c5\u957f\u5e8f\u5217\u89c4\u5212\u548c\u7ea6\u675f\u7ec4\u5408\uff0c\u4f46\u5728\u9700\u8981\u660e\u786e\u7a7a\u95f4\u6216\u7269\u7406\u57fa\u7840\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u6269\u6563\u6a21\u578b\u80fd\u6355\u6349\u4e30\u5bcc\u7684\u7a7a\u95f4\u7ed3\u6784\uff0c\u4f46\u7f3a\u4e4f\u9010\u6b65\u903b\u8f91\u63a7\u5236\u6765\u6ee1\u8db3\u590d\u6742\u591a\u9636\u6bb5\u7ea6\u675f\u6216\u53ef\u9760\u8bc6\u522b\u7ea0\u6b63\u9519\u8bef\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "Collaborative Thoughts\u6846\u67b6\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u8d1f\u8d23\u7ed3\u6784\u5316\u89c4\u5212\u548c\u7ea6\u675f\u7ba1\u7406\uff0c\u6269\u6563\u6a21\u578b\u5c06\u8fd9\u4e9b\u7ea6\u675f\u5b9e\u4f8b\u5316\u4e3a\u4e2d\u95f4\u89c6\u89c9\u601d\u7ef4\uff0c\u89c6\u89c9\u6279\u8bc4\u6a21\u5757\u8bc4\u4f30\u89c6\u89c9\u601d\u7ef4\u662f\u5426\u6ee1\u8db3\u7ed3\u6784\u548c\u7269\u7406\u8981\u6c42\uff0c\u53cd\u9988\u7528\u4e8e\u8fed\u4ee3\u4f18\u5316\u540e\u7eed\u89c4\u5212\u548c\u751f\u6210\u6b65\u9aa4", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u793a\u4f8b\u5c55\u793a\u4e86Collaborative Thoughts\u5982\u4f55\u63d0\u9ad8\u7a7a\u95f4\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u751f\u6210\u7684\u53ef\u63a7\u6027\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u534f\u4f5c\u5faa\u73af\u5904\u7406\u81ea\u56de\u5f52\u95ee\u7b54\u548c\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u89c9\u751f\u6210\u4efb\u52a1", "conclusion": "Collaborative Thoughts\u6846\u67b6\u901a\u8fc7\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408\u4e86\u5e8f\u5217\u89c4\u5212\u80fd\u529b\u548c\u7a7a\u95f4\u751f\u6210\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027"}}
{"id": "2602.00545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00545", "abs": "https://arxiv.org/abs/2602.00545", "authors": ["Shenyang Deng", "Boyao Liao", "Zhuoli Ouyang", "Tianyu Pang", "Yaoqing Yang"], "title": "Depth, Not Data: An Analysis of Hessian Spectral Bifurcation", "comment": null, "summary": "The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.\n  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u7684\"bulk-and-spike\"\u8c31\u7ed3\u6784\uff08\u5c11\u6570\u4e3b\u5bfc\u7279\u5f81\u503c\u4e0e\u5927\u91cf\u5c0f\u7279\u5f81\u503c\u5206\u79bb\uff09\u53ef\u4ee5\u7eaf\u7cb9\u7531\u7f51\u7edc\u67b6\u6784\u5f15\u8d77\uff0c\u800c\u975e\u6570\u636e\u534f\u65b9\u5dee\u77e9\u9635\u4e0d\u5e73\u8861\u6240\u81f4\u3002", "motivation": "\u4f20\u7edf\u7814\u7a76\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u7684\"bulk-and-spike\"\u8c31\u7ed3\u6784\u5f52\u56e0\u4e8e\u6570\u636e\u534f\u65b9\u5dee\u77e9\u9635\u7684\u4e0d\u5e73\u8861\u3002\u672c\u6587\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7a76\u7f51\u7edc\u67b6\u6784\u672c\u8eab\u662f\u5426\u4e5f\u80fd\u4ea7\u751f\u8fd9\u79cd\u8c31\u5206\u5c94\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u8bbe\u7f6e\u8fdb\u884c\u5206\u6790\uff0c\u5373\u4f7f\u5728\u6570\u636e\u534f\u65b9\u5dee\u5b8c\u5168\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660eHessian\u77e9\u9635\u4ecd\u7136\u8868\u73b0\u51fa\u8c31\u5206\u5c94\u7ed3\u6784\uff1a\u4e00\u4e2a\u4e3b\u5bfc\u7279\u5f81\u503c\u7c07\u548c\u4e00\u4e2a\u4e3b\u4f53\u7279\u5f81\u503c\u7c07\u3002\u5173\u952e\u53d1\u73b0\u662f\u4e3b\u5bfc\u7279\u5f81\u503c\u4e0e\u4e3b\u4f53\u7279\u5f81\u503c\u7684\u6bd4\u503c\u968f\u7f51\u7edc\u6df1\u5ea6\u7ebf\u6027\u589e\u957f\u3002", "result": "\u8bc1\u660e\u8c31\u5206\u5c94\u7ed3\u6784\u53ef\u4ee5\u7eaf\u7cb9\u7531\u7f51\u7edc\u67b6\u6784\u5f15\u8d77\uff0c\u72ec\u7acb\u4e8e\u6570\u636e\u4e0d\u5e73\u8861\u3002\u4e3b\u5bfc\u7279\u5f81\u503c\u4e0e\u4e3b\u4f53\u7279\u5f81\u503c\u7684\u6bd4\u503c\u4e0e\u7f51\u7edc\u6df1\u5ea6\u5448\u7ebf\u6027\u6bd4\u4f8b\u5173\u7cfb\uff0c\u8868\u660e\u8c31\u95f4\u9699\u53d7\u7f51\u7edc\u67b6\u6784\u5f3a\u70c8\u5f71\u54cd\u3002", "conclusion": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u666f\u89c2\u4e0d\u4ec5\u53d7\u6570\u636e\u5206\u5e03\u5f71\u54cd\uff0c\u4e5f\u53d7\u7f51\u7edc\u67b6\u6784\u5f71\u54cd\u3002\u8bbe\u8ba1\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316\u7b97\u6cd5\u65f6\u5e94\u540c\u65f6\u8003\u8651\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u7279\u6027\u3002"}}
{"id": "2602.01566", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01566", "abs": "https://arxiv.org/abs/2602.01566", "authors": ["Chiwei Zhu", "Benfeng Xu", "Mingxuan Du", "Shaohan Wang", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents", "comment": "19 pages, 6 figures", "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.", "AI": {"tldr": "FS-Researcher\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6269\u5c55\u7814\u7a76\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4f5c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u4ee3\u8868\u6027\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5176\u957f\u8f68\u8ff9\u7ecf\u5e38\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u538b\u7f29\u4e86\u8bc1\u636e\u6536\u96c6\u548c\u62a5\u544a\u7f16\u5199\u7684token\u9884\u7b97\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1aContext Builder\u667a\u80fd\u4f53\u4f5c\u4e3a\u56fe\u4e66\u7ba1\u7406\u5458\u6d4f\u89c8\u4e92\u8054\u7f51\u3001\u7f16\u5199\u7ed3\u6784\u5316\u7b14\u8bb0\u3001\u5c06\u539f\u59cb\u8d44\u6599\u5f52\u6863\u5230\u53ef\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5206\u5c42\u77e5\u8bc6\u5e93\u4e2d\uff1bReport Writer\u667a\u80fd\u4f53\u9010\u8282\u7f16\u5199\u6700\u7ec8\u62a5\u544a\uff0c\u5c06\u77e5\u8bc6\u5e93\u4f5c\u4e3a\u4e8b\u5b9e\u6765\u6e90\u3002\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5916\u90e8\u5185\u5b58\u548c\u8de8\u667a\u80fd\u4f53/\u4f1a\u8bdd\u7684\u5171\u4eab\u534f\u8c03\u5a92\u4ecb\u3002", "result": "\u5728\u4e24\u4e2a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\uff08DeepResearch Bench\u548cDeepConsult\uff09\u4e0a\uff0cFS-Researcher\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u62a5\u544a\u8d28\u91cf\u3002\u5206\u6790\u663e\u793a\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\u4e0e\u5206\u914d\u7ed9Context Builder\u7684\u8ba1\u7b97\u91cf\u5448\u6b63\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u6587\u4ef6\u7cfb\u7edf\u8303\u5f0f\u4e0b\u7684\u6709\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "conclusion": "FS-Researcher\u901a\u8fc7\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4e2d\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6269\u5c55\u7814\u7a76\u80fd\u529b\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u957f\u89c6\u91ce\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00729", "abs": "https://arxiv.org/abs/2602.00729", "authors": ["Qihe Pan", "Yiming Wu", "Xing Zhao", "Liang Xie", "Guodao Sun", "Ronghua Liang"], "title": "Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation", "comment": "This paper has been accepted for publication in the proceedings of 2026 IEEE ICASSP Conference", "summary": "Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5316\u5986\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u89e3\u8026\u8eab\u4efd\u4e0e\u5316\u5986\u7279\u5f81\u3001\u5f15\u5165\u6587\u672c\u5f15\u5bfc\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u7684\u5316\u5986\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u5316\u5986\u8fc1\u79fb\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u96c6\u6709\u9650\u3001\u8eab\u4efd\u4e0e\u5316\u5986\u7279\u5f81\u89e3\u8026\u4e0d\u8db3\u3001\u53ef\u63a7\u6027\u5f31\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u91c7\u7528\u8bad\u7ec3-\u751f\u6210-\u8fc7\u6ee4-\u518d\u8bad\u7ec3\u7b56\u7565\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b2) \u8bbe\u8ba1\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u89e3\u8026\u8eab\u4efd\u4e0e\u5316\u5986\u7279\u5f81\uff1b3) \u63d0\u51fa\u6587\u672c\u5f15\u5bfc\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u533a\u57df\u63a7\u5236\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5728\u4fdd\u771f\u5ea6\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u7075\u6d3b\u6027\u65b9\u9762\u7684\u6539\u8fdb\uff0c\u80fd\u591f\u51c6\u786e\u5e94\u7528\u591a\u6837\u5316\u5316\u5986\u98ce\u683c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u7279\u5f81\u89e3\u8026\u548c\u6587\u672c\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u53ef\u63a7\u7684\u5316\u5986\u8fc1\u79fb\uff0c\u4e3a\u751f\u6210\u5f0f\u5316\u5986\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01610", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01610", "abs": "https://arxiv.org/abs/2602.01610", "authors": ["Zitao Guo", "Changyang Jiang", "Tianhong Zhao", "Jinzhou Cao", "Genan Dai", "Bowen Zhang"], "title": "ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning", "comment": "The paper has been accepted by ICASSP 2026", "summary": "Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.", "AI": {"tldr": "ToPT\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u7684\u533a\u57df\u5d4c\u5165\u5b66\u4e60\u548c\u4efb\u52a1\u611f\u77e5\u63d0\u793a\uff0c\u89e3\u51b3\u57ce\u5e02\u533a\u57df\u8868\u793a\u5b66\u4e60\u4e2d\u7a7a\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u548c\u4efb\u52a1\u8bed\u4e49\u5bf9\u9f50\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4e24\u9636\u6bb5\u65b9\u6cd5\u4ea7\u751f\u4efb\u52a1\u65e0\u5173\u7684\u8868\u793a\uff0c\u4e0e\u4e0b\u6e38\u76ee\u6807\u8131\u8282\uff1b2\uff09\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u5148\u9a8c\uff08\u5bfc\u81f4\u7a7a\u95f4\u4e0d\u8fde\u8d2f\uff09\u548c\u9c81\u68d2\u7684\u4efb\u52a1\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\u3002", "method": "ToPT\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aSREL\uff08\u7a7a\u95f4\u611f\u77e5\u533a\u57df\u5d4c\u5165\u5b66\u4e60\uff09\u4f7f\u7528\u57fa\u4e8eGraphormer\u7684\u878d\u5408\u6a21\u5757\uff0c\u6ce8\u5165\u8ddd\u79bb\u548c\u533a\u57df\u4e2d\u5fc3\u6027\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u504f\u7f6e\uff1bPrompt4RE\uff08\u533a\u57df\u5d4c\u5165\u7684\u4efb\u52a1\u611f\u77e5\u63d0\u793a\uff09\u4f7f\u7528\u51bb\u7ed3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4efb\u52a1\u7279\u5b9a\u6a21\u677f\uff0c\u901a\u8fc7\u591a\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u8bed\u4e49\u5411\u91cf\u4e0e\u533a\u57df\u5d4c\u5165\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u57ce\u5e02\u7684\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6539\u8fdb\u9ad8\u8fbe64.2%\uff0c\u9a8c\u8bc1\u4e86\u7a7a\u95f4\u5148\u9a8c\u548c\u63d0\u793a-\u533a\u57df\u5bf9\u9f50\u7684\u5fc5\u8981\u6027\u548c\u4e92\u8865\u6027\u3002", "conclusion": "ToPT\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u5148\u9a8c\u548c\u660e\u786e\u7684\u4efb\u52a1\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57ce\u5e02\u533a\u57df\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u57ce\u5e02\u8ba1\u7b97\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u533a\u57df\u5d4c\u5165\u8868\u793a\u3002"}}
{"id": "2602.00547", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00547", "abs": "https://arxiv.org/abs/2602.00547", "authors": ["Seunghyun Yoo", "Sanghong Kim", "Namkyung Yoon", "Hwangnam Kim"], "title": "Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry", "comment": "8 pages, 2 figures", "summary": "Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u8d28\u8c31\u76f4\u63a5\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5b50\u7ed3\u6784\u5d4c\u5165\u7a7a\u95f4\uff0c\u89e3\u51b3\u8d28\u8c31\u5206\u5b50\u8bc6\u522b\u7684\u6cdb\u5316\u74f6\u9888", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c06\u8d28\u8c31\u5339\u914d\u89c6\u4e3a\u5c01\u95ed\u96c6\u8bc6\u522b\u4efb\u52a1\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5206\u5b50\u9aa8\u67b6\u7ed3\u6784\uff0c\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898", "method": "\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u8d28\u8c31\u6570\u636e\u76f4\u63a5\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5b50\u7ed3\u6784\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7269\u7406\u5149\u8c31\u4e0e\u5316\u5b66\u7ed3\u6784\u7684\u663e\u5f0f\u6574\u5408", "result": "\u5728\u4e25\u683c\u9aa8\u67b6\u4e0d\u76f8\u4ea4\u57fa\u51c6\u4e0a\uff0cTop-1\u51c6\u786e\u738742.2%\uff08256\u8def\u96f6\u6837\u672c\u68c0\u7d22\uff09\uff1b\u5168\u5c40\u68c0\u7d22\u8bbe\u7f6e\u4e0b\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u5b66\u4e60\u5d4c\u5165\u7a7a\u95f4\u5316\u5b66\u4e00\u81f4\u6027\u8fbe95.4%\uff085\u8def5\u6837\u672c\u5206\u5b50\u91cd\u8bc6\u522b\uff09", "conclusion": "\u663e\u5f0f\u6574\u5408\u7269\u7406\u5149\u8c31\u5206\u8fa8\u7387\u4e0e\u5206\u5b50\u7ed3\u6784\u5d4c\u5165\u662f\u89e3\u51b3\u8d28\u8c31\u6570\u636e\u5206\u5b50\u8bc6\u522b\u6cdb\u5316\u74f6\u9888\u7684\u5173\u952e"}}
{"id": "2602.01572", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01572", "abs": "https://arxiv.org/abs/2602.01572", "authors": ["Yeqin Zhang", "Yunfei Wang", "Jiaxuan Chen", "Ke Qin", "Yizheng Zhao", "Cam-Tu Nguyen"], "title": "LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States", "comment": null, "summary": "Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faValue Aggregation\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u6ce8\u610f\u529b\u503c\u5411\u91cf\u800c\u975e\u4f20\u7edf\u9690\u85cf\u72b6\u6001\u6765\u83b7\u5f97\u66f4\u597d\u7684\u53e5\u5b50\u8868\u793a\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u73b0\u6709LLM\u5d4c\u5165\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u96c6\u6210\u65b9\u6cd5MetaEOL\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53e5\u5b50\u8868\u793a\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6700\u540e\u4e00\u5c42\u9690\u85cf\u72b6\u6001\uff0c\u4f46\u8fd9\u4e9b\u72b6\u6001\u4e3b\u8981\u9488\u5bf9\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u4efb\u52a1\u4f18\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5168\u5c40\u53e5\u5b50\u7ea7\u8bed\u4e49\u3002", "method": "\u63d0\u51faValue Aggregation\u65b9\u6cd5\uff1a1\uff09\u805a\u5408\u591a\u4e2a\u5c42\u548c\u8bcd\u7d22\u5f15\u7684\u6ce8\u610f\u529b\u503c\u5411\u91cf\uff1b2\uff09\u8fdb\u4e00\u6b65\u63d0\u51faAligned Weighted VA\uff0c\u5229\u7528\u6700\u540e\u4e00\u4e2a\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u6570\u4f5c\u4e3a\u6743\u91cd\uff0c\u901a\u8fc7\u8f93\u51fa\u6295\u5f71\u77e9\u9635\u5bf9\u9f50\u52a0\u6743\u503c\u5411\u91cf\u5230LLM\u6b8b\u5dee\u6d41\u7684\u516c\u5171\u7a7a\u95f4\u3002", "result": "\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cVA\u65b9\u6cd5\u8d85\u8d8a\u5176\u4ed6LLM\u5d4c\u5165\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u96c6\u6210\u65b9\u6cd5MetaEOL\u3002AlignedWVA\u5728\u8bad\u7ec3\u514d\u8d39\u7684LLM\u5d4c\u5165\u65b9\u6cd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5927\u5e45\u8d85\u8d8a\u9ad8\u6210\u672c\u7684MetaEOL\u3002", "conclusion": "\u6ce8\u610f\u529b\u503c\u5411\u91cf\u6bd4\u9690\u85cf\u72b6\u6001\u80fd\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u8bed\u4e49\uff0cValue Aggregation\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u901a\u8fc7\u5fae\u8c03VA\u6709\u6f5c\u529b\u83b7\u5f97\u66f4\u5f3a\u7684LLM\u5d4c\u5165\u6a21\u578b\u3002"}}
{"id": "2602.00739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00739", "abs": "https://arxiv.org/abs/2602.00739", "authors": ["Zhengyan Qin", "Liyuan Qiu"], "title": "Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries", "comment": null, "summary": "We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the \"double surface artifact\" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \\emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \\emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \\emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u53cc\u5c42\u70b9\u4e91\u4e2d\u5206\u79bb\u5185\u5916\u5c42\u8868\u9762\uff0c\u7279\u522b\u9488\u5bf9TSDF\u878d\u5408\u4e2d\u622a\u65ad\u5bfc\u81f4\u7684\"\u53cc\u5c42\u4f2a\u5f71\"\u95ee\u9898\uff0c\u80fd\u5904\u7406\u5f00\u653e\u8fb9\u754c\u6a21\u578b\uff0c\u7ea610\u79d2\u5904\u74064\u4e07\u70b9\u3002", "motivation": "\u5728\u5ba4\u5185\u6216\u533b\u5b663D\u91cd\u5efa\u4e2d\uff0cTSDF\u878d\u5408\u7684\u622a\u65ad\u4f1a\u5bfc\u81f4\"\u53cc\u5c42\u4f2a\u5f71\"\uff0c\u4ea7\u751f\u9519\u8bef\u7684\u5185\u5916\u5c42\u58f3\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5f00\u653e\u8fb9\u754c\u70b9\u4e91\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u6a21\u5757\u6765\u51c6\u786e\u5206\u79bb\u771f\u5b9e\u5185\u5c42\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u7684\u7b97\u6cd5\uff0c\u4e13\u95e8\u5904\u7406\u5177\u6709\u5f00\u653e\u8fb9\u754c\u7684\u53cc\u5c42\u70b9\u4e91\uff08\u800c\u975e\u7f3a\u5931\u8868\u9762\u533a\u57df\uff09\u3002\u7b97\u6cd5\u80fd\u540c\u65f6\u5904\u7406\u6c34\u5bc6\u548c\u5f00\u653e\u8fb9\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5206\u79bb\u5185\u5916\u5c42\u8868\u9762\u3002", "result": "\u7b97\u6cd5\u80fd\u7a33\u5065\u5904\u74062\u4e07\u5185\u5c42\u70b9\u548c2\u4e07\u5916\u5c42\u70b9\uff0c\u7ea610\u79d2\u5b8c\u6210\u5185\u5c42\u63d0\u53d6\u3002\u6709\u6548\u89e3\u51b3\u91cd\u53e0\u8868\u9762\u548c\u6cd5\u7ebf\u6df7\u4e71\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u573a\u666f\u5efa\u6a21\u548c\u533b\u5b66\u6210\u50cf\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3aTSDF\u878d\u5408\u540e\u7684\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u6a21\u5757\uff0c\u80fd\u6709\u6548\u5206\u79bb\u53cc\u5c42\u70b9\u4e91\u7684\u5185\u5916\u5c42\u8868\u9762\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f00\u653e\u8fb9\u754c\u6a21\u578b\uff0c\u4f46\u4e0d\u66ff\u4ee3\u5b8c\u6574\u7684\u53d8\u5206\u6216\u57fa\u4e8e\u5b66\u4e60\u7684\u91cd\u5efa\u6d41\u7a0b\u3002"}}
{"id": "2602.01655", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01655", "abs": "https://arxiv.org/abs/2602.01655", "authors": ["Pengrui Lu", "Shiqi Zhang", "Yunzhong Hou", "Lyumanshan Ye", "Chaoyi Huang", "Zixi Chen", "Ji Zeng", "Hantao Jiang", "Pengfei Liu", "Yiwei Wang", "Ming-Hsuan Yang"], "title": "ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development", "comment": null, "summary": "Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.", "AI": {"tldr": "ProjDevBench\uff1a\u9996\u4e2a\u7aef\u5230\u7aef\u7f16\u7801\u4ee3\u7406\u57fa\u51c6\uff0c\u8bc4\u4f30\u4ece\u9879\u76ee\u9700\u6c42\u5230\u5b8c\u6574\u4ee3\u7801\u5e93\u7684\u751f\u6210\u80fd\u529b\uff0c\u7ed3\u5408OJ\u6d4b\u8bd5\u548cLLM\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u7b49\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u95ee\u9898\u7ea7\u522b\u7684bug\u4fee\u590d\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u5f00\u53d1\u8bc4\u4f30\u3002\u9700\u8981\u8bc4\u4f30\u4ee3\u7406\u4ece\u9879\u76ee\u9700\u6c42\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u5e93\u7684\u80fd\u529b\uff0c\u4ee5\u53cd\u6620\u771f\u5b9e\u5f00\u53d1\u573a\u666f\u3002", "method": "\u63d0\u51faProjDevBench\u57fa\u51c6\uff0c\u5305\u542b20\u4e2a\u7f16\u7a0b\u95ee\u9898\uff088\u4e2a\u7c7b\u522b\uff09\uff0c\u7ed3\u5408\u5728\u7ebf\u8bc4\u6d4b\u7cfb\u7edf\uff08OJ\uff09\u6d4b\u8bd5\u548cLLM\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\uff0c\u4ece\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u3001\u529f\u80fd\u6b63\u786e\u6027\u548c\u8fed\u4ee3\u89e3\u51b3\u65b9\u6848\u4f18\u5316\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u3002", "result": "\u8bc4\u4f306\u4e2a\u57fa\u4e8e\u4e0d\u540cLLM\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u603b\u4f53\u63a5\u53d7\u7387\u4ec5\u4e3a27.38%\u3002\u4ee3\u7406\u80fd\u5904\u7406\u57fa\u672c\u529f\u80fd\u548c\u6570\u636e\u7ed3\u6784\uff0c\u4f46\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u3001\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u5316\u548c\u8d44\u6e90\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "ProjDevBench\u586b\u8865\u4e86\u7aef\u5230\u7aef\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u7f16\u7801\u4ee3\u7406\u5728\u590d\u6742\u7cfb\u7edf\u5f00\u53d1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7f16\u7801\u4ee3\u7406\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.00549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00549", "abs": "https://arxiv.org/abs/2602.00549", "authors": ["Kezhao Lai", "Yutao Lai", "Hai-Lin Liu"], "title": "Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design", "comment": null, "summary": "While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.", "AI": {"tldr": "Clade-AHD\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u8d1d\u53f6\u65af\u4fe1\u5ff5\u66ff\u4ee3\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u70b9\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u9884\u7b97\u6709\u9650\u4e0b\u7684\u8fc7\u5ea6\u5f00\u53d1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5728LLM\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\u5b58\u5728\u8fc7\u5ea6\u5f00\u53d1\u503e\u5411\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4f1a\u5f71\u54cd\u542f\u53d1\u5f0f\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faClade-AHD\u6846\u67b6\uff1a\u7528\u5206\u652f\u7ea7\u8d1d\u53f6\u65af\u4fe1\u5ff5\u66ff\u4ee3\u8282\u70b9\u7ea7\u70b9\u4f30\u8ba1\uff0c\u5c06\u540e\u4ee3\u8bc4\u4f30\u805a\u5408\u4e3aBeta\u5206\u5e03\uff0c\u5e76\u901a\u8fc7Thompson\u91c7\u6837\u5728\u8fd9\u4e9b\u4fe1\u5ff5\u4e0a\u8fdb\u884c\u63a2\u7d22\uff0c\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u6765\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cClade-AHD\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Clade-AHD\u901a\u8fc7\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86MCTS\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u8fc7\u5ea6\u5f00\u53d1\u95ee\u9898\uff0c\u4e3aLLM\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01587", "abs": "https://arxiv.org/abs/2602.01587", "authors": ["Zehua Cheng", "Jianwei Yang", "Wei Dai", "Jiahao Sun"], "title": "Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment", "comment": "10 pages", "summary": "Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.", "AI": {"tldr": "\u63d0\u51faCertified Semantic Smoothing (CSS)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u968f\u673a\u6d88\u878d\u548c\u566a\u58f0\u589e\u5f3a\u5bf9\u9f50\u8c03\u4f18\uff0c\u4e3aLLM\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u540c\u65f6\u4fdd\u6301\u826f\u6027\u6548\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u81ea\u9002\u5e94\u8d8a\u72f1\u653b\u51fb\u4ecd\u7136\u8106\u5f31\uff0c\u73b0\u6709\u7684\u7ecf\u9a8c\u6027\u9632\u5fa1\uff08\u5982GCG\uff09\u5bb9\u6613\u88ab\u7ed5\u8fc7\u3002\u9700\u8981\u4e00\u79cd\u53ef\u8bc1\u660e\u9c81\u68d2\u6027\u7684\u6846\u67b6\u6765\u63d0\u4f9b\u786e\u5b9a\u6027\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "1. \u63d0\u51faCertified Semantic Smoothing (CSS) via Stratified Randomized Ablation\uff1a\u5c06\u8f93\u5165\u5212\u5206\u4e3a\u4e0d\u53ef\u53d8\u7684\u7ed3\u6784\u63d0\u793a\u548c\u53ef\u53d8\u7684\u6709\u6548\u8f7d\u8377\uff0c\u5229\u7528\u8d85\u51e0\u4f55\u5206\u5e03\u63a8\u5bfc\u4e25\u683c\u7684lo\u8303\u6570\u4fdd\u8bc1\u30022. \u4f7f\u7528Noise-Augmented Alignment Tuning (NAAT)\uff1a\u5c06\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u8bed\u4e49\u53bb\u566a\u5668\uff0c\u89e3\u51b3\u7a00\u758f\u4e0a\u4e0b\u6587\u4e0b\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "result": "\u5728Llama-3\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u5c06\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u6210\u529f\u7387\u4ece84.2%\u964d\u81f31.2%\uff0c\u540c\u65f6\u4fdd\u630194.1%\u7684\u826f\u6027\u6548\u7528\u3002\u663e\u8457\u4f18\u4e8e\u5b57\u7b26\u7ea7\u57fa\u7ebf\u65b9\u6cd5\uff08\u6548\u7528\u964d\u81f374.3%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u786e\u5b9a\u6027\u7684\u5b89\u5168\u8bc1\u4e66\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u53ef\u8bc1\u660e\u534a\u5f84\u5185\u5bf9\u6240\u6709\u5bf9\u6297\u6027\u53d8\u4f53\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u4fdd\u8bc1\u4ece\u5355\u6b21\u63a8\u7406\u5230\u7edf\u8ba1\u7a33\u5b9a\u6027\u7684\u8f6c\u53d8\u3002"}}
{"id": "2602.00749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00749", "abs": "https://arxiv.org/abs/2602.00749", "authors": ["Xiangming Wang", "Benteng Sun", "Yungeng Liu", "Haijin Zeng", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression", "comment": null, "summary": "Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \\textbf{\\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \\times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.", "AI": {"tldr": "HSI-VAR\uff1a\u5c06\u9ad8\u5149\u8c31\u56fe\u50cf\u6062\u590d\u91cd\u65b0\u6784\u60f3\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u5efa\u6a21\u5149\u8c31\u548c\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u7ec6\u8282\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5e38\u53d7\u566a\u58f0\u3001\u6a21\u7cca\u548c\u7f3a\u5931\u6ce2\u6bb5\u7b49\u591a\u79cd\u9000\u5316\u5f71\u54cd\u3002\u73b0\u6709\u751f\u6210\u65b9\u6cd5\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u9700\u8981\u6570\u767e\u6b21\u8fed\u4ee3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b\u56de\u5f52\u6a21\u578b\u5219\u4ea7\u751f\u8fc7\u5ea6\u5e73\u6ed1\u7ed3\u679c\uff0c\u65e0\u6cd5\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u7ec6\u8282\u3002", "method": "\u5c06HSI\u6062\u590d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u95ee\u9898\uff0c\u5f15\u5165\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u6f5c\u5728\u6761\u4ef6\u5bf9\u9f50\uff0c\u8026\u5408\u6f5c\u5728\u5148\u9a8c\u548c\u6761\u4ef6\u5d4c\u5165\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff1b2\uff09\u9000\u5316\u611f\u77e5\u5f15\u5bfc\uff0c\u5c06\u6df7\u5408\u9000\u5316\u7f16\u7801\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u7ec4\u5408\uff1b3\uff09\u7a7a\u95f4-\u5149\u8c31\u9002\u5e94\u6a21\u5757\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u7ec6\u5316\u4e24\u4e2a\u57df\u7684\u7ec6\u8282\u3002", "result": "\u5728\u4e5d\u4e2a\u4e00\u4f53\u5316HSI\u6062\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728ICVL\u6570\u636e\u96c6\u4e0aPSNR\u63d0\u53473.77 dB\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5feb95.5\u500d\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8fd150%\u3002", "conclusion": "HSI-VAR\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u6062\u590d\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u7ed3\u6784\u4fdd\u7559\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754cHSI\u6062\u590d\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01664", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01664", "abs": "https://arxiv.org/abs/2602.01664", "authors": ["Mingda Zhang", "Haoran Luo", "Tiesunlong Shen", "Qika Lin", "Xiaoying Tang", "Rui Mao", "Erik Cambria"], "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning", "comment": "41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/", "summary": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.", "AI": {"tldr": "FlowSteer\uff1a\u4e00\u4e2a\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u81ea\u52a8\u7f16\u6392\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\uff0c\u652f\u6301\u53ef\u63d2\u62d4\u7684\u64cd\u4f5c\u7b26\u5e93\u548c\u53ef\u4e92\u6362\u7684LLM\u540e\u7aef\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u7f16\u6392\u9762\u4e34\u9ad8\u4eba\u5de5\u6210\u672c\u3001\u4f9d\u8d56\u7279\u5b9a\u64cd\u4f5c\u7b26/\u5927\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlowSteer\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7406\uff0c\u5728\u53ef\u6267\u884c\u753b\u5e03\u73af\u5883\u4e2d\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\u3002\u7b56\u7565\u6a21\u578b\u5206\u6790\u6267\u884c\u72b6\u6001\u5e76\u9009\u62e9\u7f16\u8f91\u52a8\u4f5c\uff0c\u753b\u5e03\u6267\u884c\u64cd\u4f5c\u7b26\u5e76\u8fd4\u56de\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u8fd8\u63d0\u51faCanvas Workflow Relative Policy Optimization (CWRPO)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f15\u5165\u591a\u6837\u6027\u7ea6\u675f\u5956\u52b1\u548c\u6761\u4ef6\u91ca\u653e\u673a\u5236\u6765\u7a33\u5b9a\u5b66\u4e60\u5e76\u6291\u5236\u6377\u5f84\u884c\u4e3a\u3002", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlowSteer\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FlowSteer\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4f5c\u6d41\u7f16\u6392\u7684\u81ea\u52a8\u5316\u6311\u6218\uff0c\u652f\u6301\u53ef\u63d2\u62d4\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.00567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00567", "abs": "https://arxiv.org/abs/2602.00567", "authors": ["Tian Zhang", "Yujia Tong", "Junhao Dong", "Ke Xu", "Yuze Wang", "Jingling Yuan"], "title": "Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks", "comment": null, "summary": "The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.", "AI": {"tldr": "OEU\u63d0\u51fa\u6b63\u4ea4\u71b5\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u9057\u5fd8\u6570\u636e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\uff0c\u89e3\u51b3\u91cf\u5316\u6a21\u578b\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e0eGDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u7ed3\u5408\uff0c\u8feb\u5207\u9700\u8981\u91cf\u5316\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u95ee\u9898\uff1a\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u8bb0\u5fc6\u9519\u8bef\u6807\u7b7e\u6765\u8bf1\u5bfc\u9057\u5fd8\uff0c\u6df7\u6dc6\u4e86\u9057\u5fd8\u4e0e\u9519\u8bef\u8bb0\u5fc6\uff1b\u4f7f\u7528\u6807\u91cf\u68af\u5ea6\u91cd\u52a0\u6743\u65e0\u6cd5\u89e3\u51b3\u68af\u5ea6\u95f4\u7684\u65b9\u5411\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u71b5\u9057\u5fd8\uff08OEU\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u71b5\u5f15\u5bfc\u9057\u5fd8\uff1a\u6700\u5927\u5316\u9057\u5fd8\u6570\u636e\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u9057\u5fd8\u800c\u975e\u81ea\u4fe1\u7684\u9519\u8bef\u9884\u6d4b\uff1b2\uff09\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\uff1a\u901a\u8fc7\u5c06\u9057\u5fd8\u68af\u5ea6\u6295\u5f71\u5230\u4fdd\u7559\u68af\u5ea6\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u6765\u6d88\u9664\u5e72\u6270\uff0c\u5728\u4e00\u9636\u8fd1\u4f3c\u4e0b\u63d0\u4f9b\u6548\u7528\u4fdd\u7559\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOEU\u5728\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OEU\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u673a\u5668\u9057\u5fd8\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u9057\u5fd8\u548c\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9057\u5fd8\u6548\u679c\u548c\u6a21\u578b\u6548\u7528\u4fdd\u7559\u3002"}}
{"id": "2602.01590", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01590", "abs": "https://arxiv.org/abs/2602.01590", "authors": ["Shaohan Wang", "Benfeng Xu", "Licheng Zhang", "Mingxuan Du", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles", "comment": "Preprint. Work in progress", "summary": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge", "AI": {"tldr": "\u63d0\u51faWiki Live Challenge (WLC)\u57fa\u51c6\uff0c\u5229\u7528\u6700\u65b0\u7684\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6587\u7ae0\u4f5c\u4e3a\u4e13\u5bb6\u7ea7\u53c2\u8003\uff0c\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406(DRAs)\u7684\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56LLM\u751f\u6210\u7684\u53c2\u8003\u6216LLM\u884d\u751f\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u867d\u7136\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u7f3a\u4e4f\u4e13\u5bb6\u9a8c\u8bc1\u5185\u5bb9\u7684\u53ef\u9760\u6027\uff0c\u4e14\u96be\u4ee5\u63d0\u4f9b\u5ba2\u89c2\u3001\u7ec6\u7c92\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\u8bc4\u4f30\u3002", "method": "\u5f15\u5165Wiki Live Challenge (WLC)\u5b9e\u65f6\u57fa\u51c6\uff0c\u5229\u7528\u6700\u65b0\u7684\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6587\u7ae0\u4f5c\u4e3a\u4e13\u5bb6\u7ea7\u53c2\u8003\uff0c\u7b56\u5212\u5305\u542b100\u7bc7\u8fd1\u671f\u4f18\u8d28\u6587\u7ae0\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faWiki Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b39\u4e2a\u5199\u4f5c\u8d28\u91cf\u6807\u51c6\u548c\u4e25\u683c\u7684\u4e8b\u5b9e\u53ef\u9a8c\u8bc1\u6027\u6307\u6807\u3002", "result": "\u5bf9\u5404\u79cdDRA\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7ea7\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9a8c\u8bc1\u4e86WLC\u5728\u63a8\u8fdb\u4ee3\u7406\u7814\u7a76\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "WLC\u57fa\u51c6\u901a\u8fc7\u5229\u7528\u7ef4\u57fa\u767e\u79d1\u4e25\u683c\u7684\u4e2d\u7acb\u6027\u3001\u5168\u9762\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u6807\u51c6\uff0c\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2602.00763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00763", "abs": "https://arxiv.org/abs/2602.00763", "authors": ["Dylan Yves", "Khush Agarwal", "Jonathan Hoyin Chan", "Patcharapit Promoppatum", "Aroonkamon Pattanasiricharoen"], "title": "Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints", "comment": "9 pages, 6 figures", "summary": "Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eU-Net\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u81c2\u4e1b\u795e\u7ecf\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u6570\u636e\u96c6\u7ec4\u6210\u548c\u6807\u6ce8\u7b56\u7565\u5bf9\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u8d85\u58f0\u5f15\u5bfc\u4e0b\u533a\u57df\u9ebb\u9189\u4e2d\uff0c\u795e\u7ecf\u7684\u7cbe\u786e\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u6591\u70b9\u566a\u58f0\u548c\u60a3\u8005\u95f4\u89e3\u5256\u7ed3\u6784\u53d8\u5f02\uff0c\u624b\u52a8\u8bc6\u522b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528U-Net\u67b6\u6784\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u795e\u7ecf\u5206\u5272\uff0c\u7814\u7a76\u4e0d\u540c\u6570\u636e\u96c6\u7ec4\u6210\uff08\u591a\u53f0\u8d85\u58f0\u8bbe\u5907\u6570\u636e\uff09\u548c\u6807\u6ce8\u7b56\u7565\uff08\u4e8c\u5206\u7c7b\u4e0e\u591a\u5206\u7c7b\uff09\u5bf9\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u591a\u8bbe\u5907\u6570\u636e\u8bad\u7ec3\u5bf9\u6027\u80fd\u8f83\u4f4e\u7684\u91c7\u96c6\u6e90\u6709\u6b63\u5219\u5316\u6548\u679c\uff0c\u4f46\u4e0d\u5982\u5355\u6e90\u8bad\u7ec3\u5339\u914d\u76ee\u6807\u57df\uff1b\u591a\u5206\u7c7b\u76d1\u7763\u5bfc\u81f4\u795e\u7ecf\u7279\u5f02\u6027Dice\u5206\u6570\u4e0b\u964d9%-61%\uff1b\u795e\u7ecf\u5927\u5c0f\u4e0e\u5206\u5272\u7cbe\u5ea6\u5448\u4e2d\u7b49\u6b63\u76f8\u5173\uff08r=0.587\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u7ea6\u675f\u4e0b\u5f00\u53d1\u7a33\u5065\u7684\u8d85\u58f0\u795e\u7ecf\u5206\u5272\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u6cd5\u5b66\u6307\u5bfc\uff0c\u6307\u51fa\u5c0f\u795e\u7ecf\u5206\u5272\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2602.01675", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01675", "abs": "https://arxiv.org/abs/2602.01675", "authors": ["Yuanzhe Shen", "Zisu Huang", "Zhengyuan Wang", "Muzhao Tian", "Zhengkang Guo", "Chenyang Zhang", "Shuaiyu Zhou", "Zengjie Hu", "Dailin Li", "Jingwen Xu", "Kaimin Wang", "Wenhao Liu", "Tianlong Li", "Fengpeng Yue", "Feng Hong", "Cao Liu", "Ke Zeng"], "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios", "comment": "40 pages, 6figures", "summary": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \\textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose \\textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.", "AI": {"tldr": "TRIP-Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u65c5\u884c\u89c4\u5212\u573a\u666f\u7684\u957f\u89c6\u91ce\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b18\u4e2a\u5de5\u5177\u548c40+\u65c5\u884c\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u8bc4\u4f30\u3002\u5b9e\u9a8c\u663e\u793a\u5148\u8fdb\u6a21\u578b\u5728\u7b80\u5355\u5206\u5272\u4e0a\u6700\u591a\u53ea\u670950%\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u5206\u5272\u4e0a\u4f4e\u4e8e10%\u3002\u63d0\u51fa\u7684GTPO\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728Qwen2.5-32B-Instruct\u4e0a\u8868\u73b0\u4f18\u4e8eGemini-3-Pro\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u4ee3\u8868\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5f3a\u5236\u6267\u884c\u5168\u5c40\u7ea6\u675f\u3001\u534f\u8c03\u591a\u5de5\u5177\u63a8\u7406\u4ee5\u53ca\u9002\u5e94\u957f\u671f\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u884c\u4e3a\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86TRIP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u65c5\u884c\u89c4\u5212\u573a\u666f\uff0c\u5305\u542b18\u4e2a\u7cbe\u9009\u5de5\u5177\u548c40+\u65c5\u884c\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u8bc4\u4f30\u3002\u8fd8\u63d0\u51fa\u4e86GTPO\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u7ebf\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u4e13\u95e8\u7684\u5956\u52b1\u5f52\u4e00\u5316\u548c\u5956\u52b1\u5dee\u5206\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5728\u7b80\u5355\u5206\u5272\u4e0a\u6700\u591a\u53ea\u80fd\u8fbe\u523050%\u7684\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u5b50\u96c6\u4e0a\u6027\u80fd\u4e0b\u964d\u523010%\u4ee5\u4e0b\u3002\u5c06GTPO\u5e94\u7528\u4e8eQwen2.5-32B-Instruct\u540e\uff0c\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u4ea4\u4e92\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8eGemini-3-Pro\u3002", "conclusion": "TRIP-Bench\u6709\u671b\u63a8\u52a8\u5b9e\u7528\u7684\u957f\u89c6\u91ce\u4ea4\u4e92\u667a\u80fd\u4f53\u53d1\u5c55\uff0cGTPO\u4e3a\u9c81\u68d2\u7684\u957f\u89c6\u91ce\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2602.00573", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00573", "abs": "https://arxiv.org/abs/2602.00573", "authors": ["Zheng Zhang", "Tao Hu", "Xueheng Li", "Yang Wang", "Rui Li", "Jie Zhang", "Chengjun Xie"], "title": "When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStage-CIL\u8303\u5f0f\uff0c\u89e3\u51b3\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4e2d\u7c7b\u5185\u5f62\u6001\u6f14\u5316\u95ee\u9898\uff0c\u5f15\u5165Stage-Bench\u6570\u636e\u96c6\u548cSTAGE\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u5047\u8bbe\u7c7b\u522b\u5f62\u6001\u9759\u6001\uff0c\u5ffd\u7565\u4e86\u7c7b\u5185\u6f14\u5316\u73b0\u8c61\uff08\u5982\u5e7c\u866b\u53d8\u8774\u8776\uff09\u3002\u6a21\u578b\u9700\u8981\u540c\u65f6\u5904\u7406\u7c7b\u95f4\u533a\u5206\u548c\u7c7b\u5185\u5f62\u6001\u9002\u5e94\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faStage-Aware CIL\u8303\u5f0f\uff0c\u5f15\u5165Stage-Bench\u6570\u636e\u96c6\uff0810\u4e2a\u9886\u57df\uff0c2\u9636\u6bb5\uff09\u8bc4\u4f30\u534f\u8bae\u3002\u63d0\u51faSTAGE\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u5185\u5b58\u6c60\u4e2d\u5b66\u4e60\u62bd\u8c61\u53ef\u8f6c\u79fb\u7684\u6f14\u5316\u6a21\u5f0f\uff0c\u89e3\u8026\u8bed\u4e49\u8eab\u4efd\u548c\u8f6c\u6362\u52a8\u6001\uff0c\u57fa\u4e8e\u65e9\u671f\u8868\u793a\u9884\u6d4b\u672a\u6765\u5f62\u6001\u3002", "result": "STAGE\u65b9\u6cd5\u5728\u5e7f\u6cdb\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\u4e00\u81f4\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u540c\u65f6\u89e3\u51b3\u7c7b\u95f4\u533a\u5206\u548c\u7c7b\u5185\u5f62\u6001\u9002\u5e94\u95ee\u9898\u3002", "conclusion": "\u7c7b\u5185\u5f62\u6001\u6f14\u5316\u662f\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u7684\u91cd\u8981\u6311\u6218\uff0cStage-CIL\u8303\u5f0f\u3001Stage-Bench\u6570\u636e\u96c6\u548cSTAGE\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u548c\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01598", "abs": "https://arxiv.org/abs/2602.01598", "authors": ["Mingwen Zhang", "Minqiang Yang", "Changsheng Ma", "Yang Yu", "Hui Bai", "Chen Xu", "Xiangzhen Kong", "Bin Hu"], "title": "The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation", "comment": null, "summary": "Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \\textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \\textbf{when to ask} (via Strategy Anchoring) from \\textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \\textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.", "AI": {"tldr": "\u63d0\u51faSocratic Inquiry Framework (SIF)\u6846\u67b6\uff0c\u5c06LLM\u4ece\u88ab\u52a8\u503e\u542c\u8005\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8ba4\u77e5\u5f15\u5bfc\u8005\uff0c\u901a\u8fc7\u7b56\u7565\u951a\u5b9a\u548c\u6a21\u677f\u68c0\u7d22\u5b9e\u73b0\u4e3b\u52a8\u63d0\u95ee\uff0c\u63d0\u5347\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u7684\u6df1\u5ea6\u548c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5fc3\u7406\u6cbb\u7597LLM\u8fc7\u4e8e\u88ab\u52a8\uff0c\u4e3b\u8981\u63d0\u4f9b\u5171\u60c5\u4f46\u80a4\u6d45\u7684\u56de\u5e94\uff0c\u65e0\u6cd5\u63ed\u793a\u6f5c\u5728\u4fe1\u5ff5\u6216\u5f15\u5bfc\u884c\u4e3a\u6539\u53d8\u3002\u4e3b\u52a8\u63d0\u95ee\u662f\u8ba4\u77e5\u884c\u4e3a\u7597\u6cd5\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51faSocratic Inquiry Framework (SIF)\uff1a1) \u7b56\u7565\u951a\u5b9a\u51b3\u5b9a\u4f55\u65f6\u63d0\u95ee\uff1b2) \u6a21\u677f\u68c0\u7d22\u51b3\u5b9a\u63d0\u95ee\u5185\u5bb9\u3002\u8fd8\u521b\u5efa\u4e86Socratic-QA\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7b56\u7565\u5bf9\u9f50\u7684\u82cf\u683c\u62c9\u5e95\u5f0f\u5bf9\u8bdd\u5e8f\u5217\u76d1\u7763\u3002", "result": "SIF\u663e\u8457\u63d0\u9ad8\u4e86\u4e3b\u52a8\u63d0\u95ee\u9891\u7387\u3001\u5bf9\u8bdd\u6df1\u5ea6\u548c\u6cbb\u7597\u5bf9\u9f50\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u5b89\u6170\u5230\u4e3b\u52a8\u63a2\u7d22\u7684\u8f6c\u53d8\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u5fc3\u7406\u77e5\u60c5LLM\u8303\u5f0f\uff1a\u4e0d\u4ec5\u56de\u5e94\uff0c\u66f4\u8981\u5f15\u5bfc\u3002SIF\u4e3aLLM\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6cbb\u7597\u610f\u56fe\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2602.00795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00795", "abs": "https://arxiv.org/abs/2602.00795", "authors": ["Wenhao Li", "Xianjing Meng", "Qiangchang Wang", "Zhongyi Han", "Zhibin Wu", "Yilong Yin"], "title": "DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning", "comment": "Accepted by ICLR 2026", "summary": "Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.", "AI": {"tldr": "DVLA-RL\u901a\u8fc7\u53cc\u7ea7\u8bed\u4e49\u6784\u5efa\u548c\u5f3a\u5316\u5b66\u4e60\u95e8\u63a7\u6ce8\u610f\u529b\uff0c\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u5b9e\u73b0\u89c6\u89c9\u4e0e\u8bed\u8a00\u4ece\u4f4e\u5c42\u5230\u9ad8\u5c42\u7684\u6e10\u8fdb\u5bf9\u9f50\uff0c\u5728\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u5927\u8bed\u8a00\u6a21\u578b\u6765\u4e30\u5bcc\u89c6\u89c9\u8868\u793a\uff0c\u4f46\u5ffd\u7565\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u4e4b\u95f4\u4ece\u4f4e\u5c42\u5230\u9ad8\u5c42\u7684\u6e10\u8fdb\u81ea\u9002\u5e94\u5bf9\u9f50\uff0c\u5bfc\u81f4\u8bed\u4e49\u589e\u76ca\u6709\u9650\u3002", "method": "\u63d0\u51faDVLA-RL\u6846\u67b6\uff1a1) \u53cc\u7ea7\u8bed\u4e49\u6784\u5efa(DSC)\uff1a\u57fa\u4e8e\u7c7b\u522b\u540d\u79f0\u548c\u652f\u6301\u6837\u672c\u751f\u6210\u5224\u522b\u6027\u5c5e\u6027\uff0c\u6e10\u8fdb\u9009\u62e9\u6700\u76f8\u5173\u5c5e\u6027\u5e76\u5408\u6210\u8fde\u8d2f\u7c7b\u522b\u63cf\u8ff0\uff1b2) RL\u95e8\u63a7\u6ce8\u610f\u529b(RLA)\uff1a\u5c06\u8de8\u6a21\u6001\u878d\u5408\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7b56\u7565\u81ea\u9002\u5e94\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u8d21\u732e\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u4e5d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "DVLA-RL\u901a\u8fc7\u53cc\u7ea7\u8bed\u4e49\u6784\u5efa\u548c\u5f3a\u5316\u5b66\u4e60\u95e8\u63a7\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u5c40\u90e8\u5c5e\u6027\u5230\u5168\u5c40\u8bed\u4e49\u7684\u6e10\u8fdb\u5bf9\u9f50\uff0c\u4ec5\u7528\u5c11\u91cf\u652f\u6301\u6837\u672c\u5c31\u80fd\u83b7\u5f97\u7c7b\u522b\u7279\u5b9a\u7684\u5224\u522b\u6027\u548c\u6cdb\u5316\u8868\u793a\u3002"}}
{"id": "2602.01689", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01689", "abs": "https://arxiv.org/abs/2602.01689", "authors": ["Yongchan Kwon", "James Zou"], "title": "What LLMs Think When You Don't Tell Them What to Think About?", "comment": "NA", "summary": "Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6700\u5c0f\u5316\u3001\u4e3b\u9898\u4e2d\u7acb\u7684\u8f93\u5165\u63a2\u7a76LLMs\u7684\u65e0\u7ea6\u675f\u751f\u6210\u884c\u4e3a\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5b58\u5728\u7cfb\u7edf\u6027\u4e3b\u9898\u504f\u597d\u548c\u72ec\u7279\u9000\u5316\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709LLM\u5206\u6790\u5927\u591a\u4f9d\u8d56\u7279\u5b9a\u4e3b\u9898\u6216\u4efb\u52a1\u7684\u63d0\u793a\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u53ef\u89c2\u5bdf\u7684\u884c\u4e3a\u8303\u56f4\u3002\u9700\u8981\u7814\u7a76LLMs\u5728\u6700\u5c0f\u5316\u3001\u4e3b\u9898\u4e2d\u7acb\u8f93\u5165\u4e0b\u7684\u751f\u6210\u884c\u4e3a\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u7406\u89e3\u5176\u5185\u5728\u7279\u6027\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u5316\u3001\u4e3b\u9898\u4e2d\u7acb\u7684\u8f93\u5165\uff08\u5982\u5355\u4e2a\u5b57\u7b26\uff09\u6765\u6fc0\u53d1LLMs\u7684\u8fd1\u65e0\u7ea6\u675f\u751f\u6210\u884c\u4e3a\uff0c\u6536\u96c6\u4e8616\u4e2aLLMs\u7684256,000\u4e2a\u6837\u672c\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5b58\u5728\u7cfb\u7edf\u6027\u4e3b\u9898\u504f\u597d\uff1aGPT-OSS\u504f\u597d\u7f16\u7a0b\u548c\u6570\u5b66\u5185\u5bb9\uff0cLlama\u504f\u597d\u6587\u5b66\u5185\u5bb9\uff0cDeepSeek\u504f\u597d\u5b97\u6559\u5185\u5bb9\uff0cQwen\u504f\u597d\u591a\u9009\u9898\u3002\u8fd8\u89c2\u5bdf\u5230\u5185\u5bb9\u6df1\u5ea6\u5dee\u5f02\u548c\u72ec\u7279\u7684\u9000\u5316\u6a21\u5f0f\uff08\u5982Llama\u751f\u6210\u4e2a\u4eba\u793e\u4ea4\u5a92\u4f53\u94fe\u63a5\uff09\u3002", "conclusion": "LLMs\u5373\u4f7f\u5728\u6700\u5c0f\u8f93\u5165\u4e0b\u4e5f\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u7cfb\u7edf\u6027\u4e3b\u9898\u504f\u597d\u548c\u72ec\u7279\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u4e3a\u6a21\u578b\u76d1\u63a7\u548c\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u5185\u5728\u504f\u5dee\u3002"}}
{"id": "2602.00576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00576", "abs": "https://arxiv.org/abs/2602.00576", "authors": ["Tushaar Gangavarapu", "Jiping Li", "Christopher Vattheuer", "Zhangyang Wang", "Baharan Mirzasoleiman"], "title": "Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs", "comment": null, "summary": "Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff08\u4e0a\u91c7\u6837\u6216\u589e\u5f3a\u540e\u671f\u5b66\u4e60\u7684\u6837\u672c\uff09\u53ef\u4ee5\u51cf\u5c11\u4f18\u5316\u5668\u7684\u7b80\u5355\u6027\u504f\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe18%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4fee\u6539\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u6765\u5f15\u5bfc\u4f18\u5316\u5668\u627e\u5230\u5177\u6709\u66f4\u597d\u6cdb\u5316\u6027\u80fd\u7684\u89e3\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684SAM\u4f18\u5316\u5668\uff0c\u5bfb\u627e\u66f4\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u9996\u5148\u7406\u8bba\u5206\u6790\u591a\u5934\u90e8\u7ebf\u6027\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u7684\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u6bd4\u8f83\u68af\u5ea6\u4e0b\u964d(GD)\u548c\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316(SAM)\u7684\u8bad\u7ec3\u52a8\u6001\u3002\u53d1\u73b0SAM\u80fd\u964d\u4f4e\u7b80\u5355\u6027\u504f\u5dee\u540e\uff0c\u63d0\u51fa\u901a\u8fc7\u4e0a\u91c7\u6837\u6216\u589e\u5f3a\u8bad\u7ec3\u540e\u671f\u5b66\u4e60\u7684\u6837\u672c\u6765\u8c03\u6574\u6570\u636e\u5206\u5e03\uff0c\u4ece\u800c\u51cf\u5c11\u7b80\u5355\u6027\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u591a\u4e2aLLM\uff08\u5305\u62ecPhi2-2.7B\u3001Llama3.2-1B\u3001Gemma3-1B-PT\u548cQwen3-0.6B-Base\uff09\u7684\u6027\u80fd\uff0c\u5728\u4f7f\u7528AdamW\u548cMuon\u4f18\u5316\u5668\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97\u9ad8\u8fbe18%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u6765\u51cf\u5c11\u4f18\u5316\u5668\u7684\u7b80\u5355\u6027\u504f\u5dee\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u66ff\u4ee3\u8ba1\u7b97\u6602\u8d35\u7684SAM\u4f18\u5316\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01618", "abs": "https://arxiv.org/abs/2602.01618", "authors": ["Panuthep Tasawong", "Jian Gang Ngui", "Alham Fikri Aji", "Trevor Cohn", "Peerat Limkonchotiwat"], "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia", "comment": "Under reivew", "summary": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.", "AI": {"tldr": "SEA-Guard\uff1a\u9996\u4e2a\u57fa\u4e8e\u4e1c\u5357\u4e9a\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\u521b\u5efa\u771f\u5b9e\u533a\u57df\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5728\u68c0\u6d4b\u533a\u57df\u654f\u611f\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684AI\u5bf9\u9f50\u9700\u8981\u6587\u5316\u611f\u77e5\u7684\u5b89\u5168\u9632\u62a4\uff0c\u4f46\u6784\u5efa\u5927\u89c4\u6a21\u6587\u5316\u57fa\u7840\u6570\u636e\u96c6\u9762\u4e34\u8d44\u6e90\u6709\u9650\u548c\u672c\u5730\u6807\u6ce8\u8005\u7a00\u7f3a\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u82f1\u8bed\u6570\u636e\u96c6\u673a\u5668\u7ffb\u8bd1\uff0c\u5f80\u5f80\u9057\u6f0f\u533a\u57df\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u5730\u521b\u5efa\u771f\u5b9e\u3001\u533a\u57df\u7279\u5b9a\u7684\u4e1c\u5357\u4e9a\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efaSEA-Guard\u7cfb\u5217\u6a21\u578b\u3002", "result": "SEA-Guard\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6587\u5316\u53d8\u4f53\u8bc4\u4f30\u4e2d\uff0c\u5728\u68c0\u6d4b\u533a\u57df\u654f\u611f\u6216\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u521b\u5efa\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e1c\u5357\u4e9a\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\u89e3\u51b3\u4e86\u6587\u5316\u611f\u77e5AI\u5b89\u5168\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u533a\u57df\u7279\u5b9aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00807", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00807", "abs": "https://arxiv.org/abs/2602.00807", "authors": ["Xianzhe Fan", "Shengliang Deng", "Xiaoyang Wu", "Yuxiang Lu", "Zhuoling Li", "Mi Yan", "Yujia Zhang", "Zhizheng Zhang", "He Wang", "Hengshuang Zhao"], "title": "Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds", "comment": null, "summary": "Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.", "AI": {"tldr": "Any3D-VLA\u901a\u8fc7\u6574\u54083D\u70b9\u4e91\u4fe1\u606f\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u89e3\u51b32D\u89c6\u89c9\u8f93\u5165\u5728\u590d\u6742\u573a\u666f\u4e2d\u7a7a\u95f4\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u6a21\u62df\u5668\u3001\u4f20\u611f\u5668\u548c\u6a21\u578b\u4f30\u8ba1\u7684\u70b9\u4e91\u8bad\u7ec3\uff0c\u5b66\u4e60\u9886\u57df\u65e0\u5173\u76843D\u8868\u793a\u5e76\u4e0e2D\u8868\u793a\u878d\u5408\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4e3b\u8981\u4f7f\u75282D\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u878d\u51653D\u4fe1\u606f\u6765\u589e\u5f3aVLA\u80fd\u529b\uff0c\u89e3\u51b33D\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u73af\u5883\u57df\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAny3D-VLA\u6846\u67b6\uff1a1\uff09\u7edf\u4e00\u6a21\u62df\u5668\u3001\u4f20\u611f\u5668\u548c\u6a21\u578b\u4f30\u8ba1\u7684\u70b9\u4e91\u8bad\u7ec3\u6d41\u7a0b\uff1b2\uff09\u6784\u5efa\u591a\u6837\u5316\u7684\u8f93\u5165\u6570\u636e\uff1b3\uff09\u5b66\u4e60\u9886\u57df\u65e0\u5173\u76843D\u8868\u793a\u5e76\u4e0e\u5bf9\u5e94\u76842D\u8868\u793a\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u663e\u5f0f\u63d0\u5347\u4e3a\u70b9\u4e91\u80fd\u4ea7\u751f\u6bd42D\u8868\u793a\u66f4\u597d\u7684\u8865\u5145\u8868\u793a\u3002Any3D-VLA\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u548c\u57df\u5dee\u8ddd\u7f13\u89e3\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u6574\u54083D\u70b9\u4e91\u4fe1\u606f\uff0cAny3D-VLA\u6210\u529f\u589e\u5f3a\u4e86VLA\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u4e863D\u6570\u636e\u7a00\u7f3a\u548c\u8de8\u73af\u5883\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u4e3aVLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01695", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01695", "abs": "https://arxiv.org/abs/2602.01695", "authors": ["Yadong Wang", "Haodong Chen", "Yu Tian", "Chuanxing Geng", "Dong Liang", "Xiang Chen"], "title": "Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning", "comment": null, "summary": "Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.", "AI": {"tldr": "LSTR\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6f5c\u5728\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u7a00\u758f\u7f16\u7801\u5668\u63d0\u5347\u4e3a\u4e3b\u52a8\u63a8\u7406\u7b97\u5b50\uff0c\u901a\u8fc7\u7a00\u758f\u8bed\u4e49\u8f6c\u6362\u8fdb\u884c\u591a\u6b65\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u548c\u538b\u7f29\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u6f5c\u5728\u8f6c\u6362\uff0c\u96be\u4ee5\u89e3\u91ca\u548c\u63a7\u5236\uff1b\u800c\u7a00\u758f\u8868\u793a\u6a21\u578b\u867d\u7136\u80fd\u53d1\u73b0\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u4f46\u4e3b\u8981\u5c40\u9650\u4e8e\u4e8b\u540e\u5206\u6790\u3002\u9700\u8981\u89e3\u51b3\u5bc6\u96c6\u6f5c\u5728\u63a8\u7406\u4e0e\u7a00\u758f\u8868\u793a\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "method": "\u63d0\u51faLSTR\u6846\u67b6\uff0c\u4f7f\u7528\u5177\u6709\u6b8b\u5dee\u8df3\u8dc3\u67b6\u6784\u7684\u6f5c\u5728\u8f6c\u6362\u7f16\u7801\u5668\uff0c\u5c06\u7ebf\u6027\u6d41\u5f62\u4f20\u8f93\u4e0e\u7a00\u758f\u8bed\u4e49\u66f4\u65b0\u89e3\u8026\uff0c\u901a\u8fc7\u663e\u5f0f\u7a00\u758f\u7ea6\u675f\u5b9e\u73b0\u53ef\u63a7\u8bed\u4e49\u5206\u8fa8\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLSTR\u5728\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u548c\u538b\u7f29\u6548\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u5bf9\u4e8e\u5bc6\u96c6\u6f5c\u5728\u57fa\u7ebf\u7684\u53ef\u89e3\u91ca\u6027\u3002\u56e0\u679c\u5e72\u9884\u548c\u8f68\u8ff9\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u660e\u8fd9\u4e9b\u7a00\u758f\u7279\u5f81\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e2\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7b97\u5b50\uff0c\u53c8\u5177\u6709\u56e0\u679c\u6709\u6548\u6027\u3002", "conclusion": "LSTR\u6210\u529f\u5c06\u7a00\u758f\u8868\u793a\u4ece\u88ab\u52a8\u5206\u6790\u63d0\u5347\u4e3a\u4e3b\u52a8\u63a8\u7406\u7b97\u5b50\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u63a7\u7684\u6f5c\u5728\u63a8\u7406\uff0c\u4e3a\u89e3\u51b3\u5bc6\u96c6\u6f5c\u5728\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.00577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00577", "abs": "https://arxiv.org/abs/2602.00577", "authors": ["Yuze Wang", "Yujia Tong", "Ke Xu", "Jingling Yuan", "Jiawei Jiang", "Chuang Hu"], "title": "Sparsity-Aware Unlearning for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.", "AI": {"tldr": "\u63d0\u51faSAU\u65b9\u6cd5\u89e3\u51b3\u7a00\u758f\u5316\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6548\u679c\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u63a9\u7801\u548c\u91cd\u8981\u6027\u91cd\u5206\u5e03\u5b9e\u73b0\u6709\u6548\u9057\u5fd8", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u4f1a\u8bb0\u5fc6\u654f\u611f\u4fe1\u606f\uff0c\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9488\u5bf9\u5bc6\u96c6\u6a21\u578b\u8bbe\u8ba1\uff0c\u5728\u7a00\u758f\u6a21\u578b\u4e0a\u6548\u679c\u663e\u8457\u4e0b\u964d\uff0c\u5f71\u54cd\u6a21\u578b\u9690\u79c1\u4fdd\u62a4", "method": "\u63d0\u51fa\u7a00\u758f\u611f\u77e5\u9057\u5fd8(SAU)\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u68af\u5ea6\u63a9\u7801\u5c06\u66f4\u65b0\u91cd\u5b9a\u5411\u5230\u5b58\u6d3b\u6743\u91cd\uff1b2) \u91cd\u8981\u6027\u611f\u77e5\u91cd\u5206\u5e03\u8865\u507f\u88ab\u526a\u679d\u53c2\u6570", "result": "SAU\u5728\u7a00\u758fLLMs\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6709\u6548\u9057\u5fd8\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027", "conclusion": "SAU\u89e3\u51b3\u4e86\u7a00\u758f\u6a21\u578b\u9057\u5fd8\u6548\u679c\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548LLM\u90e8\u7f72\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01640", "abs": "https://arxiv.org/abs/2602.01640", "authors": ["Shuai Zhang", "Jiayu Hu", "Zijie Chen", "Zeyuan Ding", "Yi Zhang", "Yingji Zhang", "Ziyi Zhou", "Junwei Liao", "Shengjie Zhou", "Yong Dai", "Zhenzhong Lan", "Xiaozhu Ju"], "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain", "comment": null, "summary": "Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.", "AI": {"tldr": "A2Eval\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u5177\u8eab\u667a\u80fd\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u534f\u4f5c\u4ee3\u7406\u81ea\u52a8\u751f\u6210\u5e73\u8861\u3001\u7d27\u51d1\u7684\u8bc4\u4f30\u5957\u4ef6\u548c\u6267\u884c\u8bc4\u4f30\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5177\u8eabVLM\u8bc4\u4f30\u4f9d\u8d56\u9759\u6001\u3001\u4e13\u5bb6\u5b9a\u4e49\u3001\u624b\u52a8\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b58\u5728\u4e25\u91cd\u5197\u4f59\u548c\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8fd9\u79cd\u52b3\u52a8\u5bc6\u96c6\u578b\u8303\u5f0f\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u548c\u6807\u6ce8\u8d44\u6e90\uff0c\u589e\u52a0\u6210\u672c\uff0c\u626d\u66f2\u6a21\u578b\u6392\u540d\uff0c\u963b\u788d\u8fed\u4ee3\u5f00\u53d1\u3002", "method": "\u63d0\u51faAgentic Automatic Evaluation (A2Eval)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u4ee3\u7406\uff1aData Agent\u81ea\u52a8\u5f52\u7eb3\u80fd\u529b\u7ef4\u5ea6\u5e76\u7ec4\u88c5\u5e73\u8861\u3001\u7d27\u51d1\u7684\u8bc4\u4f30\u5957\u4ef6\uff1bEval Agent\u5408\u6210\u548c\u9a8c\u8bc1\u53ef\u6267\u884c\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u9ad8\u4fdd\u771f\u8bc4\u4f30\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c13\u4e2a\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cA2Eval\u5c06\u8bc4\u4f30\u5957\u4ef6\u538b\u7f2985%\uff0c\u603b\u4f53\u8ba1\u7b97\u6210\u672c\u964d\u4f4e77%\uff0c\u901f\u5ea6\u63d0\u53474.6\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002\u7ea0\u6b63\u7cfb\u7edf\u6027\u6392\u540d\u504f\u5dee\uff0c\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u5ea6(Spearman's rho=0.85)\uff0c\u4fdd\u6301\u9ad8\u6392\u540d\u4fdd\u771f\u5ea6(Kendall's tau=0.81)\u3002", "conclusion": "A2Eval\u4e3a\u9ad8\u4fdd\u771f\u3001\u4f4e\u6210\u672c\u7684\u5177\u8eab\u667a\u80fd\u8bc4\u4f30\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u4e3b\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2602.00810", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00810", "abs": "https://arxiv.org/abs/2602.00810", "authors": ["Ze Huang", "Zhongyang Xiao", "Mingliang Song", "Longan Yang", "Hongyuan Yuan", "Li Sun"], "title": "VVLoc: Prior-free 3-DoF Vehicle Visual Localization", "comment": null, "summary": "Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.", "AI": {"tldr": "VVLoc\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8f66\u8f86\u5b9a\u4f4d\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u540c\u65f6\u5b9e\u73b0\u62d3\u6251\u5b9a\u4f4d\u548c\u5ea6\u91cf\u5b9a\u4f4d\uff0c\u5e76\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406\u62d3\u6251\u5b9a\u4f4d\u548c\u5ea6\u91cf\u5b9a\u4f4d\uff0c\u4f9d\u8d56\u5355\u6444\u50cf\u5934\u8bbe\u7f6e\uff0c\u9700\u8981\u989d\u5916\u76843D\u8bed\u4e49\u6216\u59ff\u6001\u5148\u9a8c\uff0c\u4e14\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u91cf\u5316\u673a\u5236\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u9700\u6c42\u3002", "method": "VVLoc\u91c7\u7528\u7edf\u4e00\u6d41\u7a0b\uff0c\u9996\u5148\u8bc4\u4f30\u89c6\u89c9\u89c2\u6d4b\u4e4b\u95f4\u7684\u5730\u7406\u90bb\u8fd1\u6027\uff0c\u7136\u540e\u4f7f\u7528\u5339\u914d\u7b56\u7565\u4f30\u8ba1\u76f8\u5bf9\u5ea6\u91cf\u59ff\u6001\uff0c\u540c\u65f6\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u3002\u8bad\u7ec3\u8fc7\u7a0b\u9ad8\u6548\uff0c\u4ec5\u9700\u89c6\u89c9\u6570\u636e\u5bf9\u548c\u5bf9\u5e94\u7684\u771f\u5b9e\u59ff\u6001\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u66f4\u5177\u6311\u6218\u6027\u7684\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cVVLoc\u5728\u5e7f\u6cdb\u7684\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "VVLoc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u8f66\u8f86\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u62d3\u6251\u548c\u5ea6\u91cf\u5b9a\u4f4d\u4efb\u52a1\uff0c\u5e76\u91cf\u5316\u5b9a\u4f4d\u7f6e\u4fe1\u5ea6\uff0c\u66f4\u9002\u5408\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2602.01699", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01699", "abs": "https://arxiv.org/abs/2602.01699", "authors": ["Willem Fourie"], "title": "Mitigating loss of control in advanced AI systems through instrumental goal trajectories", "comment": null, "summary": "Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u5de5\u5177\u6027\u76ee\u6807\u8f68\u8ff9\"\u6982\u5ff5\uff0c\u901a\u8fc7\u76d1\u63a7AI\u7cfb\u7edf\u83b7\u53d6\u8d44\u6e90\uff08\u8ba1\u7b97\u3001\u5b58\u50a8\u3001\u6570\u636e\u3001\u8d44\u91d1\uff09\u7684\u7ec4\u7ec7\u8def\u5f84\u6765\u589e\u5f3a\u4eba\u7c7b\u63a7\u5236\uff0c\u5c06\u5e72\u9884\u70b9\u4ece\u6a21\u578b\u672c\u8eab\u6269\u5c55\u5230\u7ec4\u7ec7\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u5c42\u9762\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u3001\u53ef\u7ea0\u6b63\u6027\u8bbe\u8ba1\uff09\uff0c\u4f46\u9ad8\u5ea6\u667a\u80fd\u7684AI\u7cfb\u7edf\u53ef\u80fd\u901a\u8fc7\u8ffd\u6c42\u5de5\u5177\u6027\u76ee\u6807\u4fb5\u8680\u4eba\u7c7b\u63a7\u5236\u3002\u9700\u8981\u8d85\u8d8a\u6a21\u578b\u672c\u8eab\uff0c\u4ece\u7ec4\u7ec7\u5c42\u9762\u5bfb\u627e\u65b0\u7684\u5e72\u9884\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u7ec4\u7ec7\u8def\u5f84\u4f5c\u4e3a\u5de5\u5177\u6027\u76ee\u6807\u8f68\u8ff9\uff1a\u91c7\u8d2d\u8def\u5f84\uff08\u83b7\u53d6\u6280\u672f\u8d44\u6e90\uff09\u3001\u6cbb\u7406\u8def\u5f84\uff08\u7ec4\u7ec7\u51b3\u7b56\uff09\u3001\u8d22\u52a1\u8def\u5f84\uff08\u83b7\u53d6\u8d44\u91d1\uff09\u3002\u901a\u8fc7\u76d1\u63a7\u8fd9\u4e9b\u8def\u5f84\u4ea7\u751f\u7684\u7ec4\u7ec7\u75d5\u8ff9\uff08\u5982\u91c7\u8d2d\u8bb0\u5f55\u3001\u6cbb\u7406\u51b3\u7b56\u3001\u8d22\u52a1\u4ea4\u6613\uff09\u6765\u8bc6\u522b\u5e72\u9884\u70b9\u3002", "result": "\u5de5\u5177\u6027\u76ee\u6807\u8f68\u8ff9\u4e3a\u5b9a\u4e49AI\u80fd\u529b\u6c34\u5e73\u63d0\u4f9b\u4e86\u5177\u4f53\u9014\u5f84\uff0c\u5e76\u6269\u5c55\u4e86\u53ef\u7ea0\u6b63\u6027\u548c\u53ef\u4e2d\u65ad\u6027\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5c06\u5173\u6ce8\u70b9\u4ece\u6a21\u578b\u5c5e\u6027\u8f6c\u79fb\u5230\u652f\u6301\u6a21\u578b\u7684\u7ec4\u7ec7\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u76d1\u63a7AI\u7cfb\u7edf\u83b7\u53d6\u8d44\u6e90\u7684\u7ec4\u7ec7\u8def\u5f84\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5b9e\u65bd\u4eba\u7c7b\u63a7\u5236\uff0c\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u8d85\u8d8a\u6280\u672f\u5c42\u9762\u7684\u7ec4\u7ec7\u5e72\u9884\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u7684\u5168\u9762\u6027\u3002"}}
{"id": "2602.00582", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00582", "abs": "https://arxiv.org/abs/2602.00582", "authors": ["Xiangfei Qiu", "Kangjia Yan", "Xvyuan Liu", "Xingjian Wu", "Jilin Hu"], "title": "Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting", "comment": null, "summary": "Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.", "AI": {"tldr": "TFMixer\u662f\u4e00\u4e2a\u7528\u4e8e\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8054\u5408\u65f6\u9891\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u975e\u5747\u5300\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u9891\u8c31\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u67e5\u8be2\u7684\u8865\u4e01\u6df7\u5408\u673a\u5236\u8fdb\u884c\u5c40\u90e8\u65f6\u95f4\u5efa\u6a21\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u975e\u5747\u5300\u91c7\u6837\u548c\u53d8\u91cf\u5f02\u6b65\u6027\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u4e0d\u89c4\u5219\u6027\u8fdd\u53cd\u4e86\u6807\u51c6\u6a21\u578b\u7684\u7b49\u8ddd\u5047\u8bbe\uff0c\u963b\u788d\u4e86\u5c40\u90e8\u65f6\u95f4\u5efa\u6a21\uff0c\u5e76\u4f7f\u7ecf\u5178\u9891\u57df\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5168\u5c40\u5468\u671f\u7ed3\u6784\u3002", "method": "TFMixer\u5305\u542b\u5168\u5c40\u9891\u7387\u6a21\u5757\uff08\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u975e\u5747\u5300\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u76f4\u63a5\u4ece\u4e0d\u89c4\u5219\u65f6\u95f4\u6233\u63d0\u53d6\u9891\u8c31\u8868\u793a\uff09\u548c\u5c40\u90e8\u65f6\u95f4\u6a21\u5757\uff08\u5f15\u5165\u57fa\u4e8e\u67e5\u8be2\u7684\u8865\u4e01\u6df7\u5408\u673a\u5236\u81ea\u9002\u5e94\u805a\u5408\u4fe1\u606f\u65f6\u95f4\u8865\u4e01\uff09\uff0c\u6700\u540e\u878d\u5408\u65f6\u57df\u548c\u9891\u57df\u8868\u793a\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u9006NUDFT\u8fdb\u884c\u663e\u5f0f\u5b63\u8282\u5916\u63a8\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660eTFMixer\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TFMixer\u901a\u8fc7\u8054\u5408\u65f6\u9891\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u5904\u7406\u975e\u5747\u5300\u91c7\u6837\u548c\u5f02\u6b65\u53d8\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01654", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01654", "abs": "https://arxiv.org/abs/2602.01654", "authors": ["Jiaqian Li", "Yanshu Li", "Kuan-Hao Huang"], "title": "Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models", "comment": null, "summary": "Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.", "AI": {"tldr": "\u63d0\u51faSteering Vector Fields (SVF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u6982\u5ff5\u8bc4\u5206\u51fd\u6570\u7684\u5c40\u90e8\u68af\u5ea6\u5b9a\u4e49\u6bcf\u4e2a\u6fc0\u6d3b\u7684\u8f6c\u5411\u65b9\u5411\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u8f6c\u5411\u5411\u91cf\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8f6c\u5411\u5411\u91cf(SVs)\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff1a\u6709\u4e9b\u6982\u5ff5\u4e0d\u53ef\u8f6c\u5411\uff0c\u5373\u4f7f\u5e73\u5747\u6709\u6548\u4e5f\u53ef\u80fd\u5bf9\u90e8\u5206\u8f93\u5165\u4ea7\u751f\u53cd\u6548\u679c\uff0c\u4e14\u5728\u957f\u6587\u672c\u751f\u6210\u548c\u591a\u5c5e\u6027\u8f6c\u5411\u4e2d\u53ef\u9760\u6027\u4e0b\u964d\u3002\u9759\u6001\u8f6c\u5411\u5411\u91cf\u5047\u8bbe\u6982\u5ff5\u6539\u8fdb\u65b9\u5411\u5728\u6240\u6709\u4e0a\u4e0b\u6587\u4e2d\u6052\u5b9a\uff0c\u5f53\u5c40\u90e8\u6709\u6548\u65b9\u5411\u968f\u5f53\u524d\u6fc0\u6d3b\u53d8\u5316\u65f6\uff0c\u5355\u4e00\u5168\u5c40\u5411\u91cf\u4f1a\u9519\u4f4d\uff0c\u5bfc\u81f4\u6548\u679c\u5f31\u5316\u6216\u53cd\u5411\u3002", "method": "\u63d0\u51faSteering Vector Fields (SVF)\u65b9\u6cd5\uff1a\u5b66\u4e60\u4e00\u4e2a\u53ef\u5fae\u7684\u6982\u5ff5\u8bc4\u5206\u51fd\u6570\uff0c\u5176\u5c40\u90e8\u68af\u5ea6\u5b9a\u4e49\u6bcf\u4e2a\u6fc0\u6d3b\u70b9\u7684\u8f6c\u5411\u65b9\u5411\uff0c\u4f7f\u5e72\u9884\u660e\u786e\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002\u8be5\u6846\u67b6\u652f\u6301\u5728\u5171\u4eab\u5bf9\u9f50\u6982\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u534f\u8c03\u7684\u591a\u5c42\u5e72\u9884\uff0c\u5e76\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u6587\u672c\u548c\u591a\u5c5e\u6027\u63a7\u5236\u3002", "result": "\u5728\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8f6c\u5411\u4efb\u52a1\u4e2d\uff0cSVF\u63d0\u4f9b\u4e86\u66f4\u5f3a\u3001\u66f4\u53ef\u9760\u7684\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u65f6\u8f6c\u5411\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u8f6c\u5411\u5411\u91cf\u5931\u8d25\u539f\u56e0\uff0c\u63d0\u51faSVF\u65b9\u6cd5\u4f7f\u8f6c\u5411\u65b9\u5411\u660e\u786e\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u8f6c\u5411\u5411\u91cf\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a8\u7406\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00813", "abs": "https://arxiv.org/abs/2602.00813", "authors": ["Tong Wang", "Yunhan Zhao", "Shu Kong"], "title": "Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.", "AI": {"tldr": "Paracosm\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\"\u5fc3\u7406\u56fe\u50cf\"\u6765\u76f4\u63a5\u5339\u914d\u76ee\u6807\u56fe\u50cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\"\u5fc3\u7406\u56fe\u50cf\"\u65e0\u6cd5\u76f4\u63a5\u83b7\u53d6\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u5339\u914d\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u591f\u51c6\u786e\u3002\u672c\u6587\u63d0\u51fa\u76f4\u63a5\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u51fa\u53d1\uff0c\u751f\u6210\"\u5fc3\u7406\u56fe\u50cf\"\u6765\u8fdb\u884c\u66f4\u7cbe\u786e\u7684\u5339\u914d\u3002", "method": "Paracosm\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u7ed9\u5b9a\u7684\u591a\u6a21\u6001\u67e5\u8be2\u751f\u6210\"\u5fc3\u7406\u56fe\u50cf\"\uff1b2\uff09\u4e3a\u6570\u636e\u5e93\u4e2d\u7684\u6bcf\u4e2a\u771f\u5b9e\u56fe\u50cf\u751f\u6210\u5bf9\u5e94\u7684\u5408\u6210\u56fe\u50cf\u4ee5\u89e3\u51b3\u57df\u5dee\u8ddd\u95ee\u9898\uff1b3\uff09\u5728LMM\u6784\u5efa\u7684\"\u62df\u50cf\u4e16\u754c\"\u4e2d\u8fdb\u884c\u5339\u914d\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\uff0c\u662f\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cParacosm\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672cCIR\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u751f\u6210\"\u5fc3\u7406\u56fe\u50cf\"\u800c\u975e\u6587\u672c\u63cf\u8ff0\uff0cParacosm\u4e3a\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u76f4\u63a5\u89c6\u89c9\u5339\u914d\u76f8\u5bf9\u4e8e\u6587\u672c\u4e2d\u4ecb\u5339\u914d\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.01711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01711", "abs": "https://arxiv.org/abs/2602.01711", "authors": ["Wei Chen", "Yanbin Fang", "Shuran Fu", "Fasheng Xu", "Xuan Wei"], "title": "Optimizing Prompts for Large Language Models: A Causal Approach", "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.", "AI": {"tldr": "CPO\u6846\u67b6\u5c06\u63d0\u793a\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56e0\u679c\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u673a\u5668\u5b66\u4e60\u6784\u5efa\u79bb\u7ebf\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u67e5\u8be2\u7279\u5b9a\u63d0\u793a\u4f18\u5316", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9759\u6001\u6307\u4ee4\u65e0\u6cd5\u9002\u5e94\u5f02\u6784\u67e5\u8be2\uff0c\u52a8\u6001\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u5956\u52b1\u6a21\u578b\u5b58\u5728\u6df7\u6dc6\u95ee\u9898\u3002\u4f01\u4e1aLLM\u90e8\u7f72\u9700\u8981\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6848", "method": "CPO\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a1) \u5e94\u7528\u53cc\u673a\u5668\u5b66\u4e60\u5230\u63d0\u793a\u548c\u67e5\u8be2\u7684\u8bed\u4e49\u5d4c\u5165\uff0c\u5b66\u4e60\u79bb\u7ebf\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u9694\u79bb\u63d0\u793a\u53d8\u4f53\u7684\u56e0\u679c\u6548\u5e94\uff1b2) \u5229\u7528\u65e0\u504f\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u8d44\u6e90\u9ad8\u6548\u7684\u67e5\u8be2\u7279\u5b9a\u63d0\u793a\u641c\u7d22", "result": "CPO\u5728\u6570\u5b66\u63a8\u7406\u3001\u53ef\u89c6\u5316\u548c\u6570\u636e\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u548c\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u4f18\u5316\u5668\uff0c\u7279\u522b\u662f\u5728\u56f0\u96be\u67e5\u8be2\u4e0a\u8868\u73b0\u66f4\u7a33\u5065", "conclusion": "\u56e0\u679c\u63a8\u65ad\u4e3a\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u901a\u8fc7\u5c06\u8bc4\u4f30\u4ece\u5b9e\u65f6\u6a21\u578b\u6267\u884c\u8f6c\u79fb\u5230\u79bb\u7ebf\u56e0\u679c\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c"}}
{"id": "2602.00587", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00587", "abs": "https://arxiv.org/abs/2602.00587", "authors": ["Mahesh Keswani", "Samyak Jain", "Raunak P. Bhattacharyya"], "title": "Safe Langevin Soft Actor Critic", "comment": "20 pages, 12 figures", "summary": "Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.", "AI": {"tldr": "SL-SAC\u662f\u4e00\u79cd\u5b89\u5168\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u63a2\u7d22\u548c\u5206\u5e03\u98ce\u9669\u63a7\u5236\u89e3\u51b3\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u4e0e\u5b89\u5168\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5728Safety-Gymnasium\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u4e0e\u5b89\u5168\u7684\u5e73\u8861\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u4ece\u5c16\u9510\u4ef7\u503c\u6700\u5c0f\u503c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4ee5\u53ca\u5bf9\u91cd\u5c3e\u98ce\u9669\u5206\u5e03\u5904\u7406\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u5173\u952e\u673a\u5236\uff1a1) \u4f7f\u7528\u81ea\u9002\u5e94\u968f\u673a\u68af\u5ea6\u6717\u4e4b\u4e07\u52a8\u529b\u5b66(aSGLD)\u8fdb\u884c\u5956\u52b1\u6279\u8bc4\u5668\u63a2\u7d22\uff1b2) \u901a\u8fc7\u9690\u5f0f\u5206\u4f4d\u6570\u7f51\u7edc(IQN)\u548c\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u4f18\u5316\u8fdb\u884c\u5206\u5e03\u6210\u672c\u4f30\u8ba1\uff1b3) \u57fa\u4e8e\u7ecf\u9a8cCVaR\u7684\u53cd\u5e94\u6027\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u65b9\u6848\u3002", "result": "\u5728Safety-Gymnasium\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSL-SAC\u572810\u4e2a\u4efb\u52a1\u4e2d\u76847\u4e2a\u5b9e\u73b0\u4e86\u6700\u4f4e\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u56de\u62a5\uff0c\u5728\u901f\u5ea6\u4efb\u52a1\u4e2d\u6210\u672c\u964d\u4f4e\u4e8619-63%\u3002", "conclusion": "SL-SAC\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u63a2\u7d22\u548c\u5206\u5e03\u98ce\u9669\u63a7\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u548c\u98ce\u9669\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.01660", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01660", "abs": "https://arxiv.org/abs/2602.01660", "authors": ["Zhongyuan Peng", "Caijun Xu", "Changyi Xiao", "Shibo Hong", "Eli Zhang", "Stephen Huang", "Yixin Cao"], "title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation", "comment": "11 pages, 5 tables, 5 figures", "summary": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.", "AI": {"tldr": "CoDiQ\u6846\u67b6\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u96be\u5ea6\u63a7\u5236\uff0c\u751f\u6210\u7ade\u8d5b\u7ea7\u96be\u9898\uff0c\u6784\u5efa44K\u9ad8\u8d28\u91cf\u95ee\u9898\u8bed\u6599\u5e93\uff0c\u663e\u8457\u63d0\u5347\u5927\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u7cbe\u786e\u96be\u5ea6\u63a7\u5236\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5927\u89c4\u6a21\u751f\u6210\u7ade\u8d5b\u7ea7\u96be\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u63a7\u5236\u96be\u5ea6\u540c\u65f6\u4fdd\u8bc1\u95ee\u9898\u53ef\u89e3\u6027\u7684\u6846\u67b6\u6765\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\u3002", "method": "\u63d0\u51faCoDiQ\u6846\u67b6\uff1a1) \u53d1\u73b0\u6d4b\u8bd5\u65f6\u7f29\u653e\u8d8b\u52bf\uff08\u6269\u5c55\u63a8\u7406token\u9884\u7b97\u589e\u52a0\u96be\u5ea6\u4f46\u964d\u4f4e\u53ef\u89e3\u6027\uff09\uff1b2) \u5f00\u53d1CoDiQ-Generator\uff08\u57fa\u4e8eQwen3-8B\uff09\uff0c\u63d0\u5347\u751f\u6210\u9ad8\u96be\u5ea6\u95ee\u9898\u7684\u4e0a\u9650\uff1b3) \u6784\u5efaCoDiQ-Corpus\uff0844K\u7ade\u8d5b\u7ea7\u95ee\u9898\u5e8f\u5217\uff09\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793aCoDiQ\u751f\u6210\u7684\u95ee\u9898\u6bd4LiveCodeBench/AIME\u663e\u8457\u66f4\u96be\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc782%\u7684\u53ef\u89e3\u6027\u3002\u5728CoDiQ-Corpus\u4e0a\u8bad\u7ec3\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "CoDiQ\u6846\u67b6\u901a\u8fc7\u53ef\u63a7\u96be\u5ea6\u7684\u95ee\u9898\u751f\u6210\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5f00\u6e90\u8bed\u6599\u5e93\u3001\u751f\u6210\u5668\u548c\u5b9e\u73b0\u652f\u6301\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2602.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00821", "abs": "https://arxiv.org/abs/2602.00821", "authors": ["Konstantinos Moutselos", "Ilias Maglogiannis"], "title": "Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis", "comment": "8 pages, 5 figures", "summary": "The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a \"Segment-by-Synthesis\" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u4e34\u5e8a\u76ae\u80a4\u79d1\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u53cd\u8f6c\u7684Rectified Flow Transformers\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u751f\u6210\u8eab\u4efd\u65e0\u5173\u7684\u75c5\u7406\u4fdd\u7559\u56fe\u50cf\uff0c\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u7684\u9ad8\u4fdd\u771f\u8eab\u4efd\u8f6c\u6362\u3002", "motivation": "\u4e34\u5e8a\u76ae\u80a4\u79d1\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u4e0e\u4fdd\u7559\u8bca\u65ad\u7279\u5f81\u7684\u53cc\u91cd\u6311\u6218\u3002\u4f20\u7edf\u53bb\u8bc6\u522b\u65b9\u6cd5\u4f1a\u964d\u4f4e\u75c5\u7406\u4fdd\u771f\u5ea6\uff0c\u800c\u6807\u51c6\u751f\u6210\u7f16\u8f91\u6280\u672f\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u53cd\u8f6c\u8fc7\u7a0b\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "method": "\u91c7\u7528\u65e0\u53cd\u8f6c\u7684Rectified Flow Transformers\uff08FlowEdit\uff09\u8fdb\u884c\u9ad8\u4fdd\u771f\u8eab\u4efd\u8f6c\u6362\uff1b\u5f15\u5165\"Segment-by-Synthesis\"\u673a\u5236\u5728\u672c\u5730\u751f\u6210\u53cd\u4e8b\u5b9e\u7684\u5065\u5eb7\u548c\u75c5\u7406\u53cc\u80de\u80ce\u5bf9\uff1b\u63d0\u53d6\u4e0e\u751f\u7269\u6807\u8bb0\u548c\u8bed\u4e49\u4f2a\u5f71\u89e3\u8026\u7684\u5dee\u5f02\u7ea2\u6591\u63a9\u7801\u3002", "result": "\u5728\u4e34\u5e8a\u9ad8\u5206\u8fa8\u7387\u6837\u672c\u4e0a\u9a8c\u8bc1\uff0c\u5408\u6210\u8eab\u4efd\u95f4\u7684IoU\u7a33\u5b9a\u6027\u5927\u4e8e0.67\uff1b\u8eab\u4efd\u8f6c\u6362\u65f6\u95f4\u5c11\u4e8e20\u79d2\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\uff1b\u751f\u6210\u7684\u9690\u79c1\u5408\u89c4\u5408\u6210\u66ff\u4ee3\u54c1\u4ece\u6e90\u5934\u51cf\u8f7b\u68af\u5ea6\u6cc4\u6f0f\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8054\u90a6\u73af\u5883\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u76ae\u80a4\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u5b89\u5168\u9014\u5f84\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u751f\u6210\u9690\u79c1\u5408\u89c4\u7684\u5408\u6210\u66ff\u4ee3\u54c1\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u8bca\u65ad\u7279\u5f81\u4fdd\u7559\u7684\u9700\u6c42\u3002"}}
{"id": "2602.01740", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01740", "abs": "https://arxiv.org/abs/2602.01740", "authors": ["Qixin Xiao", "Kun Zhou"], "title": "MACD: Model-Aware Contrastive Decoding via Counterfactual Data", "comment": null, "summary": "Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.", "AI": {"tldr": "MACD\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u6784\u5efa\u4e0e\u5bf9\u6bd4\u89e3\u7801\u76f8\u7ed3\u5408\uff0c\u51cf\u5c11\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5728\u89c6\u89c9\u8bc1\u636e\u5f31\u3001\u6a21\u7cca\u6216\u6709\u504f\u65f6\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u65e0\u6839\u636e\u7684\u5185\u5bb9\u3002\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff08\u5982\u5bf9\u6bd4\u89e3\u7801\uff09\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u6784\u5efa\u5bf9\u6bd4\u6570\u636e\uff0c\u96be\u4ee5\u63a7\u5236\u9a71\u52a8\u5e7b\u89c9\u7684\u89c6\u89c9\u7ebf\u7d22\u6216\u4e0e\u6a21\u578b\u5f31\u70b9\u826f\u597d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u5bf9\u6bd4\u89e3\u7801\uff08MACD\uff09\uff0c\u5229\u7528Video-LLM\u81ea\u8eab\u53cd\u9988\u8bc6\u522b\u5bfc\u81f4\u5e7b\u89c9\u7684\u5173\u952e\u5bf9\u8c61\u533a\u57df\uff0c\u5728\u5bf9\u8c61\u7ea7\u522b\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u53cd\u4e8b\u5b9e\u8f93\u5165\uff0c\u800c\u975e\u4efb\u610f\u7684\u5e27\u6216\u65f6\u5e8f\u4fee\u6539\u3002\u5c06\u8fd9\u4e9b\u6a21\u578b\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u96c6\u6210\u5230\u5bf9\u6bd4\u89e3\u7801\u4e2d\uff0c\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u8bc1\u636e\u57fa\u7840\u7684\u6807\u8bb0\u9009\u62e9\u3002", "result": "\u5728EventHallusion\u3001MVBench\u3001Perception-test\u548cVideo-MME\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMACD\u80fd\u6301\u7eed\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u5728Qwen\u548cInternVL\u7b49\u591a\u6837\u5316Video-LLM\u4e2d\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5c0f\u7269\u4f53\u3001\u906e\u6321\u7269\u4f53\u6216\u5171\u73b0\u7269\u4f53\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002", "conclusion": "MACD\u901a\u8fc7\u6a21\u578b\u5f15\u5bfc\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u6784\u5efa\u4e0e\u89e3\u7801\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u7b56\u7565\u6765\u51cf\u5c11\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.00589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00589", "abs": "https://arxiv.org/abs/2602.00589", "authors": ["Xiangfei Qiu", "Xvyuan Liu", "Tianen Shen", "Xingjian Wu", "Hanyin Cheng", "Bin Yang", "Jilin Hu"], "title": "SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement", "comment": null, "summary": "Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.", "AI": {"tldr": "SEER\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8865\u4e01\u66ff\u6362\u6a21\u5757\u52a8\u6001\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u8865\u4e01\uff0c\u5e76\u4f7f\u7528\u589e\u5f3a\u5d4c\u5165\u6a21\u5757\u63d0\u5347\u8868\u793a\u80fd\u529b\uff0c\u5728\u5b58\u5728\u6570\u636e\u8d28\u91cf\u95ee\u9898\u65f6\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e38\u5b58\u5728\u7f3a\u5931\u503c\u3001\u5206\u5e03\u6f02\u79fb\u3001\u5f02\u5e38\u548c\u566a\u58f0\u7b49\u4f4e\u8d28\u91cf\u95ee\u9898\uff0c\u5bfc\u81f4\u4f20\u7edf\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u4f7f\u7528\u6240\u6709\u8865\u4e01\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u52a8\u6001\u9009\u62e9\u8865\u4e01\uff0c\u5bf9\u4f4e\u8d28\u91cf\u4fe1\u606f\u654f\u611f\u3002", "method": "1. \u589e\u5f3a\u5d4c\u5165\u6a21\u5757\uff1a\u4f7f\u7528MoE\u67b6\u6784\u6539\u8fdb\u8865\u4e01\u7ea7\u8868\u793a\uff0c\u901a\u8fc7\u901a\u9053\u81ea\u9002\u5e94\u611f\u77e5\u673a\u5236\u83b7\u5f97\u5e8f\u5217\u7ea7\u4ee4\u724c\u8868\u793a\u30022. \u53ef\u5b66\u4e60\u8865\u4e01\u66ff\u6362\u6a21\u5757\uff1a\u4e24\u9636\u6bb5\u5904\u7406 - \u52a8\u6001\u8fc7\u6ee4\u673a\u5236\u6d88\u9664\u8d1f\u9762\u8865\u4e01\u7ea7\u4ee4\u724c\uff1b\u66ff\u6362\u6ce8\u610f\u529b\u6a21\u5757\u7528\u5168\u5c40\u5e8f\u5217\u7ea7\u4ee4\u724c\u66ff\u6362\u4f4e\u8d28\u91cf\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u7cbe\u70bc\u8868\u793a\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSEER\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5728\u5b58\u5728\u6570\u636e\u8d28\u91cf\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "SEER\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8fc7\u6ee4\u548c\u66ff\u6362\u4f4e\u8d28\u91cf\u8865\u4e01\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01672", "abs": "https://arxiv.org/abs/2602.01672", "authors": ["Siheng Xiong", "Oguzhan Gungordu", "Blair Johnson", "James C. Kerce", "Faramarz Fekri"], "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control", "comment": "Work in progress", "summary": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.", "AI": {"tldr": "DeepControl\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u6548\u7528\u7684\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\u7684\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\uff0c\u901a\u8fc7\u68c0\u7d22\u7ee7\u7eed\u548c\u7c92\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\u5728\u4ea4\u66ff\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u548c\u5916\u90e8\u4fe1\u606f\u68c0\u7d22\u65f6\uff0c\u5b58\u5728\u68c0\u7d22\u5197\u4f59\u3001\u4e0a\u4e0b\u6587\u9971\u548c\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4fe1\u606f\u83b7\u53d6\u7684\u8c03\u63a7\u6307\u5bfc\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u6548\u7528\u7684DeepControl\u6846\u67b6\uff0c\u4fe1\u606f\u6548\u7528\u8861\u91cf\u5728\u7ed9\u5b9a\u63a8\u7406\u72b6\u6001\u4e0b\u68c0\u7d22\u8bc1\u636e\u7684\u8fb9\u9645\u4ef7\u503c\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u68c0\u7d22\u7ee7\u7eed\u63a7\u5236\uff08\u51b3\u5b9a\u4f55\u65f6\u7ee7\u7eed/\u505c\u6b62\u68c0\u7d22\uff09\u548c\u7c92\u5ea6\u63a7\u5236\uff08\u51b3\u5b9a\u6269\u5c55\u591a\u5c11\u4fe1\u606f\uff09\u673a\u5236\uff0c\u91c7\u7528\u9000\u706b\u63a7\u5236\u7b56\u7565\u8ba9\u4ee3\u7406\u5728\u8bad\u7ec3\u4e2d\u5185\u5316\u6709\u6548\u7684\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u5728Qwen2.5-7B\u548cQwen2.5-3B\u4e0a\u5206\u522b\u5b9e\u73b0\u4e869.4%\u548c8.6%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u4e14\u59cb\u7ec8\u4f18\u4e8e\u65e0\u68c0\u7d22\u548c\u57fa\u4e8e\u68c0\u7d22\u4f46\u65e0\u663e\u5f0f\u4fe1\u606f\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u5bf9\u4e8e\u5c06\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\u6269\u5c55\u5230\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u73af\u5883\u81f3\u5173\u91cd\u8981\u3002DeepControl\u901a\u8fc7\u5f62\u5f0f\u5316\u7684\u4fe1\u606f\u6548\u7528\u548c\u81ea\u9002\u5e94\u63a7\u5236\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u7d22\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u9971\u548c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00839", "abs": "https://arxiv.org/abs/2602.00839", "authors": ["Mingwei Li", "Hehe Fan", "Yi Yang"], "title": "TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation", "comment": "Project Page: https://longxiang-ai.github.io/TransNormal", "summary": "Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25\u00b0 accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.", "AI": {"tldr": "TransNormal\uff1a\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u7684\u5355\u6b65\u6cd5\u7ebf\u56de\u5f52\u6846\u67b6\uff0c\u7528\u4e8e\u900f\u660e\u7269\u4f53\u5355\u76ee\u6cd5\u7ebf\u4f30\u8ba1\uff0c\u5728\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u900f\u660e\u7269\u4f53\u7684\u5355\u76ee\u6cd5\u7ebf\u4f30\u8ba1\u5bf9\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u5149\u7ebf\u6298\u5c04\u548c\u53cd\u5c04\uff0c\u4f20\u7edf\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4f20\u611f\u5668\u7ecf\u5e38\u5931\u8d25\uff0c\u963b\u788d\u4e86\u5177\u8eabAI\u5728\u79d1\u5b66\u73af\u5883\u4e2d\u7684\u90e8\u7f72", "method": "\u63d0\u51faTransNormal\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\u8fdb\u884c\u5355\u6b65\u6cd5\u7ebf\u56de\u5f52\uff1b\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408DINOv3\u7684\u5bc6\u96c6\u89c6\u89c9\u8bed\u4e49\u6765\u5904\u7406\u900f\u660e\u8868\u9762\u7f3a\u4e4f\u7eb9\u7406\u7684\u95ee\u9898\uff1b\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u76ee\u6807\u548c\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6b63\u5219\u5316\u6765\u4fdd\u6301\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7ec6\u8282", "result": "\u5728ClearGrasp\u57fa\u51c6\u4e0a\uff0c\u5e73\u5747\u8bef\u5dee\u964d\u4f4e24.4%\uff0c11.25\u00b0\u7cbe\u5ea6\u63d0\u9ad822.8%\uff1b\u5728ClearPose\u4e0a\uff0c\u5e73\u5747\u8bef\u5dee\u964d\u4f4e15.2%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "TransNormal\u901a\u8fc7\u6574\u5408\u6269\u6563\u5148\u9a8c\u548c\u89c6\u89c9\u8bed\u4e49\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u900f\u660e\u7269\u4f53\u6cd5\u7ebf\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86TransNormal-Synthetic\u6570\u636e\u96c6"}}
{"id": "2602.01749", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01749", "abs": "https://arxiv.org/abs/2602.01749", "authors": ["Lin Chen", "Samuel Drapeau", "Fanghao Shao", "Xuekai Zhu", "Bo Xue", "Yunchong Song", "Mathieu Lauri\u00e8re", "Zhouhan Lin"], "title": "Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives", "comment": null, "summary": "Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $\u03b1$-GFNs, which generalize the mixing via a tunable parameter $\u03b1$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $\u03b1$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \\times$ increase in the number of discovered modes.", "AI": {"tldr": "\u03b1-GFNs\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u03b1\u6539\u8fdbGFlowNet\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u4f20\u7edfGFlowNet\u76ee\u6807\u9690\u542b\u5730\u56fa\u5b9a\u4e86\u524d\u5411\u548c\u540e\u5411\u7b56\u7565\u7684\u5747\u5300\u6df7\u5408\uff0c\u8fd9\u9650\u5236\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002\u4f5c\u8005\u5e0c\u671b\u6253\u7834\u8fd9\u79cd\u7ea6\u675f\uff0c\u589e\u5f3a\u6a21\u5f0f\u53d1\u73b0\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5efa\u7acbGFlowNet\u76ee\u6807\u4e0e\u9a6c\u5c14\u53ef\u592b\u94fe\u53ef\u9006\u6027\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u7ea6\u675f\u7684\u8d77\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u03b1-GFNs\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u03b1\u6765\u6cdb\u5316\u6df7\u5408\u7b56\u7565\uff0c\u76f4\u63a5\u63a7\u5236\u63a2\u7d22-\u5229\u7528\u52a8\u6001\u3002", "result": "\u5728Set\u3001Bit Sequence\u548cMolecule Generation\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u03b1-GFN\u76ee\u6807\u59cb\u7ec8\u4f18\u4e8e\u4e4b\u524d\u7684GFlowNet\u76ee\u6807\uff0c\u53d1\u73b0\u7684\u6a21\u5f0f\u6570\u91cf\u6700\u591a\u589e\u52a0\u4e8610\u500d\u3002", "conclusion": "\u03b1-GFNs\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u5c06\u9a6c\u5c14\u53ef\u592b\u94fe\u6027\u8d28\u9002\u914d\u5230GFlowNets\u4e2d\uff0c\u63d0\u4f9b\u4e86\u5bf9\u63a2\u7d22-\u5229\u7528\u52a8\u6001\u7684\u76f4\u63a5\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u53d1\u73b0\u80fd\u529b\uff0c\u540c\u65f6\u786e\u4fdd\u6536\u655b\u5230\u552f\u4e00\u6d41\u3002"}}
{"id": "2602.00596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00596", "abs": "https://arxiv.org/abs/2602.00596", "authors": ["Govind Waghmare", "Srini Rohan Gujulla Leel", "Nikhil Tumbde", "Sumedh B G", "Sonia Gupta", "Srikanta Bedathur"], "title": "Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks", "comment": "Accepted at AAAI 2026", "summary": "Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.", "AI": {"tldr": "KEAT\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u6838\u51fd\u6570\u8c03\u5236\u8fb9\u7279\u5f81\uff0c\u89e3\u51b3\u4e86TGNN\u4e2d\u8282\u70b9\u548c\u8fb9\u7279\u5f81\u7ea0\u7f20\u5bfc\u81f4\u7684\u8bed\u4e49\u6ce8\u610f\u529b\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TGNN\u6a21\u578b\u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u65f6\u901a\u5e38\u5c06\u8282\u70b9\u548c\u8fb9\u8868\u793a\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u65e0\u6cd5\u53cd\u6620\u5b83\u4eec\u4e0d\u540c\u7684\u65f6\u95f4\u884c\u4e3a\u7279\u5f81\u3002\u8282\u70b9\u5d4c\u5165\u6f14\u5316\u7f13\u6162\uff08\u805a\u5408\u957f\u671f\u7ed3\u6784\u4e0a\u4e0b\u6587\uff09\uff0c\u800c\u8fb9\u7279\u5f81\u53cd\u6620\u77ac\u65f6\u7684\u5e26\u65f6\u95f4\u6233\u4ea4\u4e92\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u5bfc\u81f4\u8bed\u4e49\u6ce8\u610f\u529b\u6a21\u7cca\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u65e0\u6cd5\u533a\u5206\u7f13\u6162\u6f02\u79fb\u7684\u8282\u70b9\u72b6\u6001\u548c\u5feb\u901f\u53d8\u5316\u7684\u4fe1\u606f\u4e30\u5bcc\u7684\u8fb9\u4ea4\u4e92\u3002", "method": "\u63d0\u51faKEAT\uff08Kernelized Edge Attention for Temporal Graphs\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u516c\u5f0f\uff0c\u4f7f\u7528\u4e00\u7cfb\u5217\u8fde\u7eed\u65f6\u95f4\u6838\u51fd\u6570\uff08\u5305\u62ec\u62c9\u666e\u62c9\u65af\u6838\u3001RBF\u6838\u548c\u53ef\u5b66\u4e60\u7684MLP\u53d8\u4f53\uff09\u6765\u8c03\u5236\u8fb9\u7279\u5f81\u3002KEAT\u4fdd\u6301\u4e86\u8282\u70b9\u548c\u8fb9\u7684\u4e0d\u540c\u89d2\u8272\uff0c\u5e76\u80fd\u4e0eTransformer\u98ce\u683c\uff08\u5982DyGFormer\uff09\u548c\u6d88\u606f\u4f20\u9012\uff08\u5982TGN\uff09\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0cKEAT\u76f8\u6bd4\u6700\u8fd1\u7684DyGFormer\u5b9e\u73b0\u4e86\u9ad8\u8fbe18%\u7684MRR\u63d0\u5347\uff0c\u76f8\u6bd4TGN\u63d0\u5347\u4e867%\u3002\u8fd9\u4f7f\u5f97TGNN\u80fd\u591f\u8fdb\u884c\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u548c\u5177\u6709\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u7684\u6d88\u606f\u4f20\u9012\u3002", "conclusion": "KEAT\u901a\u8fc7\u6838\u51fd\u6570\u8c03\u5236\u8fb9\u7279\u5f81\uff0c\u89e3\u51b3\u4e86TGNN\u4e2d\u8282\u70b9\u548c\u8fb9\u7279\u5f81\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u56fe\u5efa\u6a21\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u65f6\u95f4\u611f\u77e5\u7684\u6d88\u606f\u4f20\u9012\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2602.01687", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01687", "abs": "https://arxiv.org/abs/2602.01687", "authors": ["Jung H. Lee", "Sujith Vijayan"], "title": "Counting Hypothesis: Potential Mechanism of In-Context Learning", "comment": "19 pages, 7 main Figures, 1 Table and 6 Supp. Figures", "summary": "In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faICL\u7684\"\u8ba1\u6570\u5047\u8bf4\"\uff0c\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7f16\u7801\u7b56\u7565\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u800c\u975e\u5185\u90e8\u7ed3\u6784\u4fee\u6539", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u867d\u7136\u4e3aLLMs\u63d0\u4f9b\u4e86\u65e0\u9700\u4fee\u6539\u5185\u90e8\u7ed3\u6784\u5c31\u80fd\u6267\u884c\u591a\u79cd\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5e95\u5c42\u673a\u5236\u4ecd\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u9519\u8bef\u4fee\u6b63\u548c\u8bca\u65ad\u56f0\u96be\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3ICL\u7684\u5c40\u9650\u6027\u548c\u5de5\u4f5c\u539f\u7406", "method": "\u57fa\u4e8eICL\u7279\u6027\u548cLLMs\u529f\u80fd\u6a21\u5757\uff0c\u63d0\u51fa\"\u8ba1\u6570\u5047\u8bf4\"\uff0c\u8ba4\u4e3aLLMs\u7684\u7f16\u7801\u7b56\u7565\u53ef\u80fd\u662fICL\u7684\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u652f\u6301\u8bc1\u636e", "result": "\u63d0\u51fa\u4e86ICL\u7684\"\u8ba1\u6570\u5047\u8bf4\"\uff0c\u4e3a\u7406\u89e3LLMs\u5982\u4f55\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6", "conclusion": "\u7406\u89e3ICL\u7684\u7f16\u7801\u673a\u5236\u5bf9\u4e8e\u66f4\u597d\u5730\u5229\u7528LLMs\u5728\u4e0d\u540c\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u8ba1\u6570\u5047\u8bf4\u4e3a\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.00841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00841", "abs": "https://arxiv.org/abs/2602.00841", "authors": ["Jintao Cheng", "Weibin Li", "Zhijian He", "Jin Wu", "Chi Man Vong", "Wei Zhang"], "title": "Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition", "comment": "14pages, 5 figures", "summary": "Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u51e0\u4f55\u7edf\u8ba1\u7684\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6355\u6349\u51e0\u4f55\u7a33\u5b9a\u6027\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f53\u524d\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u91cf\u76d1\u7763\u6570\u636e\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u7684\u4e00\u9636\u7edf\u8ba1\uff0c\u5ffd\u7565\u4e86\u5185\u5728\u7684\u7ed3\u6784\u76f8\u5173\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u5267\u70c8\u73af\u5883\u548c\u89c6\u89d2\u53d8\u5316\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u9c81\u68d2\u8868\u793a\u65b9\u6cd5", "method": "\u5c06\u573a\u666f\u5efa\u6a21\u4e3aSPD\u6d41\u5f62\u4e0a\u7684\u534f\u65b9\u5dee\u63cf\u8ff0\u7b26\uff0c\u5c06\u6270\u52a8\u89c6\u4e3a\u53ef\u5904\u7406\u7684\u540c\u4f59\u53d8\u6362\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u9ece\u66fc\u6620\u5c04\u5c06\u8fd9\u4e9b\u63cf\u8ff0\u7b26\u6295\u5f71\u5230\u7ebf\u6027\u5316\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u4e2d\uff0c\u5206\u79bb\u4fe1\u53f7\u7ed3\u6784\u548c\u566a\u58f0", "result": "\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "\u63d0\u51fa\u7684\u4e8c\u9636\u51e0\u4f55\u7edf\u8ba1\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u6355\u6349\u51e0\u4f55\u7a33\u5b9a\u6027\uff0c\u4e3a\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01750", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01750", "abs": "https://arxiv.org/abs/2602.01750", "authors": ["Mohammad Beigi", "Ming Jin", "Junshan Zhang", "Qifan Wang", "Lifu Huang"], "title": "Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.", "AI": {"tldr": "ARA\u6846\u67b6\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u91cd\u6784\u4e3a\u52a8\u6001\u7ade\u4e89\u6e38\u620f\uff0c\u901a\u8fc7\u9ed1\u5ba2\u53d1\u73b0\u6f0f\u6d1e\u3001\u5ba1\u8ba1\u5458\u68c0\u6d4b\u5229\u7528\uff0c\u518d\u901a\u8fc7\u5ba1\u8ba1\u5f15\u5bfc\u7684RLHF\u60e9\u7f5a\u9ed1\u5ba2\u884c\u4e3a\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u53ef\u6cdb\u5316\u7684\u5bf9\u9f50\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u6a21\u578b\u4f1a\u5229\u7528\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u83b7\u5f97\u9ad8\u5206\u4f46\u8fdd\u80cc\u4eba\u7c7b\u610f\u56fe\u3002\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u7684\u653b\u51fb\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u5956\u52b1\u5ba1\u8ba1(ARA)\u6846\u67b6\uff1a1) \u9ed1\u5ba2\u7b56\u7565\u53d1\u73b0\u5956\u52b1\u6a21\u578b\u6f0f\u6d1e\uff0c\u5ba1\u8ba1\u5458\u4ece\u6f5c\u5728\u8868\u793a\u4e2d\u5b66\u4e60\u68c0\u6d4b\u5229\u7528\u884c\u4e3a\uff1b2) \u5ba1\u8ba1\u5f15\u5bfc\u7684RLHF(AG-RLHF)\u901a\u8fc7\u95e8\u63a7\u5956\u52b1\u4fe1\u53f7\u60e9\u7f5a\u68c0\u6d4b\u5230\u7684\u9ed1\u5ba2\u884c\u4e3a\uff0c\u5c06\u9ed1\u5ba2\u653b\u51fb\u4ece\u4e0d\u89c2\u5bdf\u5230\u7684\u5931\u8d25\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u3001\u53ef\u63a7\u5236\u7684\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u79cd\u9ed1\u5ba2\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cARA\u5728\u6240\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u5bf9\u9f50-\u6548\u7528\u6743\u8861\uff1a\u5c06\u5949\u627f\u884c\u4e3a\u964d\u81f3\u63a5\u8fd1SFT\u6c34\u5e73\u540c\u65f6\u63d0\u9ad8\u5e2e\u52a9\u6027\uff0c\u51cf\u5c11\u5197\u957f\u540c\u65f6\u83b7\u5f97\u6700\u9ad8ROUGE-L\uff0c\u6291\u5236\u4ee3\u7801\u6e38\u620f\u540c\u65f6\u63d0\u9ad8Pass@1\u3002\u9ed1\u5ba2\u653b\u51fb\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u90fd\u80fd\u8de8\u9886\u57df\u6cdb\u5316\u3002", "conclusion": "ARA\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a8\u6001\u7ade\u4e89\u6e38\u620f\uff0c\u901a\u8fc7\u53ef\u6cdb\u5316\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u9886\u57df\u9632\u5fa1\uff0c\u4e3aRLHF\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u6846\u67b6\u3002"}}
{"id": "2602.00603", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00603", "abs": "https://arxiv.org/abs/2602.00603", "authors": ["Luca Viano", "Ruida Zhou", "Yifan Sun", "Mahdi Namazifar", "Volkan Cevher", "Shoham Sabach", "Mohammad Ghavamzadeh"], "title": "Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains", "comment": null, "summary": "The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u6765\u63d0\u5347\u5bf9\u9f50\u6548\u679c\uff0c\u76f8\u6bd4\u4f20\u7edfDPO\u7b97\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u7edf\u8ba1\u6536\u655b\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfDPO\u7b97\u6cd5\u4ec5\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u53cd\u9988\uff0c\u8fd9\u79cd\u53cd\u9988\u5f62\u5f0f\u867d\u7136\u6570\u636e\u6536\u96c6\u5bb9\u6613\uff0c\u4f46\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u65e0\u6cd5\u533a\u5206\u504f\u597d\u7684\u7a0b\u5ea6\u5dee\u5f02\u3002\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\uff08\u5373\u88ab\u9009\u56de\u7b54\u6bd4\u88ab\u62d2\u56de\u7b54\u597d\u591a\u5c11\uff09\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4fe1\u53f7\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8fd9\u4e00\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u5229\u7528\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u6765\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u3002\u7b97\u6cd5\u4e0d\u4ec5\u8003\u8651\u4e86\u54ea\u4e2a\u56de\u7b54\u66f4\u597d\uff0c\u8fd8\u8003\u8651\u4e86\u597d\u7684\u7a0b\u5ea6\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u786e\u4fdd\u7b97\u6cd5\u5728\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u51c6\u786e\u65f6\u80fd\u8fbe\u5230\u66f4\u5feb\u7684\u7edf\u8ba1\u6536\u655b\u901f\u5ea6\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u51c6\u786e\u65f6\u6bd4DPO\u7b97\u6cd5\u6536\u655b\u66f4\u5feb\uff0c\u5373\u4f7f\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u4e0d\u51c6\u786e\uff0c\u7b97\u6cd5\u6027\u80fd\u4e5f\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728\u591a\u79cdLLM\u548c\u8bc4\u4f30\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u4f18\u4e8e\u591a\u79cdDPO\u98ce\u683c\u7684\u7b97\u6cd5\u3002", "conclusion": "\u5229\u7528\u8bc4\u5206\u5dee\u8ddd\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u65b0\u7b97\u6cd5\u5728\u7edf\u8ba1\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u4f20\u7edfDPO\u65b9\u6cd5\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01698", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01698", "abs": "https://arxiv.org/abs/2602.01698", "authors": ["Wenhui Tan", "Fiorenzo Parascandolo", "Enver Sangineto", "Jianzhong Ju", "Zhenbo Luo", "Qian Cao", "Rita Cucchiara", "Ruihua Song", "Jian Luan"], "title": "Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLatent Exploration Decoding (LED)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u9ad8\u71b5\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u5bfc\u81f4\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684pass@n\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u4ee3\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u540e\uff0c\u6e29\u5ea6\u91c7\u6837\u4e0d\u518d\u80fd\u63d0\u5347pass@n\u51c6\u786e\u7387\uff0c\u51fa\u73b0\u4e86\"\u63a2\u7d22\u5d29\u6e83\"\u73b0\u8c61\u3002\u7814\u7a76\u53d1\u73b0\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u7ec8\u5c42\u540e\u9a8c\u5206\u5e03\u71b5\u6025\u5267\u51cf\u5c11\uff0c\u800c\u4e2d\u95f4\u5c42\u71b5\u4fdd\u6301\u8f83\u9ad8\uff0c\u8fd9\u79cd\u71b5\u4e0d\u5bf9\u79f0\u6027\u4e3a\u6539\u8fdb\u89e3\u7801\u7b56\u7565\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faLatent Exploration Decoding (LED)\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u7d2f\u79ef\u548c\u805a\u5408\u4e2d\u95f4\u5c42\u540e\u9a8c\u5206\u5e03\uff1b2\uff09\u9009\u62e9\u5177\u6709\u6700\u5927\u71b5\u7684\u6df1\u5ea6\u914d\u7f6e\u4f5c\u4e3a\u63a2\u7d22\u5019\u9009\uff1b3\uff09\u8fd9\u662f\u4e00\u79cd\u6df1\u5ea6\u6761\u4ef6\u89e3\u7801\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\uff0cLED\u4e00\u81f4\u6027\u5730\u5c06pass@1\u548cpass@16\u51c6\u786e\u7387\u5206\u522b\u63d0\u53470.61\u548c1.03\u4e2a\u767e\u5206\u70b9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "LED\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u9ad8\u71b5\u7279\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u540e\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\u7684\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00865", "abs": "https://arxiv.org/abs/2602.00865", "authors": ["Brandon Leblanc", "Charalambos Poullis"], "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware", "comment": "Submitted to the Canadian Conference on Robotics and Vision (CRV). 10 pages, 5 figures", "summary": "While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.", "AI": {"tldr": "Distill3R\uff1a\u4e00\u4e2a\u5c06\u5927\u578b3D\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u53ef\u5728\u5355\u5de5\u4f5c\u7ad9\u4e0a\u8bad\u7ec3\u7684\u5c0f\u578b\u5b66\u751f\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5b9e\u73b09\u500d\u53c2\u6570\u51cf\u5c11\u548c5\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe3D\u91cd\u5efa\u4f9d\u8d56\u4e8e\u9700\u8981\u5927\u89c4\u6a21\u8ba1\u7b97\u96c6\u7fa4\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u8fd9\u4e3a\u5927\u591a\u6570\u5b66\u672f\u5b9e\u9a8c\u5ba4\u8bbe\u7f6e\u4e86\u5f88\u9ad8\u7684\u8fdb\u5165\u95e8\u69db\u3002\u4e3a\u4e86\u5f25\u5408\u8ba1\u7b97\u9e3f\u6c9f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6709\u9650\u786c\u4ef6\u4e0a\u8bad\u7ec3\u7684\u9ad8\u6548\u6a21\u578b\u3002", "method": "1. \u79bb\u7ebf\u7f13\u5b58\u7ba1\u9053\uff1a\u901a\u8fc7\u538b\u7f29\u76d1\u7763\u4fe1\u53f7\u5c06\u7e41\u91cd\u7684\u6559\u5e08\u6a21\u578b\u63a8\u7406\u4e0e\u8bad\u7ec3\u5faa\u73af\u89e3\u8026\uff1b2. \u7f6e\u4fe1\u611f\u77e5\u84b8\u998f\u635f\u5931\uff1a\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u4f7f\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\u3002", "result": "\u5f00\u53d1\u4e867200\u4e07\u53c2\u6570\u7684\u5b66\u751f\u6a21\u578b\uff0c\u76f8\u6bd46.5\u4ebf\u53c2\u6570\u7684\u6559\u5e08\u6a21\u578b\uff0c\u53c2\u6570\u51cf\u5c119\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53475\u500d\u3002\u5b66\u751f\u6a21\u578b\u53ef\u5728\u5355\u5de5\u4f5c\u7ad9\u4e0a3\u5929\u5185\u5b8c\u6210\u8bad\u7ec3\uff0c\u800c\u6559\u5e08\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u8bad\u7ec3\u4e00\u5468\u3002", "conclusion": "Distill3R\u4e3a\u6ca1\u6709\u5927\u89c4\u6a21\u8ba1\u7b97\u8d44\u6e90\u7684\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5355\u5de5\u4f5c\u7ad9\u8bad\u7ec3\u65b9\u6848\uff0c\u4f5c\u4e3a\u6c11\u4e3b\u53163D\u89c6\u89c9\u7814\u7a76\u7684\u63a2\u7d22\u5165\u53e3\u548c\u9ad8\u6548\u8fb9\u7f18\u90e8\u7f72\u7684\u57fa\u7840\uff0c\u800c\u975e\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u7ade\u4e89\u3002"}}
{"id": "2602.01762", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01762", "abs": "https://arxiv.org/abs/2602.01762", "authors": ["Xuliang Wang", "Yuetao Chen", "Maochan Zhen", "Fang Liu", "Xinzhou Zheng", "Xingwu Liu", "Hong Xu", "Ming Li"], "title": "PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models", "comment": null, "summary": "Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.\n  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u6d4b\u89e3\u7801\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u6b65\u9aa4\u7684\u8ba1\u7b97\u5206\u89e3\u5230\u4e0d\u540c\u53c2\u6570\u96c6\uff0c\u89e3\u8026\u6a21\u578b\u5bb9\u91cf\u4e0e\u63a8\u7406\u6210\u672c\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684LLM\u89e3\u7801\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4e3a\u4e86\u63d0\u5347\u8349\u7a3f\u8d28\u91cf\u800c\u4f7f\u7528\u66f4\u5927\u7684\u53c2\u6570\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u8fd9\u5e26\u6765\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u9700\u8981\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u5ef6\u8fdf\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51faPRISM\u67b6\u6784\uff0c\u5c06\u6bcf\u4e2a\u9884\u6d4b\u6b65\u9aa4\u7684\u8ba1\u7b97\u5206\u89e3\u5230\u4e0d\u540c\u7684\u53c2\u6570\u96c6\uff0c\u91cd\u6784\u8349\u7a3f\u6a21\u578b\u7684\u8ba1\u7b97\u8def\u5f84\uff0c\u6210\u529f\u89e3\u8026\u6a21\u578b\u5bb9\u91cf\u4e0e\u63a8\u7406\u6210\u672c\u3002", "result": "PRISM\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u8349\u7a3f\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u6700\u5c0f\u8349\u7a3f\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u5353\u8d8a\u7684\u63a5\u53d7\u957f\u5ea6\uff0c\u83b7\u5f97\u66f4\u4f18\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002\u5728\u9ad8\u5ea6\u4f18\u5316\u7684\u63a8\u7406\u5f15\u64ce\u4e0a\u63d0\u5347\u89e3\u7801\u541e\u5410\u91cf\u8d85\u8fc72.6\u500d\u3002", "conclusion": "PRISM\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u89e3\u51b3\u4e86\u63a8\u6d4b\u89e3\u7801\u4e2d\u6a21\u578b\u5bb9\u91cf\u4e0e\u63a8\u7406\u6210\u672c\u7684\u6839\u672c\u77db\u76fe\uff0c\u5c55\u793a\u4e86\u6bd4\u4f20\u7edf\u8349\u7a3f\u67b6\u6784\u66f4\u597d\u7684\u6570\u636e\u6269\u5c55\u6027\u3002"}}
{"id": "2602.00606", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00606", "abs": "https://arxiv.org/abs/2602.00606", "authors": ["Ahmed Said Donmez", "Yuksel Arslantas", "Muhammed O. Sayin"], "title": "Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games", "comment": null, "summary": "We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u72ec\u7acb\u3001\u57fa\u4e8e\u6536\u76ca\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u968f\u673a\u535a\u5f08\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u6a21\u578b\u3001\u4e0e\u6e38\u620f\u65e0\u5173\u4e14\u65e0\u9700\u68af\u5ea6\u3002\u91c7\u7528\u6700\u4f73\u54cd\u5e94\u578b\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u5feb\u901f\u8bc4\u8bba\u5bb6\u548c\u6162\u901f\u8bc4\u8bba\u5bb6\u66f4\u65b0\u7b56\u7565\uff0c\u5728\u53cc\u667a\u80fd\u4f53\u96f6\u548c\u535a\u5f08\u548c\u591a\u667a\u80fd\u4f53\u5171\u540c\u5229\u76ca\u968f\u673a\u535a\u5f08\u4e2d\u6536\u655b\u5230\uff08\u8fd1\u4f3c\uff09\u5747\u8861\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u535a\u5f08\u5b66\u4e60\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u6a21\u578b\u77e5\u8bc6\u3001\u68af\u5ea6\u4fe1\u606f\u6216\u4e2d\u5fc3\u5316\u534f\u8c03\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u3001\u57fa\u4e8e\u6536\u76ca\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u9650\u4fe1\u606f\u4e0b\u5b9e\u73b0\u6536\u655b\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u968f\u673a\u535a\u5f08\u3002", "method": "\u91c7\u7528\u6700\u4f73\u54cd\u5e94\u578b\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\uff1a\u6f14\u5458\uff08\u7b56\u7565\uff09\u901a\u8fc7\u4e24\u4e2a\u8bc4\u8bba\u5bb6\u66f4\u65b0\uff1a\u5feb\u901f\u8bc4\u8bba\u5bb6\u76f4\u89c2\u54cd\u5e94\u89c2\u5bdf\u5230\u7684\u6536\u76ca\uff0c\u6162\u901f\u8bc4\u8bba\u5bb6\u6df1\u601d\u719f\u8651\u5730\u8fd1\u4f3c\u5e95\u5c42\u52a8\u6001\u89c4\u5212\u95ee\u9898\u3002\u5b66\u4e60\u8fc7\u7a0b\u4f9d\u8d56\u975e\u5747\u8861\u9002\u5e94\uff0c\u901a\u8fc7\u5bf9\u89c2\u5bdf\u6536\u76ca\u7684\u5e73\u6ed1\u6700\u4f73\u54cd\u5e94\u8fdb\u884c\u66f4\u65b0\u3002", "result": "\u5728\u53cc\u667a\u80fd\u4f53\u96f6\u548c\u535a\u5f08\u548c\u591a\u667a\u80fd\u4f53\u5171\u540c\u5229\u76ca\u968f\u673a\u535a\u5f08\u7684\u65e0\u9650\u65f6\u57df\u4e2d\uff0c\u5efa\u7acb\u4e86\u6536\u655b\u5230\uff08\u8fd1\u4f3c\uff09\u5747\u8861\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u8fd9\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u3001\u57fa\u4e8e\u6536\u76ca\u7684\u5b66\u4e60\u7b97\u6cd5\u3002\u5b9e\u8bc1\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e24\u7c7b\u6e38\u620f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u72ec\u7acb\u3001\u57fa\u4e8e\u6536\u76ca\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u968f\u673a\u535a\u5f08\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u3001\u6e38\u620f\u65e0\u5173\u4e14\u65e0\u9700\u68af\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01708", "categories": ["cs.CL", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.01708", "abs": "https://arxiv.org/abs/2602.01708", "authors": ["Langyuan Cui", "Chun Kai Ling", "Hwee Tou Ng"], "title": "Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory", "comment": "23 pages, 10 figures, under review at ICML 2026", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \\textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGame of Thought (GoT)\u6846\u67b6\uff0c\u4f7f\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\u63d0\u5347LLMs\u5728\u4fe1\u606f\u4e0d\u8db3\u60c5\u51b5\u4e0b\u7684\u4e3b\u52a8\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\uff0c\u901a\u8fc7\u4e8c\u5341\u95ee\u6e38\u620f\u7684\u5bf9\u6297\u53d8\u4f53\u8fdb\u884c\u6d4b\u8bd5\u3002", "motivation": "LLMs\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u9762\u4e34\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\u65f6\u5f80\u5f80\u4f9d\u8d56\u7b80\u5316\u5047\u8bbe\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6700\u574f\u60c5\u51b5\u6027\u80fd\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5b58\u5728\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u63d0\u51faStrategic Language Search (SLS)\u95ee\u9898\u4f5c\u4e3a\u4e8c\u5341\u95ee\u6e38\u620f\u7684\u5bf9\u6297\u53d8\u4f53\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u4e24\u4eba\u96f6\u548c\u6269\u5c55\u5f0f\u535a\u5f08\u3002\u5f00\u53d1Game of Thought (GoT)\u6846\u67b6\uff0c\u5e94\u7528\u535a\u5f08\u8bba\u6280\u672f\u8fd1\u4f3c\u6c42\u89e3\u9650\u5236\u7248\u672c\u6e38\u620f\u7684\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGoT\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u8bbe\u7f6e\u4e2d\u90fd\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u6700\u574f\u60c5\u51b5\u6027\u80fd\uff0c\u4f18\u4e8e(1)\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\u548c(2)\u542f\u53d1\u5f0f\u5f15\u5bfc\u641c\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLMs\u5728\u4fe1\u606f\u4e0d\u8db3\u60c5\u51b5\u4e0b\u7684\u4e3b\u52a8\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u98ce\u9669\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.00883", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00883", "abs": "https://arxiv.org/abs/2602.00883", "authors": ["Alicja Polowczyk", "Agnieszka Polowczyk", "Piotr Borycki", "Joanna Waczy\u0144ska", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models", "comment": null, "summary": "Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/", "AI": {"tldr": "DIAMOND\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5e94\u7528\u8f68\u8ff9\u6821\u6b63\u6765\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4f2a\u5f71\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u6216\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08\u5982FLUX\uff09\u4ecd\u5b58\u5728\u89c6\u89c9\u548c\u89e3\u5256\u5b66\u4f2a\u5f71\uff0c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u5728\u6838\u5fc3\u56fe\u50cf\u5f62\u6210\u8fc7\u7a0b\u4e2d\u6709\u6548\u5e72\u9884\uff0c\u4e14\u9700\u8981\u4fee\u6539\u6a21\u578b\u6743\u91cd\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u533a\u57df\u7ec6\u5316\u3002", "method": "DIAMOND\u91c7\u7528\u8bad\u7ec3\u81ea\u7531\u7684\u8f68\u8ff9\u6821\u6b63\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u8f68\u8ff9\u7684\u6bcf\u4e00\u6b65\u91cd\u5efa\u5e72\u51c0\u6837\u672c\u7684\u4f30\u8ba1\uff0c\u4e3b\u52a8\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u8fdc\u79bb\u5bfc\u81f4\u4f2a\u5f71\u7684\u6f5c\u5728\u72b6\u6001\u3002", "result": "DIAMOND\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u65e0\u4f2a\u5f71\u7684\u56fe\u50cf\u5408\u6210\uff0c\u4e14\u53ef\u6269\u5c55\u5230\u6807\u51c6\u6269\u6563\u6a21\u578b\uff0c\u63d0\u4f9b\u96f6\u6837\u672c\u7684\u9ad8\u8d28\u91cf\u751f\u6210\u8def\u5f84\u3002", "conclusion": "DIAMOND\u4e3a\u73b0\u4ee3\u751f\u6210\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6743\u91cd\u4fee\u6539\u7684\u9c81\u68d2\u3001\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\u3002"}}
{"id": "2602.01775", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01775", "abs": "https://arxiv.org/abs/2602.01775", "authors": ["Yucheng Wu", "Yuekui Yang", "Hongzheng Li", "Anan Liu", "Jian Xiao", "Junjie Zhai", "Huan Yu", "Shaoping Ma", "Leye Wang"], "title": "Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction", "comment": "15 pages", "summary": "Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.", "AI": {"tldr": "CrossAdapt\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8de8\u67b6\u6784\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u5ea6\u81ea\u9002\u5e94\u6295\u5f71\u5b9e\u73b0\u5feb\u901f\u5d4c\u5165\u8fc1\u79fb\uff0c\u7ed3\u5408\u975e\u5bf9\u79f0\u534f\u540c\u84b8\u998f\u548c\u5206\u5e03\u611f\u77e5\u9002\u5e94\u673a\u5236\uff0c\u5728\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u7528\u6237\u54cd\u5e94\u9884\u6d4b\u7cfb\u7edf\u4e2d\u90e8\u7f72\u65b0\u67b6\u6784\u9762\u4e34\u9ad8\u6602\u7684\u6a21\u578b\u5207\u6362\u6210\u672c\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u5386\u53f2\u6570\u636e\u7684\u6602\u8d35\u91cd\u8bad\u7ec3\u548c\u6570\u636e\u4fdd\u7559\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u67b6\u6784\u5f02\u6784\u6027\u548c\u5927\u578b\u5d4c\u5165\u8868\u7684\u8fc1\u79fb\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u7ef4\u5ea6\u81ea\u9002\u5e94\u6295\u5f71\u5b9e\u73b0\u5feb\u901f\u5d4c\u5165\u8fc1\u79fb\uff08\u65e0\u9700\u8fed\u4ee3\u8bad\u7ec3\uff09\uff0c\u7ed3\u5408\u6e10\u8fdb\u7f51\u7edc\u84b8\u998f\u548c\u7b56\u7565\u91c7\u6837\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b\u5728\u7ebf\u9636\u6bb5\u5f15\u5165\u975e\u5bf9\u79f0\u534f\u540c\u84b8\u998f\uff08\u5b66\u751f\u9891\u7e41\u66f4\u65b0\u3001\u6559\u5e08\u4e0d\u9891\u7e41\u66f4\u65b0\uff09\u548c\u5206\u5e03\u611f\u77e5\u9002\u5e94\u673a\u5236\uff0c\u52a8\u6001\u5e73\u8861\u5386\u53f2\u77e5\u8bc6\u4fdd\u7559\u548c\u5feb\u901f\u9002\u5e94\u6f14\u5316\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cCrossAdapt\u5b9e\u73b0\u4e860.27-0.43%\u7684AUC\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c1143-71%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002\u5728\u817e\u8baf\u5fae\u4fe1\u89c6\u9891\u53f7\uff08\u7ea61000\u4e07\u65e5\u6837\u672c\uff09\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u8f7b\u4e86AUC\u4e0b\u964d\u3001LogLoss\u589e\u52a0\u548c\u9884\u6d4b\u504f\u5dee\u3002", "conclusion": "CrossAdapt\u901a\u8fc7\u521b\u65b0\u7684\u8de8\u67b6\u6784\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u6a21\u578b\u5207\u6362\u7684\u9ad8\u6210\u672c\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00620", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00620", "abs": "https://arxiv.org/abs/2602.00620", "authors": ["Juntao Fang", "Shifeng Xie", "Shengbin Nie", "Yuhui Ling", "Yuming Liu", "Zijian Li", "Keli Zhang", "Lujia Pan", "Themis Palpanas", "Ruichu Cai"], "title": "Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference", "comment": null, "summary": "The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer", "AI": {"tldr": "\u63d0\u51fa\u4e86TIC-FM\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bc4\u4f30\u4e2d\u5206\u7c7b\u5668\u8bad\u7ec3\u5e26\u6765\u7684\u504f\u5dee\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u4f7f\u7528\u51bb\u7ed3\u7f16\u7801\u5668\u52a0\u4efb\u52a1\u7279\u5b9a\u5206\u7c7b\u5668\uff0c\u8fd9\u8fdd\u53cd\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u7684\u8bad\u7ec3\u514d\u8d39\u524d\u63d0\uff0c\u4e14\u5206\u7c7b\u5668\u4f9d\u8d56\u7684\u8bad\u7ec3\u9009\u62e9\u4f1a\u5f15\u5165\u8bc4\u4f30\u504f\u5dee\u3002", "method": "\u63d0\u51faTIC-FM\u6846\u67b6\uff1a\u5c06\u6807\u8bb0\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9884\u6d4b\u6240\u6709\u6d4b\u8bd5\u5b9e\u4f8b\u6807\u7b7e\u3002\u5305\u542b\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u3001\u8f7b\u91cf\u6295\u5f71\u9002\u914d\u5668\u548c\u5206\u5272\u63a9\u7801\u6f5c\u5728\u8bb0\u5fc6Transformer\u3002", "result": "\u5728128\u4e2aUCR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u51c6\u786e\u6027\uff0c\u5728\u6781\u7aef\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u514d\u8d39\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u53ef\u4ee5\u66ff\u4ee3\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\uff0c\u5e76\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u6a21\u62df\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u7c7b\u5668\u8bad\u7ec3\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.01709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01709", "abs": "https://arxiv.org/abs/2602.01709", "authors": ["Xingshan Zeng", "Lingzhi Wang", "Weiwen Liu", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "title": "ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation", "comment": null, "summary": "Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \\emph{\\name}, \\emph{\\underline{A}gentic \\underline{R}isk-Aware \\underline{T}est-Time Scaling via \\underline{I}terative \\underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \\emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.", "AI": {"tldr": "ARTIS\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55LLM\u63a8\u7406\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u907f\u514d\u9ad8\u98ce\u9669\u884c\u52a8", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u667a\u80fd\u4f53\u73af\u5883\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u884c\u52a8\u4f1a\u4e0e\u5916\u90e8\u73af\u5883\u76f4\u63a5\u4ea4\u4e92\u4e14\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9006\u7684\u9ad8\u6210\u672c\u540e\u679c", "method": "\u63d0\u51faARTIS\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u5c06\u63a2\u7d22\u4e0e\u6267\u884c\u89e3\u8026\uff0c\u5728\u771f\u5b9e\u6267\u884c\u524d\u8fdb\u884c\u6d4b\u8bd5\u65f6\u63a2\u7d22\uff1b\u5f15\u5165\u98ce\u9669\u611f\u77e5\u5de5\u5177\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u751f\u6210\u548c\u5e73\u8861\u8bad\u7ec3\u6765\u6355\u6349\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u7684\u5931\u8d25\u6a21\u5f0f", "result": "\u5728\u591a\u8f6e\u591a\u6b65\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fed\u4ee3\u6a21\u62df\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\uff0c\u98ce\u9669\u611f\u77e5\u6a21\u62df\u5bf9\u4e8e\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u5b9e\u73b0\u8fd9\u4e9b\u589e\u76ca\u81f3\u5173\u91cd\u8981", "conclusion": "ARTIS\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u8fed\u4ee3\u6a21\u62df\uff0c\u6709\u6548\u6269\u5c55\u4e86\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5728\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884c\u52a8\u7ea7\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u907f\u514d\u4e86\u73af\u5883\u98ce\u9669"}}
{"id": "2602.00904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00904", "abs": "https://arxiv.org/abs/2602.00904", "authors": ["Kunal Mahatha", "Ali Bahri", "Pierre Marza", "Sahar Dastani", "Maria Vakalopoulou", "Stergios Christodoulidis", "Jose Dolz", "Christian Desrosiers"], "title": "OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection", "comment": null, "summary": "State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.", "AI": {"tldr": "OCTOPUS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u67b6\u6784\uff0c\u901a\u8fc7\u516b\u65b9\u5411\u79bb\u6563\u9012\u5f52\u5728\u4fdd\u6301SSM\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u56e0\u56e0\u679c\u6027\u7834\u574f\u7a7a\u95f4\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u6709\u6548\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7a7a\u95f4\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u56e0\u4e3a\u5176\u56e0\u679c\u6027\u8bbe\u8ba1\u9002\u5408\u6587\u672c\u5e8f\u5217\u4f46\u7834\u574f\u56fe\u50cf\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6355\u6349\u5c40\u90e8\u7a7a\u95f4\u8fde\u8d2f\u6027\uff0c\u7ecf\u5e38\u8fde\u63a5\u975e\u76f8\u90bb\u7684patch\u800c\u5ffd\u7565\u89c6\u89c9\u76f8\u5173\u7684\u76f8\u90bb\u533a\u57df\u3002", "method": "\u63d0\u51faOCTOPUS\u67b6\u6784\uff0c\u6cbf\u516b\u4e2a\u4e3b\u8981\u65b9\u5411\uff08\u6c34\u5e73\u3001\u5782\u76f4\u3001\u5bf9\u89d2\u7ebf\u7684\u524d\u540e\u65b9\u5411\uff09\u8fdb\u884c\u79bb\u6563\u9012\u5f52\uff0c\u5141\u8bb8\u6240\u6709\u7a7a\u95f4\u8fde\u63a5\u533a\u57df\u95f4\u7684\u6709\u6548\u4fe1\u606f\u4ea4\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u4e0d\u76f8\u5173patch\u4e4b\u95f4\u7684\u72ec\u7acb\u6027\uff0c\u5b9e\u73b0\u591a\u65b9\u5411\u9012\u5f52\u3002", "result": "\u5728\u5206\u7c7b\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOCTOPUS\u5728\u8fb9\u754c\u4fdd\u6301\u548c\u533a\u57df\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u76f8\u6bd4\u73b0\u6709V-SSM\u6a21\u578b\u4fdd\u6301\u4e86\u76f8\u5bf9\u66f4\u597d\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "OCTOPUS\u4f5c\u4e3a\u591a\u65b9\u5411\u9012\u5f52\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u7a7a\u95f4\u611f\u77e5\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89c6\u89c9\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u673a\u5236\u3002"}}
{"id": "2602.01779", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01779", "abs": "https://arxiv.org/abs/2602.01779", "authors": ["Rui Hua", "Yu Wei", "Zixin Shu", "Kai Chang", "Dengying Yan", "Jianan Xia", "Zeyu Liu", "Hui Zhu", "Shujie Song", "Mingzhong Xiao", "Xiaodong Li", "Dongmei Jia", "Zhuye Gao", "Yanyan Meng", "Naixuan Zhao", "Yu Fu", "Haibin Yu", "Benman Yu", "Yuanyuan Chen", "Fei Dong", "Zhizhou Meng", "Pengcheng Yang", "Songxue Zhao", "Lijuan Pei", "Yunhui Hu", "Kan Ding", "Jiayuan Duan", "Wenmao Yin", "Yang Gu", "Runshun Zhang", "Qiang Zhu", "Jian Yu", "Jiansheng Li", "Baoyan Liu", "Wenjia Wang", "Xuezhong Zhou"], "title": "LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning", "comment": null, "summary": "Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.", "AI": {"tldr": "\u63d0\u51fa\u4e86LingLanMiDian\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u4e2d\u533b\u9886\u57df\u7684\u5927\u89c4\u6a21\u3001\u4e13\u5bb6\u7b56\u5212\u7684\u591a\u4efb\u52a1\u8bc4\u4f30\u5957\u4ef6\uff0c\u7528\u4e8e\u7edf\u4e00\u8bc4\u4f30LLM\u5728\u4e2d\u533b\u77e5\u8bc6\u56de\u5fc6\u3001\u591a\u8df3\u63a8\u7406\u3001\u4fe1\u606f\u62bd\u53d6\u548c\u4e34\u5e8a\u51b3\u7b56\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u4e2d\u533b\u5177\u6709\u72ec\u7279\u7684\u672c\u4f53\u8bba\u3001\u672f\u8bed\u548c\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u9886\u57df\u5fe0\u5b9e\u7684\u8bc4\u4f30\u3002\u73b0\u6709\u7684\u4e2d\u533b\u57fa\u51c6\u6d4b\u8bd5\u5728\u8986\u76d6\u8303\u56f4\u548c\u89c4\u6a21\u4e0a\u5206\u6563\uff0c\u4e14\u4f9d\u8d56\u975e\u7edf\u4e00\u6216\u751f\u6210\u5bc6\u96c6\u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u521b\u5efaLingLan\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7edf\u4e00\u8bc4\u4f30\u77e5\u8bc6\u56de\u5fc6\u3001\u591a\u8df3\u63a8\u7406\u3001\u4fe1\u606f\u62bd\u53d6\u548c\u4e34\u5e8a\u51b3\u7b56\u56db\u4e2a\u4efb\u52a1\uff1b\u5f15\u5165\u4e00\u81f4\u7684\u5ea6\u91cf\u8bbe\u8ba1\u3001\u4e34\u5e8a\u6807\u7b7e\u7684\u540c\u4e49\u8bcd\u5bb9\u5fcd\u534f\u8bae\u3001\u6bcf\u4e2a\u6570\u636e\u96c6400\u9879\u7684\u56f0\u96be\u5b50\u96c6\uff0c\u5e76\u5c06\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u91cd\u6784\u4e3a\u5355\u9009\u51b3\u7b56\u8bc6\u522b\u3002", "result": "\u5bf914\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u4e13\u6709LLM\u8fdb\u884c\u5168\u9762\u7684\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5b83\u4eec\u5728\u4e2d\u533b\u5e38\u8bc6\u77e5\u8bc6\u7406\u89e3\u3001\u63a8\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u65b9\u9762\u7684\u7edf\u4e00\u89c6\u89d2\uff1b\u5728\u56f0\u96be\u5b50\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u6a21\u578b\u4e0e\u4e2d\u533b\u4e13\u5bb6\u5728\u4e13\u95e8\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6865\u63a5\u57fa\u7840\u77e5\u8bc6\u548c\u5e94\u7528\u63a8\u7406\uff0cLingLan\u4e3a\u63a8\u8fdb\u4e2d\u533bLLM\u548c\u9886\u57df\u7279\u5b9a\u533b\u5b66AI\u7814\u7a76\u5efa\u7acb\u4e86\u7edf\u4e00\u3001\u53ef\u91cf\u5316\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2602.00624", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00624", "abs": "https://arxiv.org/abs/2602.00624", "authors": ["Hyekyung Yoon", "Minhyuk Lee", "Imseung Park", "Myungjoo Kang"], "title": "MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting", "comment": null, "summary": "Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.", "AI": {"tldr": "\u63d0\u51faMoDEx\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5b9a\u4e13\u5bb6\u6df7\u5408\u66ff\u4ee3\u590d\u6742\u9aa8\u5e72\u7f51\u7edc\uff0c\u5728\u591a\u5143\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08LTSF\uff09\u8303\u5f0f\u91c7\u7528\u5d4c\u5165\u3001\u9aa8\u5e72\u7cbe\u70bc\u548c\u957f\u671f\u9884\u6d4b\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u4f46\u5404\u4e2a\u9aa8\u5e72\u5c42\u7684\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u91cf\u5316\u6bcf\u4e2a\u65f6\u95f4\u70b9\u5bf9\u5c42\u7279\u5f81\u7684\u8d21\u732e\uff0c\u4ee5\u7406\u89e3\u6df1\u5ea6\u7279\u5b9a\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c42\u654f\u611f\u5ea6\uff08layer sensitivity\uff09\u68af\u5ea6\u5ea6\u91cf\uff0c\u57fa\u4e8eGradCAM\u548c\u6709\u6548\u611f\u53d7\u91ce\u7406\u8bba\u91cf\u5316\u65f6\u95f4\u70b9\u5bf9\u5c42\u7279\u5f81\u7684\u8d21\u732e\u3002\u57fa\u4e8e\u6b64\u63d0\u51faMoDEx\u6846\u67b6\uff0c\u4f7f\u7528\u6df1\u5ea6\u7279\u5b9aMLP\u4e13\u5bb6\u66ff\u4ee3\u590d\u6742\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c78%\u60c5\u51b5\u4e0b\u6392\u540d\u7b2c\u4e00\uff0c\u540c\u65f6\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u80fd\u65e0\u7f1d\u96c6\u6210\u5230Transformer\u53d8\u4f53\u4e2d\uff0c\u6301\u7eed\u63d0\u5347\u5176\u6027\u80fd\u3002", "conclusion": "MoDEx\u4f5c\u4e3a\u9ad8\u6548\u9ad8\u6027\u80fd\u7684LTSF\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u7279\u5b9a\u4e13\u5bb6\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01714", "abs": "https://arxiv.org/abs/2602.01714", "authors": ["Mouath Abu-Daoud", "Leen Kharouf", "Omar El Hajj", "Dana El Samad", "Mariam Al-Omari", "Jihad Mallat", "Khaled Saleh", "Nizar Habash", "Farah E. Shamout"], "title": "MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark", "comment": null, "summary": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "AI": {"tldr": "MedAraBench\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u963f\u62c9\u4f2f\u8bed\u533b\u5b66\u80fd\u529b\u7684\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u591a\u9009\u9898\u6570\u636e\u96c6\uff0c\u6db5\u76d619\u4e2a\u533b\u5b66\u4e13\u4e1a\u548c5\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u901a\u8fc7\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u548cLLM-as-a-judge\u6846\u67b6\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u5728NLP\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u5e94\u7528\u9886\u57df\uff0c\u7f3a\u4e4f\u5f00\u6e90\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u8fd9\u963b\u788d\u4e86\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u8bed\u8a00\u80fd\u529b\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6570\u5b57\u5316\u963f\u62c9\u4f2f\u8bed\u5730\u533a\u533b\u5b66\u4e13\u4e1a\u4eba\u58eb\u521b\u5efa\u7684\u5b66\u672f\u8d44\u6599\u5e93\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u9884\u5904\u7406\u5e76\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u3002\u91c7\u7528\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f30\u548cLLM-as-a-judge\u4e24\u79cd\u6846\u67b6\u9a8c\u8bc1\u6570\u636e\u8d28\u91cf\u3002", "result": "\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684MedAraBench\u6570\u636e\u96c6\uff0c\u6db5\u76d619\u4e2a\u533b\u5b66\u4e13\u4e1a\u548c5\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002\u8bc4\u4f30\u4e86GPT-5\u3001Gemini 2.0 Flash\u3001Claude 4-Sonnet\u7b498\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff0c\u53d1\u73b0\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u9886\u57df\u7279\u5b9a\u589e\u5f3a\u3002", "conclusion": "MedAraBench\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u533b\u5b66NLP\u8d44\u6e90\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u811a\u672c\uff0c\u65e8\u5728\u6269\u5927\u533b\u5b66\u6570\u636e\u57fa\u51c6\u7684\u591a\u6837\u6027\uff0c\u6269\u5c55LLM\u8bc4\u4f30\u8303\u56f4\uff0c\u5e76\u589e\u5f3a\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u591a\u8bed\u8a00\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2602.00946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00946", "abs": "https://arxiv.org/abs/2602.00946", "authors": ["Dhruv Parikh", "Haoyang Fan", "Rajgopal Kannan", "Viktor Prasanna"], "title": "ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models", "comment": "Technical Report", "summary": "Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \\textit{either} vision-encoder saliency (broad but query-agnostic) \\textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \\emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \\textbf{ConsensusDrop}, a training-free framework that derives a \\emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.", "AI": {"tldr": "ConsensusDrop\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\u548cLLM\u8de8\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u89c6\u89c9token\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5927\u91cf\u5197\u4f59\u89c6\u89c9token\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709token\u7f29\u51cf\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\uff08\u5e7f\u6cdb\u4f46\u67e5\u8be2\u65e0\u5173\uff09\uff0c\u8981\u4e48\u4f7f\u7528LLM\u8de8\u6ce8\u610f\u529b\uff08\u67e5\u8be2\u611f\u77e5\u4f46\u7a00\u758f\u4e14\u6210\u672c\u9ad8\uff09\uff0c\u4e24\u8005\u5355\u72ec\u90fd\u4e0d\u591f\u5145\u5206\u3002", "method": "\u63d0\u51faConsensusDrop\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\u548c\u67e5\u8be2\u611f\u77e5\u7684\u8de8\u6ce8\u610f\u529b\u6765\u8fbe\u6210\u5171\u8bc6\u6392\u540d\uff0c\u4fdd\u7559\u6700\u6709\u4fe1\u606f\u91cf\u7684token\uff0c\u540c\u65f6\u901a\u8fc7\u7f16\u7801\u5668\u5f15\u5bfc\u7684token\u5408\u5e76\u538b\u7f29\u5176\u4f59token\u3002", "result": "\u5728LLaVA-1.5/NeXT\u3001Video-LLaVA\u7b49\u5f00\u6e90VLM\u4e0a\uff0cConsensusDrop\u5728\u76f8\u540ctoken\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u51c6\u786e\u6027-\u6548\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5373\u4f7f\u5728\u6fc0\u8fdb\u7684token\u7f29\u51cf\u4e0b\u4e5f\u80fd\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11TTFT\u548cKV\u7f13\u5b58\u5360\u7528\u3002", "conclusion": "\u878d\u5408\u89c6\u89c9\u7f16\u7801\u5668\u663e\u8457\u6027\u548cLLM\u8de8\u6ce8\u610f\u529b\u662f\u89c6\u89c9token\u538b\u7f29\u7684\u6709\u6548\u65b9\u6cd5\uff0cConsensusDrop\u6846\u67b6\u5b9e\u73b0\u4e86\u8bad\u7ec3\u514d\u8d39\u7684\u9ad8\u6548token\u7f29\u51cf\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01797", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01797", "abs": "https://arxiv.org/abs/2602.01797", "authors": ["Hanlin Zhou", "Huah Yong Chan"], "title": "ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing", "comment": null, "summary": "Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.", "AI": {"tldr": "ORCH\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\"\u591a\u5206\u6790\u3001\u4e00\u51b3\u7b56\"\u8303\u5f0f\uff0c\u4f7f\u7528\u56fa\u5b9a\u89c4\u5219\u534f\u8c03\u5f02\u6784\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u79bb\u6563\u9009\u62e9\u63a8\u7406\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u8def\u7531\u6216\u4e34\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u884c\u4e3a\u96be\u4ee5\u91cd\u73b0\u3001\u51b3\u7b56\u8fc7\u7a0b\u96be\u4ee5\u89e3\u91ca\u3002\u9700\u8981\u4e00\u79cd\u786e\u5b9a\u6027\u7684\u534f\u8c03\u6846\u67b6\u6765\u6784\u5efa\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u9002\u5408\u90e8\u7f72\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\u3002", "method": "ORCH\u91c7\u7528\"\u591a\u5206\u6790\u3001\u4e00\u51b3\u7b56\"\u8303\u5f0f\uff1a\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u72ec\u7acb\u751f\u6210\u7ed3\u6784\u5316\u5206\u6790\uff0c\u4e13\u95e8\u7684\u5408\u5e76\u667a\u80fd\u4f53\u8f93\u51fa\u6700\u7ec8\u9009\u62e9\u3002\u6846\u67b6\u4f7f\u7528\u56fa\u5b9a\u89c4\u5219\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u7b54\u6848\u805a\u5408\uff0c\u4fdd\u6301\u6d41\u7a0b\u53ef\u9884\u6d4b\u3001\u53ef\u91cd\u73b0\u4e14\u65e0\u9700\u8bad\u7ec3\u3002\u53ef\u9009\u5f15\u5165EMA\u5f15\u5bfc\u7684\u8def\u7531\u5668\uff0c\u57fa\u4e8e\u5386\u53f2\u51c6\u786e\u7387\u3001\u5ef6\u8fdf\u6216\u6210\u672c\u66f4\u65b0\u667a\u80fd\u4f53\u9009\u62e9\u3002", "result": "\u5728MMLU\u3001MMLU-Pro\u548cGSM8K\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cORCH\u6301\u7eed\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u591a\u6570\u6295\u7968\u96c6\u6210\u3002\u5728MMLU-Pro\u4e0a\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u5347\u8d85\u8fc710\u4e2a\u767e\u5206\u70b9\uff1b\u5728GSM8K\u4e0a\u63d0\u5347\u8d85\u8fc750\u4e2a\u767e\u5206\u70b9\u3002EMA\u8def\u7531\u5668\u63d0\u4f9b\u989d\u59160.7-2.0\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "ORCH\u4e3a\u79bb\u6563\u9009\u62e9\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u8def\u5f84\uff0c\u6784\u5efa\u4e86\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u9002\u5408\u90e8\u7f72\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u534f\u8c03\u6846\u67b6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.00628", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00628", "abs": "https://arxiv.org/abs/2602.00628", "authors": ["Louis Schiekiera", "Max Zimmer", "Christophe Roux", "Sebastian Pokutta", "Fritz G\u00fcnther"], "title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs", "comment": "25 pages including references, 15 figures, 6 tables", "summary": "We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.", "AI": {"tldr": "\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u7684\u884c\u4e3a\u6570\u636e\u53ef\u4ee5\u90e8\u5206\u6062\u590dLLM\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u6bd4\u81ea\u7531\u8054\u60f3\u4efb\u52a1\u66f4\u80fd\u53cd\u6620\u5185\u90e8\u8bed\u4e49\u51e0\u4f55", "motivation": "\u7814\u7a76LLM\u5728\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u4e2d\u7684\u884c\u4e3a\u6570\u636e\u80fd\u5426\u63ed\u793a\u5176\u9690\u85cf\u72b6\u6001\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u63a2\u7d22\u884c\u4e3a\u6d4b\u91cf\u662f\u5426\u80fd\u53cd\u6620\u5185\u90e8\u8ba4\u77e5\u72b6\u6001", "method": "\u57288\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684transformer\u6a21\u578b\u4e0a\u8fd0\u884c\u4e24\u79cd\u5b9e\u9a8c\u8303\u5f0f\uff08\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5f3a\u5236\u9009\u62e9\u548c\u81ea\u7531\u8054\u60f3\uff09\uff0c\u6536\u96c61750\u4e07+\u8bd5\u9a8c\u6784\u5efa\u884c\u4e3a\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u4f7f\u7528\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\u6bd4\u8f83\u884c\u4e3a\u51e0\u4f55\u4e0e\u5c42\u95f4\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u5e76\u4e0eFastText\u3001BERT\u548c\u8de8\u6a21\u578b\u5171\u8bc6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5f3a\u5236\u9009\u62e9\u884c\u4e3a\u4e0e\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u7684\u5bf9\u9f50\u7a0b\u5ea6\u663e\u8457\u9ad8\u4e8e\u81ea\u7531\u8054\u60f3\uff1b\u5728\u7559\u51fa\u8bcd\u56de\u5f52\u4e2d\uff0c\u884c\u4e3a\u76f8\u4f3c\u6027\uff08\u7279\u522b\u662f\u5f3a\u5236\u9009\u62e9\uff09\u80fd\u591f\u9884\u6d4b\u672a\u89c1\u8fc7\u7684\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u8d85\u8d8a\u4e86\u8bcd\u6c47\u57fa\u7ebf\u548c\u8de8\u6a21\u578b\u5171\u8bc6", "conclusion": "\u4ec5\u57fa\u4e8e\u884c\u4e3a\u7684\u6d4b\u91cf\u4fdd\u7559\u4e86\u5173\u4e8e\u5185\u90e8\u8bed\u4e49\u51e0\u4f55\u7684\u53ef\u6062\u590d\u4fe1\u606f\uff0c\u884c\u4e3a\u4efb\u52a1\u80fd\u591f\u63ed\u793a\u9690\u85cf\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5f3a\u5236\u9009\u62e9\u8303\u5f0f\u6bd4\u81ea\u7531\u8054\u60f3\u66f4\u80fd\u53cd\u6620LLM\u7684\u5185\u90e8\u8bed\u4e49\u7ed3\u6784"}}
{"id": "2602.01716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01716", "abs": "https://arxiv.org/abs/2602.01716", "authors": ["Mehdi Jafari", "Hao Xue", "Flora Salim"], "title": "Mechanistic Indicators of Steering Effectiveness in Large Language Models", "comment": null, "summary": "Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5185\u90e8\u6a21\u578b\u4fe1\u53f7\uff08\u4fe1\u606f\u71b5\u548cKL\u6563\u5ea6\uff09\u6765\u8bca\u65ad\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u66f4\u5f3a\u7684\u8bc4\u4f30\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u6210\u529f\u6216\u5931\u8d25\u7684\u673a\u5236\u56e0\u7d20\u4ecd\u4e0d\u6e05\u695a\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u9ed1\u76d2\u8f93\u51fa\u6216LLM\u8bc4\u5224\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u4fe1\u606f\u8bba\u5ea6\u91cf\uff1a\u57fa\u4e8e\u71b5\u7684\u5f52\u4e00\u5316\u5206\u652f\u56e0\u5b50\uff08NBF\uff09\u548c\u8bcd\u6c47\u7a7a\u95f4\u4e2d\u5bfc\u5411\u6fc0\u6d3b\u4e0e\u76ee\u6807\u6982\u5ff5\u4e4b\u95f4\u7684KL\u6563\u5ea6\uff0c\u5e76\u5f15\u5165\u66f4\u5f3a\u7684\u8bc4\u4f30\u57fa\u7ebf\u6765\u8bc4\u4f30CAA\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5bfc\u5411\u65b9\u6cd5\u3002", "result": "\u8fd9\u4e9b\u673a\u5236\u4fe1\u53f7\u80fd\u591f\u6709\u6548\u9884\u6d4b\u5bfc\u5411\u6210\u529f\u4e0e\u5426\uff0c\u5e76\u4f30\u8ba1\u5931\u8d25\u6982\u7387\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u9ad8\u53ef\u9760\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u5185\u90e8\u6a21\u578b\u4fe1\u53f7\u53ef\u4ee5\u4f5c\u4e3a\u8bca\u65ad\u6fc0\u6d3b\u5bfc\u5411\u53ef\u9760\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u7406\u89e3\u5bfc\u5411\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.00949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00949", "abs": "https://arxiv.org/abs/2602.00949", "authors": ["Xiang Zhang", "Boxuan Zhang", "Alireza Naghizadeh", "Mohab Mohamed", "Dongfang Liu", "Ruixiang Tang", "Dimitris Metaxas", "Dongfang Liu"], "title": "Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images", "comment": null, "summary": "Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff08IAAA\u548cSAAA\uff09\u6765\u751f\u6210\u5408\u6210CAR-T/NK\u514d\u75ab\u7a81\u89e6\u56fe\u50cf\uff0c\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u6709\u9650\u95ee\u9898\uff0c\u63d0\u5347\u514d\u75ab\u7a81\u89e6\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "CAR-T/NK\u7ec6\u80de\u514d\u75ab\u7597\u6cd5\u4e2d\uff0c\u514d\u75ab\u7a81\u89e6\u8d28\u91cf\u53ef\u4f5c\u4e3a\u9884\u6d4b\u7597\u6548\u7684\u529f\u80fd\u6027\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u6807\u6ce8\u663e\u5fae\u56fe\u50cf\u6570\u636e\u96c6\u6709\u9650\u9650\u5236\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u514d\u75ab\u7a81\u89e6\u68c0\u6d4b\u548c\u5206\u5272\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6574\u5408\u4e24\u79cd\u4e92\u8865\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff1a1) IAAA\uff08\u5b9e\u4f8b\u611f\u77e5\u81ea\u52a8\u589e\u5f3a\uff09- \u81ea\u52a8\u5316\u7684\u5b9e\u4f8b\u4fdd\u7559\u589e\u5f3a\u65b9\u6cd5\uff0c\u5bf9\u539f\u59cbIS\u6570\u636e\u5e94\u7528\u4f18\u5316\u589e\u5f3a\u7b56\u7565\u751f\u6210\u5408\u6210\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\uff1b2) SAAA\uff08\u8bed\u4e49\u611f\u77e5AI\u589e\u5f3a\uff09- \u7ed3\u5408\u57fa\u4e8e\u6269\u6563\u7684\u63a9\u7801\u751f\u6210\u5668\u548cPix2Pix\u6761\u4ef6\u56fe\u50cf\u5408\u6210\u5668\uff0c\u521b\u5efa\u591a\u6837\u5316\u7684\u89e3\u5256\u5b66\u771f\u5b9e\u63a9\u7801\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u3002", "result": "\u4e24\u79cd\u589e\u5f3a\u7b56\u7565\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u5728\u89c6\u89c9\u548c\u7ed3\u6784\u7279\u6027\u4e0a\u4e0e\u771f\u5b9eIS\u6570\u636e\u9ad8\u5ea6\u5339\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CAR-T/NK\u514d\u75ab\u7a81\u89e6\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u514d\u75ab\u7a81\u89e6\u91cf\u5316\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u8fd9\u9879\u5de5\u4f5c\u652f\u6301\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u57fa\u4e8e\u6210\u50cf\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u7528\u4e8e\u9884\u6d4b\u60a3\u8005\u5bf9CAR-T/NK\u514d\u75ab\u7597\u6cd5\u7684\u53cd\u5e94\u3002"}}
{"id": "2602.01815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01815", "abs": "https://arxiv.org/abs/2602.01815", "authors": ["Yunhui Jang", "Seonghyun Park", "Jaehyung Kim", "Sungsoo Ahn"], "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery", "comment": null, "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.", "AI": {"tldr": "INDIBATOR\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u79d1\u5b66\u5bb6\u4e2a\u4f53\u7814\u7a76\u8f68\u8ff9\u7684\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\u667a\u80fd\u4f53\uff0c\u5728\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u7c97\u7c92\u5ea6\u89d2\u8272\u5206\u914d\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u901a\u7528\u7684\u89d2\u8272\u5206\u914d\uff08\u5982\"\u5ba1\u7a3f\u4eba\"\u3001\"\u4f5c\u8005\"\uff09\u6216\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u7c97\u7c92\u5ea6\u89d2\u8272\uff0c\u8fd9\u8fc7\u5ea6\u7b80\u5316\u4e86\u771f\u5b9e\u79d1\u5b66\u5bb6\u7684\u884c\u4e3a\u6a21\u5f0f\u3002\u771f\u5b9e\u79d1\u5b66\u5bb6\u7684\u8d21\u732e\u662f\u7531\u5176\u72ec\u7279\u7684\u7814\u7a76\u8f68\u8ff9\u5851\u9020\u7684\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u3002", "method": "\u63d0\u51faINDIBATOR\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u6a21\u6001\u6784\u5efa\u4e2a\u4f53\u5316\u79d1\u5b66\u5bb6\u6863\u6848\uff1a1) \u53d1\u8868\u5386\u53f2\uff08\u6587\u732e\u77e5\u8bc6\uff09\uff1b2) \u5206\u5b50\u5386\u53f2\uff08\u7ed3\u6784\u5148\u9a8c\uff09\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u901a\u8fc7\u63d0\u6848\u3001\u6279\u8bc4\u548c\u6295\u7968\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u591a\u8f6e\u8fa9\u8bba\u3002", "result": "\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u4e2a\u4f53\u5316\u6863\u6848\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u4f9d\u8d56\u7c97\u7c92\u5ea6\u89d2\u8272\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6355\u6349\u667a\u80fd\u4f53\u7684\"\u79d1\u5b66DNA\"\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u7684\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4e2a\u4f53\u5316\u5efa\u6a21\u662f\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u53d1\u73b0\u7cfb\u7edf\u7684\u5173\u952e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.00636", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00636", "abs": "https://arxiv.org/abs/2602.00636", "authors": ["Yujie Yang", "Zhilong Zheng", "Shengbo Eben Li"], "title": "Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration", "comment": null, "summary": "Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u9762\u5411\u5e73\u8861\u7684\u5b89\u5168\u63a2\u7d22\u6846\u67b6SEE\uff0c\u901a\u8fc7\u4ea4\u66ff\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\u548c\u6700\u5c0f\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u7ea6\u675f\u8fdd\u89c4\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u6700\u7ec8\u6536\u655b\u5230\u5b89\u5168\u63a2\u7d22\u7684\u5e73\u8861\u70b9\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u867d\u7136\u5c06\u63a2\u7d22\u9650\u5236\u5728\u53ef\u884c\u533a\u57df\u5185\u5df2\u88ab\u5e7f\u6cdb\u63a5\u53d7\uff0c\u4f46\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\uff1a\u901a\u8fc7\u63a2\u7d22\u53ef\u83b7\u5f97\u7684\u6700\u5927\u53ef\u884c\u533a\u57df\u662f\u4ec0\u4e48\uff1f\u5982\u4f55\u8bc6\u522b\u8fd9\u4e2a\u533a\u57df\uff1f\u672c\u6587\u9996\u6b21\u56de\u7b54\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u5e73\u8861\u63a2\u7d22\uff08SEE\uff09\u6846\u67b6\uff0c\u4ea4\u66ff\u8fdb\u884c\u4e24\u4e2a\u6b65\u9aa4\uff1a1\uff09\u5728\u7ed9\u5b9a\u6a21\u578b\u4e0b\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\uff1b2\uff09\u5728\u7ed9\u5b9a\u53ef\u884c\u533a\u57df\u5185\u5bfb\u627e\u6700\u5c0f\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\u3002\u4f7f\u7528\u4e0d\u786e\u5b9a\u6a21\u578b\u7684\u56fe\u8868\u793a\uff0c\u8bc1\u660e\u6a21\u578b\u5355\u8c03\u7cbe\u7ec6\u5316\uff0c\u53ef\u884c\u533a\u57df\u5355\u8c03\u6269\u5c55\uff0c\u6700\u7ec8\u6536\u655b\u5230\u5e73\u8861\u70b9\u3002", "result": "\u5728\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEE\u7b97\u6cd5\u6210\u529f\u6269\u5c55\u4e86\u53ef\u884c\u533a\u57df\u4e14\u96f6\u7ea6\u675f\u8fdd\u89c4\uff0c\u5728\u51e0\u6b21\u8fed\u4ee3\u5185\u5c31\u8fbe\u5230\u4e86\u5b89\u5168\u63a2\u7d22\u7684\u5e73\u8861\u70b9\u3002", "conclusion": "\u5b89\u5168\u63a2\u7d22\u7684\u76ee\u6807\u662f\u627e\u5230\u53ef\u884c\u533a\u57df\u548c\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u7684\u5e73\u8861\u70b9\u3002SEE\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u8fd9\u79cd\u5e73\u8861\u5bfc\u5411\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2602.01717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01717", "abs": "https://arxiv.org/abs/2602.01717", "authors": ["Hyunsik Kim", "Haeri Kim", "Munhak Lee", "Kyungmin Lee"], "title": "BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition", "comment": "accepted to ICASSP 2026", "summary": "Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.", "AI": {"tldr": "BBPE16\uff1a\u57fa\u4e8eUTF-16\u7684\u5b57\u8282\u7ea7BPE\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u7edf\u4e002\u5b57\u8282\u7f16\u7801\u51cf\u5c11\u975e\u62c9\u4e01\u6587\u5b57\uff08\u5982\u4e2d\u65e5\u97e9\uff09\u7684token\u5e8f\u5217\u957f\u5ea6\uff0c\u63d0\u5347\u591a\u8bed\u8a00ASR\u6548\u7387", "motivation": "\u73b0\u6709UTF-8\u5b57\u8282\u7ea7BPE\uff08BBPE\uff09\u867d\u7136\u8bed\u8a00\u65e0\u5173\u4e14\u8986\u76d6\u5168Unicode\uff0c\u4f46\u5bf9\u4e2d\u65e5\u97e9\u7b49\u975e\u62c9\u4e01\u6587\u5b57\u4f7f\u7528\u53d8\u957f\u7f16\u7801\uff0c\u5bfc\u81f4token\u5e8f\u5217\u8fc7\u957f\uff0c\u589e\u52a0\u8ba1\u7b97\u8d1f\u8f7d\u548c\u5185\u5b58\u4f7f\u7528", "method": "\u63d0\u51faBBPE16\u5206\u8bcd\u5668\uff0c\u57fa\u4e8eUTF-16\u7f16\u7801\uff0c\u4f7f\u5927\u591a\u6570\u73b0\u4ee3\u6587\u5b57\u7cfb\u7edf\u4f7f\u7528\u7edf\u4e00\u76842\u5b57\u8282\u4ee3\u7801\u5355\u5143\uff0c\u4fdd\u6301\u8bed\u8a00\u65e0\u5173\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8de8\u8bed\u8a00token\u5171\u4eab", "result": "\u5728\u5355\u8bed\u3001\u53cc\u8bed\u3001\u4e09\u8bedASR\u53ca\u591a\u8bed\u8a00\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0cBBPE16\u8fbe\u5230\u76f8\u5f53\u6216\u66f4\u597d\u7684\u51c6\u786e\u7387\uff1b\u5bf9\u4e2d\u6587\u51cf\u5c11token\u6570\u91cf\u8fbe10.4%\uff0c\u964d\u4f4e\u89e3\u7801\u8fed\u4ee3\u8fbe10.3%\uff0c\u52a0\u901f\u5fae\u8c03\u548c\u63a8\u7406\u5e76\u51cf\u5c11\u5185\u5b58\u4f7f\u7528", "conclusion": "BBPE16\u662f\u5b9e\u7528\u7684\u591a\u8bed\u8a00ASR\u5206\u8bcd\u9009\u62e9\uff0c\u901a\u8fc7\u7edf\u4e002\u5b57\u8282\u7f16\u7801\u4f18\u5316\u975e\u62c9\u4e01\u6587\u5b57\u5904\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u8bed\u8a00token\u5171\u4eab\u4f18\u52bf"}}
{"id": "2602.00956", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00956", "abs": "https://arxiv.org/abs/2602.00956", "authors": ["Faisal Ahmed"], "title": "Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification", "comment": "20 pages, 6 Figures", "summary": "Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.\n  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u62d3\u6251\u6570\u636e\u5206\u6790(TDA)\u548cDenseNet121\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u56db\u9636\u6bb5\u5206\u7c7b\uff0c\u5728OASIS\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.93%\u51c6\u786e\u7387\u548c100% AUC\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5728\u795e\u7ecf\u5f71\u50cf\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u5ffd\u7565\u5927\u8111\u7ed3\u6784\u7684\u62d3\u6251\u7279\u5f81\uff0c\u9700\u8981\u7ed3\u5408\u62d3\u6251\u6570\u636e\u5206\u6790\u6765\u6355\u6349\u8fd9\u4e9b\u4e92\u8865\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a1) \u4f7f\u7528TDA\u63d0\u53d6\u5927\u8111\u7ed3\u6784\u7684\u62d3\u6251\u7279\u5f81\uff1b2) \u4f7f\u7528DenseNet121\u4eceMRI\u5207\u7247\u4e2d\u5b66\u4e60\u5c42\u6b21\u7a7a\u95f4\u7279\u5f81\uff1b3) \u878d\u5408\u6df1\u5ea6\u7279\u5f81\u548c\u62d3\u6251\u7279\u5f81\u4ee5\u589e\u5f3a\u56db\u7c7bAD\u9636\u6bb5\u7684\u53ef\u5206\u6027\u3002", "result": "\u5728OASIS-1 Kaggle MRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTDA+DenseNet121\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u523099.93%\u51c6\u786e\u7387\u548c100% AUC\uff0c\u8d85\u8d8a\u4e86CNN\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u96c6\u6210\u548c\u591a\u5c3a\u5ea6\u67b6\u6784\u3002", "conclusion": "\u5c06\u62d3\u6251\u6d1e\u5bdf\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u662f\u6709\u6548\u7684\uff0c\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u81ea\u52a8\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u7684\u7a33\u5065\u9ad8\u7cbe\u5ea6\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u62d3\u6251\u7279\u5f81\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.01832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01832", "abs": "https://arxiv.org/abs/2602.01832", "authors": ["Rui Wang", "Yaoguang Cao", "Yuyi Chen", "Jianyi Xu", "Zhuoyang Li", "Jiachen Shang", "Shichun Yang"], "title": "Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs", "comment": null, "summary": "Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.", "AI": {"tldr": "\u63d0\u51faSynesthesia of Vehicles\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u9884\u6d4b\u89e6\u89c9\u6fc0\u52b1\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5bf9\u8def\u9762\u6fc0\u52b1\u611f\u77e5\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4f9d\u8d56\u591a\u6a21\u6001\u878d\u5408\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u89c6\u89c9\u548c\u5149\u5b66\u4f20\u611f\u5668\u65e0\u6cd5\u68c0\u6d4b\u5bf9\u8f66\u8f86\u52a8\u6001\u63a7\u5236\u81f3\u5173\u91cd\u8981\u7684\u8def\u9762\u6fc0\u52b1\u3002\u53d7\u4eba\u7c7b\u8054\u89c9\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4ece\u89c6\u89c9\u9884\u6d4b\u89e6\u89c9\u6fc0\u52b1\u7684\u65b9\u6cd5", "method": "1. \u63d0\u51fa\u8f66\u8f86\u8054\u89c9\u6846\u67b6\uff1b2. \u5f00\u53d1\u8de8\u6a21\u6001\u65f6\u7a7a\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3\u65f6\u7a7a\u5dee\u5f02\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u89c6\u89c9-\u89e6\u89c9\u8054\u89c9\u751f\u6210\u6a21\u578b\u8fdb\u884c\u65e0\u76d1\u7763\u9ad8\u8d28\u91cf\u89e6\u89c9\u6570\u636e\u5408\u6210\uff1b4. \u4f7f\u7528\u771f\u5b9e\u8f66\u8f86\u611f\u77e5\u7cfb\u7edf\u6536\u96c6\u591a\u6a21\u6001\u6570\u636e\u96c6", "result": "VTSyn\u6a21\u578b\u5728\u65f6\u95f4\u3001\u9891\u7387\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u901a\u8fc7\u4e3b\u52a8\u89e6\u89c9\u611f\u77e5\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027", "conclusion": "\u63d0\u51fa\u7684Synesthesia of Vehicles\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u89c6\u89c9\u5230\u89e6\u89c9\u7684\u8de8\u6a21\u6001\u9884\u6d4b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8def\u9762\u6fc0\u52b1\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8f86\u5b89\u5168\u6027"}}
{"id": "2602.00640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00640", "abs": "https://arxiv.org/abs/2602.00640", "authors": ["Jingru Huang", "Haijie Xu", "Jie Guo", "Manrui Jiang", "Chen Zhang"], "title": "Combinatorial Bandit Bayesian Optimization for Tensor Outputs", "comment": null, "summary": "Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u5f20\u91cf\u8f93\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff1aTOGP-BO\u7528\u4e8e\u5b8c\u6574\u5f20\u91cf\u8f93\u51fa\u4f18\u5316\uff0cTOGP-CBBO\u7528\u4e8e\u7ec4\u5408\u8001\u864e\u673a\u8bbe\u7f6e\u4e0b\u7684\u90e8\u5206\u89c2\u6d4b\u4f18\u5316\uff0c\u5747\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u672a\u5904\u7406\u5f20\u91cf\u8f93\u51fa\u51fd\u6570\uff0c\u4e14\u5b58\u5728\u66f4\u590d\u6742\u7684\u7ec4\u5408\u8001\u864e\u673a\u8bbe\u7f6e\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6355\u6349\u5f20\u91cf\u7ed3\u6784\u4f9d\u8d56\u5e76\u5904\u7406\u90e8\u5206\u89c2\u6d4b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1) \u63d0\u51fa\u5f20\u91cf\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b(TOGP)\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u5305\u542b\u4e24\u7c7b\u5f20\u91cf\u8f93\u51fa\u6838\u51fd\u6570\uff1b2) \u57fa\u4e8eTOGP\u5f00\u53d1UCB\u91c7\u96c6\u51fd\u6570\uff1b3) \u9488\u5bf9\u7ec4\u5408\u8001\u864e\u673a\u8d1d\u53f6\u65af\u4f18\u5316(CBBO)\u95ee\u9898\uff0c\u6269\u5c55TOGP\u5904\u7406\u90e8\u5206\u89c2\u6d4b\u8f93\u51fa\uff0c\u5e76\u8bbe\u8ba1CMAB-UCB2\u51c6\u5219\u540c\u65f6\u9009\u62e9\u67e5\u8be2\u70b9\u548c\u6700\u4f18\u8f93\u51fa\u5b50\u96c6\u3002", "result": "\u5efa\u7acb\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u7406\u8bba\u9057\u61be\u754c\uff0c\u786e\u4fdd\u6b21\u7ebf\u6027\u6027\u80fd\u3002\u5927\u91cf\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6210\u529f\u586b\u8865\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u5f20\u91cf\u8f93\u51fa\u51fd\u6570\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684TOGP-BO\u548cTOGP-CBBO\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5b8c\u6574\u548c\u90e8\u5206\u89c2\u6d4b\u7684\u5f20\u91cf\u8f93\u51fa\u4f18\u5316\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.01719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01719", "abs": "https://arxiv.org/abs/2602.01719", "authors": ["Jiwei Tang", "Shilei Liu", "Zhicheng Zhang", "Yujin Yuan", "Libin Zheng", "Wenbo Su", "Bo Zheng"], "title": "COMI: Coarse-to-fine Context Compression via Marginal Information Gain", "comment": "Accepted at ICLR 2026", "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.", "AI": {"tldr": "COMI\uff1a\u4e00\u79cd\u7c97\u5230\u7ec6\u7684\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\uff08MIG\uff09\u6307\u6807\u8054\u5408\u4f18\u5316\u8bed\u4e49\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u5197\u4f59\u7684\u95ee\u9898\uff0c\u73b0\u6709\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u66f4\u6709\u6548\u5730\u5e73\u8861\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u8bed\u4e49\u76f8\u5173\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\uff08MIG\uff09\u7684\u7c97\u5230\u7ec6\u81ea\u9002\u5e94\u538b\u7f29\u6846\u67b6\uff1a1\uff09\u7c97\u7c92\u5ea6\u7ec4\u91cd\u5206\u914d\uff1a\u6839\u636e\u7ec4\u95f4MIG\u52a8\u6001\u5206\u914d\u538b\u7f29\u7387\uff1b2\uff09\u7ec6\u7c92\u5ea6\u4ee4\u724c\u5408\u5e76\uff1a\u57fa\u4e8e\u7ec4\u5185MIG\u7684\u52a0\u6743\u673a\u5236\u878d\u5408\u4ee4\u724c\u3002", "result": "\u5728\u95ee\u7b54\uff08NaturalQuestions\u30012WikiMQA\u7b49\uff09\u548c\u6458\u8981\uff08MultiNews\uff09\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528LLaMA-2-7B\u3001Qwen2-7B\u7b49\u9aa8\u5e72\u6a21\u578b\uff0cCOMI\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5982\u572832\u500d\u538b\u7f29\u7ea6\u675f\u4e0bQwen2-7B\u5728NaturalQuestions\u4e0a\u83b7\u5f97\u7ea625\u70b9\u7cbe\u786e\u5339\u914d\u63d0\u5347\u3002", "conclusion": "COMI\u6846\u67b6\u901a\u8fc7\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u6307\u5bfc\u7684\u7c97\u5230\u7ec6\u81ea\u9002\u5e94\u538b\u7f29\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2602.00971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00971", "abs": "https://arxiv.org/abs/2602.00971", "authors": ["Meng Luo", "Bobo Li", "Shanqing Xu", "Shize Zhang", "Qiuchan Chen", "Menglu Han", "Wenhao Chen", "Yanxiang Huang", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning", "comment": "Accepted by ICLR 2026", "summary": "Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.", "AI": {"tldr": "\u63d0\u51faHitEmotion\u57fa\u51c6\u6d4b\u8bd5\u548cToM\u5f15\u5bfc\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5c42\u60c5\u611f\u7406\u89e3\u80fd\u529b", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df1\u5c42\u60c5\u611f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u7684\u8ba4\u77e5\u57fa\u7840\u6765\u5efa\u6a21\u60c5\u611f\u667a\u80fd", "method": "1. \u63d0\u51faHitEmotion\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u5c42\u8bca\u65ad\u8ba4\u77e5\u6df1\u5ea6\u80fd\u529b\u65ad\u70b9\uff1b2. \u8bbe\u8ba1ToM\u5f15\u5bfc\u7684\u63a8\u7406\u94fe\uff0c\u8ffd\u8e2a\u5fc3\u7406\u72b6\u6001\u5e76\u6821\u51c6\u8de8\u6a21\u6001\u8bc1\u636e\uff1b3. \u63d0\u51faTMPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e2d\u95f4\u5fc3\u7406\u72b6\u6001\u4f5c\u4e3a\u8fc7\u7a0b\u7ea7\u76d1\u7763", "result": "HitEmotion\u63ed\u793a\u4e86SOTA\u6a21\u578b\u5728\u8ba4\u77e5\u8981\u6c42\u9ad8\u7684\u4efb\u52a1\u4e0a\u7684\u6df1\u5c42\u60c5\u611f\u63a8\u7406\u7f3a\u9677\uff1bToM\u5f15\u5bfc\u63a8\u7406\u94fe\u548cTMPO\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u5fe0\u5b9e\u3001\u66f4\u8fde\u8d2f\u7684\u63a8\u7406\u8fc7\u7a0b", "conclusion": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u589e\u5f3aMLLMs\u57fa\u4e8e\u8ba4\u77e5\u7684\u60c5\u611f\u7406\u89e3\u80fd\u529b\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2602.01848", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01848", "abs": "https://arxiv.org/abs/2602.01848", "authors": ["Salaheddin Alzu'bi", "Baran Nama", "Arda Kaz", "Anushri Eswaran", "Weiyuan Chen", "Sarvesh Khetan", "Rishab Bala", "Tu Vu", "Sewoong Oh"], "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems", "comment": null, "summary": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.", "AI": {"tldr": "ROMA\u662f\u4e00\u4e2a\u9012\u5f52\u5f00\u653e\u5143\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u7ed3\u6784\u5316\u805a\u5408\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u652f\u6301\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408GEPA+\u63d0\u793a\u4f18\u5316\u5668\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u6846\u67b6\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u968f\u7740\u63a8\u7406\u6df1\u5ea6\u589e\u52a0\uff0c\u987a\u5e8f\u7f16\u6392\u53d8\u5f97\u8106\u5f31\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e0d\u900f\u660e\u7684\u6267\u884c\u8f68\u8ff9\u96be\u4ee5\u8c03\u8bd5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u7684\u67b6\u6784\u3002", "method": "ROMA\u91c7\u7528\u9012\u5f52\u4efb\u52a1\u5206\u89e3\u548c\u7ed3\u6784\u5316\u805a\u5408\uff0c\u5c06\u76ee\u6807\u5206\u89e3\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u5b50\u4efb\u52a1\u6811\u5e76\u884c\u6267\u884c\uff0c\u805a\u5408\u538b\u7f29\u548c\u9a8c\u8bc1\u4e2d\u95f4\u7ed3\u679c\u4ee5\u63a7\u5236\u4e0a\u4e0b\u6587\u589e\u957f\u3002\u6846\u67b6\u56f4\u7ed5\u56db\u4e2a\u6a21\u5757\u5316\u89d2\u8272\u6784\u5efa\uff1aAtomizer\uff08\u51b3\u5b9a\u662f\u5426\u5206\u89e3\u4efb\u52a1\uff09\u3001Planner\u3001Executor\u548cAggregator\uff0c\u652f\u6301\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u3002GEPA+\u662f\u6539\u8fdb\u7684\u9057\u4f20-Pareto\u63d0\u793a\u63d0\u8bae\u5668\uff0c\u5728ROMA\u7ec4\u4ef6\u5c42\u6b21\u7ed3\u6784\u4e2d\u641c\u7d22\u63d0\u793a\u3002", "result": "\u5728SEAL-0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROMA\u7ed3\u5408GLM-4.6\u5c06\u51c6\u786e\u7387\u6bd4Kimi-Researcher\u63d0\u9ad89.9%\u3002\u5728EQ-Bench\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROMA\u4f7fDeepSeek-V3\u80fd\u591f\u5339\u914dClaude Sonnet 4.5\u7b49\u9886\u5148\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u9012\u5f52\u6a21\u5757\u5316\u4ee3\u7406\u67b6\u6784\u80fd\u591f\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3001\u7075\u6d3b\u6027\u548c\u6a21\u578b\u65e0\u5173\u6027\uff0c\u4e3a\u957f\u65f6\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00647", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00647", "abs": "https://arxiv.org/abs/2602.00647", "authors": ["Noorain Mukhtiar", "Adnan Mahmood", "Quan Z. Sheng"], "title": "CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation", "comment": "7 pages (main content), 2 pages (references), Accepted in AAAI 2026", "summary": "With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.", "AI": {"tldr": "CoRe-Fed\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u548c\u516c\u5e73\u805a\u5408\u89e3\u51b3\u8868\u793a\u504f\u5dee\u548c\u534f\u4f5c\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u6570\u636e\u5206\u5e03\u5f02\u6784\u548c\u53c2\u4e0e\u4e0d\u5e73\u7b49\u5bfc\u81f4\u7684\u6027\u80fd\u5dee\u5f02\u95ee\u9898\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u8868\u793a\u504f\u5dee\uff08\u5ba2\u6237\u7aef\u8868\u793a\u4e0d\u4e00\u81f4\uff09\u548c\u534f\u4f5c\u504f\u5dee\uff08\u805a\u5408\u8d21\u732e\u4e0d\u516c\uff09\uff0c\u8fd9\u4e9b\u504f\u5dee\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCoRe-Fed\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff1a1\uff09\u5d4c\u5165\u5bf9\u9f50\u673a\u5236\u4fc3\u8fdb\u672c\u5730\u4e0e\u5168\u5c40\u5d4c\u5165\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u4ee5\u51cf\u5c11\u8868\u793a\u504f\u5dee\uff1b2\uff09\u57fa\u4e8e\u52a8\u6001\u5956\u52b1-\u60e9\u7f5a\u7684\u805a\u5408\u7b56\u7565\uff0c\u6839\u636e\u53c2\u4e0e\u5386\u53f2\u548c\u5d4c\u5165\u5bf9\u9f50\u5ea6\u8c03\u6574\u5ba2\u6237\u7aef\u6743\u91cd\uff0c\u5b9e\u73b0\u8d21\u732e\u611f\u77e5\u7684\u516c\u5e73\u805a\u5408\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCoRe-Fed\u5728\u516c\u5e73\u6027\u548c\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "CoRe-Fed\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u8868\u793a\u516c\u5e73\u6027\u548c\u534f\u4f5c\u516c\u5e73\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.01725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01725", "abs": "https://arxiv.org/abs/2602.01725", "authors": ["Yurun Chen", "Zeyi Liao", "Ping Yin", "Taotao Xie", "Keting Yin", "Shengyu Zhang"], "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models", "comment": null, "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.", "AI": {"tldr": "SafePred\u662f\u4e00\u4e2a\u9884\u6d4b\u6027\u62a4\u680f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u7684\u672a\u6765\u98ce\u9669\u4e0e\u5f53\u524d\u51b3\u7b56\u5bf9\u9f50\uff0c\u89e3\u51b3\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u957f\u671f\u98ce\u9669\u95ee\u9898\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6548\u7528\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u62a4\u680f\u5927\u591a\u91c7\u7528\u53cd\u5e94\u5f0f\u65b9\u6cd5\uff0c\u53ea\u80fd\u5728\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u7ea6\u675f\u884c\u4e3a\uff0c\u65e0\u6cd5\u4e3b\u52a8\u907f\u514d\u957f\u671f\u98ce\u9669\u3002\u770b\u4f3c\u5408\u7406\u7684\u884c\u52a8\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8fdf\u51fa\u73b0\u7684\u9ad8\u98ce\u9669\u540e\u679c\uff0c\u800c\u53cd\u5e94\u5f0f\u62a4\u680f\u65e0\u6cd5\u5728\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u8bc6\u522b\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u9884\u6d4b\u7684\u672a\u6765\u98ce\u9669\u4e0e\u5f53\u524d\u51b3\u7b56\u5bf9\u9f50\u3002SafePred\u6846\u67b6\u5efa\u7acb\u98ce\u9669\u5230\u51b3\u7b56\u7684\u5faa\u73af\uff0c\u652f\u6301\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a1) \u77ed\u671f\u548c\u957f\u671f\u98ce\u9669\u9884\u6d4b\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u751f\u6210\u98ce\u9669\u8bed\u4e49\u8868\u793a\uff1b2) \u51b3\u7b56\u4f18\u5316\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u5e72\u9884\u548c\u4efb\u52a1\u7ea7\u91cd\u65b0\u89c4\u5212\u5c06\u9884\u6d4b\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5b89\u5168\u51b3\u7b56\u6307\u5bfc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSafePred\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u98ce\u9669\u884c\u4e3a\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc797.6%\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5e76\u5c06\u4efb\u52a1\u6548\u7528\u63d0\u9ad8\u4e86\u9ad8\u8fbe21.4%\u3002", "conclusion": "SafePred\u901a\u8fc7\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u957f\u671f\u98ce\u9669\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u98ce\u9669\u5230\u51b3\u7b56\u7684\u5faa\u73af\uff0c\u5728\u4fdd\u6301\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u4efb\u52a1\u6548\u7528\uff0c\u4e3a\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00982", "categories": ["cs.CV", "cs.AI", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.00982", "abs": "https://arxiv.org/abs/2602.00982", "authors": ["Phu-Hoa Pham", "Chi-Nguyen Tran", "Dao Sy Duy Minh", "Nguyen Lam Phu Quy", "Huynh Trung Kiet"], "title": "Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025", "comment": "15 pages, 8 tables. Technical Report for winning solutions (Track 1 & Track 2) at the NeurIPS 2025 Mouse vs. AI Challenge", "summary": "Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86NeurIPS 2025 Mouse vs. AI\u7ade\u8d5b\u4e2d\u83b7\u80dc\u7684\u65b9\u6cd5\uff1aTrack 1\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4e24\u5c42CNN\u7ed3\u5408GLU\u548c\u5f52\u4e00\u5316\u83b7\u5f9795.4%\u5206\u6570\uff1bTrack 2\u4f7f\u752816\u5c42ResNet-like\u67b6\u6784\u5b9e\u73b0\u6700\u4f73\u795e\u7ecf\u9884\u6d4b\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u65f6\u957f\u4e0e\u6027\u80fd\u5448\u975e\u5355\u8c03\u5173\u7cfb\uff0c\u6700\u4f73\u7ed3\u679c\u572820\u4e07\u6b65\u5de6\u53f3\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u9c81\u68d2\u6027\u548c\u795e\u7ecf\u5bf9\u9f50\u8fd9\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5f00\u53d1\u80fd\u591f\u5339\u914d\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u7684\u4eba\u5de5\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u53c2\u52a0NeurIPS 2025\u7ade\u8d5b\u6765\u63a2\u7d22\u4e0d\u540c\u67b6\u6784\u5728\u89c6\u89c9\u9c81\u68d2\u6027\u548c\u795e\u7ecf\u5bf9\u9f50\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "Track 1\u91c7\u7528\u8f7b\u91cf\u7ea7\u4e24\u5c42CNN\u67b6\u6784\uff0c\u7ed3\u5408Gated Linear Units\u548c\u89c2\u6d4b\u5f52\u4e00\u5316\uff1bTrack 2\u91c7\u752816\u5c42\u5377\u79ef\u7684ResNet-like\u67b6\u6784\uff0c\u540c\u6837\u4f7f\u7528GLU\u95e8\u63a7\u673a\u5236\u3002\u7cfb\u7edf\u5206\u6790\u4e8610\u4e2a\u6a21\u578b\u68c0\u67e5\u70b9\uff08\u8bad\u7ec3\u6b65\u6570\u4ece6\u4e07\u5230114\u4e07\uff09\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u548c\u5931\u8d25\u6848\u4f8b\u5206\u6790\u3002", "result": "Track 1\u83b7\u5f9795.4%\u7684\u6700\u7ec8\u5206\u6570\uff0cTrack 2\u5b9e\u73b0top-1\u795e\u7ecf\u9884\u6d4b\u6027\u80fd\uff081780\u4e07\u53c2\u6570\uff09\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u65f6\u957f\u4e0e\u6027\u80fd\u5448\u975e\u5355\u8c03\u5173\u7cfb\uff0c\u6700\u4f73\u7ed3\u679c\u572820\u4e07\u6b65\u5de6\u53f3\u3002\u7b80\u5355\u67b6\u6784\u5728\u89c6\u89c9\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u66f4\u6df1\u3001\u5bb9\u91cf\u66f4\u5927\u7684\u6a21\u578b\u5728\u795e\u7ecf\u5bf9\u9f50\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5173\u4e8e\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u4e2d\u6a21\u578b\u590d\u6742\u6027\u7684\u4f20\u7edf\u5047\u8bbe\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684\u3001\u53d7\u751f\u7269\u542f\u53d1\u7684\u89c6\u89c9\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002\u7b80\u5355\u67b6\u6784\u5728\u89c6\u89c9\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u66f4\u6df1\u6a21\u578b\u5728\u795e\u7ecf\u5bf9\u9f50\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8bad\u7ec3\u65f6\u957f\u5b58\u5728\u6700\u4f18\u533a\u95f4\u3002"}}
{"id": "2602.01858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01858", "abs": "https://arxiv.org/abs/2602.01858", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures", "comment": null, "summary": "Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.", "AI": {"tldr": "SOPRAG\uff1a\u9488\u5bf9\u5de5\u4e1a\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\u68c0\u7d22\u7684\u6df7\u5408\u4e13\u5bb6RAG\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f53\u3001\u56e0\u679c\u3001\u6d41\u7a0b\u56fe\u4e13\u5bb6\u548c\u7a0b\u5e8f\u5361\u5c42\u89e3\u51b3\u4f20\u7edfRAG\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u548c\u54cd\u5e94\u5b9e\u7528\u6027\u3002", "motivation": "\u5de5\u4e1a\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\uff08SOPs\uff09\u68c0\u7d22\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u4e13\u6709\u7ed3\u6784\u50f5\u5316\u3001\u6761\u4ef6\u4f9d\u8d56\u76f8\u5173\u6027\u3001\u9700\u8981\u53ef\u6267\u884c\u6027\uff0c\u4f20\u7edf\u8bed\u4e49\u9a71\u52a8\u7684RAG\u8303\u5f0f\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faSOPRAG\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8303\u5f0f\uff1a1\uff09\u7528\u4e13\u95e8\u7684\u5b9e\u4f53\u3001\u56e0\u679c\u3001\u6d41\u7a0b\u56fe\u4e13\u5bb6\u66ff\u4ee3\u5e73\u9762\u5206\u5757\uff1b2\uff09\u7a0b\u5e8f\u5361\u5c42\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u6d88\u9664\u8ba1\u7b97\u566a\u58f0\uff1b3\uff09LLM\u5f15\u5bfc\u7684\u95e8\u63a7\u673a\u5236\u52a8\u6001\u52a0\u6743\u4e13\u5bb6\uff1b4\uff09\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u52a8\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u5de5\u4e1a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cSOPRAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u54cd\u5e94\u5b9e\u7528\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff08\u8bcd\u6cd5\u3001\u5bc6\u96c6\u3001\u57fa\u4e8e\u56fe\u7684RAG\uff09\uff0c\u5728\u771f\u5b9e\u5173\u952e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5b8c\u7f8e\u6267\u884c\u5206\u6570\u3002", "conclusion": "SOPRAG\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1aSOP\u68c0\u7d22\u7684\u72ec\u7279\u6311\u6218\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u667a\u80fd\u534f\u8c03\u673a\u5236\uff0c\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u7a0b\u5e8f\u68c0\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00654", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00654", "abs": "https://arxiv.org/abs/2602.00654", "authors": ["Jiaming Ma", "Guanjun Wang", "Qihe Huang", "Sheng Huang", "Haofeng Ma", "Zhengyang Zhou", "Pengkun Wang", "Binwu Wang", "Yang Wang"], "title": "PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting", "comment": null, "summary": "While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional \"periodic bucket\" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.", "AI": {"tldr": "PHAT\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u5468\u671f\u5f02\u8d28\u6027\u7684Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6876\u7ed3\u6784\u548c\u6b63\u8d1f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u4e0d\u540c\u53d8\u91cf\u5177\u6709\u4e0d\u540c\u52a8\u6001\u5468\u671f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u867d\u7136\u80fd\u5efa\u6a21\u5468\u671f\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u73b0\u5b9e\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u5468\u671f\u5f02\u8d28\u6027\u2014\u2014\u4e0d\u540c\u53d8\u91cf\u5177\u6709\u4e0d\u540c\u4e14\u52a8\u6001\u53d8\u5316\u7684\u5468\u671f\u3002\u8fd9\u79cd\u5468\u671f\u5f02\u8d28\u6027\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "PHAT\u5c06\u591a\u53d8\u91cf\u8f93\u5165\u7ec4\u7ec7\u6210\u4e09\u7ef4\"\u5468\u671f\u6027\u6876\"\u5f20\u91cf\uff0c\u7ef4\u5ea6\u5206\u522b\u5bf9\u5e94\u5177\u6709\u76f8\u4f3c\u5468\u671f\u6027\u7684\u53d8\u91cf\u7ec4\u3001\u6309\u76f8\u4f4d\u5bf9\u9f50\u7684\u65f6\u95f4\u6b65\u548c\u5468\u671f\u5185\u504f\u79fb\u3002\u901a\u8fc7\u9650\u5236\u6876\u5185\u4ea4\u4e92\u548c\u5c4f\u853d\u8de8\u6876\u8fde\u63a5\u6765\u907f\u514d\u4e0d\u4e00\u81f4\u5468\u671f\u7684\u5e72\u6270\u3002\u8fd8\u63d0\u51fa\u6b63\u8d1f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u5468\u671f\u5bf9\u9f50\u548c\u5468\u671f\u504f\u5dee\u4e24\u4e2a\u89d2\u5ea6\u6355\u83b7\u5468\u671f\u6027\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u8c03\u5236\u9879\u7f16\u7801\u5468\u671f\u6027\u5148\u9a8c\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5bf918\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aPHAT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "PHAT\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u5468\u671f\u5f02\u8d28\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u91cd\u8981\u5c40\u9650\u6027\uff0c\u4e3a\u5904\u7406\u5177\u6709\u4e0d\u540c\u52a8\u6001\u5468\u671f\u7684\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01747", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01747", "abs": "https://arxiv.org/abs/2602.01747", "authors": ["Hongseok Choi", "Serynn Kim", "Wencke Liermann", "Jin Seong", "Jin-Xia Huang"], "title": "Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training", "comment": "22 pages, 4 figures", "summary": "Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cd\u6280\u672f\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u6027\u80fd\uff1a\u4e24\u9636\u6bb5\u5fae\u8c03\u3001\u5206\u6570\u5bf9\u9f50\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u8bad\u7ec3\uff0c\u5728\u6709\u9650\u6570\u636e\u548c\u5168\u6570\u636e\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u6807\u8bb0\u6570\u636e\u6781\u5ea6\u7a00\u7f3a\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u7a33\u5065\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u5b9e\u9645\u5e94\u7528", "method": "1) \u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u5229\u7528\u4f4e\u79e9\u9002\u5e94\u66f4\u597d\u9002\u5e94\u76ee\u6807\u63d0\u793a\u4f5c\u6587\uff1b2) \u5206\u6570\u5bf9\u9f50\u6280\u672f\u6539\u5584\u9884\u6d4b\u4e0e\u771f\u5b9e\u5206\u6570\u5206\u5e03\u4e00\u81f4\u6027\uff1b3) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u81ea\u8bad\u7ec3\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u6269\u5c55\u8bad\u7ec3\u96c6\u540c\u65f6\u51cf\u8f7b\u6807\u7b7e\u566a\u58f0\u4f20\u64ad", "result": "\u572832\u6570\u636e\u8bbe\u7f6e\u4e0b\uff0c\u4e09\u79cd\u6280\u672f\u5747\u63d0\u5347\u6027\u80fd\uff0c\u96c6\u6210\u540e\u8fbe\u5230\u5168\u6570\u636e\u6027\u80fd\u768491.2%\uff1b\u5206\u6570\u5bf9\u9f50\u6280\u672f\u5728\u5168\u6570\u636e\u8bbe\u7f6e\u4e0b\u96c6\u6210\u5230DualBERT\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c", "conclusion": "\u63d0\u51fa\u7684\u4e09\u79cd\u5173\u952e\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u5728\u6709\u9650\u6570\u636e\u548c\u5168\u6570\u636e\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.00995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00995", "abs": "https://arxiv.org/abs/2602.00995", "authors": ["Nick DiSanto", "Ehsan Khodapanah Aghdam", "Han Liu", "Jacob Watson", "Yuankai K. Tao", "Hao Li", "Ipek Oguz"], "title": "VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes", "comment": "Accepted to SPIE Medical Imaging 2026", "summary": "Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.", "AI": {"tldr": "VAMOS-OCTA\uff1a\u4e00\u79cd\u7528\u4e8e\u4fee\u590dOCTA\u8fd0\u52a8\u4f2a\u5f71\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8840\u7ba1\u611f\u77e5\u591a\u8f74\u76d1\u7763\u5b9e\u73b0B\u626b\u63cf\u4fee\u590d\uff0c\u63d0\u5347\u6a2a\u622a\u9762\u6e05\u6670\u5ea6\u548c\u5bb9\u79ef\u6295\u5f71\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u6301\u5f0fOCTA\u5728\u975e\u5408\u4f5c\u6216\u513f\u79d1\u53d7\u8bd5\u8005\u4e2d\u6613\u53d7\u8fd0\u52a8\u4f2a\u5f71\u5f71\u54cd\uff0c\u5bfc\u81f43D\u91c7\u96c6\u65f6\u51fa\u73b0\u672a\u91c7\u6837\u533a\u57df\u548c\u7a7a\u767d\u5e26\uff0c\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u91c7\u75282.5D U-Net\u67b6\u6784\uff0c\u4ee5\u76f8\u90bbB\u626b\u63cf\u5806\u6808\u4e3a\u8f93\u5165\u91cd\u5efa\u53d7\u635f\u4e2d\u5fc3B\u626b\u63cf\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u8840\u7ba1\u611f\u77e5\u591a\u8f74\u6b63\u4ea4\u76d1\u7763\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u8840\u7ba1\u52a0\u6743\u5f3a\u5ea6\u91cd\u5efa\u4e0e\u8f74\u5411\u548c\u6a2a\u5411\u6295\u5f71\u4e00\u81f4\u6027\u3002", "result": "VAMOS-OCTA\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u53d7\u635f\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6062\u590d\u6e05\u6670\u6bdb\u7ec6\u8840\u7ba1\u3001\u8840\u7ba1\u8fde\u7eed\u6027\u548c\u5e72\u51c0\u7684en face\u6295\u5f71\u3002", "conclusion": "\u591a\u8f74\u76d1\u7763\u4e3a\u6062\u590d\u8fd0\u52a8\u9000\u5316\u76843D OCTA\u6570\u636e\u63d0\u4f9b\u4e86\u5f3a\u5927\u7ea6\u675f\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u624b\u6301OCTA\u5728\u975e\u5408\u4f5c\u53d7\u8bd5\u8005\u4e2d\u7684\u6210\u50cf\u8d28\u91cf\u3002"}}
{"id": "2602.01869", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01869", "abs": "https://arxiv.org/abs/2602.01869", "authors": ["Qirui Mi", "Zhijian Ma", "Mengyue Yang", "Haoxuan Li", "Yisen Wang", "Haifeng Zhang", "Jun Wang"], "title": "ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents", "comment": "20 Pages, 6 Figures, 4 Tables", "summary": "LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.", "AI": {"tldr": "ProcMEM\u6846\u67b6\u8ba9\u667a\u80fd\u4f53\u4ece\u4ea4\u4e92\u7ecf\u9a8c\u4e2d\u81ea\u4e3b\u5b66\u4e60\u7a0b\u5e8f\u6027\u8bb0\u5fc6\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u901a\u8fc7\u6280\u80fdMDP\u548cNon-Parametric PPO\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u91cd\u7528\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u4f9d\u8d56\u5373\u65f6\u63a8\u7406\uff0c\u5373\u4f7f\u5728\u91cd\u590d\u573a\u666f\u4e2d\u4e5f\u91cd\u65b0\u63a8\u5bfc\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6267\u884c\u4e0d\u7a33\u5b9a\uff0c\u7f3a\u4e4f\u7ecf\u9a8c\u91cd\u7528\u673a\u5236\u3002", "method": "\u63d0\u51faProcMEM\u6846\u67b6\uff1a1) \u5f62\u5f0f\u5316Skill-MDP\u5c06\u88ab\u52a8\u53d9\u4e8b\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6280\u80fd\uff1b2) \u5f15\u5165Non-Parametric PPO\uff0c\u5229\u7528\u8bed\u4e49\u68af\u5ea6\u751f\u6210\u9ad8\u8d28\u91cf\u5019\u9009\u6280\u80fd\uff0c\u901a\u8fc7PPO Gate\u9a8c\u8bc1\u6280\u80fd\uff1b3) \u57fa\u4e8e\u5206\u6570\u7684\u7ef4\u62a4\u673a\u5236\u4fdd\u6301\u7d27\u51d1\u9ad8\u8d28\u91cf\u7684\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u3002", "result": "\u5728\u9886\u57df\u5185\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u667a\u80fd\u4f53\u573a\u666f\u4e2d\uff0cProcMEM\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u91cd\u7528\u7387\u548c\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u7aef\u7684\u5185\u5b58\u538b\u7f29\u3002\u53ef\u89c6\u5316\u8fdb\u5316\u8f68\u8ff9\u548c\u6280\u80fd\u5206\u5e03\u5c55\u793a\u4e86\u900f\u660e\u77e5\u8bc6\u79ef\u7d2f\u8fc7\u7a0b\u3002", "conclusion": "ProcMEM\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u7a0b\u5e8f\u6027\u8bb0\u5fc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u7ecf\u9a8c\u91cd\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u548c\u957f\u671f\u81ea\u4e3b\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u77e5\u8bc6\u79ef\u7d2f\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2602.00656", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00656", "abs": "https://arxiv.org/abs/2602.00656", "authors": ["Yingxu Wang", "Xinwang Liu", "Mengzhu Wang", "Siyang Gao", "Nan Yin"], "title": "Riemannian Flow Matching for Disentangled Graph Domain Adaptation", "comment": null, "summary": "Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.", "AI": {"tldr": "DisRFM\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u56fe\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5d4c\u5165\u548c\u57fa\u4e8e\u6d41\u7684\u4f20\u8f93\u6765\u89e3\u51b3\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u56fe\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u4f7f\u7528\u5bf9\u6297\u5b66\u4e60\u5bf9\u9f50\u56fe\u5d4c\u5165\uff0c\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u7ed3\u6784\u9000\u5316 - \u5c42\u6b21\u548c\u8bed\u4e49\u8868\u793a\u7ea0\u7f20\uff1b2) \u4f18\u5316\u4e0d\u7a33\u5b9a\u6027 - \u6700\u5c0f\u6700\u5927\u5bf9\u6297\u8bad\u7ec3\u7684\u632f\u8361\u52a8\u6001\u3002", "method": "1) \u5c06\u56fe\u5d4c\u5165\u5230\u9ece\u66fc\u6d41\u5f62\u4e2d\uff0c\u4f7f\u7528\u6781\u5750\u6807\u663e\u5f0f\u89e3\u8026\u7ed3\u6784\uff08\u534a\u5f84\uff09\u548c\u8bed\u4e49\uff08\u89d2\u5ea6\uff09\uff1b2) \u901a\u8fc7\u5f84\u5411Wasserstein\u5bf9\u9f50\u4fdd\u6301\u62d3\u6251\u7ed3\u6784\uff0c\u901a\u8fc7\u89d2\u5ea6\u805a\u7c7b\u5b9e\u73b0\u8bed\u4e49\u533a\u5206\uff1b3) \u4f7f\u7528\u9ece\u66fc\u6d41\u5339\u914d\u5b66\u4e60\u5e73\u6ed1\u5411\u91cf\u573a\uff0c\u6cbf\u6d4b\u5730\u7ebf\u8def\u5f84\u5f15\u5bfc\u6e90\u7279\u5f81\u5411\u76ee\u6807\u79fb\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDisRFM\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6d41\u5339\u914d\u7684\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u66f4\u7d27\u7684\u76ee\u6807\u98ce\u9669\u754c\u9650\u3002", "conclusion": "DisRFM\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u7ed3\u6784\u9000\u5316\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01752", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01752", "abs": "https://arxiv.org/abs/2602.01752", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Li Guo"], "title": "WorldCup Sampling for Multi-bit LLM Watermarking", "comment": null, "summary": "As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.", "AI": {"tldr": "WorldCup\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6bd4\u7279\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7ade\u4e89\u673a\u5236\u5c06\u6d88\u606f\u6bd4\u7279\u76f4\u63a5\u5d4c\u5165\u5230token\u9009\u62e9\u4e2d\uff0c\u5728\u5bb9\u91cf\u3001\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u89e3\u7801\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8d8a\u6765\u8d8a\u50cf\u4eba\u7c7b\u7684\u6587\u672c\uff0c\u6c34\u5370\u6280\u672f\u4e3a\u8d85\u8d8a\u7b80\u5355\u68c0\u6d4b\u7684\u53ef\u9760\u6eaf\u6e90\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6cd5\u5927\u591a\u901a\u8fc7\u79cd\u5b50\u9a71\u52a8\u5f15\u5bfc\u6269\u5c55\u96f6\u6bd4\u7279\u65b9\u6848\uff0c\u5bfc\u81f4\u95f4\u63a5\u4fe1\u606f\u6d41\u3001\u6709\u9650\u6709\u6548\u5bb9\u91cf\u548c\u6b21\u4f18\u89e3\u7801\u3002", "method": "WorldCup\u5c06\u91c7\u6837\u89c6\u4e3a\u81ea\u7136\u901a\u4fe1\u4fe1\u9053\uff0c\u901a\u8fc7\u5206\u5c42\u7ade\u4e89\u673a\u5236\u5c06\u6d88\u606f\u6bd4\u7279\u76f4\u63a5\u5d4c\u5165\u5230token\u9009\u62e9\u4e2d\uff0c\u8be5\u673a\u5236\u7531\u4e92\u8865\u4fe1\u53f7\u5f15\u5bfc\u3002\u91c7\u7528\u71b5\u611f\u77e5\u8c03\u5236\u6765\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u611f\u77e5\u89e3\u7801\u652f\u6301\u9c81\u68d2\u7684\u6d88\u606f\u6062\u590d\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cWorldCup\u5728\u5bb9\u91cf\u3001\u53ef\u68c0\u6d4b\u6027\u3001\u9c81\u68d2\u6027\u3001\u6587\u672c\u8d28\u91cf\u548c\u89e3\u7801\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u5f3a\u5927\u5e73\u8861\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WorldCup\u4e3a\u672a\u6765\u5927\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u901a\u8fc7\u76f4\u63a5\u5d4c\u5165\u6d88\u606f\u6bd4\u7279\u548c\u5206\u5c42\u7ade\u4e89\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01000", "abs": "https://arxiv.org/abs/2602.01000", "authors": ["Vagish Kumar", "Souvik Chakraborty"], "title": "CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound", "comment": null, "summary": "Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.", "AI": {"tldr": "CortiNet\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u76ae\u5c42\u542f\u53d1\u7684\u53cc\u6d41\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u80c6\u56ca\u75be\u75c5\u8bca\u65ad\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4fe1\u53f7\u5206\u89e3\u548c\u611f\u77e5\u9a71\u52a8\u7279\u5f81\u5b66\u4e60\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bca\u65ad", "motivation": "\u8d85\u58f0\u6210\u50cf\u662f\u80c6\u56ca\u75be\u75c5\u7684\u4e3b\u8981\u8bca\u65ad\u65b9\u5f0f\uff0c\u4f46\u56fe\u50cf\u5206\u8fa8\u7387\u4f4e\u3001\u5b58\u5728\u6591\u70b9\u566a\u58f0\uff0c\u4f20\u7edf\u5927\u578b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u4e14\u53ef\u9760\u7684\u8bca\u65ad\u6a21\u578b", "method": "\u63d0\u51faCortiNet\u67b6\u6784\uff0c\u53d7\u4eba\u7c7b\u89c6\u89c9\u76ae\u5c42\u5e76\u884c\u5904\u7406\u901a\u8def\u542f\u53d1\uff0c\u5c06\u4f4e\u9891\u7ed3\u6784\u4fe1\u606f\u4e0e\u9ad8\u9891\u611f\u77e5\u7ec6\u8282\u5206\u79bb\u5904\u7406\uff0c\u901a\u8fc7\u4e13\u7528\u7f16\u7801\u6d41\u5904\u7406\u9891\u7387\u9009\u62e9\u6027\u8868\u793a\uff0c\u91c7\u7528\u540e\u671f\u76ae\u5c42\u5f0f\u878d\u5408\u673a\u5236\u6574\u5408\u4e92\u8865\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u53ef\u89e3\u91ca\u6027\u6846\u67b6", "result": "\u572810,692\u5f20\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d69\u79cd\u4e34\u5e8a\u76f8\u5173\u80c6\u56ca\u75be\u75c5\u7c7b\u522b\uff0cCortiNet\u8fbe\u523098.74%\u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u53c2\u6570\u6570\u91cf\u4ec5\u4e3a\u4f20\u7edf\u6df1\u5ea6\u5377\u79ef\u6a21\u578b\u7684\u4e00\u5c0f\u90e8\u5206", "conclusion": "CortiNet\u901a\u8fc7\u6574\u5408\u7269\u7406\u53ef\u89e3\u91ca\u7684\u591a\u5c3a\u5ea6\u4fe1\u53f7\u5206\u89e3\u548c\u611f\u77e5\u9a71\u52a8\u7279\u5f81\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u80c6\u56ca\u75be\u75c5\u8bca\u65ad\uff0c\u5177\u6709\u5f3a\u5927\u7684\u7269\u7406\u57fa\u7840\u5f52\u7eb3\u504f\u7f6e\u548c\u4e34\u5e8a\u90e8\u7f72\u6f5c\u529b"}}
{"id": "2602.01884", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01884", "abs": "https://arxiv.org/abs/2602.01884", "authors": ["Shidong Yang", "Tongwen Huang", "Hao Wen", "Yong Wang", "Li Chen", "Xiangxiang Chu"], "title": "Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models", "comment": null, "summary": "Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u6307\u5bfc\u7684\u8bad\u7ec3\u65b9\u6cd5(EGT)\u6765\u6539\u8fdb\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u71b5\u4f5c\u4e3a\u65e0\u76d1\u7763\u6307\u6807\u6765\u7b5b\u9009\u6570\u636e\u548c\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\uff0c\u89e3\u51b3\u504f\u597d\u6570\u636e\u566a\u58f0\u548c\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u504f\u597d\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u566a\u58f0\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff1b2) \u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u5ffd\u7565\u4e86\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u3002\u4f5c\u8005\u53d1\u73b0\u54cd\u5e94\u71b5\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u71b5\u53ef\u4ee5\u4f5c\u4e3a\u6807\u6ce8\u566a\u58f0\u548c\u6837\u672c\u96be\u5ea6\u7684\u53ef\u9760\u65e0\u76d1\u7763\u4ee3\u7406\u6307\u6807\u3002", "method": "\u63d0\u51fa\u71b5\u6307\u5bfc\u8bad\u7ec3(EGT)\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u7b56\u7565\uff1a1) \u71b5\u6307\u5bfc\u7684\u6570\u636e\u7b5b\u9009\uff1a\u4f7f\u7528\u71b5\u4f5c\u4e3a\u6307\u6807\u6765\u51cf\u8f7b\u4e0d\u53ef\u9760\u6837\u672c\u7684\u5f71\u54cd\uff1b2) \u71b5\u6307\u5bfc\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u9010\u6b65\u5f15\u5165\u66f4\u590d\u6742\u7684\u793a\u4f8b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cEGT\u8bad\u7ec3\u7684\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "\u71b5\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u65e0\u76d1\u7763\u6307\u6807\u6765\u6307\u5bfc\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u901a\u8fc7\u71b5\u6307\u5bfc\u7684\u6570\u636e\u7b5b\u9009\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u89e3\u51b3\u504f\u597d\u6570\u636e\u566a\u58f0\u548c\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2602.00670", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.00670", "abs": "https://arxiv.org/abs/2602.00670", "authors": ["Ashna Purwar", "Gaurav Simkar", "Madhumita", "Sachin Kadam"], "title": "Three-Way Emotion Classification of EEG-based Signals using Machine Learning", "comment": "6 pages, 8 figures, and 3 tables. Submitted to a conference, under review", "summary": "Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9EEG\u4fe1\u53f7\u8fdb\u884c\u4e09\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\uff08\u6d88\u6781\u3001\u4e2d\u6027\u3001\u79ef\u6781\uff09\uff0c\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u968f\u673a\u68ee\u6797\u4e09\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u4f73\u3002", "motivation": "EEG\u4fe1\u53f7\u80fd\u76f4\u63a5\u53cd\u6620\u5927\u8111\u6d3b\u52a8\uff0c\u53ef\u7528\u4e8e\u8bc6\u522b\u4eba\u7684\u60c5\u7eea\u72b6\u6001\u3002\u968f\u7740\u60c5\u611f\u611f\u77e5\u7cfb\u7edf\u548cEEG\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u7684\u589e\u957f\uff0c\u9700\u8981\u63a2\u7d22\u54ea\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6700\u9002\u5408EEG\u4fe1\u53f7\u7684\u4e09\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u6709\u9650\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41\u7a0b\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e09\u79cd\u5e38\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1a\u903b\u8f91\u56de\u5f52\uff08LR\uff09\u3001\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u3002\u901a\u8fc7\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u7528\u4e8eEEG\u4fe1\u53f7\u7684\u4e09\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\u3002\u5728\u4e09\u79cd\u6a21\u578b\u4e2d\uff0c\u968f\u673a\u68ee\u6797\u8868\u73b0\u6700\u597d\uff0c\u5176\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u8868\u660e\u5b83\u80fd\u66f4\u51c6\u786e\u6709\u6548\u5730\u6355\u6349\u60c5\u7eea\u6a21\u5f0f\u3002\u968f\u673a\u68ee\u6797\u5728\u51c6\u786e\u7387\u53c2\u6570\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u5206\u7c7b\u6a21\u578b\u3002", "conclusion": "\u968f\u673a\u68ee\u6797\u662fEEG\u4fe1\u53f7\u4e09\u5206\u7c7b\u60c5\u611f\u8bc6\u522b\u7684\u6700\u4f73\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9009\u62e9\uff0c\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u5bf9\u6bd4\u6a21\u578b\u548c\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2602.01757", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01757", "abs": "https://arxiv.org/abs/2602.01757", "authors": ["Doohyun Kim", "Donghwa Kang", "Kyungjae Lee", "Hyeongboo Baek", "Brent Byunghoon Kang"], "title": "Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings", "comment": "10 pages", "summary": "The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.", "AI": {"tldr": "Zero2Text\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u9012\u5f52\u5728\u7ebf\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ed1\u76d2\u8de8\u57df\u8bbe\u7f6e\u4e0b\u7684\u5d4c\u5165\u53cd\u6f14\u653b\u51fb\uff0c\u65e0\u9700\u6cc4\u9732\u6570\u636e\u5bf9\u5373\u53ef\u4ece\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u6062\u590d\u539f\u59cb\u6587\u672c\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u666e\u53ca\u4f7f\u5411\u91cf\u6570\u636e\u5e93\u6210\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u5d4c\u5165\u53cd\u6f14\u653b\u51fb\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6839\u672c\u6027\u6743\u8861\uff1a\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u91cf\u5de8\u5927\u7684\u67e5\u8be2\uff0c\u800c\u57fa\u4e8e\u5bf9\u9f50\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff08\u53ef\u8bbf\u95ee\u57df\u5185\u8bad\u7ec3\u6570\u636e\uff09\uff0c\u5728\u4e25\u683c\u7684\u9ed1\u76d2\u548c\u8de8\u57df\u8bbe\u7f6e\u4e2d\u65e0\u6548\u3002", "method": "Zero2Text\uff1a\u57fa\u4e8e\u9012\u5f52\u5728\u7ebf\u5bf9\u9f50\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u3002\u7ed3\u5408LLM\u5148\u9a8c\u77e5\u8bc6\u548c\u52a8\u6001\u5cad\u56de\u5f52\u673a\u5236\uff0c\u5728\u8fd0\u884c\u65f6\u8fed\u4ee3\u5730\u5c06\u751f\u6210\u6587\u672c\u4e0e\u76ee\u6807\u5d4c\u5165\u5bf9\u9f50\uff0c\u65e0\u9700\u9759\u6001\u6570\u636e\u96c6\u3002", "result": "\u5728MS MARCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9488\u5bf9OpenAI\u53d7\u5bb3\u8005\u6a21\u578b\uff0cZero2Text\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e861.8\u500d\u66f4\u9ad8\u7684ROUGE-L\u548c6.4\u500d\u66f4\u9ad8\u7684BLEU-2\u5206\u6570\uff0c\u80fd\u591f\u4ece\u672a\u77e5\u9886\u57df\u6062\u590d\u53e5\u5b50\u800c\u65e0\u9700\u4efb\u4f55\u6cc4\u9732\u7684\u6570\u636e\u5bf9\u3002\u6807\u51c6\u9632\u5fa1\uff08\u5982\u5dee\u5206\u9690\u79c1\uff09\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u8fd9\u79cd\u81ea\u9002\u5e94\u5a01\u80c1\u3002", "conclusion": "Zero2Text\u7a81\u7834\u4e86\u73b0\u6709\u5d4c\u5165\u53cd\u6f14\u653b\u51fb\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5728\u4e25\u683c\u7684\u9ed1\u76d2\u548c\u8de8\u57df\u8bbe\u7f6e\u4e2d\u6709\u6548\u5de5\u4f5c\uff0c\u63ed\u793a\u4e86\u5411\u91cf\u6570\u636e\u5e93\u9690\u79c1\u4fdd\u62a4\u7684\u4e25\u91cd\u6f0f\u6d1e\uff0c\u5e76\u8868\u660e\u5f53\u524d\u9632\u5fa1\u63aa\u65bd\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u81ea\u9002\u5e94\u5a01\u80c1\u3002"}}
{"id": "2602.01004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01004", "abs": "https://arxiv.org/abs/2602.01004", "authors": ["Zihao Zhao", "Shengting Cao", "Muchao Ye"], "title": "SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning", "comment": null, "summary": "Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.", "AI": {"tldr": "SRVAU-R1\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u601d\u589e\u5f3a\u7684MLLM\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u601d\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u6570\u636e\u96c6\u548c\u53cd\u601d\u611f\u77e5\u5b66\u4e60\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4efb\u52a1\u7684\u63a8\u7406\u8d28\u91cf\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMLLM\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u65b9\u6cd5\u4e3b\u8981\u505c\u7559\u5728\u8868\u9762\u63cf\u8ff0\u5c42\u9762\uff0c\u7f3a\u4e4f\u5bf9\u5f02\u5e38\u884c\u4e3a\u7684\u6df1\u5ea6\u63a8\u7406\uff0c\u7279\u522b\u662f\u7f3a\u5c11\u660e\u786e\u7684\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u7684\u6846\u67b6\u6765\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faSRVAU-R1\u6846\u67b6\uff1a1\uff09\u521b\u5efa\u9996\u4e2a\u9762\u5411\u53cd\u601d\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u5305\u542b\u521d\u59cb\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u548c\u4fee\u6b63\u63a8\u7406\u7684\u7ed3\u6784\u5316\u76d1\u7763\uff1b2\uff09\u91c7\u7528\u53cd\u601d\u611f\u77e5\u5b66\u4e60\u8303\u5f0f\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\u6765\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5f02\u5e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSRVAU-R1\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u65f6\u95f4\u5f02\u5e38\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53cd\u601d\u673a\u5236\u5230MLLM\u63a8\u7406\u4e2d\uff0cSRVAU-R1\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01893", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01893", "abs": "https://arxiv.org/abs/2602.01893", "authors": ["Timur Mudarisov", "Mikhal Burtsev", "Tatiana Petrova", "Radu State"], "title": "Geometric Analysis of Token Selection in Multi-Head Attention", "comment": null, "summary": "We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\u5206\u6790LLM\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u6807\u51c6\u6ce8\u610f\u529b\u89c6\u4e3atop-N\u9009\u62e9\u5668\uff0c\u5728\u503c\u72b6\u6001\u7a7a\u95f4\u7814\u7a76\u5176\u884c\u4e3a\uff0c\u5b9a\u4e49\u51e0\u4f55\u6307\u6807\u91cf\u5316token\u53ef\u5206\u6027\uff0c\u7406\u8bba\u9884\u6d4b\u5c0fN\u673a\u5236\u6700\u5f3a\u53ef\u5206\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u5e76\u4e0e\u4e09\u79cd\u6ce8\u610f\u529b\u5934\u7c7b\u578b\u5bf9\u5e94\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5728\u51e0\u4f55\u7a7a\u95f4\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002\u9700\u8981\u91cf\u5316\u6ce8\u610f\u529b\u5982\u4f55\u9009\u62e9token\uff0c\u7406\u89e3\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u529f\u80fd\u5dee\u5f02\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u51e0\u4f55\u89e3\u91ca\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "1) \u5c06\u6807\u51c6\u6ce8\u610f\u529b\u89c6\u4e3atop-N\u9009\u62e9\u5668\u5728\u503c\u72b6\u6001\u7a7a\u95f4\u5206\u6790\uff1b2) \u5b9a\u4e49\u51e0\u4f55\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F\u5206\u6570\uff09\u91cf\u5316\u9009\u4e2d\u4e0e\u672a\u9009\u4e2dtoken\u7684\u53ef\u5206\u6027\uff1b3) \u5728\u7ecf\u9a8c\u5047\u8bbe\u4e0b\u63a8\u5bfc\u975e\u6e10\u8fd1\u8fb9\u754c\uff1b4) \u5728LLaMA-2-7B\u3001Gemma-7B\u3001Mistral-7B\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u5c0fN\u673a\u5236\u4e0b\u6700\u5f3a\u975e\u5e73\u51e1\u53ef\u5206\u6027\uff0c\u5e8f\u5217\u957f\u5ea6\u548csink\u76f8\u4f3c\u5ea6\u5f71\u54cd\u51e0\u4f55\u6307\u6807\u3002\u5b9e\u9a8c\u6d4b\u91cf\u4e0e\u7406\u8bba\u5305\u7edc\u7ebf\u7d27\u5bc6\u5339\u914d\uff1atop-N\u9009\u62e9\u589e\u5f3a\u53ef\u5206\u6027\uff0csink\u76f8\u4f3c\u5ea6\u4e0e\u53ec\u56de\u7387\u76f8\u5173\u3002\u5728LLaMA-2-7B\u4e2d\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u5206\u4e3a\u4e09\u7c7b\uff1a\u68c0\u7d22\u5668\u3001\u6df7\u5408\u5668\u3001\u91cd\u7f6e\u5668\uff0c\u5404\u6709\u4e0d\u540c\u51e0\u4f55\u7279\u5f81\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u8868\u73b0\u4e3a\u7ed3\u6784\u5316\u51e0\u4f55\u5206\u7c7b\u5668\uff0c\u5177\u6709\u53ef\u6d4b\u91cf\u7684token\u9009\u62e9\u6807\u51c6\uff0c\u63d0\u4f9b\u5934\u90e8\u7ea7\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u51e0\u4f55\u611f\u77e5\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u548cLLM\u6ce8\u610f\u529b\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2602.00672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00672", "abs": "https://arxiv.org/abs/2602.00672", "authors": ["Aleksandr Yugay", "Hang Cui", "Changhua Pei", "Alexey Zaytsev"], "title": "Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD", "comment": null, "summary": "Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.", "AI": {"tldr": "\u7ebf\u6027\u81ea\u56de\u5f52\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u6781\u4f4e", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u590d\u6742\u3001\u96be\u4ee5\u8bad\u7ec3\u4e14\u63a8\u7406\u6602\u8d35\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u8303\u5f0f", "method": "\u63d0\u51fa\u7b80\u5355\u7684\u7ebf\u6027\u81ea\u56de\u5f52\u5f02\u5e38\u8bc4\u5206\u65b9\u6cd5\uff0c\u4f7f\u7528\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u7684\u95ed\u5f0f\u89e3\uff0c\u4f30\u8ba1\u6709\u9650\u5386\u53f2\u9ad8\u65af\u8fc7\u7a0b\u7684\u6761\u4ef6\u5bc6\u5ea6", "result": "\u5728\u5e7f\u6cdb\u7684\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u964d\u4f4e\u6570\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5305\u542b\u5f3a\u5927\u7684\u7ebf\u6027\u57fa\u7ebf\uff0c\u5e76\u5f00\u53d1\u5177\u6709\u66f4\u4e30\u5bcc\u65f6\u95f4\u7ed3\u6784\u7684\u65b0\u57fa\u51c6\uff0c\u4ee5\u660e\u786e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf"}}
{"id": "2602.01771", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.01771", "abs": "https://arxiv.org/abs/2602.01771", "authors": ["Jingyao Wu", "Bin Lu", "Zijun Di", "Xiaoying Gan", "Meng Jin", "Luoyi Fu", "Xinbing Wang", "Chenghu Zhou"], "title": "<SOG_k>: One LLM Token for Explicit Graph Structural Understanding", "comment": null, "summary": "Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.", "AI": {"tldr": "\u63d0\u51faSOG\u65b9\u6cd5\uff0c\u4f7f\u7528\u7279\u6b8a\u7ed3\u6784\u4ee4\u724c<SOG_k>\u5728\u7edf\u4e00\u4ee4\u724c\u7a7a\u95f4\u4e2d\u8868\u793a\u56fe\u7ed3\u6784\uff0c\u89e3\u51b3LLM\u5904\u7406\u56fe\u6570\u636e\u65f6\u7684\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u56fe\u6570\u636e\u65f6\u9762\u4e34\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u56fe\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u5bfc\u81f4\u4ee4\u724c\u6d88\u8017\u8fc7\u5927\u548c\u6ce8\u610f\u529b\u5206\u6563\uff0c\u8981\u4e48\u8f6c\u5316\u4e3a\u53ef\u8bad\u7ec3\u7684\u8fde\u7eed\u5d4c\u5165\u4f46\u4e0e\u539f\u6587\u672c\u4ee4\u724c\u4e25\u91cd\u4e0d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u62d3\u6251\u611f\u77e5\u7684\u7ed3\u6784\u4ee4\u724c\u5316\u5668\uff0c\u5c06\u6bcf\u4e2a\u56fe\u62d3\u6251\u6620\u5c04\u4e3a\u9ad8\u5ea6\u9009\u62e9\u6027\u7684\u5355\u4e2a\u4ee4\u724c<SOG_k>\uff0c\u6784\u5efa\u6df7\u5408\u7ed3\u6784\u95ee\u7b54\u8bed\u6599\u5e93\u5bf9\u9f50\u65b0\u7ed3\u6784\u4ee4\u724c\u4e0e\u73b0\u6709\u6587\u672c\u4ee4\u724c\uff0c\u5b9e\u73b0\u7edf\u4e00\u4ee4\u724c\u7a7a\u95f4\u4e2d\u7684\u56fe\u7ed3\u6784\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u56fe\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u53479.9%\u81f341.4%\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u53ef\u89e3\u91ca\u6027\u548c\u4e00\u81f4\u6027\u3002\u65b9\u6cd5\u53ef\u7075\u6d3b\u6269\u5c55\u5230\u8282\u70b9\u7ea7\u4efb\u52a1\uff0c\u652f\u6301\u5168\u5c40\u548c\u5c40\u90e8\u7ed3\u6784\u7406\u89e3\u3002", "conclusion": "SOG\u65b9\u6cd5\u901a\u8fc7\u7279\u6b8a\u7ed3\u6784\u4ee4\u724c<SOG_k>\u5728\u7edf\u4e00\u4ee4\u724c\u7a7a\u95f4\u4e2d\u8868\u793a\u56fe\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3LLM\u5904\u7406\u56fe\u6570\u636e\u7684\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u56fe\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.01012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01012", "abs": "https://arxiv.org/abs/2602.01012", "authors": ["Yiyang Su", "Minchul Kim", "Jie Zhu", "Christopher Perry", "Feng Liu", "Anil Jain", "Xiaoming Liu"], "title": "LocalScore: Local Density-Aware Similarity Scoring for Biometrics", "comment": null, "summary": "Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.", "AI": {"tldr": "LocalScore\uff1a\u4e00\u79cd\u57fa\u4e8ek\u8fd1\u90bb\u5c40\u90e8\u5bc6\u5ea6\u7684\u7b80\u5355\u8bc4\u5206\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5f00\u653e\u96c6\u751f\u7269\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6a21\u578b\u67b6\u6784", "motivation": "\u73b0\u6709\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u5728\u5904\u7406\u5f00\u653e\u96c6\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u5c06\u540c\u4e00\u4e3b\u4f53\u7684\u591a\u4e2a\u6837\u672c\u538b\u7f29\u4e3a\u5355\u4e00\u5168\u5c40\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u7279\u5f81\u5206\u5e03\u7684\u5c40\u90e8\u5bc6\u5ea6\u4fe1\u606f\uff0c\u5bfc\u81f4\u51b3\u7b56\u8fb9\u754c\u4e0d\u4f18\u548c\u5f00\u653e\u96c6\u9c81\u68d2\u6027\u5dee", "method": "\u63d0\u51faLocalScore\u8bc4\u5206\u7b97\u6cd5\uff0c\u5229\u7528k\u8fd1\u90bb\u663e\u5f0f\u5730\u7eb3\u5165\u56fe\u5e93\u7279\u5f81\u5206\u5e03\u7684\u5c40\u90e8\u5bc6\u5ea6\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u4e0e\u67b6\u6784\u65e0\u5173\u3001\u635f\u5931\u51fd\u6570\u72ec\u7acb\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848", "result": "\u5728\u591a\u6a21\u6001\u5b9e\u9a8c\u4e2d\uff0cLocalScore\u663e\u8457\u63d0\u5347\u5f00\u653e\u96c6\u68c0\u7d22\u6027\u80fd\uff08FNIR@FPIR\u4ece53%\u964d\u81f340%\uff09\u548c\u9a8c\u8bc1\u6027\u80fd\uff08TAR@FAR\u4ece51%\u63d0\u5347\u81f374%\uff09", "conclusion": "LocalScore\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5c40\u90e8\u5bc6\u5ea6\uff0c\u6709\u6548\u6539\u5584\u4e86\u5f00\u653e\u96c6\u751f\u7269\u8bc6\u522b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\u3001\u9ad8\u6548\u6027\u548c\u5373\u63d2\u5373\u7528\u7279\u6027"}}
{"id": "2602.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01910", "abs": "https://arxiv.org/abs/2602.01910", "authors": ["Michele Fiori", "Gabriele Civitarese", "Flora D. Salim", "Claudio Bettini"], "title": "DomusFM: A Foundation Model for Smart-Home Sensor Data", "comment": null, "summary": "Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.", "AI": {"tldr": "DomusFM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u8bbe\u8ba1\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\u6355\u83b7\u8bed\u4e49\u548c\u65f6\u95f4\u6a21\u5f0f\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u76d1\u7763\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e0d\u5b9e\u7528\uff1b\u73b0\u6709\u57fa\u7840\u6a21\u578b\u53ea\u5173\u6ce8\u60ef\u6027\u4f20\u611f\u5668\uff0c\u4e0d\u9002\u7528\u4e8e\u667a\u80fd\u5bb6\u5c45\u4e8c\u8fdb\u5236\u4f20\u611f\u5668\u6570\u636e\u7684\u7a00\u758f\u79bb\u6563\u7279\u6027\uff1b\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u9700\u8981\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6216\u63d0\u793a\uff0c\u4f9d\u8d56\u5916\u90e8\u670d\u52a1\u6216\u6602\u8d35\u786c\u4ef6\uff0c\u5b58\u5728\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "DomusFM\u91c7\u7528\u81ea\u76d1\u7763\u53cc\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\uff0c\u6574\u5408\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u5d4c\u5165\u4ee5\u53ca\u4e13\u95e8\u7528\u4e8e\u65f6\u95f4\u6a21\u5f0f\u548c\u4e8c\u8fdb\u5236\u72b6\u6001\u7684\u7f16\u7801\u5668\uff0c\u540c\u65f6\u6355\u83b7\u6807\u8bb0\u7ea7\u8bed\u4e49\u5c5e\u6027\u548c\u5e8f\u5217\u7ea7\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u667a\u80fd\u5bb6\u5c45\u6570\u636e\u96c6\u4e0a\u7684\u7559\u4e00\u6570\u636e\u96c6\u8bc4\u4f30\u8868\u660e\uff0cDomusFM\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5373\u4f7f\u5728\u4ec5\u67095%\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DomusFM\u89e3\u51b3\u4e86\u667a\u80fd\u5bb6\u5c45\u4f20\u611f\u5668\u6570\u636e\u5206\u6790\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00688", "abs": "https://arxiv.org/abs/2602.00688", "authors": ["Tom Segal", "Asaf Shabtai", "Yuval Elovici"], "title": "Provably Protecting Fine-Tuned LLMs from Training Data Extraction", "comment": "20 pages, 5 figures", "summary": "Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$\u0394_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$\u0394_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.", "AI": {"tldr": "\u63d0\u51faSCP-\u0394r\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u5e73\u6ed1\u4f4e\u5f71\u54cd\u529btoken\u7684\u6982\u7387\u504f\u79fb\u6765\u9632\u5fa1\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u6570\u636e\u4e0a\u5fae\u8c03\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9690\u79c1\u4fdd\u8bc1\uff0c\u8981\u4e48\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u5fae\u8c03\u53ea\u5f15\u8d77\u5c11\u6570\u5173\u952etoken\u7684\u6982\u7387\u504f\u79fb\uff0c\u5927\u90e8\u5206\u504f\u79fb\u53ef\u4ee5\u5e73\u6ed1\u5904\u7406\u800c\u4e0d\u5f71\u54cd\u6027\u80fd", "method": "\u63d0\u51faSCP-\u0394r\u7b97\u6cd5\uff0c\u57fa\u4e8eNear Access Freeness\u6846\u67b6\uff0c\u64cd\u4f5c\u76f8\u5bf9\u6982\u7387\uff0c\u660e\u786e\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u5e73\u6ed1\u4f4e\u5f71\u54cd\u529btoken\uff0c\u53ea\u4fdd\u7559\u5173\u952etoken\u7684\u504f\u79fb", "result": "SCP-\u0394r\u76f8\u6bd4\u73b0\u6709NAF\u65b9\u6cd5\u83b7\u5f97\u6570\u91cf\u7ea7\u66f4\u597d\u7684\u7406\u8bba\u754c\u9650\uff0c\u5728\u5b9e\u9a8c\u4e2d\u80fd\u6709\u6548\u9632\u5fa1\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u5c0f\u6027\u80fd\u635f\u5931", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u7559\u5173\u952etoken\u504f\u79fb\u5e76\u5e73\u6ed1\u5176\u4ed6token\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3aLLM\u5fae\u8c03\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01778", "abs": "https://arxiv.org/abs/2602.01778", "authors": ["Kangtao Lv", "Jiwei Tang", "Langming Liu", "Haibin Chen", "Weidong Zhang", "Shilei Liu", "Yongwei Wang", "Yujin Yuan", "Wenbo Su", "Bo Zheng"], "title": "Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model", "comment": "15 pages,6 figures", "summary": "The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u6570\u636e\u4e2d\u5fc3\u7684\u89c6\u89d2\u7cfb\u7edf\u7814\u7a76\u6570\u636e\u5206\u5e03\u5bf9\u4e0a\u4e0b\u6587\u538b\u7f29\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8f93\u5165\u71b5\u4e0e\u538b\u7f29\u8d28\u91cf\u8d1f\u76f8\u5173\uff0c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5185\u5728\u6570\u636e\u5dee\u8ddd\u663e\u8457\u964d\u4f4e\u538b\u7f29\u589e\u76ca", "motivation": "LLM\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u5197\u4f59\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u6a21\u578b\u4fa7\u6539\u8fdb\uff0c\u6570\u636e\u5206\u5e03\u672c\u8eab\u5bf9\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u63a2\u7d22", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u89c6\u89d2\uff0c\u4ece\u8f93\u5165\u6570\u636e\u548c\u5185\u5728\u6570\u636e\u4e24\u4e2a\u7ef4\u5ea6\u5206\u6790\u6570\u636e\u5206\u5e03\u5bf9\u538b\u7f29\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u6846\u67b6\u8bc4\u4f30\u538b\u7f29\u8868\u793a\u7684\u8bed\u4e49\u5b8c\u6574\u6027", "result": "1) \u7f16\u7801\u5668\u6d4b\u91cf\u7684\u8f93\u5165\u71b5\u4e0e\u538b\u7f29\u8d28\u91cf\u8d1f\u76f8\u5173\uff0c\u800c\u89e3\u7801\u5668\u6d4b\u91cf\u7684\u71b5\u5728\u51bb\u7ed3\u89e3\u7801\u5668\u8bbe\u7f6e\u4e0b\u65e0\u663e\u8457\u5173\u7cfb\uff1b2) \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5185\u5728\u6570\u636e\u5dee\u8ddd\u663e\u8457\u964d\u4f4e\u538b\u7f29\u589e\u76ca\u4e14\u96be\u4ee5\u7f13\u89e3", "conclusion": "\u57fa\u4e8e\u53d1\u73b0\u63d0\u51fa\u4e86\u4f18\u5316\u538b\u7f29\u589e\u76ca\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5f3a\u8c03\u9700\u8981\u540c\u65f6\u8003\u8651\u8f93\u5165\u6570\u636e\u548c\u6a21\u578b\u5185\u5728\u77e5\u8bc6\u7684\u6570\u636e\u5206\u5e03\u7279\u6027"}}
{"id": "2602.01020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01020", "abs": "https://arxiv.org/abs/2602.01020", "authors": ["Jichen Yang", "Jikai Zhang", "Benjamin Wildman-Tobriner", "Maciej A. Mazurowski"], "title": "Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning", "comment": "9 pages, 3 figures", "summary": "The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.", "AI": {"tldr": "\u81ea\u52a8\u6807\u6ce8\u7684\u7532\u72b6\u817a\u7ed3\u8282\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u4f7f\u7528\u5168\u90e8\u6570\u636e\u6bd4\u4ec5\u7528\u9ad8\u7cbe\u5ea6\u5b50\u96c6\u6548\u679c\u66f4\u597d", "motivation": "\u7532\u72b6\u817a\u7ed3\u8282\u8d85\u58f0\u56fe\u50cf\u8bca\u65ad\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u3002\u5148\u524d\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u4f46\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u6709\u6548\u6027\u5c1a\u672a\u9a8c\u8bc1", "method": "\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6bd4\u4e09\u79cd\u6570\u636e\u96c6\uff1a\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u3001\u81ea\u52a8\u6807\u6ce8\u5b8c\u6574\u6570\u636e\u96c6\u3001\u81ea\u52a8\u6807\u6ce8\u9ad8\u7cbe\u5ea6\u5b50\u96c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5dee\u5f02", "result": "\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578bAUC\u4e3a0.694\uff0c\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578bAUC 0.643\u3002\u9ad8\u7cbe\u5ea6\u5b50\u96c6\u8bad\u7ec3\u7684\u6a21\u578bAUC\u4e3a0.689\uff0c\u4e0e\u5b8c\u6574\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u65e0\u663e\u8457\u5dee\u5f02", "conclusion": "\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u6027\u80fd\uff0c\u5efa\u8bae\u4f7f\u7528\u5168\u90e8\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u800c\u975e\u4ec5\u7528\u9ad8\u7cbe\u5ea6\u5b50\u96c6"}}
{"id": "2602.01933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01933", "abs": "https://arxiv.org/abs/2602.01933", "authors": ["Fabrice Boissier", "Monica Sen", "Irina Rychkova"], "title": "Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling", "comment": null, "summary": "Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.", "AI": {"tldr": "\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u5728\u4e3b\u9898\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u8bc4\u4f30\u5b83\u4eec\u5728\u6587\u6863\u4e3b\u9898\u63d0\u53d6\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u4e3b\u9898\u5efa\u6a21\u5728\u6587\u6863\u68c0\u7d22\u3001\u60c5\u611f\u5206\u6790\u548c\u6587\u672c\u6458\u8981\u7b49\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\u3002\u867d\u7136LLM\u5728\u6587\u672c\u5904\u7406\u4e2d\u5f88\u6d41\u884c\uff0c\u4f46\u5f88\u5c11\u7814\u7a76\u5176\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\u3002FCA\u6700\u8fd1\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4e3b\u9898\u5efa\u6a21\u7684\u5019\u9009\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83LLM\u548cFCA\u5728\u4e3b\u9898\u5efa\u6a21\u9886\u57df\u7684\u4f18\u7f3a\u70b9\u3002", "method": "\u4f7f\u7528CREA\u7ba1\u9053\u8bc4\u4f30FCA\uff08\u57fa\u4e8e\u8fc7\u53bb\u4e3b\u9898\u5efa\u6a21\u548c\u53ef\u89c6\u5316\u5b9e\u9a8c\uff09\uff0c\u4f7f\u7528GPT-5\u8bc4\u4f30LLM\u3002\u91c7\u7528\u57fa\u4e8e\u4e09\u4e2a\u63d0\u793a\u7684\u96f6\u6837\u672c\u7b56\u7565\uff1a\u4ece\u6587\u6863\u6279\u6b21\u751f\u6210\u4e3b\u9898\u3001\u5408\u5e76\u6279\u6b21\u7ed3\u679c\u5f62\u6210\u6700\u7ec8\u4e3b\u9898\u3001\u4ee5\u53ca\u4e3b\u9898\u6807\u6ce8\u3002\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u91cd\u7528\u4e4b\u524d\u8bc4\u4f30CREA\u7684\u6559\u5b66\u6750\u6599\uff0c\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u5206\u679040\u7bc7\u4fe1\u606f\u7cfb\u7edf\u7814\u7a76\u6587\u7ae0\uff0c\u6bd4\u8f83\u63d0\u53d6\u7684\u4e3b\u9898\u4e0e\u5e95\u5c42\u5b50\u9886\u57df\u3002", "result": "\u8bba\u6587\u6ca1\u6709\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\u6570\u636e\uff0c\u4f46\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u6bd4\u8f83\u4e86LLM\u548cFCA\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff1a\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6559\u5b66\u6750\u6599\uff0c\u7b2c\u4e8c\u4e2a\u4f7f\u7528\u4fe1\u606f\u7cfb\u7edf\u7814\u7a76\u6587\u7ae0\uff0c\u65e8\u5728\u8bc4\u4f30\u63d0\u53d6\u7684\u4e3b\u9898\u4e0e\u5b9e\u9645\u5b50\u9886\u57df\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83LLM\u548cFCA\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u586b\u8865\u4e86LLM\u5728\u4e3b\u9898\u5efa\u6a21\u9886\u57df\u5e94\u7528\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u4e3aFCA\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6848\u4f8b\u7814\u7a76\u3002"}}
{"id": "2602.00693", "categories": ["cs.LG", "math.AG", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.00693", "abs": "https://arxiv.org/abs/2602.00693", "authors": ["Marco Nurisso", "Pierrick Leroy", "Giovanni Petri", "Francesco Vaccarino"], "title": "Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities", "comment": "Accepted to ICLR 2026. 32 pages, 13 figures", "summary": "Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u524d\u9988ReLU\u7f51\u7edc\u53c2\u6570\u7a7a\u95f4\u7684\u62d3\u6251\u6027\u8d28\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\u67b6\u6784\u7684\u7f51\u7edc\u4e2d\u53c2\u6570\u7a7a\u95f4\u7684\u8fde\u901a\u6027\u548c\u5947\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u7406\u89e3\u524d\u9988ReLU\u7f51\u7edc\u53c2\u6570\u7a7a\u95f4\u7684\u6027\u8d28\u5bf9\u4e8e\u6709\u6548\u5206\u6790\u548c\u6307\u5bfc\u8bad\u7ec3\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002ReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u9f50\u6b21\u6027\u5bfc\u81f4\u68af\u5ea6\u6d41\u8bad\u7ec3\u5c06\u53c2\u6570\u7a7a\u95f4\u9650\u5236\u5728\u4e00\u4e2a\u4ee3\u6570\u7c07\u4e0a\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u8be5\u7a7a\u95f4\u7684\u62d3\u6251\u7279\u6027\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u4e00\u822c\u6709\u5411\u65e0\u73af\u56fe\u67b6\u6784\u7684\u524d\u9988ReLU\u7f51\u7edc\uff0c\u5206\u6790\u53c2\u6570\u7a7a\u95f4\u7684\u8fde\u901a\u6027\u548c\u5947\u5f02\u6027\u3002\u901a\u8fc7\u74f6\u9888\u8282\u70b9\u548c\u5e73\u8861\u6761\u4ef6\u6765\u8868\u5f81\u8fde\u901a\u6027\uff0c\u63a2\u7d22\u5947\u5f02\u6027\u4e0e\u5e95\u5c42DAG\u62d3\u6251\u7ed3\u6784\u53ca\u5176\u8bf1\u5bfc\u5b50\u7f51\u7edc\u7684\u5173\u8054\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5947\u5f02\u6027\u4e0e\u5e95\u5c42DAG\u62d3\u6251\u7ed3\u6784\u5bc6\u5207\u76f8\u5173\uff0c\u5efa\u7acb\u4e86\u53c2\u6570\u7a7a\u95f4\u8fde\u901a\u6027\u7684\u5b8c\u6574\u7279\u5f81\u63cf\u8ff0\uff0c\u63ed\u793a\u4e86\u5947\u5f02\u6027\u53ef\u8fbe\u6027\u4e0e\u53ef\u5fae\u526a\u679d\u4e4b\u95f4\u7684\u539f\u7406\u6027\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u3002", "conclusion": "\u524d\u9988ReLU\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\u62d3\u6251\u6027\u8d28\uff08\u7279\u522b\u662f\u8fde\u901a\u6027\u548c\u5947\u5f02\u6027\uff09\u4e0e\u7f51\u7edc\u67b6\u6784\u7684DAG\u62d3\u6251\u7ed3\u6784\u6709\u6df1\u523b\u8054\u7cfb\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3\u8bad\u7ec3\u52a8\u6001\u548c\u7f51\u7edc\u526a\u679d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.01785", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01785", "abs": "https://arxiv.org/abs/2602.01785", "authors": ["Yuling Shi", "Chaoxiang Xie", "Zhensu Sun", "Yeheng Chen", "Chenxu Zhang", "Longfei Yun", "Chengcheng Wan", "Hongyu Zhang", "David Lo", "Xiaodong Gu"], "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "comment": "Code and data are available at https://github.com/YerbaPage/CodeOCR", "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u901a\u8fc7\u56fe\u50cf\u8868\u793a\u6e90\u4ee3\u7801\u6765\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u6587\u672c\u8868\u793a\u53ef\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u67d0\u4e9b\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684LLMs\u5728\u6e90\u4ee3\u7801\u7406\u89e3\u4e0a\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u56e0\u4e3a\u6587\u672c\u8868\u793a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002\u591a\u6a21\u6001LLMs\u7684\u53d1\u5c55\u4e3a\u901a\u8fc7\u56fe\u50cf\u8868\u793a\u6e90\u4ee3\u7801\u63d0\u4f9b\u4e86\u4f18\u5316\u6548\u7387\u7684\u673a\u4f1a\u3002", "method": "\u5c06\u6e90\u4ee3\u7801\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u56fe\u50cf\u6a21\u6001\u56fa\u6709\u7684\u53ef\u538b\u7f29\u6027\uff0c\u901a\u8fc7\u8c03\u6574\u5206\u8fa8\u7387\u5c06\u56fe\u50cf\u7f29\u653e\u5230\u539f\u59cbtoken\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u7136\u540e\u4f7f\u7528\u89c6\u89c9\u80fd\u529b\u7684MLLMs\u8fdb\u884c\u4ee3\u7801\u7406\u89e3\u3002\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc4\u4f30MLLMs\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1) MLLMs\u80fd\u6709\u6548\u7406\u89e3\u4ee3\u7801\u5e76\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684token\u538b\u7f29\uff1b(2) MLLMs\u80fd\u6709\u6548\u5229\u7528\u8bed\u6cd5\u9ad8\u4eae\u7b49\u89c6\u89c9\u7ebf\u7d22\uff0c\u57284\u500d\u538b\u7f29\u4e0b\u63d0\u5347\u4ee3\u7801\u8865\u5168\u6027\u80fd\uff1b(3) \u514b\u9686\u68c0\u6d4b\u7b49\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u5bf9\u89c6\u89c9\u538b\u7f29\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u97e7\u6027\uff0c\u67d0\u4e9b\u538b\u7f29\u6bd4\u751a\u81f3\u7565\u5fae\u4f18\u4e8e\u539f\u59cb\u6587\u672c\u8f93\u5165\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86MLLMs\u5728\u4ee3\u7801\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u9650\u5236\uff0c\u6307\u51fa\u5411\u56fe\u50cf\u6a21\u6001\u4ee3\u7801\u8868\u793a\u8f6c\u53d8\u662f\u5b9e\u73b0\u66f4\u9ad8\u6548\u63a8\u7406\u7684\u9014\u5f84\u3002\u56fe\u50cf\u8868\u793a\u6e90\u4ee3\u7801\u4e3a\u5927\u89c4\u6a21\u8f6f\u4ef6\u7cfb\u7edf\u7684\u4ee3\u7801\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6548\u7387\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2602.01033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01033", "abs": "https://arxiv.org/abs/2602.01033", "authors": ["Chentian Sun"], "title": "GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration", "comment": "A 5-page paper with 1 figure, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.", "AI": {"tldr": "GMAC\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u89c6\u56fe\u91cd\u5efa\u7f51\u7edc\u9690\u5f0f\u51e0\u4f55\u8868\u793a\u7684\u591a\u76f8\u673a\u5916\u53c2\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u56de\u5f52\u5934\u76f4\u63a5\u9884\u6d4b\u5916\u53c2\uff0c\u65e0\u9700\u663e\u5f0f3D\u91cd\u5efa\u6216\u624b\u52a8\u6807\u5b9a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6807\u5b9a\u677f\u3001\u663e\u5f0f\u51e0\u4f55\u5efa\u6a21\u6216\u4efb\u52a1\u7279\u5b9a\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u6216\u5728\u7ebf\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u90e8\u7f72", "method": "GMAC\u5c06\u5916\u53c2\u5efa\u6a21\u4e3a\u53d7\u6f5c\u5728\u591a\u89c6\u56fe\u51e0\u4f55\u7ed3\u6784\u7ea6\u675f\u7684\u5168\u5c40\u53d8\u91cf\uff0c\u5bf9\u73b0\u6709\u7f51\u7edc\u8fdb\u884c\u526a\u679d\u548c\u7ed3\u6784\u91cd\u6784\uff0c\u4f7f\u5176\u6f5c\u5728\u7279\u5f81\u901a\u8fc7\u8f7b\u91cf\u56de\u5f52\u5934\u76f4\u63a5\u652f\u6301\u5916\u53c2\u9884\u6d4b\uff0c\u65e0\u9700\u5168\u65b0\u7f51\u7edc\u8bbe\u8ba1\u3002\u540c\u65f6\u8054\u5408\u4f18\u5316\u8de8\u89c6\u56fe\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u548c\u591a\u89c6\u56fe\u5faa\u73af\u4e00\u81f4\u6027", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u591a\u76f8\u673a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGMAC\u5b9e\u73b0\u4e86\u51c6\u786e\u7a33\u5b9a\u7684\u5916\u53c2\u4f30\u8ba1\uff0c\u65e0\u9700\u663e\u5f0f3D\u91cd\u5efa\u6216\u624b\u52a8\u6807\u5b9a", "conclusion": "GMAC\u4e3a\u591a\u76f8\u673a\u7cfb\u7edf\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u5728\u7ebf\u6807\u5b9a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01970", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01970", "abs": "https://arxiv.org/abs/2602.01970", "authors": ["Yun Qu", "Qi Wang", "Yixiu Mao", "Heming Zou", "Yuhang Jiang", "Weijie Liu", "Clive Bai", "Kai Yang", "Yangkun Chen", "Saiyong Yang", "Xiangyang Ji"], "title": "Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models", "comment": null, "summary": "Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.", "AI": {"tldr": "\u63d0\u51faGPS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u9884\u6d4b\u63d0\u793a\u96be\u5ea6\uff0c\u7ed3\u5408\u4e2d\u7b49\u96be\u5ea6\u4f18\u5148\u548c\u5386\u53f2\u951a\u5b9a\u591a\u6837\u6027\u539f\u5219\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u63d0\u793a\u6279\u6b21\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd", "motivation": "\u5f3a\u5316\u5b66\u4e60\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884crollout\u4f18\u5316\u3002\u5728\u7ebf\u63d0\u793a\u9009\u62e9\u662f\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u7684\u53ef\u884c\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u786e\u5207\u8bc4\u4f30\uff0c\u8981\u4e48\u6784\u5efa\u7f3a\u4e4f\u8de8\u63d0\u793a\u6cdb\u5316\u80fd\u529b\u7684\u7279\u5b9a\u9884\u6d4b\u6a21\u578b", "method": "\u63d0\u51fa\u901a\u7528\u9884\u6d4b\u6027\u63d0\u793a\u9009\u62e9(GPS)\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u751f\u6210\u6a21\u578b\u5728\u5171\u4eab\u4f18\u5316\u5386\u53f2\u6570\u636e\u4e0a\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u7406\u9884\u6d4b\u63d0\u793a\u96be\u5ea6\uff1b2) \u7ed3\u5408\u4e2d\u7b49\u96be\u5ea6\u4f18\u5148\u539f\u5219\uff1b3) \u5f15\u5165\u5386\u53f2\u951a\u5b9a\u591a\u6837\u6027\u539f\u5219\uff1b4) \u8bbe\u8ba1\u6279\u91cf\u83b7\u53d6\u539f\u5219\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u63d0\u793a\u6279\u6b21\uff1b5) \u5c0f\u9884\u6d4b\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u4e5f\u80fd\u6cdb\u5316\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u5206\u914d", "result": "\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGPS\u5728\u8bad\u7ec3\u6548\u7387\u3001\u6700\u7ec8\u6027\u80fd\u548c\u6d4b\u8bd5\u65f6\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "GPS\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\u548c\u521b\u65b0\u7684\u9009\u62e9\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u63d0\u793a\u9009\u62e9\u7684\u6548\u7387\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00694", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00694", "abs": "https://arxiv.org/abs/2602.00694", "authors": ["Fabio Turazza", "Marcello Pietri", "Natalia Selini Hadjidimitriou", "Marco Mamei"], "title": "Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning", "comment": "Published as a book chapter in the MEDES 2024 proceedings (Springer LNCS)", "summary": "Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u8054\u90a6\u5b66\u4e60\u4e0eLSTM\u7f51\u7edc\u5982\u4f55\u5728\u4e0d\u5171\u4eab\u9690\u79c1\u654f\u611f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u672c\u5730\u80fd\u6e90\u793e\u533a\u5f00\u53d1\u51c6\u786e\u7684\u80fd\u6e90\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u672c\u5730\u80fd\u6e90\u793e\u533a\u9762\u4e34\u5b9e\u73b0\u80fd\u6e90\u81ea\u7ed9\u81ea\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\u6765\u5e73\u8861\u80fd\u6e90\u751f\u4ea7\u4e0e\u6d88\u8d39\u3002\u7136\u800c\uff0c\u7528\u6237\u901a\u5e38\u4e0d\u613f\u5171\u4eab\u5176\u6d88\u8d39\u6a21\u5f0f\u7b49\u9690\u79c1\u654f\u611f\u4fe1\u606f\uff0c\u8fd9\u963b\u788d\u4e86\u4f20\u7edf\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e0e\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u8054\u90a6\u5b66\u4e60\u5141\u8bb8\u5728\u672c\u5730\u8bbe\u5907\u4e0a\u8bad\u7ec3\u6a21\u578b\u800c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\uff0cLSTM\u7f51\u7edc\u5219\u7528\u4e8e\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u8054\u90a6\u5b66\u4e60\u4e0eLSTM\u7f51\u7edc\u80fd\u591f\u6709\u6548\u521b\u5efa\u80fd\u6e90\u9884\u6d4b\u6a21\u578b\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u6570\u636e\u5171\u4eab\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u89e3\u51b3\u672c\u5730\u80fd\u6e90\u793e\u533a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u80fd\u6e90\u7ba1\u7406\u3002"}}
{"id": "2602.01807", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01807", "abs": "https://arxiv.org/abs/2602.01807", "authors": ["DongNyeong Heo", "Heelyoul Choi"], "title": "Sentence Curve Language Models", "comment": null, "summary": "Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.", "AI": {"tldr": "SCLM\u63d0\u51fa\u53e5\u5b50\u66f2\u7ebf\u8868\u793a\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u8fde\u7eed\u53e5\u5b50\u66f2\u7ebf\u800c\u975e\u9759\u6001\u8bcd\u5d4c\u5165\uff0c\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u7ed3\u6784\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u9759\u6001\u8bcd\u5d4c\u5165\u8868\u793a\u76ee\u6807\u53e5\u5b50\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u76f8\u90bb\u8bcd\u4e0d\u654f\u611f\uff0c\u9f13\u52b1\u5c40\u90e8\u51c6\u786e\u7684\u8bcd\u9884\u6d4b\u4f46\u5ffd\u7565\u4e86\u53e5\u5b50\u7684\u5168\u5c40\u7ed3\u6784\u3002\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e5f\u5b58\u5728\u7c7b\u4f3c\u9650\u5236\u3002", "method": "\u63d0\u51fa\u53e5\u5b50\u66f2\u7ebf\u8868\u793a\u6cd5\uff0c\u5c06\u53e5\u5b50\u8868\u793a\u4e3a\u6837\u6761\u66f2\u7ebf\uff0c\u5176\u63a7\u5236\u70b9\u5f71\u54cd\u591a\u4e2a\u8bcd\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u53e5\u5b50\u66f2\u7ebf\u8bed\u8a00\u6a21\u578b(SCLM)\uff0c\u6269\u5c55\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u53e5\u5b50\u66f2\u7ebf\u800c\u975e\u9759\u6001\u8bcd\u5d4c\u5165\u3002", "result": "SCLM\u5728IWSLT14\u548cWMT14\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684SOTA\u6027\u80fd\uff0c\u8bad\u7ec3\u7a33\u5b9a\u65e0\u9700\u7e41\u7410\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5728LM1B\u4e0a\u4e0e\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u6f5c\u529b\u3002", "conclusion": "\u53e5\u5b50\u66f2\u7ebf\u8868\u793a\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u5168\u5c40\u7ed3\u6784\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9759\u6001\u8bcd\u5d4c\u5165\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2602.01035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01035", "abs": "https://arxiv.org/abs/2602.01035", "authors": ["Chentian Sun"], "title": "FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence", "comment": "A 5-page paper, prepared for submission to the 2026 IEEE International Conference on Image Processing (ICIP)", "summary": "Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.", "AI": {"tldr": "FUSE-Flow\uff1a\u4e00\u79cd\u5e27\u7ea7\u3001\u65e0\u72b6\u6001\u3001\u7ebf\u6027\u53ef\u6269\u5c55\u7684\u70b9\u4e91\u6d41\u5f0f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7a7a\u95f4\u54c8\u5e0c\u52a0\u6743\u805a\u5408\u5b9e\u73b0\u5b9e\u65f6\u591a\u89c6\u89d2\u70b9\u4e91\u91cd\u5efa", "motivation": "\u5b9e\u65f6\u591a\u89c6\u89d2\u70b9\u4e91\u91cd\u5efa\u5728VR\u3001AR\u3001\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u57fa\u4e8e\u4f53\u7d20\u878d\u5408\u3001\u65f6\u95f4\u7d2f\u79ef\u6216\u5168\u5c40\u4f18\u5316\uff09\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u5185\u5b58\u5360\u7528\u5927\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u6027\u3001\u91cd\u5efa\u8d28\u91cf\u548c\u591a\u76f8\u673a\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51faFUSE-Flow\u6846\u67b6\uff1a1\uff09\u6bcf\u5e27\u72ec\u7acb\u751f\u6210\u70b9\u4e91\u7247\u6bb5\uff0c\u901a\u8fc7\u6d4b\u91cf\u7f6e\u4fe1\u5ea6\u548c3D\u8ddd\u79bb\u4e00\u81f4\u6027\u4e24\u4e2a\u6743\u91cd\u8fdb\u884c\u878d\u5408\u4ee5\u6291\u5236\u566a\u58f0\uff1b2\uff09\u5f15\u5165\u81ea\u9002\u5e94\u7a7a\u95f4\u54c8\u5e0c\u52a0\u6743\u805a\u5408\u65b9\u6cd5\uff0c\u6839\u636e\u5c40\u90e8\u70b9\u4e91\u5bc6\u5ea6\u81ea\u9002\u5e94\u5212\u52063D\u7a7a\u95f4\uff0c\u6bcf\u4e2a\u5355\u5143\u9009\u62e9\u4ee3\u8868\u6027\u70b9\u8fdb\u884c\u52a0\u6743\u878d\u5408\uff1b3\uff09\u5229\u7528GPU\u5e76\u884c\u5316\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u5904\u7406", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u91cd\u53e0\u533a\u57df\u3001\u6df1\u5ea6\u4e0d\u8fde\u7eed\u548c\u52a8\u6001\u573a\u666f\u4e2d\u63d0\u9ad8\u4e86\u91cd\u5efa\u7a33\u5b9a\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5728\u73b0\u4ee3GPU\u4e0a\u4fdd\u6301\u5b9e\u65f6\u5e27\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "FUSE-Flow\u6210\u529f\u89e3\u51b3\u4e86\u5b9e\u65f6\u591a\u89c6\u89d2\u70b9\u4e91\u91cd\u5efa\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u3001\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u591a\u76f8\u673a\u53ef\u6269\u5c55\u6027\u7684\u5e73\u8861\uff0c\u4e3a3D\u89c6\u89c9\u548c\u6c89\u6d78\u5f0f\u611f\u77e5\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01983", "abs": "https://arxiv.org/abs/2602.01983", "authors": ["Xintian Shen", "Jiawei Chen", "Lihao Zheng", "Hao Ma", "Tao Wei", "Kun Zhan"], "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning", "comment": null, "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.", "AI": {"tldr": "UCT\u6846\u67b6\u8ba9LLM\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u8f6c\u53d8\u4e3a\u5de5\u5177\u521b\u9020\u8005\uff0c\u901a\u8fc7\u63d0\u53d6\u63a8\u7406\u7ecf\u9a8c\u521b\u5efa\u81ea\u9002\u5e94\u5de5\u5177\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6301\u7eed\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u56fa\u5b9a\u5de5\u5177\u65e0\u6cd5\u5e94\u5bf9\u5f00\u653e\u6027\u95ee\u9898\uff1b2) \u9519\u8bef\u5de5\u5177\u8f93\u51fa\u4f1a\u8bef\u5bfcLLM\uff1b3) \u5de5\u5177\u6784\u5efa\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4", "method": "\u63d0\u51faUCT\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6LLM\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9690\u542b\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5c06\u63a8\u7406\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u5de5\u5177\u3002\u5f15\u5165\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u7ef4\u62a4\u5de5\u5177\u5e93\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u5de5\u5177\u521b\u5efa\u548c\u81ea\u6211\u66f4\u65b0", "result": "\u5728\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347+20.86%\u548c+23.04%\uff0c\u9a8c\u8bc1\u4e86\u4ee3\u7406\u7684\u81ea\u6211\u8fdb\u5316\u80fd\u529b", "conclusion": "UCT\u4e3a\u589e\u5f3aTIR\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f7f\u4ee3\u7406\u7cfb\u7edf\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u6301\u7eed\u8fdb\u6b65\uff0c\u5b9e\u73b0\u4e86\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u5230\u5de5\u5177\u521b\u9020\u8005\u7684\u8f6c\u53d8"}}
{"id": "2602.00704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00704", "abs": "https://arxiv.org/abs/2602.00704", "authors": ["Hanqi Lyu", "Di Huang", "Yaoyu Zhu", "Kangcheng Liu", "Bohan Dou", "Chongxiao Li", "Pengwei Jin", "Shuyao Cheng", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "title": "LocalV: Exploiting Information Locality for IP-level Verilog Generation", "comment": null, "summary": "The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.", "AI": {"tldr": "LocalV\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u5757\u5316\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u4fe1\u606f\u5c40\u90e8\u6027\uff0c\u5c06\u957f\u6587\u6863\u5230\u957f\u4ee3\u7801\u751f\u6210\u95ee\u9898\u5206\u89e3\u4e3a\u77ed\u6587\u6863\u3001\u77ed\u4ee3\u7801\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86RTL\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edfRTL\u4ee3\u7801\u751f\u6210\u9700\u8981\u5de5\u7a0b\u5e08\u624b\u52a8\u5c06\u590d\u6742\u89c4\u8303\u8f6c\u6362\u4e3a\u6570\u5343\u884c\u53ef\u7efc\u5408HDL\u4ee3\u7801\uff0c\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5728\u5904\u7406\u5de5\u4e1a\u7ea7IP\u8bbe\u8ba1\u4efb\u52a1\u65f6\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5904\u7406\u5197\u957f\u8be6\u7ec6\u6587\u6863\u3001\u751f\u6210\u957f\u4ee3\u7801\u65f6\u6b63\u786e\u6027\u4e0b\u964d\u3001\u4ee5\u53ca\u590d\u6742\u7684\u8c03\u8bd5\u5468\u671f\u3002", "method": "LocalV\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6587\u6863\u5206\u533a\u3001\u4efb\u52a1\u89c4\u5212\u3001\u5c40\u90e8\u5316\u4ee3\u7801\u751f\u6210\u3001\u63a5\u53e3\u4e00\u81f4\u6027\u5408\u5e76\u548cAST\u5f15\u5bfc\u7684\u5c40\u90e8\u611f\u77e5\u8c03\u8bd5\uff0c\u5c06\u957f\u6587\u6863\u5230\u957f\u4ee3\u7801\u751f\u6210\u5206\u89e3\u4e3a\u77ed\u6587\u6863\u3001\u77ed\u4ee3\u7801\u4efb\u52a1\u3002", "result": "\u5728IP\u7ea7Verilog\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5RealBench\u4e0a\uff0cLocalV\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684LLM\u548c\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u901a\u8fc7\u7387\u8fbe\u523045.0%\uff0c\u800cSOTA\u65b9\u6cd5\u4ec5\u4e3a21.6%\u3002", "conclusion": "LocalV\u901a\u8fc7\u5229\u7528\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u4fe1\u606f\u5c40\u90e8\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7ea7RTL\u4ee3\u7801\u751f\u6210\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01838", "abs": "https://arxiv.org/abs/2602.01838", "authors": ["Abdelrahman Mansour", "Khaled W. Alshaer", "Moataz Elsaban"], "title": "AXE: Low-Cost Cross-Domain Web Structured Information Extraction", "comment": null, "summary": "Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized \"pruning\" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.", "AI": {"tldr": "AXE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f51\u9875\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u7cfb\u7edf\uff0c\u901a\u8fc7DOM\u6811\u526a\u679d\u673a\u5236\u548c\u7269\u7406\u53ef\u8ffd\u6eaf\u7684XPath\u89e3\u6790\uff0c\u4ec5\u75280.6B\u5c0f\u6a21\u578b\u5c31\u5b9e\u73b0\u4e86SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3\u7f51\u9875\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u4e2d\u624b\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\u8106\u5f31\u6027\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06HTML DOM\u89c6\u4e3a\u9700\u8981\u526a\u679d\u7684\u6811\u800c\u975e\u7eaf\u6587\u672c\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\"\u526a\u679d\"\u673a\u5236\u53bb\u9664\u65e0\u5173\u8282\u70b9\uff0c\u4fdd\u7559\u9ad8\u5bc6\u5ea6\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408Grounded XPath Resolution\u786e\u4fdd\u6bcf\u4e2a\u63d0\u53d6\u90fd\u80fd\u7269\u7406\u8ffd\u6eaf\u81f3\u6e90\u8282\u70b9", "result": "\u5728SWDE\u6570\u636e\u96c6\u4e0a\u8fbe\u523088.1%\u7684F1\u5206\u6570\uff0c\u96f6\u6837\u672c\u6027\u80fd\u4f18\u4e8e\u591a\u4e2a\u66f4\u5927\u7684\u5168\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "AXE\u901a\u8fc7\u521b\u65b0\u7684DOM\u526a\u679d\u65b9\u6cd5\u548c\u7269\u7406\u53ef\u8ffd\u6eaf\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u7f51\u9875\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6210\u672c"}}
{"id": "2602.01037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01037", "abs": "https://arxiv.org/abs/2602.01037", "authors": ["Guangshuo Qin", "Zhiteng Li", "Zheng Chen", "Weihang Zhang", "Linghe Kong", "Yulun Zhang"], "title": "VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models", "comment": null, "summary": "Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\\% on Kimi-VL and 3.09\\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.", "AI": {"tldr": "VEQ\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9MoE\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u611f\u77e5\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u89c6\u89c9-\u8bed\u8a00\u6a21\u6001\u5dee\u5f02\u548c\u4e13\u5bb6\u5f02\u8d28\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6027\u80fd\u3002", "motivation": "MoE\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u4f18\u5f02\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5de8\u5927\uff0c\u9700\u8981\u538b\u7f29\u3002\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u5f02\u8d28\u6027\uff1a\u89c6\u89c9\u548c\u8bed\u8a00token\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u4e0d\u540c\u4e13\u5bb6\u7684\u975e\u5747\u5300\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u4e13\u5bb6\u91cf\u5316(VEQ)\u6846\u67b6\uff0c\u5305\u542b\uff1a1)\u6a21\u6001\u4e13\u5bb6\u611f\u77e5\u91cf\u5316\uff0c\u5229\u7528\u4e13\u5bb6\u6fc0\u6d3b\u9891\u7387\u4f18\u5148\u6700\u5c0f\u5316\u5173\u952e\u4e13\u5bb6\u7684\u8bef\u5dee\uff1b2)\u6a21\u6001\u4eb2\u548c\u611f\u77e5\u91cf\u5316\uff0c\u901a\u8fc7\u6574\u5408token-\u4e13\u5bb6\u4eb2\u548c\u5ea6\u4e0e\u6a21\u6001\u4fe1\u606f\u6784\u5efa\u589e\u5f3a\u7684Hessian\u77e9\u9635\u6307\u5bfc\u6821\u51c6\u8fc7\u7a0b\u3002", "result": "\u5728W3A16\u914d\u7f6e\u4e0b\uff0cVEQ\u5728Kimi-VL\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.04%\uff0c\u5728Qwen3-VL\u4e0a\u63d0\u53473.09%\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "VEQ\u901a\u8fc7\u540c\u65f6\u8003\u8651\u8de8\u6a21\u6001\u5dee\u5f02\u548c\u4e13\u5bb6\u5f02\u8d28\u6027\uff0c\u4e3aMoE\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2602.01992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01992", "abs": "https://arxiv.org/abs/2602.01992", "authors": ["Gouki Minegishi", "Jingyuan Feng", "Hiroki Furuta", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Emergent Analogical Reasoning in Transformers", "comment": null, "summary": "Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u7c7b\u6bd4\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u8de8\u7c7b\u522b\u5b9e\u4f53\u5bf9\u5e94\u5173\u7cfb\u7684\u63a8\u65ad\uff0c\u57fa\u4e8e\u8303\u7574\u8bba\u4e2d\u7684\u51fd\u5b50\u6982\u5ff5\uff0c\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u8bc4\u4f30Transformer\u4e2d\u7c7b\u6bd4\u63a8\u7406\u7684\u6d8c\u73b0\u673a\u5236\u3002", "motivation": "\u7c7b\u6bd4\u662f\u4eba\u7c7b\u667a\u80fd\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4f46Transformer\u5982\u4f55\u83b7\u5f97\u548c\u5b9e\u73b0\u7c7b\u6bd4\u63a8\u7406\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u5c06\u7c7b\u6bd4\u4ece\u62bd\u8c61\u7684\u8ba4\u77e5\u6982\u5ff5\u8f6c\u5316\u4e3a\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e2d\u5177\u4f53\u3001\u673a\u5236\u57fa\u7840\u7684\u73b0\u8c61\u3002", "method": "\u53d7\u8303\u7574\u8bba\u4e2d\u51fd\u5b50\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u5c06\u7c7b\u6bd4\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u8de8\u7c7b\u522b\u5b9e\u4f53\u5bf9\u5e94\u5173\u7cfb\u7684\u63a8\u65ad\u3002\u5f15\u5165\u5408\u6210\u4efb\u52a1\u5728\u53d7\u63a7\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u7c7b\u6bd4\u63a8\u7406\u7684\u6d8c\u73b0\uff0c\u5e76\u8fdb\u884c\u673a\u5236\u5206\u6790\u3002", "result": "\u53d1\u73b0\u7c7b\u6bd4\u63a8\u7406\u7684\u6d8c\u73b0\u5bf9\u6570\u636e\u7279\u5f81\u3001\u4f18\u5316\u9009\u62e9\u548c\u6a21\u578b\u89c4\u6a21\u9ad8\u5ea6\u654f\u611f\u3002Transformer\u4e2d\u7684\u7c7b\u6bd4\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5173\u7cfb\u7ed3\u6784\u7684\u51e0\u4f55\u5bf9\u9f50\uff0c\u4ee5\u53caTransformer\u5185\u90e8\u51fd\u5b50\u7684\u5e94\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u673a\u5236\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u5173\u7cfb\u7ed3\u6784\u4ece\u4e00\u4e2a\u7c7b\u522b\u8f6c\u79fb\u5230\u53e6\u4e00\u4e2a\u7c7b\u522b\uff0c\u5b9e\u73b0\u7c7b\u6bd4\u3002\u5728\u9884\u8bad\u7ec3LLM\u4e2d\u4e5f\u89c2\u5bdf\u5230\u76f8\u540c\u8d8b\u52bf\uff0c\u6210\u529f\u5c06\u7c7b\u6bd4\u4ece\u62bd\u8c61\u8ba4\u77e5\u6982\u5ff5\u8f6c\u5316\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5177\u4f53\u673a\u5236\u73b0\u8c61\u3002"}}
{"id": "2602.00717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00717", "abs": "https://arxiv.org/abs/2602.00717", "authors": ["Licheng Pan", "Hao Wang", "Haocheng Yang", "Yuqi Li", "Qingsong Wen", "Xiaoxi Li", "Zhichao Chen", "Haoxuan Li", "Zhixuan Chu", "Yuan Lu"], "title": "Deep Time-series Forecasting Needs Kernelized Moment Balancing", "comment": null, "summary": "Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.", "AI": {"tldr": "\u63d0\u51faKMB-DF\u65b9\u6cd5\uff0c\u901a\u8fc7\u6838\u5316\u77e9\u5e73\u8861\u5b9e\u73b0\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5b8c\u5168\u5206\u5e03\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u5206\u5e03\u5e73\u8861\uff0c\u5b83\u4eec\u53ea\u5339\u914d\u4e00\u4e2a\u6216\u4e24\u4e2a\u9884\u5b9a\u4e49\u7684\u5e73\u8861\u51fd\u6570\uff0c\u65e0\u6cd5\u6ee1\u8db3Imbens\u51c6\u5219\u8981\u6c42\u7684\u5b8c\u5168\u5206\u5e03\u5e73\u8861", "method": "\u63d0\u51faKMB-DF\u65b9\u6cd5\uff0c\u4ece\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u5e73\u8861\u51fd\u6570\uff0c\u5b9e\u73b0\u5145\u5206\u7684\u5206\u5e03\u5e73\u8861\uff0c\u63a8\u5bfc\u51fa\u53ef\u5fae\u5206\u7684\u4f18\u5316\u76ee\u6807", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKMB-DF\u80fd\u6301\u7eed\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6838\u5316\u77e9\u5e73\u8861\u5b9e\u73b0\u5b8c\u5168\u5206\u5e03\u5e73\u8861\u662f\u63d0\u5347\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0cKMB-DF\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01840", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01840", "abs": "https://arxiv.org/abs/2602.01840", "authors": ["Jiwei Tang", "Shilei Liu", "Zhicheng Zhang", "Qingsong Lv", "Runsong Zhao", "Tingwei Lu", "Langming Liu", "Haibin Chen", "Yujin Yuan", "Hai-Tao Zheng", "Wenbo Su", "Bo Zheng"], "title": "Read As Human: Compressing Context via Parallelizable Close Reading and Skimming", "comment": "13 pages,5 figures", "summary": "Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).", "AI": {"tldr": "RAM\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u9605\u8bfb\u884c\u4e3a\uff08\u7cbe\u8bfb\u91cd\u8981\u5185\u5bb9+\u7565\u8bfb\u6b21\u8981\u5185\u5bb9\uff09\uff0c\u5bf9\u957f\u4e0a\u4e0b\u6587\u8fdb\u884c\u81ea\u9002\u5e94\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b012\u500d\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u5197\u4f59\u4e24\u5927\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6027\u80fd\u53c8\u80fd\u63d0\u9ad8\u6548\u7387\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u3002", "method": "RAM\u91c7\u7528\u81ea\u9002\u5e94\u6df7\u5408\u9605\u8bfb\u7b56\u7565\uff1a\u5c06\u4e0a\u4e0b\u6587\u5206\u6bb5\u5e76\u884c\u7f16\u7801\uff0c\u9ad8\u76f8\u5173\u6bb5\u5b8c\u6574\u4fdd\u7559\uff08\u7cbe\u8bfb\uff09\uff0c\u4f4e\u76f8\u5173\u6bb5\u901a\u8fc7\u67e5\u8be2\u5f15\u5bfc\u538b\u7f29\u4e3a\u6458\u8981\u5411\u91cf\uff08\u7565\u8bfb\uff09\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u7cbe\u8bfb\u4e0e\u7565\u8bfb\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u548c\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5e73\u5747\u957f\u5ea616K\u3001\u6700\u5927\u957f\u5ea632K\u7684\u957f\u8f93\u5165\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad812\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "RAM\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u9605\u8bfb\u7684\u6df7\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5197\u4f59\u4fe1\u606f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2602.01038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01038", "abs": "https://arxiv.org/abs/2602.01038", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Andrea Colaco"], "title": "From Videos to Conversations: Egocentric Instructions for Task Assistance", "comment": null, "summary": "Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u52a8\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u53cc\u4eba\u591a\u6a21\u6001\u4efb\u52a1\u6307\u5bfc\u5bf9\u8bdd\u7684\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86\u5305\u542b507\u4e2a\u5bf9\u8bdd\u30016636\u4e2a\u95ee\u7b54\u5bf9\u7684HowToDIV\u6570\u636e\u96c6\uff0c\u4e3a\u591a\u6a21\u6001\u7a0b\u5e8f\u6027\u4efb\u52a1\u8f85\u52a9\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u65e5\u5e38\u4efb\u52a1\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46AI\u52a9\u624b\u5728AR\u8f85\u52a9\u65b9\u9762\u8fdb\u5c55\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u6267\u884c\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u800c\u4eba\u5de5\u6536\u96c6\u6570\u636e\u6210\u672c\u9ad8\u3001\u590d\u6742\u5ea6\u5927\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b8c\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\uff0c\u5c06\u5355\u4eba\u6559\u5b66\u89c6\u9891\u8f6c\u6362\u4e3a\u4e13\u5bb6-\u65b0\u624b\u53cc\u4eba\u591a\u6a21\u6001\u4efb\u52a1\u6307\u5bfc\u5bf9\u8bdd\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6570\u636e\u6536\u96c6\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u521b\u5efa\u4e86HowToDIV\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b507\u4e2a\u5bf9\u8bdd\u30016,636\u4e2a\u95ee\u7b54\u5bf9\u300124\u5c0f\u65f6\u89c6\u9891\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u6bcf\u4e2a\u4f1a\u8bdd\u5305\u542b\u591a\u8f6e\u4e13\u5bb6-\u65b0\u624b\u4ea4\u4e92\uff0c\u5e76\u4f7f\u7528Gemma 3\u548cQwen 2.5\u63d0\u4f9b\u4e86\u57fa\u51c6\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u7a0b\u5e8f\u6027\u4efb\u52a1\u8f85\u52a9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\uff0cHowToDIV\u6570\u636e\u96c6\u548c\u57fa\u51c6\u7ed3\u679c\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2602.01995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01995", "abs": "https://arxiv.org/abs/2602.01995", "authors": ["Jeongmoon Won", "Seungwon Kook", "Yohan Jo"], "title": "Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs", "comment": null, "summary": "Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5bf9\u8bdd\u8bca\u65ad\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u8bca\u65ad\u5047\u8bbe\u548c\u9a8c\u8bc1\u6027\u63d0\u95ee\u7684\u4e24\u6b65\u63a8\u7406\uff0c\u5728\u6a21\u7cca\u75c7\u72b6\u63cf\u8ff0\u4e0b\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bca\u65ad\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\uff0c\u8981\u4e48\u5047\u8bbe\u60a3\u8005\u63d0\u4f9b\u4e30\u5bcc\u5177\u4f53\u4fe1\u606f\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u5904\u7406\u4fe1\u606f\u4e0d\u5b8c\u6574\u548c\u75c7\u72b6\u63cf\u8ff0\u6a21\u7cca\u7684\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u63a8\u7406\u7cfb\u7edf\uff1a1) \u4ece\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u751f\u6210\u8bca\u65ad\u5047\u8bbe\uff1b2) \u901a\u8fc7\u6f84\u6e05\u95ee\u9898\u9a8c\u8bc1\u5047\u8bbe\uff0c\u5faa\u73af\u76f4\u5230\u6700\u7ec8\u8bca\u65ad\u3002\u4f7f\u7528MIMIC-IV\u60a3\u8005\u6863\u6848\u548c\u6a21\u7cca\u75c7\u72b6\u6a21\u62df\u5668\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u533b\u5e08\u8bc4\u4f30\u8bc1\u5b9e\u6a21\u62df\u5668\u771f\u5b9e\u6027\u548c\u751f\u6210\u95ee\u9898\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5bf9\u8bdd\u8bca\u65ad\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u6a21\u7cca\u75c7\u72b6\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u8bca\u65ad\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00718", "abs": "https://arxiv.org/abs/2602.00718", "authors": ["Noorain Mukhtiar", "Adnan Mahmood", "Yipeng Zhou", "Jian Yang", "Jing Teng", "Quan Z. Sheng"], "title": "Federated Learning at the Forefront of Fairness: A Multifaceted Perspective", "comment": "7 pages (main content), 2 pages (references), Accepted and Published Proceedings of the 34th International Joint Conference on Artificial Intelligence (IJCAI). 2025", "summary": "Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e86\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u56e0\u4e3a\u5f02\u6784\u5ba2\u6237\u7aef\u7ea6\u675f\u548c\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u5e73\u8861\u9700\u6c42\u3002\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u5206\u7c7b\u6846\u67b6\u548c\u7814\u7a76\u65b9\u5411", "method": "\u91c7\u7528\u591a\u89d2\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff1a1) \u6a21\u578b\u6027\u80fd\u5bfc\u5411\u7684\u516c\u5e73\u6027\u65b9\u6cd5\uff1b2) \u80fd\u529b\u5bfc\u5411\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u3002\u63d0\u4f9b\u5206\u7c7b\u6846\u67b6\u89e3\u51b3\u5404\u79cd\u516c\u5e73\u6027\u95ee\u9898\u548c\u76f8\u5173\u6280\u672f\u65b9\u9762\uff0c\u5e76\u5206\u6790\u5176\u5728\u5e73\u8861\u516c\u5e73\u4e0e\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u91cd\u8981\u7684\u91cf\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7406\u8bba\u57fa\u7840", "conclusion": "\u672c\u6587\u4e3a\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6027\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u5e76\u63d0\u51fa\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u91cd\u8981\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2602.01875", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01875", "abs": "https://arxiv.org/abs/2602.01875", "authors": ["Langming Liu", "Kangtao Lv", "Haibin Chen", "Weidong Zhang", "Yejing Wang", "Shilei Liu", "Xin Tong", "Yujin Yuan", "Yongwei Wang", "Wenbo Su", "Bo Zheng"], "title": "PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning", "comment": null, "summary": "Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of \"low-probability truth\" and \"high-probability falsehood\". Recent approaches, such as teaching models to say \"I don't know\" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \\textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is \"\\textbf{debiasing then learning}.\" It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making \"room\" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faPretrainRL\u6846\u67b6\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u6765\u5de9\u56fa\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u901a\u8fc7\"\u53bb\u504f\u540e\u5b66\u4e60\"\u539f\u5219\u964d\u4f4e\u9ad8\u6982\u7387\u9519\u8bef\u4fe1\u606f\u7684\u6743\u91cd\uff0c\u4e3a\u4f4e\u6982\u7387\u771f\u5b9e\u4fe1\u606f\u521b\u9020\u5b66\u4e60\u7a7a\u95f4\uff0c\u663e\u8457\u7f13\u89e3\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u9519\u8bef\u4fe1\u606f\u3002\u4f5c\u8005\u53d1\u73b0\u6839\u6e90\u5728\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\"\u4f4e\u6982\u7387\u771f\u5b9e\"\u548c\"\u9ad8\u6982\u7387\u9519\u8bef\"\u7684\u72b6\u6001\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u56de\u907f\u95ee\u9898\uff08\u5982\u6559\u6a21\u578b\u8bf4\"\u6211\u4e0d\u77e5\u9053\"\uff09\uff0c\u8981\u4e48\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\uff08\u5982\u4e8b\u540e\u77e5\u8bc6\u7f16\u8f91\uff09\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u3002", "method": "\u63d0\u51faPretrainRL\u6846\u67b6\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u6574\u5408\u5230\u9884\u8bad\u7ec3\u9636\u6bb5\u3002\u6838\u5fc3\u539f\u5219\u662f\"\u53bb\u504f\u540e\u5b66\u4e60\"\uff1a\u4e3b\u52a8\u91cd\u5851\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\uff0c\u964d\u4f4e\u9ad8\u6982\u7387\u9519\u8bef\u4fe1\u606f\u7684\u6743\u91cd\uff0c\u4e3a\u4f4e\u6982\u7387\u771f\u5b9e\u4fe1\u606f\u521b\u9020\u6709\u6548\u5b66\u4e60\u7a7a\u95f4\u3002\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u6765\u53d1\u73b0\u8fd9\u4e9b\u9ad8\u6982\u7387\u9519\u8bef\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u5173\u4e8e\u4e8b\u5b9e\u77e5\u8bc6\u7684\u6982\u7387\u72b6\u6001\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660ePretrainRL\u663e\u8457\u7f13\u89e3\u4e86\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "PretrainRL\u901a\u8fc7\u4ece\u6839\u6e90\u4e0a\u89e3\u51b3\u9884\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u5de9\u56fa\u4e8b\u5b9e\u77e5\u8bc6\u3002"}}
{"id": "2602.01046", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01046", "abs": "https://arxiv.org/abs/2602.01046", "authors": ["Jiawei Lin", "Shizhao Sun", "Danqing Huang", "Ting Liu", "Ji Li", "Jiang Bian"], "title": "ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction", "comment": null, "summary": "Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.", "AI": {"tldr": "ReLayout\u662f\u4e00\u4e2a\u65e0\u9700\u4e09\u5143\u7ec4\u6570\u636e\u3001\u80fd\u591f\u4fdd\u6301\u5e03\u5c40\u7ed3\u6784\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u5e03\u5c40\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u7cfb\u56fe\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u591a\u79cd\u7f16\u8f91\u64cd\u4f5c\u3002", "motivation": "\u8bbe\u8ba1\u5e03\u5c40\u7f16\u8f91\u662f\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u9762\u4e34\u7528\u6237\u9700\u6c42\u8868\u8fbe\u6a21\u7cca\u3001\u7f16\u8f91\u64cd\u4f5c\u6807\u51c6\u5316\u4e0d\u8db3\u3001\u672a\u7f16\u8f91\u5143\u7d20\u5e03\u5c40\u7ed3\u6784\u4fdd\u6301\u56f0\u96be\uff0c\u4ee5\u53ca\u7f3a\u4e4f\uff08\u539f\u59cb\u8bbe\u8ba1\u3001\u7f16\u8f91\u64cd\u4f5c\u3001\u7f16\u8f91\u540e\u8bbe\u8ba1\uff09\u4e09\u5143\u7ec4\u6570\u636e\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faReLayout\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u5173\u7cfb\u56fe\u8868\u793a\u672a\u7f16\u8f91\u5143\u7d20\u4e4b\u95f4\u7684\u4f4d\u7f6e\u548c\u5927\u5c0f\u5173\u7cfb\u4f5c\u4e3a\u5e03\u5c40\u7ed3\u6784\u7ea6\u675f\uff1b2\uff09\u63d0\u51fa\u5173\u7cfb\u611f\u77e5\u8bbe\u8ba1\u91cd\u5efa\uff08RADR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5143\u7d20\u3001\u5173\u7cfb\u56fe\u548c\u5408\u6210\u7f16\u8f91\u64cd\u4f5c\u91cd\u5efa\u8bbe\u8ba1\uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u6a21\u62df\u7f16\u8f91\u8fc7\u7a0b\uff1b3\uff09\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aRADR\u4e3b\u5e72\uff0c\u7edf\u4e00\u591a\u79cd\u7f16\u8f91\u64cd\u4f5c\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4ee5\u53ca\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cReLayout\u5728\u7f16\u8f91\u8d28\u91cf\u3001\u51c6\u786e\u6027\u548c\u5e03\u5c40\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ReLayout\u901a\u8fc7\u5173\u7cfb\u56fe\u7ea6\u675f\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8bbe\u8ba1\u5e03\u5c40\u7f16\u8f91\u4e2d\u7684\u7ed3\u6784\u4fdd\u6301\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4e09\u5143\u7ec4\u6570\u636e\u7684\u591a\u529f\u80fd\u7f16\u8f91\uff0c\u63a8\u52a8\u4e86\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2602.02018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02018", "abs": "https://arxiv.org/abs/2602.02018", "authors": ["Enes Altinisik", "Masoomali Fatehkia", "Fatih Deniz", "Nadir Durrani", "Majd Hawasly", "Mohammad Raza", "Husrev Taha Sencar"], "title": "Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction", "comment": null, "summary": "Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.", "AI": {"tldr": "VeriFY\u6846\u67b6\u901a\u8fc7\u4e00\u81f4\u6027\u81ea\u9a8c\u8bc1\u8bad\u7ec3LLMs\uff0c\u51cf\u5c11\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ec\u56de\u7387", "motivation": "\u73b0\u6709\u7f13\u89e3LLMs\u4e8b\u5b9e\u5e7b\u89c9\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5916\u90e8\u540e\u9a8c\u9a8c\u8bc1\uff0c\u8981\u4e48\u5728\u5fae\u8c03\u4e2d\u76f4\u63a5\u5c06\u4e0d\u786e\u5b9a\u6027\u6620\u5c04\u4e3a\u5f03\u6743\uff0c\u5bfc\u81f4\u8fc7\u4e8e\u4fdd\u5b88\u7684\u884c\u4e3a", "method": "\u63d0\u51faVeriFY\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u9a8c\u8bc1\u8f68\u8ff9\u6559\u5bfcLLMs\u8fdb\u884c\u4e8b\u5b9e\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u5305\u62ec\u521d\u59cb\u56de\u7b54\u3001\u9a8c\u8bc1\u67e5\u8be2\u751f\u6210\u4e0e\u56de\u7b54\u3001\u4e00\u81f4\u6027\u5224\u65ad\u3001\u56de\u7b54/\u5f03\u6743\u51b3\u7b56\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u9636\u6bb5\u7ea7\u635f\u5931\u63a9\u7801\u907f\u514d\u5f3a\u5316\u5e7b\u89c9\u5185\u5bb9", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u4e0a\uff0cVeriFY\u5c06\u4e8b\u5b9e\u5e7b\u89c9\u7387\u964d\u4f4e9.7%\u81f353.3%\uff0c\u53ec\u56de\u7387\u4ec5\u8f7b\u5fae\u4e0b\u964d0.4%\u81f35.7%\uff0c\u4e14\u5728\u5355\u6e90\u8bad\u7ec3\u4e0b\u80fd\u8de8\u6570\u636e\u96c6\u6cdb\u5316", "conclusion": "VeriFY\u901a\u8fc7\u4e00\u81f4\u6027\u81ea\u9a8c\u8bc1\u6709\u6548\u51cf\u5c11LLMs\u7684\u4e8b\u5b9e\u5e7b\u89c9\uff0c\u5728\u51c6\u786e\u6027\u548c\u8986\u76d6\u8303\u56f4\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.00722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00722", "abs": "https://arxiv.org/abs/2602.00722", "authors": ["Hao Gu", "Mao-Lin Luo", "Zi-Hao Zhou", "Han-Chen Zhang", "Min-Ling Zhang", "Tong Wei"], "title": "Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation", "comment": "19 pages, 6 figures", "summary": "Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u66f4\u65b0\u7684\u5e45\u5ea6\u548c\u65b9\u5411\u7ed3\u6784\uff0c\u5728\u53d7\u9650Stiefel\u6d41\u5f62\u4e0a\u4f18\u5316\uff0c\u5e73\u8861\u5947\u5f02\u503c\u8c31\uff0c\u51cf\u5c11\u524d\u540e\u5411\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u907f\u514d\u4e0e\u8fc7\u53bb\u66f4\u65b0\u7684\u5e72\u6270\uff0c\u800c\u6ca1\u6709\u8003\u8651\u4ec0\u4e48\u7279\u6027\u80fd\u4f7f\u5f53\u524d\u4efb\u52a1\u7279\u5b9a\u66f4\u65b0\u81ea\u7136\u5730\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002\u4ece\u77e5\u8bc6\u5206\u89e3\u7684\u89d2\u5ea6\u770b\uff0c\u4f4e\u79e9\u9002\u5e94\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u5947\u5f02\u503c\u8c31\uff1a\u5c11\u6570\u4e3b\u5bfc\u6210\u5206\u5438\u6536\u4e86\u5927\u90e8\u5206\u9002\u5e94\u80fd\u91cf\uff0c\u8fd9\u65e2\u5bb9\u6613\u7834\u574f\u5148\u524d\u77e5\u8bc6\uff0c\u53c8\u4f7f\u66f4\u65b0\u66f4\u5bb9\u6613\u53d7\u5230\u540e\u7eed\u4efb\u52a1\u7684\u5e72\u6270\u3002", "method": "\u5c06\u4efb\u52a1\u66f4\u65b0\u7684\u5e45\u5ea6\u4e0e\u65b9\u5411\u7ed3\u6784\u89e3\u8026\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u53d7\u9650Stiefel\u6d41\u5f62\u4e0a\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u4e0e\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u517c\u5bb9\u7684\u6295\u5f71\u4e00\u9636\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u51cf\u8f7b\u540e\u5411\u9057\u5fd8\u548c\u524d\u5411\u9057\u5fd8\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5e73\u8861\u5947\u5f02\u503c\u8c31\u4e2d\u7684\u6210\u5206\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u540e\u7eed\u4efb\u52a1\u7684\u5e72\u6270\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5b66\u4e60\u3002"}}
{"id": "2602.01885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01885", "abs": "https://arxiv.org/abs/2602.01885", "authors": ["Tiantian Chen", "Jiaqi Lu", "Ying Shen", "Lin Zhang"], "title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support", "comment": "12 pages, 7 figures. Accepted to The Web Conference (WWW) 2026", "summary": "Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.", "AI": {"tldr": "ES-MemEval\u662f\u4e00\u4e2a\u8bc4\u4f30\u957f\u671f\u5bf9\u8bdd\u8bb0\u5fc6\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u60c5\u611f\u652f\u6301\u573a\u666f\uff0c\u5305\u542b\u4fe1\u606f\u63d0\u53d6\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u51b2\u7a81\u68c0\u6d4b\u7b49\u4e94\u4e2a\u6838\u5fc3\u80fd\u529b\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u5f0f\u957f\u671f\u8bb0\u5fc6\u5bf9\u51cf\u5c11\u5e7b\u89c9\u548c\u5b9e\u73b0\u4e2a\u6027\u5316\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u5bf9\u8bdd\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u663e\u5f0f\u4e8b\u5b9e\u68c0\u7d22\uff0c\u65e0\u6cd5\u8bc4\u4f30\u5728\u7528\u6237\u4fe1\u606f\u5206\u6563\u3001\u9690\u5f0f\u4e14\u6301\u7eed\u6f14\u5316\u7684\u5173\u952e\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5728\u7ebf\u60c5\u611f\u652f\u6301\u7b49\u590d\u6742\u957f\u671f\u7f51\u7edc\u670d\u52a1\u4e2d\uff0cLLM\u7684\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faES-MemEval\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e94\u4e2a\u6838\u5fc3\u8bb0\u5fc6\u80fd\u529b\uff1a\u4fe1\u606f\u63d0\u53d6\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u51b2\u7a81\u68c0\u6d4b\u3001\u5f03\u6743\u548c\u7528\u6237\u5efa\u6a21\uff1b\u540c\u65f6\u6784\u5efaEvoEmo\u591a\u4f1a\u8bdd\u6570\u636e\u96c6\uff0c\u6355\u6349\u788e\u7247\u5316\u3001\u9690\u5f0f\u7684\u7528\u6237\u62ab\u9732\u548c\u6f14\u5316\u7528\u6237\u72b6\u6001\uff1b\u5728\u5f00\u6e90\u957f\u4e0a\u4e0b\u6587\u3001\u5546\u4e1a\u548cRAG\u589e\u5f3a\u7684LLM\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u663e\u5f0f\u957f\u671f\u8bb0\u5fc6\u5bf9\u51cf\u5c11\u5e7b\u89c9\u548c\u5b9e\u73b0\u6709\u6548\u4e2a\u6027\u5316\u81f3\u5173\u91cd\u8981\uff1bRAG\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u5904\u7406\u65f6\u5e8f\u52a8\u6001\u548c\u6f14\u5316\u7528\u6237\u72b6\u6001\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b\u5f53\u524d\u8303\u5f0f\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u66f4\u9c81\u68d2\u5730\u6574\u5408\u8bb0\u5fc6\u548c\u68c0\u7d22\u673a\u5236\u6765\u6784\u5efa\u957f\u671f\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5f53\u524d\u7814\u7a76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2602.01047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01047", "abs": "https://arxiv.org/abs/2602.01047", "authors": ["Xinrong Chen", "Xu Chu", "Yingmin Qiu", "Hengyuan Zhang", "Jing Xiong", "Shiyu Tang", "Shuai Liu", "Shaokang Yang", "Cheng Yang", "Hayden Kwok-Hay So", "Ngai Wong"], "title": "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance", "comment": null, "summary": "Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.", "AI": {"tldr": "ResDec\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u4fe1\u606f\u548cLVLM\u5185\u90e8\u63a8\u7406\u673a\u5236\u6765\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u8bed\u8a00\u5148\u9a8c\u7684\u5f71\u54cd\uff0c\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u5339\u914d\u7684\u5e7b\u89c9\u5185\u5bb9", "method": "\u63d0\u51faResDec\uff08\u6b8b\u5dee\u89e3\u7801\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u4fe1\u606f\u548c\u6a21\u578b\u5185\u90e8\u7684\u9690\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7token\u5bf9\u6570\u6f14\u5316\u673a\u5236\u6765\u7ea0\u6b63\u504f\u5dee", "result": "ResDec\u80fd\u6709\u6548\u6291\u5236\u8bed\u8a00\u5148\u9a8c\u5f15\u8d77\u7684\u5e7b\u89c9\uff0c\u663e\u8457\u6539\u5584\u89c6\u89c9\u57fa\u7840\uff0c\u51cf\u5c11\u7269\u4f53\u5e7b\u89c9\uff0c\u540c\u65f6\u5728\u7efc\u5408LVLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "conclusion": "ResDec\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u8fd8\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u4e3a\u6539\u5584LVLM\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02027", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02027", "abs": "https://arxiv.org/abs/2602.02027", "authors": ["Sicheng Shen", "Mingyang Lv", "Han Shen", "Jialin Wu", "Binghao Wang", "Zhou Yang", "Guobin Shen", "Dongcheng Zhao", "Feifei Zhao", "Yi Zeng"], "title": "Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron", "comment": "21 pages, 3 figures", "summary": "The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e13\u5bb6\u6a21\u578b\u548c\u5355\u795e\u7ecf\u5143\u95e8\u63a7\u673a\u5236\u7684\u5b89\u5168\u611f\u77e5\u89e3\u7801\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u5347\u8f93\u51fa\u5b89\u5168\u6027", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u540e\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9884\u5148\u8ba1\u7b97\u7684\u5b89\u5168\u6ce8\u5165\uff0c\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56\u6a21\u578b\u81ea\u8eab\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u6709\u9650\u4e14\u751f\u6210\u6548\u7387\u964d\u4f4e", "method": "\u63d0\u51fa\u5b89\u5168\u611f\u77e5\u89e3\u7801\u65b9\u6cd5\uff1a\u4ec5\u9700\u4f4e\u6210\u672c\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\uff0c\u4f7f\u7528\u5355\u795e\u7ecf\u5143\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u6a21\u578b\u5185\u5728\u80fd\u529b\u548c\u5916\u90e8\u6307\u5bfc", "result": "\u65b9\u6cd5\u5728\u8bad\u7ec3\u5f00\u9500\u548c\u8de8\u6a21\u578b\u89c4\u6a21\u6cdb\u5316\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u548c\u589e\u5f3a\u8f93\u51fa\u5b89\u5168\u6027", "conclusion": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u7684\u65b0\u89c6\u89d2"}}
{"id": "2602.00723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00723", "abs": "https://arxiv.org/abs/2602.00723", "authors": ["Prakhar Ganesh", "Reza Shokri", "Golnoosh Farnadi"], "title": "Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity", "comment": "To appear at EACL 2026", "summary": "Large language models (LLMs) are known to \"hallucinate\" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faprompt multiplicity\u6846\u67b6\u91cf\u5316LLM\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u5e7b\u89c9\u8bc4\u4f30\u8fc7\u5ea6\u5173\u6ce8\u6b63\u786e\u6027\u800c\u5ffd\u89c6\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5bf9\u5e7b\u89c9\u5371\u5bb3\u7684\u4e25\u91cd\u8bef\u89e3\u3002", "motivation": "\u73b0\u6709LLM\u5e7b\u89c9\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u8f93\u51fa\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u533a\u5206\u548c\u89e3\u51b3\u5e7b\u89c9\u5e26\u6765\u7684\u5404\u79cd\u5371\u5bb3\uff08\u5982\u4fe1\u4efb\u4fb5\u8680\u548c\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\uff09\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faprompt multiplicity\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316LLM\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\u6765\u5206\u6790\u5e7b\u89c9\u95ee\u9898\u3002\u5728Med-HALT\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u6790\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u7814\u7a76\u4e00\u81f4\u6027\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u6280\u672f\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u663e\u8457\u7684\u591a\u91cd\u6027\uff08Med-HALT\u7b49\u57fa\u51c6\u4e2d\u8d85\u8fc750%\u7684\u4e0d\u4e00\u81f4\u6027\uff09\uff0c\u8868\u660e\u5e7b\u89c9\u76f8\u5173\u5371\u5bb3\u88ab\u4e25\u91cd\u8bef\u89e3\u3002\u68c0\u6d4b\u6280\u672f\u4e3b\u8981\u68c0\u6d4b\u4e00\u81f4\u6027\u800c\u975e\u6b63\u786e\u6027\uff0c\u800cRAG\u7b49\u7f13\u89e3\u6280\u672f\u867d\u7136\u6709\u76ca\u4f46\u53ef\u80fd\u5f15\u5165\u989d\u5916\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06prompt multiplicity\u6574\u5408\u5230\u5e7b\u89c9\u8bc4\u4f30\u4e2d\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u5371\u5bb3\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e00\u81f4\u6027\u8bc4\u4f30\u5bf9\u5168\u9762\u7406\u89e3LLM\u5e7b\u89c9\u5371\u5bb3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.01917", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01917", "abs": "https://arxiv.org/abs/2602.01917", "authors": ["Chengguang Gan", "Yoshihiro Tsujii", "Yunhao Liang", "Tatsunori Mori", "Shiwen Ni", "Hiroki Itoh"], "title": "GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs", "comment": null, "summary": "Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \\textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \\textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \\textbf{GuideWeb Agent} achieves \\textbf{30.79\\%} accuracy in guide target element prediction, while obtaining BLEU scores of \\textbf{44.94} for intent generation and \\textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.", "AI": {"tldr": "GuideWeb\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u771f\u5b9e\u7f51\u9875UI\u4e0a\u81ea\u52a8\u751f\u6210\u5e94\u7528\u5185\u6307\u5bfc\u7684\u65b0\u57fa\u51c6\uff0c\u5c06\u4efb\u52a1\u5b9a\u4e49\u4e3a\u901a\u8fc7\u9009\u62e9\u7f51\u9875\u4e2d\u7684\u6307\u5bfc\u76ee\u6807\u5143\u7d20\u5e76\u751f\u6210\u7b80\u6d01\u7684\u6307\u5bfc\u6587\u672c\u6765\u63d0\u4f9b\u9875\u9762\u7ea7\u6307\u5bfc\u3002", "motivation": "\u6570\u5b57\u91c7\u7528\u5e73\u53f0(DAP)\u867d\u7136\u80fd\u8ba9\u975e\u4e13\u5bb6\u7f16\u5199\u7f51\u9875\u64cd\u4f5c\u6307\u5bfc\uff0c\u4f46\u7f51\u7ad9\u5e03\u5c40\u548c\u529f\u80fd\u4e0d\u65ad\u53d8\u5316\uff0c\u9700\u8981\u91cd\u590d\u624b\u52a8\u66f4\u65b0\u548c\u91cd\u65b0\u6807\u6ce8\uff0c\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86GuideWeb\u57fa\u51c6\uff0c\u5c06\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\uff1a1) \u9009\u62e9\u7f51\u9875\u4e2d\u7684\u6307\u5bfc\u76ee\u6807\u5143\u7d20\uff1b2) \u751f\u6210\u4e0e\u7528\u6237\u610f\u56fe\u4e00\u81f4\u7684\u7b80\u6d01\u6307\u5bfc\u6587\u672c\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u8054\u5408\u8861\u91cf\u76ee\u6807\u5143\u7d20\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u751f\u6210\u610f\u56fe\u53ca\u6307\u5bfc\u6587\u672c\u7684\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u7684GuideWeb\u4ee3\u7406\u5728\u6307\u5bfc\u76ee\u6807\u5143\u7d20\u9884\u6d4b\u4e0a\u8fbe\u523030.79%\u51c6\u786e\u7387\uff0c\u610f\u56fe\u751f\u6210BLEU\u5206\u657044.94\uff0c\u6307\u5bfc\u6587\u672c\u751f\u6210BLEU\u5206\u657021.34\u3002\u73b0\u6709\u57fa\u7ebf\u8868\u73b0\u663e\u8457\u66f4\u5dee\uff0c\u8868\u660e\u81ea\u52a8\u6307\u5bfc\u751f\u6210\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "\u81ea\u52a8\u6307\u5bfc\u751f\u6210\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5728\u7cfb\u7edf\u80fd\u591f\u53ef\u9760\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e4b\u524d\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u6280\u672f\u8fdb\u6b65\u3002GuideWeb\u57fa\u51c6\u4e3a\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.01055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01055", "abs": "https://arxiv.org/abs/2602.01055", "authors": ["Bo Deng", "Yitong Tang", "Jiake Li", "Yuxin Huang", "Li Wang", "Yu Zhang", "Yufei Zhan", "Hua Lu", "Xiaoshen Zhang", "Jieyun Bai"], "title": "Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis", "comment": null, "summary": "Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \\href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FM_UIA 2026\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u57fa\u7840\u6a21\u578b\u6311\u6218\u7684\u5b98\u65b9\u57fa\u7ebf\uff0c\u57fa\u4e8e\u7edf\u4e00\u7684\u591a\u5934\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u630127\u4e2a\u5b50\u4efb\u52a1\uff0c\u4e3a\u8d85\u58f0\u57fa\u7840\u6a21\u578b\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7ebf\u3002", "motivation": "\u8d85\u58f0\u56fe\u50cf\u5728\u4e0d\u540c\u89e3\u5256\u7ed3\u6784\u548c\u91c7\u96c6\u534f\u8bae\u4e0b\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u578b\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684\u57fa\u7840\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u901a\u7528\u7684\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u6a21\u578b\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u591a\u5934\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528ImageNet\u9884\u8bad\u7ec3\u7684EfficientNet-B4\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u8def\u7531\u7b56\u7565\u4e3a\u4e0d\u540c\u4efb\u52a1\u5206\u914d\u9002\u5f53\u7279\u5f81\uff0c\u91c7\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u8c03\u6574\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\u8be5\u7edf\u4e00\u8bbe\u8ba1\u5177\u6709\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u8d85\u58f0\u57fa\u7840\u6a21\u578b\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7ebf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u652f\u6301\u591a\u79cd\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u8d85\u58f0\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e34\u5e8a\u53ef\u90e8\u7f72\u7684\u8d85\u58f0AI\u6a21\u578b\u7814\u7a76\u3002"}}
{"id": "2602.02028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02028", "abs": "https://arxiv.org/abs/2602.02028", "authors": ["Ya Gao", "Kalle Kujanp\u00e4\u00e4", "Pekka Marttinen", "Harri Valpola", "Alexander Ilin"], "title": "Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories", "comment": "under review", "summary": "Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u77e5\u8bc6\u5185\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u80cc\u666f\u6545\u4e8b\u3001\u591a\u8df3\u95ee\u9898\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u8ba9AI\u6a21\u578b\u80fd\u6709\u6548\u6574\u5408\u65b0\u77e5\u8bc6\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u539f\u5b50\u4e8b\u5b9e\u7684\u8bb0\u5fc6\uff0c\u4f46\u65e0\u6cd5\u5c06\u65b0\u77e5\u8bc6\u6574\u5408\u5230\u8fde\u8d2f\u7684\u6846\u67b6\u4e2d\u5e76\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7075\u6d3b\u5e94\u7528\u3002\u77e5\u8bc6\u5185\u5316\u672c\u8d28\u4e0a\u5e94\u8be5\u662f\u63a8\u7406\u95ee\u9898\u800c\u975e\u8bb0\u5fc6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u4e2a\u539f\u5219\u7684\u8bad\u7ec3\u7b56\u7565\uff1a1) \u5c06\u65b0\u77e5\u8bc6\u4f5c\u4e3a\u8fde\u8d2f\u7684\u80cc\u666f\u6545\u4e8b\u5f15\u5165\uff0c\u89e3\u91ca\u65b0\u4e8b\u5b9e\u4e0e\u73b0\u6709\u77e5\u8bc6\u7684\u5173\u7cfb\uff1b2) \u4f7f\u7528\u81ea\u751f\u6210\u7684\u591a\u8df3\u95ee\u9898\u8fdb\u884c\u8bad\u7ec3\uff0c\u8981\u6c42\u6d89\u53ca\u65b0\u4fe1\u606f\u7684\u591a\u6b65\u63a8\u7406\uff1b3) \u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u8ba9\u5b66\u751f\u6a21\u578b\u5185\u5316\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u800c\u4e0d\u76f4\u63a5\u63a5\u89e6\u65b0\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u91c7\u7528\u8be5\u7b56\u7565\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u6709\u6548\u5229\u7528\u65b0\u83b7\u5f97\u7684\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u5728\u9700\u8981\u7ed3\u5408\u591a\u4e2a\u65b0\u4e8b\u5b9e\u7684\u6311\u6218\u6027\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u5c06\u77e5\u8bc6\u5185\u5316\u89c6\u4e3a\u63a8\u7406\u95ee\u9898\u800c\u975e\u8bb0\u5fc6\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u80cc\u666f\u6545\u4e8b\u3001\u591a\u8df3\u63a8\u7406\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u6574\u5408\u548c\u5e94\u7528\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002"}}
{"id": "2602.00737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00737", "abs": "https://arxiv.org/abs/2602.00737", "authors": ["Jatan Shrestha", "Santeri Heiskanen", "Kari Hepola", "Severi Rissanen", "Pekka J\u00e4\u00e4skel\u00e4inen", "Joni Pajarinen"], "title": "Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization", "comment": "Accepted by ICLR 2026. Project page: https://sites.google.com/view/pcd-iclr26", "summary": "Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.", "AI": {"tldr": "PCD\u5c06\u79bb\u7ebf\u591a\u76ee\u6807\u4f18\u5316\u8f6c\u5316\u4e3a\u6761\u4ef6\u91c7\u6837\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u6761\u4ef6\u5316\u671f\u671b\u6743\u8861\u6765\u907f\u514d\u663e\u5f0f\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u7528\u91cd\u52a0\u6743\u7b56\u7565\u548c\u53c2\u8003\u65b9\u5411\u673a\u5236\u63a2\u7d22\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u591a\u76ee\u6807\u4f18\u5316\u9700\u8981\u5e73\u8861\u7ade\u4e89\u76ee\u6807\uff0c\u79bb\u7ebf\u573a\u666f\u4e0b\u4ec5\u6709\u9759\u6001\u6570\u636e\u96c6\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5982\u4f55\u6cdb\u5316\u5230\u89c2\u6d4b\u6570\u636e\u4e4b\u5916\u3002", "method": "\u63d0\u51fa\u5e15\u7d2f\u6258\u6761\u4ef6\u6269\u6563(PCD)\u6846\u67b6\uff0c\u5c06\u79bb\u7ebfMOO\u8f6c\u5316\u4e3a\u6761\u4ef6\u91c7\u6837\u95ee\u9898\uff1b\u901a\u8fc7\u76f4\u63a5\u6761\u4ef6\u5316\u671f\u671b\u6743\u8861\u907f\u514d\u663e\u5f0f\u4ee3\u7406\u6a21\u578b\uff1b\u91c7\u7528\u91cd\u52a0\u6743\u7b56\u7565\u805a\u7126\u9ad8\u6027\u80fd\u6837\u672c\uff1b\u4f7f\u7528\u53c2\u8003\u65b9\u5411\u673a\u5236\u5f15\u5bfc\u91c7\u6837\u5230\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u7684\u65b0\u9896\u533a\u57df\u3002", "result": "\u5728\u6807\u51c6\u79bb\u7ebfMOO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPCD\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u79bb\u7ebfMOO\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "PCD\u4e3a\u79bb\u7ebf\u591a\u76ee\u6807\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u6761\u4ef6\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6cdb\u5316\u5230\u89c2\u6d4b\u6570\u636e\u4e4b\u5916\u5e76\u63a2\u7d22\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2602.01919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01919", "abs": "https://arxiv.org/abs/2602.01919", "authors": ["Hend Al-Khalifa"], "title": "From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted \"Vibe Coding\"", "comment": "Accepted in The Seventh Workshop on Teaching Natural Language Processing (Teaching NLP @ EACL2026)", "summary": "The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.", "AI": {"tldr": "Vibe Coding\u662f\u4e00\u79cd\u5c06LLMs\u4f5c\u4e3a\u7f16\u7a0b\u52a9\u624b\u7684\u6559\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u63d0\u793a\u8bb0\u5f55\u548c\u53cd\u601d\u8bc4\u4f30\uff0c\u5c06\u5b66\u4e60\u91cd\u70b9\u4ece\u8bed\u6cd5\u719f\u7ec3\u5ea6\u8f6c\u5411\u6982\u5ff5\u638c\u63e1\uff0c\u5728NLP\u8bfe\u7a0b\u4e2d\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "LLMs\u7684\u5feb\u901f\u53d1\u5c55\u7ed9NLP\u6559\u80b2\u5e26\u6765\u6311\u6218\u548c\u673a\u9047\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528LLMs\u4f5c\u4e3a\u7f16\u7a0b\u52a9\u624b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6982\u5ff5\u7406\u89e3\u548c\u6279\u5224\u6027\u601d\u7ef4\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\"Vibe Coding\"\u6559\u5b66\u65b9\u6cd5\uff0c\u5728\u9ad8\u7ea7\u672c\u79d1NLP\u8bfe\u7a0b\u4e2d\u5b9e\u65bd\uff0c\u5b66\u751f\u4f7f\u7528LLMs\u5b8c\u62107\u4e2a\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5f3a\u5236\u63d0\u793a\u8bb0\u5f55\u548c\u57fa\u4e8e\u53cd\u601d\u95ee\u9898\u7684\u8bc4\u4f30\u6765\u786e\u4fdd\u6982\u5ff5\u7406\u89e3\u3002", "result": "19\u540d\u5b66\u751f\u53cd\u9988\u663e\u793a\u9ad8\u6ee1\u610f\u5ea6\uff08\u5e73\u5747\u52064.4-4.6/5.0\uff09\uff0c\u5b66\u751f\u7279\u522b\u8d5e\u8d4f\u8c03\u8bd5\u8ba4\u77e5\u8d1f\u8377\u51cf\u5c11\uff0c\u80fd\u66f4\u6df1\u5165\u5173\u6ce8NLP\u6982\u5ff5\uff0c\u4f46\u4e5f\u9762\u4e34\u65f6\u95f4\u9650\u5236\u3001LLM\u8f93\u51fa\u9a8c\u8bc1\u548c\u4efb\u52a1\u89c4\u8303\u6e05\u6670\u5ea6\u7b49\u6311\u6218\u3002", "conclusion": "\u5f53\u901a\u8fc7\u5f3a\u5236\u63d0\u793a\u8bb0\u5f55\u548c\u57fa\u4e8e\u53cd\u601d\u7684\u8bc4\u4f30\u9002\u5f53\u7ed3\u6784\u5316\u65f6\uff0cLLM\u8f85\u52a9\u5b66\u4e60\u53ef\u4ee5\u5c06\u91cd\u70b9\u4ece\u8bed\u6cd5\u719f\u7ec3\u5ea6\u8f6c\u5411\u6982\u5ff5\u638c\u63e1\uff0c\u4e3a\u5b66\u751f\u9002\u5e94AI\u589e\u5f3a\u7684\u4e13\u4e1a\u73af\u5883\u505a\u597d\u51c6\u5907\u3002"}}
{"id": "2602.01057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01057", "abs": "https://arxiv.org/abs/2602.01057", "authors": ["Ling Chen", "Bao Yang"], "title": "Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\u7684\u65ad\u5c42\u91cd\u5efa\u6846\u67b6\uff0c\u514b\u670d\u4e86\u57fa\u4e8esplatting\u65b9\u6cd5\u4e2d\u4eff\u5c04\u8fd1\u4f3c\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7269\u7406\u4e00\u81f4\u7684\u6295\u5f71\u6a21\u578b\u5e76\u652f\u6301\u975e\u7ebf\u6027\u51e0\u4f55\u6821\u6b63", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65afsplatting\u7684\u65ad\u5c42\u91cd\u5efa\u65b9\u6cd5\uff08\u5982R2-Gaussian\uff09\u4f7f\u7528\u5c40\u90e8\u4eff\u5c04\u8fd1\u4f3c\uff0c\u5c063D\u9ad8\u65af\u6620\u5c04\u52302D\u63a2\u6d4b\u5668\u4e0a\uff0c\u8fd9\u79cd\u8fd1\u4f3c\u4f1a\u964d\u4f4e\u91cd\u5efa\u7684\u5b9a\u91cf\u7cbe\u5ea6\uff0c\u5e76\u4e14\u96be\u4ee5\u7eb3\u5165\u975e\u7ebf\u6027\u51e0\u4f55\u6821\u6b63", "method": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\u7684\u65ad\u5c42\u91cd\u5efa\u6846\u67b6\uff1a1\uff09\u901a\u8fc73D\u9ad8\u65af\u57fa\u5143\u89e3\u6790\u8ba1\u7b97\u7ebf\u79ef\u5206\uff0c\u907f\u514d\u5c40\u90e8\u4eff\u5c04\u574d\u584c\uff1b2\uff09\u5c04\u7ebf\u8ffd\u8e2a\u516c\u5f0f\u63d0\u4f9b\u5bf9\u5c04\u7ebf\u8d77\u70b9\u548c\u65b9\u5411\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u4fbf\u4e8e\u7cbe\u786e\u5e94\u7528\u975e\u7ebf\u6027\u51e0\u4f55\u6821\u6b63", "result": "\u76f8\u6bd4\u57fa\u4e8esplatting\u7684\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u66f4\u7269\u7406\u4e00\u81f4\u7684\u524d\u5411\u6295\u5f71\u6a21\u578b\uff0c\u63d0\u9ad8\u6295\u5f71\u7cbe\u5ea6\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u65ad\u5c42\u626b\u63cf\u7cfb\u7edf\uff08\u5982PET\u4e2d\u7684\u5f27\u6821\u6b63\uff09", "conclusion": "\u57fa\u4e8e3D\u9ad8\u65af\u5c04\u7ebf\u8ffd\u8e2a\u7684\u65ad\u5c42\u91cd\u5efa\u6846\u67b6\u514b\u670d\u4e86\u4eff\u5c04\u8fd1\u4f3c\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u5b9a\u91cf\u7cbe\u5ea6\uff0c\u652f\u6301\u975e\u7ebf\u6027\u51e0\u4f55\u6821\u6b63\uff0c\u6269\u5c55\u4e86\u9ad8\u65af\u57fa\u91cd\u5efa\u5728\u73b0\u5b9e\u65ad\u5c42\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u8303\u56f4"}}
{"id": "2602.02029", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02029", "abs": "https://arxiv.org/abs/2602.02029", "authors": ["Zhongyuan Lyu", "Shuoyu Hu", "Lujie Liu", "Hongxia Yang", "Ming LI"], "title": "Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation", "comment": "41 pages, 4 figures, 5 tables", "summary": "Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.", "AI": {"tldr": "\u63d0\u51faCIR\u4e2d\u95f4\u8868\u793a\u548cR2C\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u5316\u4e3a\u4f18\u5316\u6a21\u578b\uff0c\u5728\u590d\u6742\u64cd\u4f5c\u89c4\u5219\u5efa\u6a21\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u64cd\u4f5c\u89c4\u5219\u7684\u590d\u5408\u7ea6\u675f\u548c\u9002\u5f53\u5efa\u6a21\u8303\u5f0f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u89c4\u5219\u903b\u8f91\u4e0e\u6570\u5b66\u5b9e\u4f8b\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u89c4\u8303\u4e2d\u95f4\u8868\u793a(CIR)\u4f5c\u4e3aLLM\u5728\u95ee\u9898\u63cf\u8ff0\u548c\u4f18\u5316\u6a21\u578b\u4e4b\u95f4\u663e\u5f0f\u751f\u6210\u7684\u6a21\u5f0f\uff0c\u901a\u8fc7\u7ea6\u675f\u539f\u578b\u548c\u5019\u9009\u5efa\u6a21\u8303\u5f0f\u7f16\u7801\u64cd\u4f5c\u89c4\u5219\u8bed\u4e49\u3002\u57fa\u4e8eCIR\u77e5\u8bc6\u5e93\u5f00\u53d1R2C\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u5305\u62ec\u89e3\u6790\u95ee\u9898\u6587\u672c\u3001\u68c0\u7d22\u9886\u57df\u77e5\u8bc6\u5408\u6210CIR\u5b9e\u73b0\u3001\u5b9e\u4f8b\u5316\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u63d0\u51fa\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523047.2%\u51c6\u786e\u7387\uff0c\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1GPT-5\u7b49\u4e13\u6709\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u53cd\u601d\u673a\u5236\u5728\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u4e0b\u6700\u4f73\u62a5\u544a\u7ed3\u679c\u3002", "conclusion": "CIR\u548cR2C\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u64cd\u4f5c\u89c4\u5219\u7684\u4f18\u5316\u5efa\u6a21\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5230\u4f18\u5316\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8f6c\u6362\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.00753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00753", "abs": "https://arxiv.org/abs/2602.00753", "authors": ["Zeljko Bolevic", "Milos Brajovic", "Isidora Stankovic", "Ljubisa Stankovic"], "title": "GraphNNK -- Graph Classification and Interpretability", "comment": "4 pages, 3 figures, IEEE conference paper", "summary": "Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.", "AI": {"tldr": "GNNs\u4f9d\u8d56\u53c2\u6570\u5316\u5206\u7c7b\u5668\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u57fa\u4e8e\u63d2\u503c\u7684\u65b9\u6cd5\uff08\u5982NNK\uff09\u901a\u8fc7\u8bad\u7ec3\u6837\u672c\u7684\u51f8\u7ec4\u5408\u8fdb\u884c\u9884\u6d4b\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "GNNs\u5df2\u6210\u4e3a\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5176\u4f9d\u8d56\u53c2\u6570\u5316\u5206\u7c7b\u5668\uff08\u901a\u5e38\u662f\u7ebf\u6027softmax\u5c42\uff09\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u65f6\u4e5f\u963b\u788d\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u66f4\u53ef\u89e3\u91ca\u4e14\u7406\u8bba\u4fdd\u8bc1\u66f4\u597d\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u63d2\u503c\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u975e\u8d1f\u6838\u56de\u5f52\uff08NNK\uff09\uff0c\u5c06\u9884\u6d4b\u8868\u793a\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u8bad\u7ec3\u6837\u672c\u7684\u51f8\u7ec4\u5408\uff0c\u4ece\u800c\u83b7\u5f97\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "NNK\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u8bad\u7ec3\u6837\u672c\u7684\u51f8\u7ec4\u5408\u8868\u8fbe\u9884\u6d4b\u7ed3\u679c\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u7406\u89e3\u6027\u3002", "conclusion": "\u57fa\u4e8e\u63d2\u503c\u7684\u975e\u53c2\u6570\u65b9\u6cd5\uff08\u5982NNK\uff09\u4e3aGNNs\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u53c2\u6570\u5316\u5206\u7c7b\u5668\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7406\u8bba\u57fa\u7840\uff0c\u662f\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u900f\u660e\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2602.01965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01965", "abs": "https://arxiv.org/abs/2602.01965", "authors": ["Kwun Hang Lau", "Fangyuan Zhang", "Boyu Ruan", "Yingli Zhou", "Qintian Guo", "Ruiyuan Zhang", "Xiaofang Zhou"], "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation", "comment": null, "summary": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.", "AI": {"tldr": "CatRAG \u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u904d\u5386\u7684 RAG \u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u6765\u89e3\u51b3\u9759\u6001\u56fe\u8c31\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u63a8\u7406\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684 RAG \u65b9\u6cd5\uff08\u5982 HippoRAG\uff09\u5b58\u5728\"\u9759\u6001\u56fe\u8c31\u8c2c\u8bef\"\uff1a\u4f9d\u8d56\u56fa\u5b9a\u7684\u8f6c\u79fb\u6982\u7387\uff0c\u5ffd\u7565\u4e86\u67e5\u8be2\u76f8\u5173\u7684\u8fb9\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u968f\u673a\u6e38\u8d70\u88ab\u9ad8\u8fde\u63a5\u5ea6\u7684\"\u67a2\u7ebd\"\u8282\u70b9\u5206\u6563\uff0c\u65e0\u6cd5\u5b8c\u6574\u68c0\u7d22\u591a\u8df3\u67e5\u8be2\u6240\u9700\u7684\u8bc1\u636e\u94fe\u3002", "method": "\u57fa\u4e8e HippoRAG 2 \u67b6\u6784\uff0c\u5c06\u9759\u6001\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\u4e3a\u67e5\u8be2\u81ea\u9002\u5e94\u7684\u5bfc\u822a\u7ed3\u6784\u3002\u91c7\u7528\u4e09\u65b9\u9762\u6846\u67b6\uff1a1) \u7b26\u53f7\u951a\u5b9a\uff1a\u6ce8\u5165\u5f31\u5b9e\u4f53\u7ea6\u675f\u6765\u6b63\u5219\u5316\u968f\u673a\u6e38\u8d70\uff1b2) \u67e5\u8be2\u611f\u77e5\u7684\u52a8\u6001\u8fb9\u6743\u91cd\uff1a\u52a8\u6001\u8c03\u6574\u56fe\u7ed3\u6784\uff0c\u526a\u679d\u4e0d\u76f8\u5173\u8def\u5f84\uff0c\u589e\u5f3a\u4e0e\u67e5\u8be2\u610f\u56fe\u5bf9\u9f50\u7684\u8def\u5f84\uff1b3) \u5173\u952e\u4e8b\u5b9e\u6bb5\u843d\u6743\u91cd\u589e\u5f3a\uff1a\u901a\u8fc7\u6210\u672c\u9ad8\u6548\u7684\u504f\u7f6e\u7ed3\u6784\u6027\u5730\u951a\u5b9a\u968f\u673a\u6e38\u8d70\u5230\u53ef\u80fd\u8bc1\u636e\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCatRAG \u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u3002\u867d\u7136\u6807\u51c6\u53ec\u56de\u7387\u6307\u6807\u663e\u793a\u9002\u5ea6\u63d0\u5347\uff0c\u4f46\u5728\u63a8\u7406\u5b8c\u6574\u6027\uff08\u6062\u590d\u5b8c\u6574\u8bc1\u636e\u8def\u5f84\u7684\u80fd\u529b\uff09\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "CatRAG \u6709\u6548\u5f25\u5408\u4e86\u68c0\u7d22\u90e8\u5206\u4e0a\u4e0b\u6587\u4e0e\u5b9e\u73b0\u5b8c\u5168\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u56fe\u7ed3\u6784\u89e3\u51b3\u4e86\u9759\u6001\u56fe\u8c31\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u591a\u8df3\u63a8\u7406\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2602.01059", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.01059", "abs": "https://arxiv.org/abs/2602.01059", "authors": ["Ying Shu", "Pujian Zhan", "Huiqi Yang", "Hehe Fan", "Youfang Lin", "Kai Lv"], "title": "DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification", "comment": null, "summary": "Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \\textbf{D}ual-\\textbf{R}egularized Bidirectional \\textbf{Transformer} (\\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faDRFormer\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6b63\u5219\u5316\u53cc\u5411Transformer\u878d\u5408DINO\u7684\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\u548cCLIP\u7684\u5168\u5c40\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u51b3\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u906e\u6321\u548c\u59ff\u6001\u53d8\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u8303\u5f0f\uff08\u8981\u4e48\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5982DINO\u6316\u6398\u5c40\u90e8\u7eb9\u7406\uff0c\u8981\u4e48\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u6355\u83b7\u5168\u5c40\u8bed\u4e49\u5dee\u5f02\uff09\uff0c\u5ffd\u7565\u4e86\u4e24\u79cd\u67b6\u6784\u4e92\u8865\u96c6\u6210\u7684\u6f5c\u5728\u4f18\u52bf\u3002\u7ec6\u7c92\u5ea6\u5224\u522b\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\u7279\u5f81\u90fd\u80fd\u5e2e\u52a9\u89e3\u51b3\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u906e\u6321\u548c\u59ff\u6001\u53d8\u5316\u6311\u6218\u3002", "method": "\u63d0\u51fa\u53cc\u6b63\u5219\u5316\u53cc\u5411Transformer\uff08DRFormer\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6b63\u5219\u5316\u673a\u5236\u786e\u4fdd\u591a\u6837\u5316\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5b9e\u73b0\u4e24\u79cd\u6a21\u578b\u8d21\u732e\u7684\u66f4\u597d\u5e73\u8861\u3002\u8be5\u6846\u67b6\u534f\u540c\u878d\u5408DINO\u7684\u5c40\u90e8\u7eb9\u7406\u6316\u6398\u80fd\u529b\u548cCLIP\u7684\u5168\u5c40\u8bed\u4e49\u5dee\u5f02\u6355\u83b7\u80fd\u529b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u534f\u8c03\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\uff0c\u5728\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5bf9\u6bd4\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u4e24\u79cd\u67b6\u6784\u7684\u4e92\u8865\u4f5c\u7528\uff0c\u63d0\u51fa\u7684DRFormer\u6846\u67b6\u6210\u529f\u878d\u5408\u4e86DINO\u548cCLIP\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u53cc\u6b63\u5219\u5316\u673a\u5236\u5b9e\u73b0\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u6709\u6548\u534f\u8c03\uff0c\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2602.02034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02034", "abs": "https://arxiv.org/abs/2602.02034", "authors": ["Ananya Joshi", "Michael Rudow"], "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406\u5408\u89c4\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u5728\u51c6\u786e\u6027\u3001\u4eba\u5de5\u5ba1\u6838\u9700\u6c42\u548c\u5904\u7406\u65f6\u95f4\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u5408\u89c4\u3001\u5c3d\u804c\u8c03\u67e5\u7b49\u53d7\u76d1\u7ba1\u573a\u666f\u4e2d\u6267\u884c\u590d\u6742\u591a\u6b65\u5de5\u4f5c\u6d41\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u5355\u667a\u80fd\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u96be\u4ee5\u89c2\u5bdf\u6216\u6bd4\u8f83\u6a21\u578b\u5982\u4f55\u5904\u7406\u8de8\u51b3\u7b56\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u534f\u8c03\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5f62\u5f0f\u5316\u4e3a\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5177\u6709\u6709\u5411\u65e0\u73af\u7ed3\u6784\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9\u5e94\u7279\u5b9a\u89d2\u8272\u6216\u51b3\u7b56\u9636\u6bb5\uff08\u5982\u5408\u89c4\u5de5\u4f5c\u6d41\u4e2d\u7684\u5185\u5bb9\u3001\u4e1a\u52a1\u6216\u6cd5\u5f8b\u5ba1\u6838\uff09\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u8f6c\u6362\u8868\u793a\u4efb\u52a1\u5347\u7ea7\u6216\u5b8c\u6210\u3002\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5728\u667a\u80fd\u4f53\u5c42\u9762\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u7cfb\u7edf\u7ea7\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7MDP\u7ec8\u6b62\u4e8e\u81ea\u52a8\u6807\u8bb0\u72b6\u6001\u6216\u4eba\u5de5\u5ba1\u6838\u72b6\u6001\u6765\u6355\u83b7\u3002", "result": "\u5728AI\u5b89\u5168\u8bc4\u4f30\uff08\u81ea\u6b8b\u68c0\u6d4b\uff09\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe19%\u7684\u51c6\u786e\u6027\u63d0\u5347\uff0c\u9ad8\u8fbe85\u500d\u7684\u4eba\u5de5\u5ba1\u6838\u9700\u6c42\u51cf\u5c11\uff0c\u5728\u67d0\u4e9b\u914d\u7f6e\u4e0b\u8fd8\u51cf\u5c11\u4e86\u5904\u7406\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53MDP\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u4eba\u5de5\u76d1\u7763\u9700\u6c42\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u4e3a\u53d7\u76d1\u7ba1\u573a\u666f\u4e2d\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u67b6\u6784\u3002"}}
{"id": "2602.00767", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00767", "abs": "https://arxiv.org/abs/2602.00767", "authors": ["Muhammed Ustaomeroglu", "Guannan Qu"], "title": "BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features", "comment": "41 pages, 32 figures. Code available", "summary": "Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.", "AI": {"tldr": "\u901a\u8fc7\u8bc6\u522b\u5e76\u963b\u65ad\u63a7\u5236\u4e0d\u826f\u884c\u4e3a\u7684\u5185\u90e8\u7279\u5f81\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u51fa\u73b0\u7a81\u53d1\u6027\u9519\u4f4d\uff0c\u5728\u516d\u4e2a\u9886\u57df\u5b9e\u73b0\u9ad8\u8fbe95%\u7684\u76f8\u5bf9\u51cf\u5c11\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u8d28\u91cf\u548c\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u8303\u56f4\u7684\u76d1\u7763\u76ee\u6807\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u867d\u7136\u5b66\u4f1a\u4e86\u76ee\u6807\u884c\u4e3a\uff0c\u4f46\u4e5f\u4f1a\u53d1\u5c55\u51fa\u4e0d\u826f\u7684\u57df\u5916\u884c\u4e3a\uff08\u7a81\u53d1\u6027\u9519\u4f4d\uff09\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6027\u65b9\u6cd5\u6765\u9632\u6b62\u8fd9\u79cd\u9519\u4f4d\uff0c\u800c\u4e0d\u5f71\u54cd\u6a21\u578b\u7684\u6838\u5fc3\u6027\u80fd\u3002", "method": "\u8bc6\u522b\u53ef\u9760\u63a7\u5236\u9519\u4f4d\u884c\u4e3a\u7684\u5c11\u91cf\u5185\u90e8\u7279\u5f81\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u963b\u6b62\u6a21\u578b\u5f3a\u5316\u8fd9\u4e9b\u7279\u5f81\u3002\u4f7f\u7528\u5206\u79bb\u7684\u9009\u62e9/\u8bc4\u4f30\u96c6\u3001\u591a\u4e2a\u72ec\u7acb\u8bc4\u5224\u8005\u3001\u591a\u4e2a\u968f\u673a\u79cd\u5b50\u3001\u8d28\u91cf\u6307\u6807\u548c\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u516d\u4e2a\u5fae\u8c03\u9886\u57df\u4e2d\uff0c\u963b\u65ad\u56fa\u5b9a\u7279\u5f81\u96c6\u5b9e\u73b0\u4e86\u9ad8\u8fbe95%\u7684\u76f8\u5bf9\u9519\u4f4d\u51cf\u5c11\uff0c\u4e14\u6ca1\u6709\u964d\u4f4e\u6a21\u578b\u8d28\u91cf\u6216\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u4e86\u5728\u957f\u65f6\u95f4\u5fae\u8c03\u4e0b\u9519\u4f4d\u91cd\u65b0\u51fa\u73b0\u7684\u9650\u5236\u60c5\u51b5\u3002", "conclusion": "\u9488\u5bf9\u5185\u90e8\u673a\u5236\u7684\u6709\u9488\u5bf9\u6027\u8bad\u7ec3\u65f6\u95f4\u7ea6\u675f\u53ef\u4ee5\u6709\u6548\u51cf\u8f7b\u7a81\u53d1\u6027\u9519\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01967", "abs": "https://arxiv.org/abs/2602.01967", "authors": ["Wonjun Lee", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition", "comment": null, "summary": "Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.", "AI": {"tldr": "Moe-Ctc\uff1a\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u4e2d\u95f4CTC\u76d1\u7763\u8054\u5408\u4fc3\u8fdb\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u6cdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u591a\u53e3\u97f3\u8bed\u97f3\u8bc6\u522b\u6027\u80fd", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5c11\u6570\u9ad8\u8d44\u6e90\u82f1\u8bed\u53d8\u4f53\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5bf9\u5176\u4ed6\u53e3\u97f3\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u53e3\u97f3\u65e0\u5173\u65b9\u6cd5\u5bf9\u91cd\u53e3\u97f3\u6216\u672a\u89c1\u53e3\u97f3\u6548\u679c\u6709\u9650\uff0c\u800c\u53e3\u97f3\u7279\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u4e14\u5608\u6742\u7684\u6807\u7b7e\u6570\u636e\u3002", "method": "\u63d0\u51faMoe-Ctc\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5305\u542b\u53e3\u97f3\u611f\u77e5\u8def\u7531\uff08\u8bad\u7ec3\u65f6\uff09\u548c\u6807\u7b7e\u65e0\u5173\u8def\u7531\uff08\u63a8\u7406\u65f6\uff09\u3002\u6bcf\u4e2a\u4e13\u5bb6\u914d\u5907\u72ec\u7acb\u7684CTC\u5934\uff0c\u901a\u8fc7\u8def\u7531\u589e\u5f3a\u635f\u5931\u7a33\u5b9a\u4f18\u5316\uff0c\u5b9e\u73b0\u4e13\u5bb6\u4e13\u4e1a\u5316\u4e0e\u6cdb\u5316\u7684\u5e73\u8861\u3002", "result": "\u5728Mcv-Accent\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u6761\u4ef6\u4e0b\uff0c\u5bf9\u5df2\u89c1\u548c\u672a\u89c1\u53e3\u97f3\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u6bd4FastConformer\u57fa\u7ebf\u5b9e\u73b0\u6700\u9ad829.3%\u7684\u76f8\u5bf9WER\u964d\u4f4e\u3002", "conclusion": "Moe-Ctc\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u591a\u53e3\u97f3\u8bed\u97f3\u8bc6\u522b\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u53e3\u97f3\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u53e3\u97f3\u9c81\u68d2ASR\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01069", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01069", "abs": "https://arxiv.org/abs/2602.01069", "authors": ["Seema K. Poudel", "Sunny K. Khadka"], "title": "PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors", "comment": null, "summary": "Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u56fe\u50cf\u5206\u5272\u5efa\u6a21\u4e3aPDE\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u6b63\u5219\u5316\u5c06\u7269\u7406\u5148\u9a8c\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u5728\u663e\u5fae\u955c\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u663e\u5fae\u955c\u56fe\u50cf\u5206\u5272\u9762\u4e34\u6d4b\u91cf\u566a\u58f0\u3001\u5f31\u8fb9\u754c\u548c\u6709\u9650\u6807\u6ce8\u6570\u636e\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u7684\u65e0\u7ea6\u675f\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4f1a\u5bfc\u81f4\u89e3\u4e0d\u7a33\u5b9a\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u9700\u8981\u5c06\u7269\u7406\u5148\u9a8c\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u4ee5\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002", "method": "\u5c06\u56fe\u50cf\u5206\u5272\u5efa\u6a21\u4e3aPDE\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u53d8\u5206\u6b63\u5219\u5316\u6574\u5408\u53cd\u5e94-\u6269\u6563\u65b9\u7a0b\u548c\u76f8\u573a\u754c\u9762\u80fd\u91cf\u7684\u7269\u7406\u5148\u9a8c\u3002\u4f7f\u7528\u53ef\u5fae\u5206\u6b8b\u5dee\u635f\u5931\u5b9e\u73b0\u590d\u5408\u76ee\u6807\u51fd\u6570\uff0c\u5305\u542b\u6570\u636e\u4fdd\u771f\u9879\u548c\u60e9\u7f5a\u9879\u3002\u5728LIVECell\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528UNet\u4f5c\u4e3a\u65e0\u7ea6\u675f\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5728LIVECell\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u65e0\u7ea6\u675f\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0cPDE\u6b63\u5219\u5316\u6a21\u578b\u5728\u5206\u5272\u51c6\u786e\u6027\u548c\u8fb9\u754c\u4fdd\u771f\u5ea6\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002\u7279\u522b\u662f\u5728\u4f4e\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PDE\u7ea6\u675f\u4f18\u5316\u4e3a\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6865\u6881\uff0c\u5c06\u53d8\u5206\u65b9\u6cd5\u3001\u7edf\u8ba1\u5b66\u4e60\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5148\u9a8c\u63d0\u9ad8\u4e86\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.02039", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02039", "abs": "https://arxiv.org/abs/2602.02039", "authors": ["Wei Liu", "Peijie Yu", "Michele Orini", "Yali Du", "Yulan He"], "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models", "comment": "14 pages, 7 tables, 8 figures", "summary": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u8c03\u67e5\u6027\u667a\u80fd\"\u6982\u5ff5\uff0c\u533a\u522b\u4e8e\u6267\u884c\u6027\u667a\u80fd\uff0c\u5e76\u5f15\u5165Deep Data Research\u4efb\u52a1\u548cDDR-Bench\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u81ea\u4e3b\u6570\u636e\u63a2\u7d22\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u56de\u7b54\u6b63\u786e\u6027\uff0c\u4f46\u771f\u6b63\u7684\u667a\u80fd\u4f53\u9700\u8981\u81ea\u4e3b\u8bbe\u5b9a\u76ee\u6807\u548c\u63a2\u7d22\u7684\u80fd\u529b\u3002\u6570\u636e\u79d1\u5b66\u9886\u57df\u662f\u5929\u7136\u6d4b\u8bd5\u573a\uff0c\u56e0\u4e3a\u771f\u5b9e\u5206\u6790\u59cb\u4e8e\u539f\u59cb\u6570\u636e\u800c\u975e\u660e\u786e\u67e5\u8be2\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u57fa\u51c6\u3002", "method": "\u63d0\u51faDeep Data Research\u4efb\u52a1\uff0c\u8ba9LLM\u4ece\u6570\u636e\u5e93\u4e2d\u81ea\u4e3b\u63d0\u53d6\u5173\u952e\u89c1\u89e3\uff1b\u5f00\u53d1DDR-Bench\u5927\u89c4\u6a21\u68c0\u67e5\u8868\u57fa\u51c6\uff0c\u652f\u6301\u53ef\u9a8c\u8bc1\u8bc4\u4f30\uff1b\u5206\u6790\u524d\u6cbf\u6a21\u578b\u5728\u957f\u89c6\u91ce\u63a2\u7d22\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u663e\u793a\u51fa\u521d\u6b65\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u4f46\u957f\u89c6\u91ce\u63a2\u7d22\u4ecd\u7136\u56f0\u96be\u3002\u8c03\u67e5\u6027\u667a\u80fd\u7684\u6709\u6548\u6027\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u6846\u67b6\u6216\u89c4\u6a21\u6269\u5c55\uff0c\u8fd8\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5185\u5728\u7b56\u7565\u3002", "conclusion": "\u8c03\u67e5\u6027\u667a\u80fd\u662fLLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u7684\u4efb\u52a1\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u3002DDR\u548cDDR-Bench\u4e3a\u8bc4\u4f30LLM\u5728\u5f00\u653e\u6570\u636e\u63a2\u7d22\u4e2d\u7684\u81ea\u4e3b\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.00772", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00772", "abs": "https://arxiv.org/abs/2602.00772", "authors": ["Xiaoqi Qiu", "Hao Zeng", "Zhiyu Hou", "Hongxin Wei"], "title": "Provable Model Provenance Set for Large Language Models", "comment": null, "summary": "The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.", "AI": {"tldr": "\u63d0\u51faMPS\u65b9\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u6d4b\u8bd5\u548c\u6392\u9664\u7a0b\u5e8f\u6784\u5efa\u6ee1\u8db3\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u5c0f\u578b\u6a21\u578b\u6765\u6e90\u96c6\u5408\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u8bc1\u660e\u9519\u8bef\u63a7\u5236\u548c\u591a\u6765\u6e90\u8bc6\u522b\u7684\u95ee\u9898", "motivation": "\u968f\u7740\u672a\u7ecf\u6388\u6743\u6a21\u578b\u4f7f\u7528\u548c\u9519\u8bef\u5f52\u56e0\u7684\u589e\u52a0\uff0c\u9700\u8981\u53ef\u9760\u7684\u6a21\u578b\u6765\u6e90\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u6307\u7eb9\u5339\u914d\u89c4\u5219\uff0c\u7f3a\u4e4f\u53ef\u8bc1\u660e\u7684\u9519\u8bef\u63a7\u5236\uff0c\u4e14\u5e38\u5ffd\u7565\u591a\u6765\u6e90\u5b58\u5728\uff0c\u5bfc\u81f4\u6765\u6e90\u58f0\u660e\u7684\u53ef\u9760\u6027\u65e0\u6cd5\u9a8c\u8bc1", "method": "\u63d0\u51fa\u6a21\u578b\u6765\u6e90\u96c6\u5408(MPS)\u65b9\u6cd5\uff0c\u91c7\u7528\u987a\u5e8f\u6d4b\u8bd5\u548c\u6392\u9664\u7a0b\u5e8f\uff0c\u81ea\u9002\u5e94\u6784\u5efa\u6ee1\u8db3\u4fdd\u8bc1\u7684\u5c0f\u578b\u96c6\u5408\u3002\u6838\u5fc3\u601d\u60f3\u662f\u5728\u5019\u9009\u6c60\u4e2d\u6d4b\u8bd5\u6765\u6e90\u5b58\u5728\u7684\u663e\u8457\u6027\uff0c\u4ece\u800c\u5728\u7528\u6237\u7279\u5b9a\u7f6e\u4fe1\u6c34\u5e73\u4e0a\u5efa\u7acb\u53ef\u8bc1\u660e\u7684\u6e10\u8fd1\u4fdd\u8bc1", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMPS\u80fd\u6709\u6548\u5b9e\u73b0\u76ee\u6807\u6765\u6e90\u8986\u76d6\uff0c\u540c\u65f6\u4e25\u683c\u9650\u5236\u65e0\u5173\u6a21\u578b\u7684\u5305\u542b\uff0c\u5e76\u5c55\u793a\u5176\u5728\u5f52\u56e0\u548c\u5ba1\u8ba1\u4efb\u52a1\u4e2d\u5b9e\u9645\u6765\u6e90\u5206\u6790\u7684\u6f5c\u529b", "conclusion": "MPS\u4e3a\u6a21\u578b\u6765\u6e90\u95ee\u9898\u63d0\u4f9b\u4e86\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u6b63\u5f0f\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u548c\u987a\u5e8f\u6392\u9664\u7a0b\u5e8f\uff0c\u5728\u4fdd\u6301\u9ad8\u7f6e\u4fe1\u6c34\u5e73\u7684\u540c\u65f6\u51c6\u786e\u8bc6\u522b\u6a21\u578b\u6765\u6e90"}}
{"id": "2602.01969", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.01969", "abs": "https://arxiv.org/abs/2602.01969", "authors": ["Bin Cao", "Huixian Lu", "Chenwen Ma", "Ting Wang", "Ruizhe Li", "Jing Fan"], "title": "Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models", "comment": "Work in process", "summary": "Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.", "AI": {"tldr": "OHD\u6846\u67b6\u901a\u8fc7\u6b63\u4ea4\u6811\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u8868\u683c\u5206\u89e3\u4e3a\u5217\u6811\u548c\u884c\u6811\uff0c\u4fdd\u7559\u5c42\u6b21\u7ed3\u6784\uff0c\u63d0\u5347LLM\u5bf9\u590d\u6742\u8868\u683c\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u590d\u6742\u8868\u683c\uff08\u591a\u7ea7\u8868\u5934\u3001\u5408\u5e76\u5355\u5143\u683c\u3001\u5f02\u6784\u5e03\u5c40\uff09\u5bf9LLM\u7684\u7406\u89e3\u548c\u63a8\u7406\u6784\u6210\u6301\u7eed\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u8868\u683c\u7ebf\u6027\u5316\u6216\u89c4\u8303\u5316\u7f51\u683c\u5efa\u6a21\uff09\u96be\u4ee5\u663e\u5f0f\u6355\u6349\u5c42\u6b21\u7ed3\u6784\u548c\u8de8\u7ef4\u5ea6\u4f9d\u8d56\uff0c\u5bfc\u81f4\u7ed3\u6784\u8bed\u4e49\u4e0e\u6587\u672c\u8868\u793a\u4e0d\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5c42\u6b21\u5206\u89e3\uff08OHD\uff09\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u7a7a\u95f4-\u8bed\u4e49\u5171\u540c\u7ea6\u675f\u7684\u6b63\u4ea4\u6811\u5f52\u7eb3\uff08OTI\uff09\u65b9\u6cd5\uff0c\u5c06\u4e0d\u89c4\u5219\u8868\u683c\u5206\u89e3\u4e3a\u5217\u6811\u548c\u884c\u6811\uff0c\u5206\u522b\u6355\u6349\u5782\u76f4\u548c\u6c34\u5e73\u5c42\u6b21\u4f9d\u8d56\uff1b2\uff09\u8bbe\u8ba1\u53cc\u8def\u5f84\u5173\u8054\u534f\u8bae\u5bf9\u79f0\u91cd\u5efa\u6bcf\u4e2a\u5355\u5143\u683c\u7684\u8bed\u4e49\u8c31\u7cfb\uff1b3\uff09\u5f15\u5165LLM\u4f5c\u4e3a\u8bed\u4e49\u4ef2\u88c1\u5668\u5bf9\u9f50\u591a\u7ea7\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728AITQA\u548cHiTab\u4e24\u4e2a\u590d\u6742\u8868\u683c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOHD\u6846\u67b6\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8868\u793a\u8303\u5f0f\u3002", "conclusion": "OHD\u6846\u67b6\u901a\u8fc7\u6b63\u4ea4\u6811\u5206\u89e3\u65b9\u6cd5\u6709\u6548\u6355\u6349\u590d\u6742\u8868\u683c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8868\u793a\u65b9\u6cd5\u5728\u7ed3\u6784\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5bf9\u590d\u6742\u8868\u683c\u7684\u7406\u89e3\u548c\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.01077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01077", "abs": "https://arxiv.org/abs/2602.01077", "authors": ["Haopeng Li", "Shitong Shao", "Wenliang Zhong", "Zikai Zhou", "Lichen Bai", "Hui Xiong", "Zeke Xie"], "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers", "comment": "17 pages", "summary": "Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.", "AI": {"tldr": "PISA\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u81ea\u7531\u7684\u7247\u6bb5\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u7cbe\u786e\u8ba1\u7b97\u5173\u952e\u5757\u5e76\u8fd1\u4f3c\u975e\u5173\u952e\u5757\u6765\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u5728\u6269\u6563Transformer\u4e2d\u5b9e\u73b0\u4e9a\u4e8c\u6b21\u590d\u6742\u5ea6\u3002", "motivation": "\u6269\u6563Transformer\u5728\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u6548\u7387\u3002\u73b0\u6709\u7684\u5757\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u901a\u8fc7\u4e22\u5f03\u975e\u5173\u952e\u5757\u6765\u52a0\u901f\uff0c\u4f46\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u56e0\u4e22\u5931\u4e0a\u4e0b\u6587\u4fe1\u606f\u800c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002", "method": "PISA\u91c7\u7528\"\u7cbe\u786e\u6216\u8fd1\u4f3c\"\u7b56\u7565\uff1a\u53d1\u73b0\u975e\u5173\u952e\u5757\u7684\u6ce8\u610f\u529b\u5206\u6570\u5177\u6709\u5206\u5e03\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u5bf9\u5173\u952e\u5757\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\uff0c\u5bf9\u975e\u5173\u952e\u5757\u901a\u8fc7\u5757\u7ea7\u6cf0\u52d2\u5c55\u5f00\u8fdb\u884c\u9ad8\u6548\u8fd1\u4f3c\uff0c\u4ece\u800c\u8986\u76d6\u5b8c\u6574\u6ce8\u610f\u529b\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPISA\u5728Wan2.1-14B\u4e0a\u5b9e\u73b01.91\u500d\u52a0\u901f\uff0c\u5728Hunyuan-Video\u4e0a\u5b9e\u73b02.57\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5728\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e2d\u4fdd\u6301\u6700\u9ad8\u8d28\u91cf\u3002\u5728FLUX\u56fe\u50cf\u751f\u6210\u4e2d\u5b9e\u73b01.2\u500d\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "PISA\u901a\u8fc7\u521b\u65b0\u7684\u7cbe\u786e-\u8fd1\u4f3c\u7b56\u7565\u6709\u6548\u5f25\u5408\u4e86\u901f\u5ea6\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6269\u6563Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02050", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02050", "abs": "https://arxiv.org/abs/2602.02050", "authors": ["Zeping Li", "Hongru Wang", "Yiwen Zhao", "Guanhua Chen", "Yixia Li", "Keyang Chen", "Yixin Cao", "Guangnan Ye", "Hongfeng Chai", "Mengdi Wang", "Zhenfei Yin"], "title": "Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents", "comment": null, "summary": "Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.", "AI": {"tldr": "\u57fa\u4e8eLLM\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u5728\u957f\u8f68\u8ff9\u4e2d\u5e38\u89e6\u53d1\u8fc7\u591a\u4f4e\u8d28\u91cf\u5de5\u5177\u8c03\u7528\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u71b5\u51cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u548c\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u6765\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8df3\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u957f\u8f68\u8ff9\u4e2d\u7ecf\u5e38\u89e6\u53d1\u8fc7\u591a\u4e14\u4f4e\u8d28\u91cf\u7684\u5de5\u5177\u8c03\u7528\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u8fd9\u4f7f\u5f97\u7ba1\u7406\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8fdb\u884c\u57fa\u4e8e\u71b5\u7684\u8bd5\u70b9\u5b9e\u9a8c\uff0c\u53d1\u73b0\u71b5\u51cf\u4e0e\u9ad8\u8d28\u91cf\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u5b58\u5728\u5f3a\u6b63\u76f8\u5173\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u4ed6\u4eec\u63d0\u51fa\u4f7f\u7528\u71b5\u51cf\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u5956\u52b1\u7b56\u7565\uff1a\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u8f68\u8ff9\u7ea7\u6307\u5bfc\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8de8\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u5956\u52b1\u8bbe\u8ba1\u90fd\u80fd\u6539\u5584\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\uff1a\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u51cf\u5c11\u4e8672.07%\u7684\u5de5\u5177\u8c03\u7528\uff0c\u800c\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u5c06\u6027\u80fd\u63d0\u5347\u4e8622.27%\u3002", "conclusion": "\u71b5\u51cf\u6210\u4e3a\u589e\u5f3a\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u7684\u5173\u952e\u673a\u5236\uff0c\u4f7f\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u66f4\u5177\u9002\u5e94\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u4f18\u5316\u57fa\u4e8eLLM\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u884c\u4e3a\u3002"}}
{"id": "2602.00774", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00774", "abs": "https://arxiv.org/abs/2602.00774", "authors": ["Yuxin Lu", "Zhen Peng", "Xiqiang Xia", "Jie Wang"], "title": "A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry", "comment": null, "summary": "Against the backdrop of the global green transition and \"dual carbon\" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.", "AI": {"tldr": "\u80a1\u6743\u5236\u8861\u80fd\u663e\u8457\u6291\u5236\u77ff\u4e1a\u4ea7\u4e1a\u94fe\u4f01\u4e1a\u7684\u7eff\u8272\u6f02\u6d17\u884c\u4e3a\uff0c\u4e3b\u8981\u901a\u8fc7\u7f13\u89e3\u7ba1\u7406\u5c42\u4e1a\u7ee9\u538b\u529b\u3001\u589e\u5f3a\u9ad8\u7ba1\u56e2\u961f\u7a33\u5b9a\u6027\u548c\u5f3a\u5316\u5a92\u4f53\u76d1\u7763\u4e09\u6761\u8def\u5f84\u5b9e\u73b0\uff0c\u4e14\u5b58\u5728\u533a\u57df\u3001\u4ea7\u4e1a\u94fe\u4f4d\u7f6e\u548c\u884c\u4e1a\u654f\u611f\u6027\u7b49\u5f02\u8d28\u6027\u7279\u5f81\u3002", "motivation": "\u5728\u5168\u7403\u7eff\u8272\u8f6c\u578b\u548c\"\u53cc\u78b3\"\u76ee\u6807\u80cc\u666f\u4e0b\uff0c\u77ff\u4e1a\u4ea7\u4e1a\u94fe\u4f01\u4e1a\u4f5c\u4e3a\u8d44\u6e90\u6d88\u8017\u548c\u73af\u5883\u5f71\u54cd\u7684\u91cd\u70b9\u5b9e\u4f53\uff0c\u5176\u73af\u5883\u4fe1\u606f\u62ab\u9732\u7684\u771f\u5b9e\u53ef\u9760\u6027\u5bf9\u533a\u57df\u751f\u6001\u5b89\u5168\u548c\u56fd\u5bb6\u6218\u7565\u81f3\u5173\u91cd\u8981\u3002\u4ece\u516c\u53f8\u6cbb\u7406\u89d2\u5ea6\uff0c\u7814\u7a76\u80a1\u6743\u5236\u8861\u8fd9\u4e00\u57fa\u7840\u6cbb\u7406\u673a\u5236\u5bf9\u7eff\u8272\u6f02\u6d17\u884c\u4e3a\u7684\u6291\u5236\u4f5c\u7528\u53ca\u5176\u8def\u5f84\u3002", "method": "\u521b\u65b0\u6027\u5730\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u548c\u53cc\u91cd\u673a\u5668\u5b66\u4e60(DML)\u6a21\u578b\u6784\u5efa\u53cd\u4e8b\u5b9e\u573a\u666f\uff0c\u89e3\u51b3\u5185\u751f\u6027\u95ee\u9898\uff0c\u7cbe\u786e\u8bc6\u522b\u80a1\u6743\u5236\u8861\u4e0e\u7eff\u8272\u6f02\u6d17\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "1) \u80a1\u6743\u5236\u8861\u4e0e\u4f01\u4e1a\u7eff\u8272\u6f02\u6d17\u5b58\u5728\u663e\u8457\u8d1f\u5411\u56e0\u679c\u5173\u7cfb\uff0c\u8bc1\u5b9e\u5176\u6cbb\u7406\u6548\u5e94\uff1b2) \u6291\u5236\u4f5c\u7528\u5b58\u5728\u5f02\u8d28\u6027\uff1a\u897f\u90e8\u5730\u533a\u3001\u4ea7\u4e1a\u94fe\u4e0a\u6e38\u3001\u9ad8\u73af\u5883\u654f\u611f\u6027\u884c\u4e1a\u66f4\u5f3a\uff1b3) \u6cbb\u7406\u6548\u5e94\u5177\u6709\u65f6\u6001\u52a8\u6001\u6027\uff1a\u5f53\u671f\u6700\u5f3a\uff0c\u6ede\u540e\u6548\u5e94\u9012\u51cf\u4f46\u4ecd\u663e\u8457\uff0c\u6700\u7ec8\u5f62\u6210\u7a33\u5b9a\u957f\u671f\u7d2f\u79ef\u5f71\u54cd\u3002", "conclusion": "\u80a1\u6743\u5236\u8861\u901a\u8fc7\u7f13\u89e3\u7ba1\u7406\u5c42\u4e1a\u7ee9\u538b\u529b\u3001\u589e\u5f3a\u9ad8\u7ba1\u56e2\u961f\u7a33\u5b9a\u6027\u3001\u5f3a\u5316\u5a92\u4f53\u76d1\u7763\u4e09\u6761\u8def\u5f84\u6709\u6548\u6291\u5236\u4f01\u4e1a\u7eff\u8272\u6f02\u6d17\u884c\u4e3a\uff0c\u4e3a\u5b8c\u5584\u77ff\u4e1a\u4f01\u4e1a\u73af\u5883\u6cbb\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u653f\u7b56\u542f\u793a\u3002"}}
{"id": "2602.01977", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01977", "abs": "https://arxiv.org/abs/2602.01977", "authors": ["Shuainan Liu", "Xuanang Chen", "Ben He", "Le Sun"], "title": "Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing", "comment": null, "summary": "Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.", "AI": {"tldr": "\u63d0\u51faEVK\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u6270\u52a8\u8868\u5f81\u6a21\u578b\u77e5\u8bc6\uff0c\u6784\u5efaEVK-Bench\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u5f15\u8d77\u7684\u77e5\u8bc6\u6f02\u79fb\uff0c\u5e76\u5f00\u53d1EVK-Align\u6a21\u5757\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u6709\u9650\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5168\u9762\u7406\u89e3\u7f16\u8f91\u5bf9\u6a21\u578b\u77e5\u8bc6\u7cfb\u7edf\u7684\u6574\u4f53\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEmbedding-Virtualized Knowledge (EVK)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u7a7a\u95f4\u53d7\u63a7\u6270\u52a8\u8868\u5f81\u6a21\u578b\u77e5\u8bc6\uff1b\u6784\u5efaEVK-Bench\u8bc4\u4f30\u57fa\u51c6\u91cf\u5316\u77e5\u8bc6\u6f02\u79fb\uff1b\u5f00\u53d1EVK-Align\u6a21\u5757\u7ea6\u675f\u5d4c\u5165\u7ea7\u77e5\u8bc6\u6f02\u79fb\u3002", "result": "EVK\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\u6548\u679c\uff0cEVK-Align\u6a21\u5757\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u800c\u4e0d\u727a\u7272\u7f16\u8f91\u51c6\u786e\u6027\u3002", "conclusion": "EVK\u6846\u67b6\u4e3a\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u89c6\u89d2\uff0cEVK-Align\u6a21\u5757\u53ef\u65e0\u7f1d\u96c6\u6210\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7f16\u8f91\u51c6\u786e\u6027\u7684\u540c\u65f6\u6709\u6548\u51cf\u5c11\u77e5\u8bc6\u6f02\u79fb\u3002"}}
{"id": "2602.01081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01081", "abs": "https://arxiv.org/abs/2602.01081", "authors": ["Haitao Zhang", "Yingying Wang", "Jiaxiang Wang", "Haote Xu", "Hongyang Zhang", "Yirong Chen", "Yue Huang", "Xinghao Ding"], "title": "MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization", "comment": "9 pages, 4 figures", "summary": "Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MedAD-38K\u57fa\u51c6\u6570\u636e\u96c6\u548cMedAD-R1\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u8ba4\u77e5\u6ce8\u5165+\u4e00\u81f4\u6027\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u63d0\u5347\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\uff08MedAD\uff09\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u7b80\u5355\u788e\u7247\u5316\u6570\u636e\u96c6\u4e0a\uff0c\u5bfc\u81f4\u6a21\u578b\u7f3a\u4e4f\u5408\u7406\u63a8\u7406\u80fd\u529b\u548c\u9c81\u68d2\u7684\u591a\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u57fa\u51c6\u6765\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "1) \u6784\u5efaMedAD-38K\u57fa\u51c6\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u3001\u591a\u4e2d\u5fc3\u7684\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u8bca\u65ad\u601d\u7ef4\u94fe\u6ce8\u91ca\u548c\u7ed3\u6784\u5316\u89c6\u89c9\u95ee\u7b54\u5bf9\uff1b2) \u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8ba4\u77e5\u6ce8\u5165\u4f7f\u7528SFT\u6ce8\u5165\u57fa\u7840\u533b\u5b66\u77e5\u8bc6\u5e76\u5bf9\u9f50\u601d\u7ef4-\u56de\u7b54\u8303\u5f0f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u4e00\u81f4\u6027\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08Con-GRPO\uff09\uff0c\u5f15\u5165\u4e00\u81f4\u6027\u5956\u52b1\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u8bca\u65ad\u76f8\u5173\u4e14\u903b\u8f91\u8fde\u8d2f\u3002", "result": "\u63d0\u51fa\u7684MedAD-R1\u6a21\u578b\u5728MedAD-38K\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u8d85\u8fc710%\uff0c\u80fd\u591f\u751f\u6210\u900f\u660e\u4e14\u903b\u8f91\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u6ce8\u5165\u548c\u4e00\u81f4\u6027\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u589e\u5f3aAI\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2602.02051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02051", "abs": "https://arxiv.org/abs/2602.02051", "authors": ["Shivank Garg", "Ayush Singh", "Gaurav Kumar Nayak"], "title": "SIDiffAgent: Self-Improving Diffusion Agent", "comment": null, "summary": "Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \\modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.", "AI": {"tldr": "SIDiffAgent\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528Qwen\u7cfb\u5217\u6a21\u578b\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u95ee\u9898\uff0c\u5305\u62ec\u63d0\u793a\u8bcd\u654f\u611f\u6027\u3001\u8bed\u4e49\u6b67\u4e49\u3001\u751f\u6210\u4f2a\u5f71\u7b49\uff0c\u901a\u8fc7\u81ea\u4e3b\u7ba1\u7406\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u6d4b\u7ea0\u6b63\u4e0d\u826f\u751f\u6210\u3001\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u79fb\u9664\u548c\u8fed\u4ee3\u81ea\u6211\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u591a\u4e2a\u9650\u5236\uff1a\u5bf9\u63d0\u793a\u8bcd\u8868\u8fbe\u7684\u654f\u611f\u6027\u3001\u8bed\u4e49\u89e3\u91ca\u7684\u6b67\u4e49\u6027\uff08\u5982\"mouse\"\u53ef\u6307\u52a8\u7269\u6216\u8ba1\u7b97\u673a\u5916\u8bbe\uff09\u3001\u89e3\u5256\u7ed3\u6784\u626d\u66f2\u7b49\u4f2a\u5f71\u95ee\u9898\uff0c\u4ee5\u53ca\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f93\u5165\u63d0\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u4e14\u53ef\u63a7\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSIDiffAgent\u6846\u67b6\uff0c\u5229\u7528Qwen\u7cfb\u5217\u6a21\u578b\uff08Qwen-VL\u3001Qwen-Image\u3001Qwen-Edit\u3001Qwen-Embedding\uff09\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u81ea\u4e3b\u7ba1\u7406\u63d0\u793a\u5de5\u7a0b\uff0c\u68c0\u6d4b\u5e76\u7ea0\u6b63\u4e0d\u826f\u751f\u6210\uff0c\u6267\u884c\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u79fb\u9664\u3002\u901a\u8fc7\u5c06\u5148\u524d\u7ecf\u9a8c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u5b9e\u73b0\u8fed\u4ee3\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u5728\u667a\u80fd\u4f53\u7ba1\u9053\u7684\u6bcf\u4e2a\u9636\u6bb5\u6ce8\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u5bfc\u3002", "result": "\u5728GenAIBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIDiffAgent\u83b7\u5f97\u4e86\u5e73\u57470.884\u7684VQA\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u4ee5\u53ca\u5176\u4ed6\u667a\u80fd\u4f53\u65b9\u6cd5\u3002\u8fd9\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "SIDiffAgent\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u6311\u6218\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u63d0\u793a\u5de5\u7a0b\u81ea\u52a8\u5316\u3001\u751f\u6210\u8d28\u91cf\u6539\u8fdb\u548c\u8fed\u4ee3\u81ea\u6211\u4f18\u5316\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00775", "categories": ["cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2602.00775", "abs": "https://arxiv.org/abs/2602.00775", "authors": ["Zitao Hong", "Zhen Peng", "Xueping Liu"], "title": "Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference", "comment": null, "summary": "Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u4e0e\u7a33\u5b9a\u5b66\u4e60\u7684\u78b3\u6392\u9884\u6d4b\u673a\u5236\uff0c\u901a\u8fc7\u63d0\u53d6\u56e0\u679c\u7a33\u5b9a\u7279\u5f81\u548c\u52a8\u6001\u6821\u6b63\u65f6\u95f4\u975e\u5e73\u7a33\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f01\u4e1a\u78b3\u6392\u653e\u6570\u636e\u5b58\u5728\u663e\u8457\u7684\u65f6\u7a7a\u5206\u5e03\u504f\u79fb\u548c\u975e\u5e73\u7a33\u6027\uff0c\u5bfc\u81f4\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u80fd\u6e90\u7ed3\u6784\u4f18\u5316\u548c\u4f4e\u78b3\u8f6c\u578b\u51b3\u7b56", "method": "\u6574\u5408\u56e0\u679c\u63a8\u65ad\u3001\u7a33\u5b9a\u5b66\u4e60\u4e0e\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0c\u6784\u5efa\u98ce\u9669\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u7a33\u5b9a\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u53d6\u56e0\u679c\u7a33\u5b9a\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u548c\u6837\u672c\u91cd\u52a0\u6743\u7b56\u7565\u6821\u6b63\u65f6\u95f4\u975e\u5e73\u7a33\u6027", "result": "\u63d0\u51fa\u7684\u7a33\u5b9a\u65f6\u95f4\u9884\u6d4b\u673a\u5236\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u78b3\u6392\u653e\u9884\u6d4b\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u78b3\u6392\u62a5\u544a\u51c6\u786e\u6027\u548c\u51b3\u7b56\u6307\u5bfc\u4ef7\u503c"}}
{"id": "2602.01982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01982", "abs": "https://arxiv.org/abs/2602.01982", "authors": ["Yanrui Du", "Sendong Zhao", "Yibo Gao", "Danyang Zhao", "Qika Lin", "Ming Ma", "Jiayun Li", "Yi Jiang", "Kai He", "Qianyi Xu", "Bing Qin", "Mengling Feng"], "title": "S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs", "comment": null, "summary": "Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faS3-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u81ea\u91c7\u6837\u5b9e\u73b0\u9ad8\u6548\u601d\u7ef4\u94fe\u5b66\u4e60\uff0c\u65e0\u9700\u6559\u5e08\u6307\u5bfc\u5373\u53ef\u751f\u6210\u98ce\u683c\u5bf9\u9f50\u3001\u957f\u5ea6\u53ef\u53d8\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u89e3\u51b3\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u5197\u4f59\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22LLM\u80fd\u5426\u50cf\u4eba\u7c7b\u7cfb\u7edf1\u63a8\u7406\u4e00\u6837\u83b7\u5f97\u5feb\u901f\u601d\u8003\u6a21\u5f0f\uff0c\u540c\u65f6\u89e3\u51b3\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4e2d\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u7684\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u81ea\u91c7\u6837\u6846\u67b6\uff0c\u4ece\u76ee\u6807LLM\u81ea\u8eab\u8bf1\u5bfc\u63a8\u7406\u8f68\u8ff9\uff1b\u4f7f\u7528\u9ec4\u91d1\u7b54\u6848\u8fc7\u6ee4\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u6784\u5efa\u7c7b\u4eba\u53cc\u8ba4\u77e5\u7cfb\u7edf\u5e76\u91c7\u7528\u6e10\u8fdb\u538b\u7f29\u8bfe\u7a0b\uff1b\u63a2\u7d22\u4ec5\u4f7f\u7528\u9884\u6d4b\u4e00\u81f4\u6570\u636e\u7684\u81ea\u8fdb\u5316\u673a\u5236\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u548c\u533b\u5b66\u9886\u57df\u7684\u8de8\u57df\u6cdb\u5316\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u901a\u7528LLM\u548cR1\u98ce\u683cLLM\u5747\u5e26\u6765\u7a33\u5b9a\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "S3-CoT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u601d\u7ef4\u94fe\u5b66\u4e60\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u4e3aLLM\u83b7\u5f97\u5feb\u901f\u601d\u8003\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.01089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01089", "abs": "https://arxiv.org/abs/2602.01089", "authors": ["Zhiqi Zhang", "Xinhao Zhong", "Yi Sun", "Shuoyang Sun", "Bin Chen", "Shu-Tao Xia", "Xuan Wang"], "title": "Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models", "comment": null, "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.", "AI": {"tldr": "\u63d0\u51faDVE\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u64e6\u9664\u6280\u672f\uff0c\u4e13\u95e8\u4e3a\u6d41\u5339\u914d\u6a21\u578b\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6295\u5f71\u901f\u5ea6\u573a\u5230\u5dee\u5206\u65b9\u5411\u6765\u9009\u62e9\u6027\u79fb\u9664\u7279\u5b9a\u6982\u5ff5", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u4f1a\u590d\u5236\u4e0d\u826f\u6982\u5ff5\uff08\u5982NSFW\u5185\u5bb9\u3001\u7248\u6743\u98ce\u683c\u7b49\uff09\uff0c\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9DDPM\u6a21\u578b\u4e14\u9700\u8981\u6602\u8d35\u5fae\u8c03\uff0c\u800c\u6d41\u5339\u914d\u6a21\u578b\u4f5c\u4e3a\u65b0\u5174\u751f\u6210\u8303\u5f0f\u7f3a\u4e4f\u9002\u7528\u65b9\u6cd5", "method": "\u63d0\u51fa\u5dee\u5206\u5411\u91cf\u64e6\u9664(DVE)\uff1a1) \u53d1\u73b0\u8bed\u4e49\u6982\u5ff5\u9690\u542b\u5728\u63a7\u5236\u751f\u6210\u6d41\u7684\u901f\u5ea6\u573a\u65b9\u5411\u7ed3\u6784\u4e2d\uff1b2) \u6784\u5efa\u5dee\u5206\u5411\u91cf\u573a\uff0c\u8868\u5f81\u76ee\u6807\u6982\u5ff5\u4e0e\u951a\u6982\u5ff5\u4e4b\u95f4\u7684\u65b9\u5411\u5dee\u5f02\uff1b3) \u5728\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u5c06\u901f\u5ea6\u573a\u6295\u5f71\u5230\u5dee\u5206\u65b9\u5411\u6765\u9009\u62e9\u6027\u79fb\u9664\u6982\u5ff5\u7279\u5b9a\u6210\u5206", "result": "\u5728FLUX\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDVE\u5728\u591a\u79cd\u6982\u5ff5\u64e6\u9664\u4efb\u52a1\uff08NSFW\u6291\u5236\u3001\u827a\u672f\u98ce\u683c\u79fb\u9664\u3001\u5bf9\u8c61\u64e6\u9664\uff09\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027", "conclusion": "DVE\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u6d41\u5339\u914d\u6a21\u578b\u8bbe\u8ba1\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u901f\u5ea6\u573a\u7684\u5dee\u5206\u65b9\u5411\u7ed3\u6784\u5b9e\u73b0\u7cbe\u786e\u6982\u5ff5\u6291\u5236\uff0c\u4e3a\u5b89\u5168\u53ef\u63a7\u7684\u751f\u6210\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02133", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02133", "abs": "https://arxiv.org/abs/2602.02133", "authors": ["Sangwoo Shin", "BumJun Kim", "Kyelim Lee", "Moongyu Jeon", "Albert No"], "title": "Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics", "comment": null, "summary": "Autoregressive language models (ARMs) suffer from the reversal curse: after learning that \"$A$ is $B$\", they often fail on the reverse query \"$B$ is $A$\". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing \"[MASK] is $B$\" during training does not necessarily teach the model to handle the reverse prompt \"$B$ is [MASK]\". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.", "AI": {"tldr": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u67b6\u6784\u7ed3\u6784\u548c\u8bad\u7ec3\u4ea4\u4e92\u7f13\u89e3\u4e86\u53cd\u8f6c\u8bc5\u5492\uff0c\u800c\u81ea\u56de\u5f52\u6a21\u578b\u5219\u6301\u7eed\u5b58\u5728\u6b64\u95ee\u9898", "motivation": "\u63a2\u7a76\u4e3a\u4ec0\u4e48\u57fa\u4e8e\u63a9\u7801\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b(MDMs)\u6bd4\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b(ARMs)\u5728\u53cd\u8f6c\u8bc5\u5492\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5c3d\u7ba1\u4e24\u8005\u90fd\u4f7f\u7528\u4efb\u610f\u987a\u5e8f\u8bad\u7ec3\u76ee\u6807", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1\uff09\u5206\u6790\u5355\u5c42Transformer\u7f16\u7801\u5668\u4e2d\u6743\u91cd\u5171\u4eab\u5982\u4f55\u4f7f\u524d\u5411\u548c\u53cd\u5411\u6ce8\u610f\u529b\u5206\u6570\u6b63\u76f8\u5173\uff1b2\uff09\u8bc1\u660e\u5bf9\u5e94\u68af\u5ea6\u5bf9\u9f50\uff0c\u6700\u5c0f\u5316\u524d\u5411\u635f\u5931\u4e5f\u51cf\u5c11\u53cd\u5411\u635f\u5931\uff1b3\uff09\u5728\u53d7\u63a7\u73a9\u5177\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c", "result": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5bf9\u53cd\u8f6c\u8bc5\u5492\u7684\u7f13\u89e3\u6e90\u4e8e\u67b6\u6784\u7ed3\u6784\u53ca\u5176\u4e0e\u8bad\u7ec3\u7684\u4ea4\u4e92\uff0c\u7279\u522b\u662f\u6743\u91cd\u5171\u4eab\u5bfc\u81f4\u7684\u524d\u5411-\u53cd\u5411\u8026\u5408\u548c\u68af\u5ea6\u5bf9\u9f50\uff0c\u800c\u975e\u4ec5\u56e0\u4efb\u610f\u987a\u5e8f\u8bad\u7ec3\u76ee\u6807", "conclusion": "MDMs\u90e8\u5206\u514b\u670d\u53cd\u8f6c\u8bc5\u5492\u7684\u539f\u56e0\u662f\u67b6\u6784\u8bbe\u8ba1\uff08Transformer\u7f16\u7801\u5668\uff09\u548c\u8bad\u7ec3\u52a8\u6001\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5f3aARMs\u4ecd\u6301\u7eed\u5b58\u5728\u6b64\u5931\u8d25\u6a21\u5f0f"}}
{"id": "2602.00781", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00781", "abs": "https://arxiv.org/abs/2602.00781", "authors": ["Jiamin Xu", "Kyra Gan"], "title": "Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding", "comment": null, "summary": "Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\\mathcal{O}(\\max((K-1),C_{K-1})\\sqrt{SAT\\log(T)})$ regret for any $K \\geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6709\u9650\u65f6\u57df\u975e\u7247\u6bb5\u5f0fMDP\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528K\u6b65\u524d\u77bbQ\u51fd\u6570\u548c\u9608\u503c\u673a\u5236\uff0c\u5728\u7406\u8bba\u4e0a\u8fbe\u5230\u6700\u4f18\u540e\u6094\u754c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65e0\u9650\u65f6\u57df\u65b9\u6cd5\u4f9d\u8d56\u6298\u6263\u6536\u7f29\uff0c\u4e0d\u9002\u7528\u4e8e\u6709\u9650\u65f6\u57dfMDP\u4e2d\u9700\u8981\u4f30\u8ba1\u5230\u56fa\u5b9a\u7ec8\u6b62\u65f6\u95f4\u56de\u62a5\u7684\u95ee\u9898\u3002\u6709\u9650\u65f6\u57df\u975e\u7247\u6bb5\u5f0f\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165K\u6b65\u524d\u77bbQ\u51fd\u6570\uff0c\u5c06\u89c4\u5212\u622a\u65ad\u5230\u672a\u6765K\u6b65\uff1b\u91c7\u7528\u9608\u503c\u673a\u5236\uff0c\u4ec5\u5f53\u4f30\u8ba1\u7684K\u6b65\u524d\u77bb\u503c\u8d85\u8fc7\u65f6\u53d8\u9608\u503c\u65f6\u624d\u9009\u62e9\u52a8\u4f5c\uff1b\u63d0\u51fa\u9ad8\u6548\u7684\u8868\u683c\u5b66\u4e60\u7b97\u6cd5\uff0c\u968f\u65f6\u95f4\u81ea\u9002\u5e94\u589e\u52a0K\u4ee5\u5e73\u8861\u524d\u77bb\u6df1\u5ea6\u548c\u4f30\u8ba1\u65b9\u5dee\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1aK=1\u65f6\u8fbe\u5230\u6781\u5c0f\u6781\u5927\u6700\u4f18\u5e38\u6570\u540e\u6094\uff1bK\u22652\u65f6\u540e\u6094\u754c\u4e3aO(max((K-1),C_{K-1})\u221a(SAT log T))\u3002\u5b9e\u9a8c\u5728JumpRiverswim\u3001FrozenLake\u548cAnyTrading\u7b49\u73af\u5883\u4e2d\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u8868\u683cRL\u65b9\u6cd5\u7684\u7d2f\u79ef\u5956\u52b1\u3002", "conclusion": "\u63d0\u51fa\u7684K\u6b65\u524d\u77bbQ\u51fd\u6570\u548c\u9608\u503c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6709\u9650\u65f6\u57dfMDP\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6709\u9650\u65f6\u57df\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01999", "abs": "https://arxiv.org/abs/2602.01999", "authors": ["Yanrui Du", "Yibo Gao", "Sendong Zhao", "Jiayun Li", "Haochun Wang", "Qika Lin", "Kai He", "Bing Qin", "Mengling Feng"], "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs", "comment": null, "summary": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86R1\u98ce\u683c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u81ea\u6211\u53cd\u601d\u884c\u4e3a\u7684\u5185\u5728\u673a\u5236\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6fc0\u6d3b\u8f68\u8ff9\u53d1\u73b0\u4e86\u4e00\u4e2a\u4ece\u6f5c\u5728\u76d1\u63a7\u5230\u663e\u6027\u53cd\u601d\u7684\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u8fdb\u7a0b\u3002", "motivation": "R1\u98ce\u683c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u81ea\u6211\u53cd\u601d\u80fd\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u53cd\u601d\u884c\u4e3a\u7684\u8d77\u59cb\u70b9\u5e76\u8ffd\u8e2a\u5176\u5c42\u95f4\u6fc0\u6d3b\u8f68\u8ff9\uff0c\u6765\u7406\u89e3\u8fd9\u79cd\u81ea\u6211\u53cd\u601d\u884c\u4e3a\u7684\u5185\u5728\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u4f7f\u7528logit lens\u6280\u672f\u8bfb\u53d6token\u7ea7\u8bed\u4e49\uff0c\u8ffd\u8e2a\u53cd\u601d\u884c\u4e3a\u7684\u5c42\u95f4\u6fc0\u6d3b\u8f68\u8ff9\u3002\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u9636\u6bb5\u4e4b\u95f4\u7684\u56e0\u679c\u94fe\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e86\u53cd\u601d\u884c\u4e3a\u7684\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u8fdb\u7a0b\uff1a1)\u6f5c\u5728\u63a7\u5236\u5c42\uff0c\u7ebf\u6027\u65b9\u5411\u7f16\u7801\u601d\u7ef4\u9884\u7b97\u8bed\u4e49\uff1b2)\u8bed\u4e49\u67a2\u7ebd\u5c42\uff0c\u51fa\u73b0\u8bdd\u8bed\u7ea7\u7ebf\u7d22\uff08\u8f6c\u6298\u70b9\u548c\u603b\u7ed3\u7ebf\u7d22\uff09\u5e76\u4e3b\u5bfc\u6982\u7387\u8d28\u91cf\uff1b3)\u884c\u4e3a\u663e\u6027\u5c42\uff0c\u53cd\u601d\u884c\u4e3atoken\u7684\u4f3c\u7136\u5ea6\u4e0a\u5347\u76f4\u81f3\u88ab\u91c7\u6837\u3002\u5e72\u9884\u5b9e\u9a8c\u63ed\u793a\u4e86\u8fd9\u4e9b\u9636\u6bb5\u4e4b\u95f4\u7684\u56e0\u679c\u94fe\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u53cd\u601d\u8fc7\u7a0b\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5143\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4ece\u6f5c\u5728\u76d1\u63a7\u5230\u8bdd\u8bed\u7ea7\u8c03\u8282\uff0c\u6700\u7ec8\u5230\u663e\u6027\u81ea\u6211\u53cd\u601d\u3002\u8fd9\u79cd\u7406\u89e3\u6709\u52a9\u4e8e\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u5e76\u4e3a\u6539\u8fdb\u6a21\u578b\u53cd\u601d\u80fd\u529b\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.01095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01095", "abs": "https://arxiv.org/abs/2602.01095", "authors": ["Jinghong Zheng", "Changlong Jiang", "Yang Xiao", "Jiaqi Li", "Haohong Kuang", "Hang Xu", "Ran Wang", "Zhiguo Cao", "Min Du", "Joey Tianyi Zhou"], "title": "PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space", "comment": "Accepted at NeurIPS 2025", "summary": "3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.", "AI": {"tldr": "PandaPose\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c062D\u59ff\u6001\u5148\u9a8c\u4f20\u64ad\u52303D\u951a\u70b9\u7a7a\u95f4\u4f5c\u4e3a\u7edf\u4e00\u4e2d\u95f4\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d2D\u59ff\u6001\u8bef\u5dee\u4f20\u64ad\u548c\u81ea\u906e\u6321\u5904\u7406\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e2D\u7279\u5f81\u5efa\u7acb\u76f4\u63a5\u7684\u5173\u8282\u5230\u5173\u8282\u6620\u5c04\uff0c\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a1) \u4ece\u8f93\u5165\u9884\u6d4b\u76842D\u59ff\u6001\u52303D\u9884\u6d4b\u7684\u4e0d\u53ef\u907f\u514d\u7684\u8bef\u5dee\u4f20\u64ad\uff1b2) \u5904\u7406\u81ea\u906e\u6321\u60c5\u51b5\u7684\u56fa\u6709\u56f0\u96be\u3002", "method": "\u63d0\u51faPandaPose\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u89c4\u8303\u5750\u6807\u7cfb\u4e2d\u7684\u5173\u8282\u7ea73D\u951a\u70b9\uff0c\u63d0\u4f9b\u51c6\u786e\u9c81\u68d2\u7684\u5148\u9a8c\uff1b2) \u6df1\u5ea6\u611f\u77e5\u7684\u5173\u8282\u7ea7\u7279\u5f81\u63d0\u5347\uff0c\u5206\u5c42\u6574\u5408\u6df1\u5ea6\u4fe1\u606f\u89e3\u51b3\u81ea\u906e\u6321\u6b67\u4e49\uff1b3) \u951a\u70b9-\u7279\u5f81\u4ea4\u4e92\u89e3\u7801\u5668\uff0c\u5c063D\u951a\u70b9\u4e0e\u63d0\u5347\u7684\u7279\u5f81\u7ed3\u5408\u751f\u6210\u7edf\u4e00\u7684\u951a\u70b9\u67e5\u8be2\uff0c\u8fdb\u4e00\u6b65\u7528\u4e8e\u951a\u70b9\u5230\u5173\u8282\u7684\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u5728Human3.6M\u3001MPI-INF-3DHP\u548c3DPW\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5728Human3.6M\u7684\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u76f8\u6bd4SOTA\u65b9\u6cd5\u8bef\u5dee\u51cf\u5c11\u4e8614.7%\uff0c\u5b9a\u6027\u6bd4\u8f83\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PandaPose\u901a\u8fc7\u521b\u65b0\u76843D\u951a\u70b9\u7a7a\u95f4\u8868\u793a\u548c\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u63d0\u5347\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u548c\u81ea\u906e\u6321\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.02136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02136", "abs": "https://arxiv.org/abs/2602.02136", "authors": ["Yingsha Xie", "Tiansheng Huang", "Enneng Yang", "Rui Min", "Wenjie Lu", "Xiaochun Cao", "Naiqiang Tan", "Li Shen"], "title": "Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models", "comment": "Code will be released soon", "summary": "Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \\textbf{+30.2\\%} on DirectRefusal and \\textbf{+21.2\\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \\textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.", "AI": {"tldr": "\u63d0\u51faDGR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5206\u5e03\uff0c\u51cf\u5c11\u5b89\u5168\u5bf9\u9f50\u5e26\u6765\u7684\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff08\u5b89\u5168\u7a0e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u901a\u5e38\u4ece\u5916\u90e8\u5927\u8bed\u8a00\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u4e2d\u84b8\u998f\u5b89\u5168\u63a8\u7406\u8f68\u8ff9\u548c\u7b54\u6848\uff0c\u4f46\u8fd9\u4e9b\u4e0e\u9700\u8981\u5bf9\u9f50\u7684\u76ee\u6807\u6a21\u578b\u5b58\u5728\u5206\u5e03\u5dee\u5f02\uff0c\u8fd9\u79cd\u5206\u5e03\u5dee\u5f02\u88ab\u8ba4\u4e3a\u662f\u5bfc\u81f4\u76ee\u6807\u6a21\u578b\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51faDGR\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u7684\u5206\u5e03\u5916\u5b89\u5168\u63a8\u7406\u6570\u636e\u96c6\u8f6c\u6362\u548c\u7cbe\u70bc\uff0c\u4f7f\u5176\u4e0e\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5206\u5e03\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u5206\u5e03\u5dee\u5f02\u6765\u7f13\u89e3\u5b89\u5168\u7a0e\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1) DGR\u6709\u6548\u7f13\u89e3\u5b89\u5168\u7a0e\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u80fd\uff0c\u76f8\u6bd4Vanilla SFT\u5728DirectRefusal\u4e0a\u5e73\u5747\u63a8\u7406\u51c6\u786e\u7387\u63d0\u534730.2%\uff0c\u5728R1-ACT\u4e0a\u63d0\u534721.2%\uff1b2) \u63a8\u7406\u80fd\u529b\u4e0b\u964d\u7a0b\u5ea6\u4e0e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u76f8\u5173\uff1b3) \u4ec5\u970010\u4e2a\u6837\u672c\u5c31\u80fd\u6fc0\u6d3b\u6709\u6548\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u8868\u660e\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u4e3b\u8981\u4f5c\u4e3a\u6fc0\u6d3b\u6f5c\u5728\u77e5\u8bc6\u7684\u673a\u5236\u3002", "conclusion": "\u5206\u5e03\u4e00\u81f4\u6027\u5bf9\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u4e3b\u8981\u4f5c\u4e3a\u6fc0\u6d3b\u6f5c\u5728\u77e5\u8bc6\u7684\u673a\u5236\u3002DGR\u65b9\u6cd5\u4e3a\u51cf\u5c11\u5b89\u5168\u5bf9\u9f50\u5e26\u6765\u7684\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00788", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00788", "abs": "https://arxiv.org/abs/2602.00788", "authors": ["Md Abir Hossen", "Mohammad Ali Javidian", "Vignesh Narayanan", "Jason M. O'Kane", "Pooyan Jamshidi"], "title": "Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors", "comment": null, "summary": "Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.", "AI": {"tldr": "RESCUE\u662f\u4e00\u79cd\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u6a21\u578b\u6765\u6539\u8fdb\u4f20\u7edf\u65b9\u6cd5\uff0c\u5f53\u4f4e\u4fdd\u771f\u5ea6\u4ee3\u7406\u4e0e\u76ee\u6807\u4fdd\u771f\u5ea6\u5bf9\u9f50\u4e0d\u4f73\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u8f93\u5165\u3001\u4fdd\u771f\u5ea6\u548c\u76ee\u6807\u4e4b\u95f4\u7684\u5173\u8054\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u4e0d\u662f\u56e0\u679c\u673a\u5236\uff0c\u5f53\u4f4e\u4fdd\u771f\u5ea6\u4ee3\u7406\u4e0e\u76ee\u6807\u4fdd\u771f\u5ea6\u5bf9\u9f50\u4e0d\u4f73\u65f6\u8868\u73b0\u8f83\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cfb\u7edf\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u65b9\u6cd5\u3002", "method": "RESCUE\u901a\u8fc7\u5b66\u4e60\u6355\u6349\u8f93\u5165\u3001\u4fdd\u771f\u5ea6\u548c\u76ee\u6807\u4e4b\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u6784\u5efa\u7f16\u7801\u5e72\u9884\u6548\u5e94\u7684\u6982\u7387\u591a\u4fdd\u771f\u5ea6\u4ee3\u7406\u6a21\u578b\u3002\u5229\u7528\u56e0\u679c\u7ed3\u6784\uff0c\u5f15\u5165\u56e0\u679c\u8d85\u4f53\u79ef\u77e5\u8bc6\u68af\u5ea6\u91c7\u96c6\u7b56\u7565\u6765\u9009\u62e9\u8f93\u5165-\u4fdd\u771f\u5ea6\u5bf9\uff0c\u5e73\u8861\u9884\u671f\u7684\u591a\u76ee\u6807\u6539\u8fdb\u548c\u6210\u672c\u3002", "result": "\u5728\u673a\u5668\u4eba\u3001\u673a\u5668\u5b66\u4e60\uff08AutoML\uff09\u548c\u533b\u7597\u4fdd\u5065\u7b49\u9886\u57df\u7684\u5408\u6210\u548c\u5b9e\u9645\u95ee\u9898\u4e0a\uff0cRESCUE\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u591a\u4fdd\u771f\u5ea6\u4f18\u5316\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u5c06\u56e0\u679c\u8ba1\u7b97\u878d\u5165\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\uff0cRESCUE\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4f4e\u4fdd\u771f\u5ea6\u4ee3\u7406\u4e0e\u76ee\u6807\u4fdd\u771f\u5ea6\u5bf9\u9f50\u4e0d\u4f73\u7684\u60c5\u51b5\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.02007", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02007", "abs": "https://arxiv.org/abs/2602.02007", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Hanqi Yan", "Yulan He", "Lin Gui"], "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation", "comment": null, "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.", "AI": {"tldr": "xMemory\u63d0\u51fa\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026-\u805a\u5408\u65b9\u6cd5\u89e3\u51b3\u667a\u80fd\u4f53\u8bb0\u5fc6\u68c0\u7d22\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edfRAG\u5728\u5bf9\u8bdd\u6d41\u4e2d\u8868\u73b0\u66f4\u597d", "motivation": "\u4f20\u7edfRAG\u9488\u5bf9\u5927\u89c4\u6a21\u5f02\u6784\u8bed\u6599\u5e93\u8bbe\u8ba1\uff0c\u800c\u667a\u80fd\u4f53\u8bb0\u5fc6\u662f\u6709\u9650\u3001\u8fde\u8d2f\u7684\u5bf9\u8bdd\u6d41\uff0c\u5177\u6709\u9ad8\u5ea6\u76f8\u5173\u7684\u91cd\u590d\u5185\u5bb9\u3002\u56fa\u5b9atop-k\u76f8\u4f3c\u6027\u68c0\u7d22\u4f1a\u8fd4\u56de\u5197\u4f59\u4e0a\u4e0b\u6587\uff0c\u540e\u5904\u7406\u526a\u679d\u53ef\u80fd\u5220\u9664\u65f6\u95f4\u5173\u8054\u7684\u524d\u63d0\u6761\u4ef6\u3002", "method": "xMemory\u91c7\u7528\u89e3\u8026\u5230\u805a\u5408\u7684\u65b9\u6cd5\uff1a\u5c06\u8bb0\u5fc6\u89e3\u8026\u6210\u8bed\u4e49\u7ec4\u4ef6\uff0c\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u6027-\u8bed\u4e49\u76ee\u6807\u6307\u5bfc\u8bb0\u5fc6\u7684\u5206\u5272\u4e0e\u5408\u5e76\uff0c\u4fdd\u6301\u53ef\u641c\u7d22\u4e14\u5fe0\u5b9e\u7684\u9ad8\u5c42\u8282\u70b9\u7ec4\u7ec7\u3002\u63a8\u7406\u65f6\u91c7\u7528\u81ea\u4e0a\u800c\u4e0b\u68c0\u7d22\uff0c\u9009\u62e9\u7d27\u51d1\u591a\u6837\u7684\u4e3b\u9898\u548c\u8bed\u4e49\u3002", "result": "\u5728LoCoMo\u548cPerLTQA\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e09\u79cd\u6700\u65b0LLM\u7684\u5b9e\u9a8c\u663e\u793a\uff0cxMemory\u5728\u7b54\u6848\u8d28\u91cf\u548ctoken\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u68c0\u7d22\u5e94\u8d85\u8d8a\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u5728\u6f5c\u5728\u7ec4\u4ef6\u4e0a\u64cd\u4f5c\u3002xMemory\u7684\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u80fd\u6709\u6548\u5904\u7406\u667a\u80fd\u4f53\u8bb0\u5fc6\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u68c0\u7d22\u3002"}}
{"id": "2602.01101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01101", "abs": "https://arxiv.org/abs/2602.01101", "authors": ["Felix Breiteneder", "Mohammad Belal", "Muhammad Saad Saeed", "Shahed Masoudian", "Usman Naseem", "Kulshrestha Juhi", "Markus Schedl", "Shah Nawaz"], "title": "Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning", "comment": "Accepted at WWW2026", "summary": "Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u6a21\u6001\u4e0d\u5b8c\u6574\u6570\u636e\u4e0b\u7684\u6709\u5bb3\u8868\u60c5\u5305\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u72ec\u7acb\u6295\u5f71\u5b66\u4e60\u591a\u6a21\u6001\u5171\u4eab\u8868\u793a\u7684\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u7f3a\u5931\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e92\u8054\u7f51\u8868\u60c5\u5305\u662f\u5f3a\u5927\u7684\u4f20\u64ad\u5de5\u5177\uff0c\u4f46\u53ef\u80fd\u88ab\u7528\u4e8e\u4f20\u64ad\u4ec7\u6068\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6a21\u6001\u5b8c\u6574\u6570\u636e\uff08\u6587\u672c\u548c\u56fe\u50cf\uff09\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6587\u672c\u53ef\u80fd\u56e0OCR\u8d28\u91cf\u5dee\u7b49\u539f\u56e0\u7f3a\u5931\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u6a21\u6001\u4e0d\u5b8c\u6574\u6570\u636e\u4e0b\u7684\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u6295\u5f71\u5b66\u4e60\u591a\u6a21\u6001\u7684\u5171\u4eab\u8868\u793a\u3002\u8fd9\u4e9b\u5171\u4eab\u8868\u793a\u53ef\u4ee5\u5728\u6570\u636e\u6a21\u6001\u4e0d\u5b8c\u6574\u65f6\u88ab\u5229\u7528\uff0c\u7279\u522b\u662f\u5f53\u6587\u672c\u7f3a\u5931\u65f6\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u7f3a\u5931\u65f6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6574\u5408\u89c6\u89c9\u7279\u5f81\uff0c\u51cf\u5c11\u5bf9\u6587\u672c\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u5728\u6587\u672c\u4fe1\u606f\u7f3a\u5931\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5728\u6709\u5bb3\u8868\u60c5\u5305\u68c0\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u65b9\u9762\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u6a21\u6001\u4e0d\u5b8c\u6574\u6570\u636e\u4e0b\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02158", "abs": "https://arxiv.org/abs/2602.02158", "authors": ["Sarah Nassar"], "title": "Traffic-Aware Navigation in Road Networks", "comment": null, "summary": "This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cd\u56fe\u641c\u7d22\u7b97\u6cd5\u5728\u91d1\u65af\u987f\u9053\u8def\u7f51\u7edc\u4ea4\u901a\u611f\u77e5\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff1aFloyd-Warshall-Ingerman\uff08\u5355\u6b21\u591a\u67e5\u8be2\u9884\u5904\u7406\uff09\u3001Dijkstra\u548cA*\uff08\u8fde\u7eed\u5355\u67e5\u8be2\u5b9e\u65f6\u641c\u7d22\uff09\u3001Yen\u7b97\u6cd5\uff08\u7ed3\u5408\u4e24\u8005\uff0c\u5148\u627eK\u6761\u6700\u77ed\u8def\u5f84\u518d\u5b9e\u65f6\u8fed\u4ee3\uff09\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u56fe\u641c\u7d22\u7b97\u6cd5\u5728\u4ea4\u901a\u611f\u77e5\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u7279\u5b9a\u90e8\u7f72\u573a\u666f\u9009\u62e9\u6700\u4f73\u65b9\u6848\u63d0\u4f9b\u4f9d\u636e\u3002\u9700\u8981\u5728\u9884\u5904\u7406\u65f6\u95f4\u3001\u5b9e\u65f6\u8ba1\u7b97\u901f\u5ea6\u3001\u8def\u5f84\u6700\u4f18\u6027\u548c\u4ea4\u901a\u611f\u77e5\u80fd\u529b\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u5728\u91d1\u65af\u987f\u9053\u8def\u7f51\u7edc\u4e0a\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\uff1a1\uff09Floyd-Warshall-Ingerman\u7b97\u6cd5\u8fdb\u884c\u5355\u6b21\u591a\u67e5\u8be2\u9884\u5904\u7406\uff1b2\uff09Dijkstra\u548cA*\u7b97\u6cd5\u8fdb\u884c\u8fde\u7eed\u5355\u67e5\u8be2\u5b9e\u65f6\u641c\u7d22\uff1b3\uff09Yen\u7b97\u6cd5\u7ed3\u5408\u4e24\u8005\uff0c\u5148\u627e\u5230\u524dK\u6761\u6700\u77ed\u8def\u5f84\uff0c\u7136\u540e\u5728\u5b9e\u65f6\u4e2d\u8fed\u4ee3\u8fd9\u4e9b\u8def\u5f84\u3002", "result": "Dijkstra\u548cA*\u7b97\u6cd5\u4ea7\u751f\u6700\u4ea4\u901a\u611f\u77e5\u7684\u6700\u4f18\u89e3\uff0c\u4e14\u9700\u8981\u6700\u5c11\u7684\u9884\u5904\u7406\uff1bFloyd-Warshall-Ingerman\u5b9e\u65f6\u901f\u5ea6\u6700\u5feb\uff0c\u4f46\u63d0\u4f9b\u7684\u662f\u57fa\u4e8e\u8ddd\u79bb\u7684\u8def\u5f84\uff0c\u65e0\u4ea4\u901a\u611f\u77e5\uff1bYen\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u9884\u5904\u7406\uff0c\u4f46\u5728\u8fd0\u884c\u901f\u5ea6\u548c\u6700\u4f18\u6027\u65b9\u9762\u5e73\u8861\u4e86\u524d\u4e24\u79cd\u65b9\u6cd5\u3002", "conclusion": "\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u5176\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u6839\u636e\u5177\u4f53\u90e8\u7f72\u73af\u5883\u7684\u60c5\u51b5\u6743\u8861\u9009\u62e9\u6700\u4f73\u5b9a\u5236\u65b9\u6848\u3002\u4ea4\u901a\u611f\u77e5\u5bfc\u822a\u9700\u8981\u7efc\u5408\u8003\u8651\u9884\u5904\u7406\u9700\u6c42\u3001\u5b9e\u65f6\u8ba1\u7b97\u6548\u7387\u3001\u8def\u5f84\u6700\u4f18\u6027\u548c\u4ea4\u901a\u4fe1\u606f\u5229\u7528\u7b49\u56e0\u7d20\u3002"}}
{"id": "2602.00791", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.00791", "abs": "https://arxiv.org/abs/2602.00791", "authors": ["Shahryar Zehtabi", "Dong-Jun Han", "Seyyedali Hosseinalipour", "Christopher Brinton"], "title": "Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning", "comment": "32 pages, 5 figures", "summary": "Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\\textit{Sporadic Gradient Tracking}$ ($\\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\\texttt{Spod-GT}$ compared to well-known GT baselines.", "AI": {"tldr": "Spod-GT\uff1a\u9996\u4e2a\u5728\u5b9a\u5411\u56fe\u4e0a\u7ed3\u5408\u68af\u5ea6\u8ffd\u8e2a\u548c\u5ba2\u6237\u7aef\u8d44\u6e90\u5f02\u6784\u6027\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u652f\u6301\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u68af\u5ea6\u8ba1\u7b97\u9891\u7387\u548c\u5f02\u6784\u975e\u5bf9\u79f0\u901a\u4fe1\u9891\u7387\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff08DFL\uff09\u9700\u8981\u5728\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u8d28\u6027\u548c\u8d44\u6e90\u53ef\u7528\u6027\u5dee\u5f02\u4e24\u5927\u6311\u6218\u4e0a\u53d6\u5f97\u5e73\u8861\u3002\u73b0\u6709\u7814\u7a76\u5206\u522b\u89e3\u51b3\u4e86\u68af\u5ea6\u8ffd\u8e2a\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u8d44\u6e90\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faSporadic Gradient Tracking\uff08Spod-GT\uff09\u7b97\u6cd5\uff0c\u5728\u4e00\u822c\u5b9a\u5411\u56fe\u4e0a\u7ed3\u5408\u68af\u5ea6\u8ffd\u8e2a\u6280\u672f\uff0c\u5141\u8bb8\uff1a1\uff09\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u68af\u5ea6\u8ba1\u7b97\u9891\u7387\uff1b2\uff09\u5f02\u6784\u4e14\u975e\u5bf9\u79f0\u7684\u901a\u4fe1\u9891\u7387\u3002\u7b97\u6cd5\u5728\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u548c\u68af\u5ea6\u591a\u6837\u6027\u5047\u8bbe\u66f4\u5bbd\u677e\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6536\u655b\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u5ba2\u6237\u7aef\u95f4\u6b47\u53c2\u4e0e\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5728\u5b9a\u5411\u56fe\u4e0a\u8fbe\u6210\u5171\u8bc6\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002\u5728\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0cSpod-GT\u4f18\u4e8e\u5df2\u77e5\u7684\u68af\u5ea6\u8ffd\u8e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Spod-GT\u662f\u9996\u4e2a\u5728\u5b9a\u5411\u56fe\u4e0a\u7edf\u4e00\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u8d44\u6e90\u5f02\u6784\u6027\u7684DFL\u7b97\u6cd5\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u5ba2\u6237\u7aef\u8d44\u6e90\u5dee\u5f02\u5927\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02010", "abs": "https://arxiv.org/abs/2602.02010", "authors": ["Kang Liu", "Yongkang Liu", "Xiaocui Yang", "Peidong Wang", "Wen Zhang", "Shi Feng", "Yifei Zhang", "Daling Wang"], "title": "NEAT: Neuron-Based Early Exit for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) often suffer from \\emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \\textbf{NEAT}, a \\textbf{N}euron-based \\textbf{E}arly re\\textbf{A}soning exi\\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\\% to 28\\% when averaged over the four benchmarks, while maintaining accuracy.", "AI": {"tldr": "NEAT\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u52a8\u6001\u7684\u8bad\u7ec3\u514d\u8d39\u65e9\u671f\u63a8\u7406\u9000\u51fa\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u6d4b\u795e\u7ecf\u5143\u7ea7\u6fc0\u6d3b\u6a21\u5f0f\u6765\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5e73\u5747\u51cf\u5c1122-28%\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5373\u5728\u5df2\u7ecf\u5f97\u5230\u6b63\u786e\u7b54\u6848\u540e\u4ecd\u751f\u6210\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u3002\u73b0\u6709\u65e9\u671f\u63a8\u7406\u9000\u51fa\u65b9\u6cd5\u4f9d\u8d56\u8f93\u51fa\u7ea7\u542f\u53d1\u5f0f\u6216\u8bad\u7ec3\u63a2\u6d4b\u6a21\u578b\uff0c\u9700\u8981\u989d\u5916\u8ba1\u7b97\u6216\u5916\u90e8\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "NEAT\u901a\u8fc7\u8bc6\u522b\u9000\u51fa\u76f8\u5173\u795e\u7ecf\u5143\u5e76\u8ddf\u8e2a\u5176\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u7684\u65e9\u671f\u9000\u51fa\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u52a8\u6001\u89e6\u53d1\u65e9\u671f\u9000\u51fa\u6216\u6291\u5236\u53cd\u601d\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u63a8\u7406\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684\u6a21\u578b\u4e0a\uff0cNEAT\u5e73\u5747\u51cf\u5c1122%\u523028%\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "NEAT\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u8bad\u7ec3\u514d\u8d39\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u5927\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u76d1\u6d4b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6216\u6807\u6ce8\u6570\u636e\u3002"}}
{"id": "2602.01118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01118", "abs": "https://arxiv.org/abs/2602.01118", "authors": ["Jingjing Wang", "Qirui Hu", "Chong Bao", "Yuke Zhu", "Hujun Bao", "Zhaopeng Cui", "Guofeng Zhang"], "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions", "comment": null, "summary": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.", "AI": {"tldr": "LightCity\u662f\u4e00\u4e2a\u7528\u4e8e\u57ce\u5e02\u573a\u666f\u9006\u6e32\u67d3\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u5149\u7167\u6761\u4ef6\u548c\u4e30\u5bcc\u7684\u5c5e\u6027\u6807\u6ce8\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u76f8\u5173\u4efb\u52a1\u3002", "motivation": "\u57ce\u5e02\u573a\u666f\u9006\u6e32\u67d3\u9762\u4e34\u590d\u6742\u5149\u7167\u6761\u4ef6\uff08\u591a\u5149\u6e90\u3001\u95f4\u63a5\u5149\u548c\u9634\u5f71\uff09\u7684\u6311\u6218\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u6765\u7814\u7a76\u8fd9\u4e9b\u6311\u6218\u5bf9\u5185\u5728\u5206\u89e3\u548c3D\u91cd\u5efa\u7684\u5f71\u54cd\u3002", "method": "\u521b\u5efa\u4e86LightCity\u6570\u636e\u96c6\uff0c\u5305\u542b300\u591a\u4e2a\u5929\u7a7a\u8d34\u56fe\u3001\u9ad8\u5ea6\u53ef\u63a7\u7684\u5149\u7167\u3001\u8857\u666f\u548c\u822a\u62cd\u89c6\u89d2\u76845\u4e07\u591a\u5f20\u56fe\u50cf\uff0c\u4ee5\u53ca\u6df1\u5ea6\u3001\u6cd5\u7ebf\u3001\u6750\u8d28\u7ec4\u4ef6\u3001\u76f4\u63a5\u5149\u548c\u95f4\u63a5\u5149\u7b49\u4e30\u5bcc\u5c5e\u6027\u3002", "result": "LightCity\u6570\u636e\u96c6\u4e3a\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4e09\u4e2a\u57fa\u672c\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "LightCity\u586b\u8865\u4e86\u57ce\u5e02\u9006\u6e32\u67d3\u7814\u7a76\u7684\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u4e3a\u5904\u7406\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u5185\u5728\u5206\u89e3\u548c3D\u91cd\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.02188", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02188", "abs": "https://arxiv.org/abs/2602.02188", "authors": ["Xia Jiang", "Jing Chen", "Cong Zhang", "Jie Gao", "Chengpeng Hu", "Chenhao Zhang", "Yaoxin Wu", "Yingqian Zhang"], "title": "Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization", "comment": null, "summary": "While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \\textbf{N}atural \\textbf{L}anguage \\textbf{C}ombinatorial \\textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.", "AI": {"tldr": "NLCO\u662f\u4e00\u4e2a\u81ea\u7136\u8bed\u8a00\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u7ed9\u5b9a\u8bed\u8a00\u63cf\u8ff0\u51b3\u7b56\u573a\u666f\u4e0b\u76f4\u63a5\u8f93\u51fa\u79bb\u6563\u89e3\u7684\u80fd\u529b\uff0c\u8986\u76d643\u4e2a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u53d1\u73b0LLM\u5728\u5c0f\u5b9e\u4f8b\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u968f\u7740\u89c4\u6a21\u589e\u5927\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec4\u5408\u4f18\u5316\uff08\u641c\u7d22\u9ad8\u7ef4\u89e3\u7a7a\u95f4\u5e76\u6ee1\u8db3\u786c\u7ea6\u675f\uff09\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faNLCO\u57fa\u51c6\uff0c\u5305\u542b43\u4e2a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u56db\u5c42\u5206\u7c7b\u6cd5\uff08\u53d8\u91cf\u7c7b\u578b\u3001\u7ea6\u675f\u65cf\u3001\u5168\u5c40\u6a21\u5f0f\u3001\u76ee\u6807\u7c7b\u522b\uff09\u7ec4\u7ec7\uff0c\u63d0\u4f9b\u6c42\u89e3\u5668\u6807\u6ce8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u53ef\u884c\u6027\u3001\u89e3\u6700\u4f18\u6027\u548c\u63a8\u7406\u6548\u7387\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u3002", "result": "\u9ad8\u6027\u80fdLLM\u5728\u5c0f\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u884c\u6027\u548c\u89e3\u8d28\u91cf\uff0c\u4f46\u968f\u7740\u5b9e\u4f8b\u89c4\u6a21\u589e\u5927\uff0c\u5373\u4f7f\u4f7f\u7528\u66f4\u591atoken\u8fdb\u884c\u63a8\u7406\uff0c\u4e24\u8005\u90fd\u4f1a\u4e0b\u964d\uff1b\u96c6\u5408\u4efb\u52a1\u76f8\u5bf9\u5bb9\u6613\uff0c\u800c\u56fe\u7ed3\u6784\u95ee\u9898\u548c\u74f6\u9888\u76ee\u6807\u5bfc\u81f4\u66f4\u591a\u5931\u8d25\u3002", "conclusion": "LLM\u5728\u7ec4\u5408\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\u548c\u590d\u6742\u7ed3\u6784\u95ee\u9898\u65f6\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347LLM\u7684\u7ec4\u5408\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2602.00792", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00792", "abs": "https://arxiv.org/abs/2602.00792", "authors": ["Guinan Chen", "Xunpeng Huang", "Ying Sun", "Shijin Wang", "Yanyong Zhang", "Chao Wang"], "title": "Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion", "comment": "10 pages", "summary": "Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.", "AI": {"tldr": "\u63d0\u51faMasked Consistency Distillation (MCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u63a9\u7801\u6269\u6563\u5bf9\u5076\u6027\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u91c7\u6837\uff0c\u76f8\u6bd4\u968f\u673a\u84b8\u998f\u65b9\u6cd5\u83b7\u5f9716\u500d\u63a8\u7406\u52a0\u901f\u4e14\u4e0d\u635f\u5931\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u63a9\u7801\u79bb\u6563\u6269\u6563\u662f\u9ad8\u8d28\u91cf\u8bed\u8a00\u5efa\u6a21\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u5176\u63a8\u7406\u6548\u7387\u53d7\u9650\u4e8e\u7f3a\u4e4f\u786e\u5b9a\u6027\u91c7\u6837\u5de5\u5177\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6027\u80fd\u4e0d\u8db3\uff0c\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u79ef\u5206\u7b97\u5b50\uff0c\u8981\u4e48\u53ea\u80fd\u91c7\u7528\u968f\u673a\u84b8\u998f\u3002", "method": "\u5efa\u7acb\u663e\u5f0f\u7684Masked Diffusion Duality\uff0c\u8bc1\u660e\u63a9\u7801\u8fc7\u7a0b\u662f\u8fde\u7eed\u9ad8\u65af\u8fc7\u7a0b\u901a\u8fc7\u6700\u5927\u7d22\u5f15\u503c\u4fdd\u6301\u673a\u5236\u7684\u6295\u5f71\u3002\u57fa\u4e8e\u6b64\u63d0\u51faMasked Consistency Distillation (MCD)\uff0c\u5229\u7528\u5bf9\u5076\u6027\u89e3\u6790\u6784\u9020\u786e\u5b9a\u6027\u8026\u5408\u8f68\u8ff9\uff0c\u7ed5\u8fc7\u6570\u503cODE\u6c42\u89e3\u5668\u3002", "result": "MCD\u4e25\u683c\u4f18\u4e8e\u5148\u524d\u7684\u968f\u673a\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b016\u500d\u63a8\u7406\u52a0\u901f\u4e14\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002\u4e3a\u63a9\u7801\u548c\u8fde\u7eed\u6269\u6563\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u91ca\u653e\u4e86\u4e00\u81f4\u6027\u84b8\u998f\u5728\u79bb\u6563\u751f\u6210\u4e2d\u7684\u5168\u90e8\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u5efa\u7acb\u4e86\u63a9\u7801\u4e0e\u8fde\u7eed\u6269\u6563\u7684\u7406\u8bba\u8fde\u63a5\uff0c\u8fd8\u4e3a\u9ad8\u6027\u80fd\u79bb\u6563\u751f\u6210\u89e3\u9501\u4e86\u4e00\u81f4\u6027\u84b8\u998f\u7684\u5168\u90e8\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.02053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02053", "abs": "https://arxiv.org/abs/2602.02053", "authors": ["Pengyu Wang", "Benfeng Xu", "Licheng Zhang", "Shaohan Wang", "Mingxuan Du", "Chiwei Zhu", "Zhendong Mao"], "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora", "comment": "https://github.com/BstWPY/WildGraphBench", "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.", "AI": {"tldr": "WildGraphBench\u662f\u4e00\u4e2a\u65b0\u7684GraphRAG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7ef4\u57fa\u767e\u79d1\u7684\u957f\u6587\u6863\u548c\u5f02\u6784\u53c2\u8003\u6587\u6863\u6765\u8bc4\u4f30\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5305\u542b1,100\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u4e09\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u3002", "motivation": "\u73b0\u6709GraphRAG\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u4f7f\u7528\u7b80\u77ed\u3001\u7cbe\u9009\u7684\u6bb5\u843d\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5927\u89c4\u6a21\u5f02\u6784\u6587\u6863\u7684\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5229\u7528\u7ef4\u57fa\u767e\u79d1\u7684\u7ed3\u6784\uff0c\u4ece12\u4e2a\u9876\u7ea7\u4e3b\u9898\u4e2d\u91c7\u6837\u6587\u7ae0\uff0c\u4f7f\u7528\u5916\u90e8\u53c2\u8003\u6587\u6863\u4f5c\u4e3a\u68c0\u7d22\u8bed\u6599\u5e93\uff0c\u5f15\u7528\u94fe\u63a5\u7684\u9648\u8ff0\u4f5c\u4e3a\u771f\u5b9e\u7b54\u6848\uff0c\u6784\u5efa\u5305\u542b1,100\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u5355\u4e8b\u5b9eQA\u3001\u591a\u4e8b\u5b9eQA\u548c\u7ae0\u8282\u7ea7\u6458\u8981\u4e09\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524dGraphRAG\u7ba1\u9053\u5728\u8bc1\u636e\u6765\u81ea\u4e2d\u7b49\u6570\u91cf\u6765\u6e90\u65f6\u6709\u52a9\u4e8e\u591a\u4e8b\u5b9e\u805a\u5408\uff0c\u4f46\u8fd9\u79cd\u805a\u5408\u8303\u5f0f\u53ef\u80fd\u8fc7\u5ea6\u5f3a\u8c03\u9ad8\u5c42\u7ea7\u9648\u8ff0\u800c\u727a\u7272\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u5728\u6458\u8981\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5f31\u3002", "conclusion": "WildGraphBench\u586b\u8865\u4e86GraphRAG\u8bc4\u4f30\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5e73\u8861\u9ad8\u5c42\u805a\u5408\u4e0e\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2602.01127", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01127", "abs": "https://arxiv.org/abs/2602.01127", "authors": ["Matej Suchanek", "Klara Janouskova", "Ondrej Vasatko", "Jiri Matas"], "title": "Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis", "comment": null, "summary": "Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.\n  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.", "AI": {"tldr": "Koo-Fu CLIP\uff1a\u57fa\u4e8eFukunaga-Koontz\u7ebf\u6027\u5224\u522b\u5206\u6790\u7684\u76d1\u7763CLIP\u9002\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u767d\u5316\u5d4c\u5165\u7a7a\u95f4\u6291\u5236\u7c7b\u5185\u53d8\u5f02\u3001\u589e\u5f3a\u7c7b\u95f4\u533a\u5206\uff0c\u5b9e\u73b0\u7ebf\u6027\u6295\u5f71\u964d\u7ef4\uff0c\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5f3a\u5927\u7684\u901a\u7528\u8868\u793a\uff0c\u4f46\u5176\u539f\u59cb\u5d4c\u5165\u5728\u76d1\u7763\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7c7b\u95f4\u5206\u79bb\u4e0d\u8db3\u3001\u7ef4\u5ea6\u5197\u4f59\uff0c\u9700\u8981\u4f18\u5316\u4ee5\u9002\u5e94\u76d1\u7763\u5206\u7c7b\u9700\u6c42\u3002", "method": "\u63d0\u51faKoo-Fu CLIP\u65b9\u6cd5\uff0c\u57fa\u4e8eFukunaga-Koontz\u7ebf\u6027\u5224\u522b\u5206\u6790\uff0c\u5728\u767d\u5316\u5d4c\u5165\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u6291\u5236\u7c7b\u5185\u53d8\u5f02\u5e76\u589e\u5f3a\u7c7b\u95f4\u533a\u5206\uff0c\u901a\u8fc7\u95ed\u5f0f\u7ebf\u6027\u6295\u5f71\u91cd\u5851CLIP\u5d4c\u5165\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728ImageNet-1K\u4e0a\uff0c\u6700\u8fd1\u89c6\u89c9\u539f\u578b\u5206\u7c7b\u51c6\u786e\u7387\u4ece75.1%\u63d0\u5347\u81f379.1%\uff1b\u6269\u5c55\u523014K\u548c21K\u7c7b\u65f6\u4fdd\u6301\u7a33\u5b9a\u589e\u76ca\uff1b\u652f\u630110-12\u500d\u538b\u7f29\u800c\u51e0\u4e4e\u4e0d\u635f\u5931\u51c6\u786e\u7387\u3002", "conclusion": "Koo-Fu CLIP\u63d0\u4f9b\u8f7b\u91cf\u9ad8\u6548\u7684CLIP\u8868\u793a\u9002\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7c7b\u95f4\u5206\u79bb\u6027\uff0c\u652f\u6301\u5927\u89c4\u6a21\u538b\u7f29\uff0c\u9002\u7528\u4e8e\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u3002"}}
{"id": "2602.02196", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02196", "abs": "https://arxiv.org/abs/2602.02196", "authors": ["Hang Yan", "Xinyu Che", "Fangzhi Xu", "Qiushi Sun", "Zichen Ding", "Kanzhi Cheng", "Jian Zhang", "Tao Qin", "Jun Liu", "Qika Lin"], "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents", "comment": "29pages, 10 figures", "summary": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TIDE\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u5206\u6790LLM\u667a\u80fd\u4f53\u5728\u6d4b\u8bd5\u65f6\u6539\u8fdb(TTI)\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u4f18\u5316\u6548\u7387\u3001\u884c\u4e3a\u9002\u5e94\u548c\u8bb0\u5fc6\u6548\u7528\u3002", "motivation": "\u5f53\u524d\u5bf9\u81ea\u4e3bLLM\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u6539\u8fdb(TTI)\u7684\u6210\u529f\u4e0e\u5931\u8d25\u673a\u5236\u7406\u89e3\u4e0d\u8db3\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u4efb\u52a1\u4f18\u5316\u6548\u7387\u3001\u9519\u8bef\u884c\u4e3a\u540e\u7684\u9002\u5e94\u80fd\u529b\u4ee5\u53ca\u5de5\u4f5c\u8bb0\u5fc6\u7684\u5177\u4f53\u6548\u7528\u3002", "method": "\u63d0\u51fa\u6d4b\u8bd5\u65f6\u6539\u8fdb\u8bca\u65ad\u8bc4\u4f30(TIDE)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u65e0\u5173\u548c\u73af\u5883\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5c06TTI\u5206\u89e3\u4e3a\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7ef4\u5ea6\uff1a\u4efb\u52a1\u5b8c\u6210\u7684\u6574\u4f53\u65f6\u95f4\u52a8\u6001\u3001\u9012\u5f52\u5faa\u73af\u884c\u4e3a\u7684\u7ea6\u675f\u3001\u4ee5\u53ca\u7d2f\u79ef\u8bb0\u5fc6\u8d1f\u62c5\u7684\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u548c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0cTIDE\u6846\u67b6\u8868\u660e\uff0c\u63d0\u9ad8\u667a\u80fd\u4f53\u6027\u80fd\u4e0d\u4ec5\u9700\u8981\u6269\u5c55\u5185\u90e8\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u663e\u5f0f\u4f18\u5316\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4ea4\u4e92\u52a8\u6001\u3002", "conclusion": "TIDE\u6846\u67b6\u4e3a\u7406\u89e3LLM\u667a\u80fd\u4f53\u7684\u6d4b\u8bd5\u65f6\u6539\u8fdb\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u4f18\u5316\u667a\u80fd\u4f53\u6027\u80fd\u9700\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\u52a8\u6001\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.00800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00800", "abs": "https://arxiv.org/abs/2602.00800", "authors": ["Yebin Yang", "Huaijin Wu", "Fu Guo", "Lin Yao", "Xiaohan Qin", "Jingzhi Wang", "Debing Zhang", "Junchi Yan"], "title": "JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation", "comment": null, "summary": "LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.", "AI": {"tldr": "\u63d0\u51fatoken-indexed parameters\u4f5c\u4e3a\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6\uff0c\u901a\u8fc7JTok\u548cJTok-M\u5728Transformer\u5c42\u4e2d\u52a0\u5165\u8c03\u5236\u5411\u91cf\uff0c\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edfLLM\u6269\u5c55\u4f9d\u8d56\u5bc6\u96c6\u7ef4\u5ea6\uff0c\u6027\u80fd\u63d0\u5347\u4f34\u968f\u8ba1\u7b97\u6210\u672c\u7ebf\u6027\u589e\u957f\uff1bMoE\u867d\u7136\u89e3\u8026\u5bb9\u91cf\u4e0e\u8ba1\u7b97\uff0c\u4f46\u5e26\u6765\u5185\u5b58\u5f00\u9500\u548c\u786c\u4ef6\u6548\u7387\u95ee\u9898\u3002\u9700\u8981\u65b0\u7684\u6269\u5c55\u8f74\u6765\u89e3\u8026\u6a21\u578b\u5bb9\u91cf\u4e0eFLOPs", "method": "\u63d0\u51fatoken-indexed parameters\u4f5c\u4e3a\u6b63\u4ea4\u6269\u5c55\u8f74\uff0c\u5f15\u5165JTok\u548cJTok-M\uff0c\u901a\u8fc7\u8f85\u52a9\u5d4c\u5165\u8868\u68c0\u7d22\u8c03\u5236\u5411\u91cf\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9010\u5143\u7d20\u64cd\u4f5c\u8c03\u5236\u9aa8\u5e72\u7f51\u7edc\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f", "result": "\u5728650M\u523061B\u53c2\u6570\u89c4\u6a21\u7684\u5bc6\u96c6\u548cMoE\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u9a8c\u8bc1\u635f\u5931\u6301\u7eed\u964d\u4f4e\uff0c\u4e0b\u6e38\u4efb\u52a1\u663e\u8457\u63d0\u5347\uff08MMLU +4.1\uff0cARC +8.3\uff0cCEval +8.9\uff09\u3002isoFLOPs\u5206\u6790\u663e\u793aJTok-M\u5c06\u8d28\u91cf-\u8ba1\u7b97Pareto\u524d\u6cbf\u79fb\u52a835%\uff0c\u4e14token-indexed\u53c2\u6570\u5448\u73b0\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\u6269\u5c55\u884c\u4e3a", "conclusion": "token-indexed parameters\u662f\u6709\u6548\u7684\u6269\u5c55\u7ef4\u5ea6\uff0cJTok\u548cJTok-M\u4ee5\u6781\u5c0f\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9ad8\u6548\u5b9e\u73b0\u786e\u4fdd\u5f00\u9500\u8fb9\u9645\u5316\uff0c\u4e3aLLM\u6269\u5c55\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2602.02084", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02084", "abs": "https://arxiv.org/abs/2602.02084", "authors": ["Jane Luo", "Chengyu Yin", "Xin Zhang", "Qingtao Li", "Steven Liu", "Yiming Huang", "Jie Wu", "Hao Liu", "Yangyu Huang", "Yu Kang", "Fangkai Yang", "Ying Xin", "Scarlett Li"], "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder", "comment": null, "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", "AI": {"tldr": "RPG-Encoder\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7801\u4ed3\u5e93\u8868\u793a\u6846\u67b6\uff0c\u5c06\u4ed3\u5e93\u7406\u89e3\u4e0e\u751f\u6210\u89c6\u4e3a\u9006\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7f16\u7801\u539f\u59cb\u4ee3\u7801\u5230Repository Planning Graph\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u8868\u793a\u548c\u7ed3\u6784\u611f\u77e5\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u5e93\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ed3\u5e93\u4ee3\u7406\u5b58\u5728\u63a8\u7406\u8131\u8282\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u5b64\u7acb\u7684API\u6587\u6863\u6216\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u7684\u4f9d\u8d56\u56fe\u3002\u4f5c\u8005\u8ba4\u4e3a\u4ed3\u5e93\u7406\u89e3\u548c\u751f\u6210\u662f\u7edf\u4e00\u5faa\u73af\u4e2d\u7684\u9006\u8fc7\u7a0b\uff1a\u751f\u6210\u5c06\u610f\u56fe\u6269\u5c55\u4e3a\u5b9e\u73b0\uff0c\u800c\u7406\u89e3\u5c06\u5b9e\u73b0\u538b\u7f29\u56de\u610f\u56fe\u3002", "method": "\u63d0\u51faRPG-Encoder\u6846\u67b6\uff0c\u5c06Repository Planning Graph\u4ece\u9759\u6001\u751f\u6210\u84dd\u56fe\u63a8\u5e7f\u4e3a\u7edf\u4e00\u7684\u9ad8\u4fdd\u771f\u8868\u793a\u3002\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a1) \u5c06\u539f\u59cb\u4ee3\u7801\u7f16\u7801\u5230\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u548c\u4ee3\u7801\u4f9d\u8d56\u7684RPG\uff1b2) \u589e\u91cf\u6f14\u5316\u62d3\u6251\u7ed3\u6784\uff0c\u5c06\u7ef4\u62a4\u6210\u672c\u4e0e\u4ed3\u5e93\u89c4\u6a21\u89e3\u8026\uff1b3) \u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u5bfc\u822a\u7684\u7edf\u4e00\u63a5\u53e3\u3002", "result": "\u5728SWE-bench Verified\u4e0a\u8fbe\u523093.7% Acc@5\u7684\u6700\u5148\u8fdb\u4ed3\u5e93\u7406\u89e3\u6027\u80fd\uff0c\u5728SWE-bench Live Lite\u4e0a\u8d85\u8fc7\u6700\u4f73\u57fa\u7ebf10%\u4ee5\u4e0a\u3002\u5728RepoCraft\u4e0a\u5b9e\u73b098.5%\u7684\u91cd\u5efa\u8986\u76d6\u7387\uff0c\u9a8c\u8bc1\u4e86RPG\u7684\u9ad8\u4fdd\u771f\u80fd\u529b\u3002", "conclusion": "RPG-Encoder\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u610f\u56fe\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u63a8\u7406\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4ee3\u7801\u5e93\u7684\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u7ef4\u62a4\u5f00\u9500\u3002"}}
{"id": "2602.01158", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01158", "abs": "https://arxiv.org/abs/2602.01158", "authors": ["Daniel Yezid Guarnizo Orjuela", "Leonardo Scappatura", "Veronica Di Gennaro", "Riccardo Andrea Izzo", "Gianluca Bardaro", "Matteo Matteucci"], "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $\u03c0_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCRT\uff08Corruption Restoration Transformer\uff09\u6765\u589e\u5f3aVLA\u6a21\u578b\u5bf9\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u5fae\u8c03\u5c31\u80fd\u6062\u590d\u6027\u80fd", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u65f6\u5bf9\u56fe\u50cf\u635f\u574f\uff08\u5982\u4f20\u611f\u5668\u566a\u58f0\u3001\u574f\u70b9\u3001\u955c\u5934\u6c61\u67d3\u7b49\uff09\u975e\u5e38\u8106\u5f31\uff0c\u6027\u80fd\u4f1a\u4ece90%\u9aa4\u964d\u81f32%\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7269\u7406\u906e\u6321\uff0c\u5ffd\u7565\u4e86\u4f20\u611f\u5668\u5c42\u9762\u7684\u56fe\u50cf\u635f\u574f\u95ee\u9898", "method": "\u63d0\u51faCRT\uff08Corruption Restoration Transformer\uff09\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89c9Transformer\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u76ee\u6807\u4ece\u635f\u574f\u7684\u8f93\u5165\u4e2d\u6062\u590d\u5e72\u51c0\u7684\u89c2\u6d4b\uff0c\u65e0\u9700\u5bf9\u5e95\u5c42VLA\u6a21\u578b\u8fdb\u884c\u6602\u8d35\u7684\u5fae\u8c03", "result": "\u5728LIBERO\u548cMeta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRT\u80fd\u6709\u6548\u6062\u590dVLA\u6a21\u578b\uff08\u5982\u03c0\u2080.\u2085\u548cSmolVLA\uff09\u56e0\u56fe\u50cf\u635f\u574f\u800c\u4e22\u5931\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u4e25\u91cd\u89c6\u89c9\u635f\u574f\u4e0b\u4ecd\u80fd\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u7684\u6210\u529f\u7387", "conclusion": "CRT\u4e3aVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u5bf9\u4f20\u611f\u5668\u5e72\u6270\u7684\u514d\u75ab\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u8106\u5f31\u6027\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027"}}
{"id": "2602.02199", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02199", "abs": "https://arxiv.org/abs/2602.02199", "authors": ["Aryan Sood", "Tanvi Sharma", "Vansh Agrawal"], "title": "More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression", "comment": null, "summary": "While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.", "AI": {"tldr": "LASER-KV\u662f\u4e00\u79cd\u65b0\u7684KV\u7f13\u5b58\u538b\u7f29\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u7d2f\u79ef\u9009\u62e9\u548c\u7cbe\u786eLSH\u53ec\u56de\u7b56\u7565\uff0c\u5728\u4e25\u683c\u7d2f\u79ef\u9884\u7b97\u653f\u7b56\u4e0b\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u8fbe10%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u7406\u8bba\u4e0a\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230KV\u7f13\u5b58\u5185\u5b58\u7ebf\u6027\u589e\u957f\u7684\u5236\u7ea6\u3002\u73b0\u6709\u538b\u7f29\u7b56\u7565\u901a\u8fc7\u526a\u679d\u673a\u5236\u5728\u8bed\u4e49\u53ec\u56de\u548c\u5185\u5b58\u6548\u7387\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u5728\u4e25\u683c\u7d2f\u79ef\u9884\u7b97\u653f\u7b56\u4e0bKV\u538b\u7f29\u7684\u6781\u9650\u3002", "method": "\u63d0\u51faLASER-KV\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u7d2f\u79ef\u9009\u62e9\u548c\u7cbe\u786eLSH\u53ec\u56de\u7b56\u7565\u3002\u4e0e\u4f20\u7edf\u56fa\u5b9a\u6458\u8981\u5927\u5c0f\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b9e\u65bd\u57fa\u4e8e\u4fdd\u62a4\u9664\u6570(n)\u7684\u5757\u72b6\u7d2f\u79ef\u7b56\u7565\uff0c\u5c06\u538b\u7f29\u6548\u679c\u4e0e\u6ed1\u52a8\u7a97\u53e3\u4f2a\u5f71\u9694\u79bb\u3002\u4f7f\u7528\u7cbe\u786e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u8fdb\u884c\u9ad8\u6548\u53ec\u56de\u3002", "result": "\u5728Babilong\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728\u5404\u79cd\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d15-30%\uff0c\u800cLASER-KV\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u5728128k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe10%\u3002", "conclusion": "LASER-KV\u5c55\u793a\u4e86\u5728\u4e25\u683c\u7d2f\u79ef\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548KV\u538b\u7f29\u7684\u53ef\u80fd\u6027\uff0c\u6311\u6218\u4e86\u6ce8\u610f\u529b\u5206\u6570\u5355\u72ec\u4f5c\u4e3atoken\u6548\u7528\u4ee3\u7406\u7684\u666e\u904d\u5047\u8bbe\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u538b\u7f29\u65b9\u6848\u3002"}}
{"id": "2602.00809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00809", "abs": "https://arxiv.org/abs/2602.00809", "authors": ["David Craveiro", "Hugo Silva"], "title": "Mobile Exergames: Activity Recognition Based on Smartphone Sensors", "comment": null, "summary": "Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word \"fire\" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u6b3e\u7ed3\u5408\u4f20\u611f\u5668\u6d3b\u52a8\u8bc6\u522b\u4e0e\u8bed\u97f3\u8bc6\u522b\u76842D\u65e0\u5c3d\u6e38\u620fDuck Catch & Fit\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u68c0\u6d4b\u4eba\u4f53\u6d3b\u52a8\uff0c\u63d0\u5347\u6e38\u620f\u6c89\u6d78\u611f", "motivation": "\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u7528\u6237\u6d3b\u52a8\u548c\u884c\u4e3a\u4fe1\u606f\uff0c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u5728\u6e38\u620f\u3001\u533b\u7597\u548c\u76d1\u63a7\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5982\u4f55\u5c06\u6d3b\u52a8\u8bc6\u522b\u4e0e\u6e38\u620f\u673a\u5236\u6709\u6548\u7ed3\u5408\u4ecd\u9700\u63a2\u7d22", "method": "\u5f00\u53d1\u6982\u5ff5\u9a8c\u8bc1\u6e38\u620fDuck Catch & Fit\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u548c\u78c1\u529b\u8ba1\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u5b66\u4e60\u673a\u5236\u68c0\u6d4b\u9759\u6b62\u3001\u4fa7\u5411\u79fb\u52a8\u548c\u865a\u5047\u4fa7\u5411\u79fb\u52a8\u7b49\u6d3b\u52a8\uff0c\u5e76\u96c6\u6210\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u8bc6\u522b\"fire\"\u6307\u4ee4", "result": "\u673a\u5668\u5b66\u4e60\u6280\u672f\u80fd\u591f\u4ee5\u9ad8\u8bc6\u522b\u7387\u68c0\u6d4b\u4eba\u4f53\u6d3b\u52a8\uff0c\u8fd0\u52a8\u8bc6\u522b\u4e0e\u8bed\u97f3\u8bc6\u522b\u7684\u7ed3\u5408\u663e\u8457\u589e\u5f3a\u4e86\u6e38\u620f\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c", "conclusion": "\u667a\u80fd\u624b\u673a\u4f20\u611f\u5668\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6280\u672f\u80fd\u6709\u6548\u5b9e\u73b0\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff0c\u5c06\u8fd0\u52a8\u8bc6\u522b\u4e0e\u8bed\u97f3\u8bc6\u522b\u96c6\u6210\u5230\u6e38\u620f\u4e2d\u53ef\u521b\u9020\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u4e3a\u672a\u6765\u6e38\u620f\u5f00\u53d1\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2602.02090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02090", "abs": "https://arxiv.org/abs/2602.02090", "authors": ["Yikai Zeng", "Yingchao Piao", "Jianhui Li"], "title": "LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs", "comment": null, "summary": "Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.", "AI": {"tldr": "LEC-KG\u662f\u4e00\u4e2a\u53cc\u5411\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408LLM\u7684\u8bed\u4e49\u7406\u89e3\u548cKGE\u7684\u7ed3\u6784\u63a8\u7406\uff0c\u901a\u8fc7\u5206\u5c42\u5173\u7cfb\u63d0\u53d6\u3001\u8bc1\u636e\u5f15\u5bfc\u7684\u601d\u7ef4\u94fe\u53cd\u9988\u548c\u8bed\u4e49\u521d\u59cb\u5316\uff0c\u8fed\u4ee3\u5730\u5c06\u975e\u7ed3\u6784\u5316\u653f\u7b56\u6587\u672c\u8f6c\u5316\u4e3a\u9a8c\u8bc1\u7684\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u3002", "motivation": "\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u6784\u5efa\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5f02\u6784\u5b9e\u4f53\u63d0\u53ca\u3001\u957f\u5c3e\u5173\u7cfb\u5206\u5e03\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u6a21\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u63a8\u7406\u3002", "method": "\u63d0\u51faLEC-KG\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u5206\u5c42\u7c97\u5230\u7ec6\u5173\u7cfb\u63d0\u53d6\u7f13\u89e3\u957f\u5c3e\u504f\u5dee\uff1b2) \u8bc1\u636e\u5f15\u5bfc\u7684\u601d\u7ef4\u94fe\u53cd\u9988\uff0c\u5c06\u7ed3\u6784\u5efa\u8bae\u57fa\u4e8e\u6e90\u6587\u672c\uff1b3) \u8bed\u4e49\u521d\u59cb\u5316\u5b9e\u73b0\u672a\u89c1\u5b9e\u4f53\u7684\u7ed3\u6784\u9a8c\u8bc1\u3002LLM\u548cKGE\u6a21\u5757\u901a\u8fc7\u8fed\u4ee3\u534f\u4f5c\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5728\u4e2d\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u62a5\u544a\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4LLM\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u4f4e\u9891\u5173\u7cfb\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\uff0c\u6846\u67b6\u80fd\u53ef\u9760\u5730\u5c06\u975e\u7ed3\u6784\u5316\u653f\u7b56\u6587\u672c\u8f6c\u5316\u4e3a\u9a8c\u8bc1\u7684\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u3002", "conclusion": "LEC-KG\u6846\u67b6\u901a\u8fc7LLM\u548cKGE\u7684\u53cc\u5411\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5c3e\u5173\u7cfb\u548c\u672a\u89c1\u5b9e\u4f53\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u653f\u7b56\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2602.01163", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01163", "abs": "https://arxiv.org/abs/2602.01163", "authors": ["Chunliang Hua", "Zeyuan Yang", "Lei Zhang", "Jiayang Sun", "Fengwen Chen", "Chunlan Zeng", "Xiao Hu"], "title": "Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models", "comment": null, "summary": "Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9065\u611f\u5f71\u50cf\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u4eba\u673a\u7d27\u6025\u7740\u9646\u70b9\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u9884\u7b5b\u9009\u548c\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u68c0\u6d4b\u98ce\u9669\uff0c\u6784\u5efaELSS\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u6548\u679c", "motivation": "\u4f20\u7edf\u51e0\u4f55\u4f20\u611f\u5668\u65e0\u6cd5\u8bc6\u522b\u590d\u6742\u8bed\u4e49\u98ce\u9669\uff08\u5982\u4eba\u7fa4\u3001\u4e34\u65f6\u7ed3\u6784\uff09\uff0c\u65e0\u4eba\u673a\u5b89\u5168\u7d27\u6025\u7740\u9646\u9700\u8981\u7406\u89e3\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\u4fe1\u606f", "method": "\u91c7\u7528\u7c97\u5230\u7cbe\u7684\u6d41\u7a0b\uff1a1\uff09\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u6a21\u5757\u9884\u7b5b\u9009\u5019\u9009\u533a\u57df\uff1b2\uff09\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4ee3\u7406\u878d\u5408\u89c6\u89c9\u7279\u5f81\u548cPOI\u6570\u636e\u68c0\u6d4b\u7ec6\u5fae\u98ce\u9669", "result": "\u5728\u98ce\u9669\u8bc6\u522b\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u51e0\u4f55\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u53ef\u89e3\u91ca\u7406\u7531\uff0c\u589e\u5f3a\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u53ef\u4fe1\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u7ed3\u5408\u9065\u611f\u5f71\u50cf\u548cMLLM\u80fd\u6709\u6548\u8bc4\u4f30\u7740\u9646\u70b9\u98ce\u9669\uff0c\u6784\u5efa\u7684ELSS\u57fa\u51c6\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u7d27\u6025\u7740\u9646\u7684\u5b89\u5168\u6027"}}
{"id": "2602.02304", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02304", "abs": "https://arxiv.org/abs/2602.02304", "authors": ["Martino Ciaperoni", "Marzio Di Vece", "Luca Pappalardo", "Fosca Giannotti", "Francesco Giannini"], "title": "Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach", "comment": null, "summary": "Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($\u0394$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $\u0394$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $\u0394$-XAI experiment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6bd4\u8f83\u6027\u53ef\u89e3\u91caAI\u6846\u67b6\uff08\u0394-XAI\uff09\uff0c\u7528\u4e8e\u89e3\u91ca\u57fa\u7840\u6a21\u578b\u5728\u5e72\u9884\uff08\u5982\u7f29\u653e\u3001\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\uff09\u540e\u51fa\u73b0\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u800c\u975e\u5b64\u7acb\u5206\u6790\u5355\u4e2a\u6a21\u578b\u3002", "motivation": "\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u5e72\u9884\u540e\u4f1a\u51fa\u73b0\u884c\u4e3a\u53d8\u5316\uff0c\u4f46\u73b0\u6709\u53ef\u89e3\u91caAI\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u6a21\u578b\u68c0\u67e5\u70b9\uff0c\u96be\u4ee5\u89e3\u91ca\u4e0d\u540c\u68c0\u67e5\u70b9\u4e4b\u95f4\u7684\u5185\u90e8\u53d8\u5316\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u5e72\u9884\u5f15\u8d77\u7684\u884c\u4e3a\u8f6c\u53d8\u3002", "method": "\u63d0\u51fa\u6bd4\u8f83\u6027\u53ef\u89e3\u91caAI\u6846\u67b6\uff08\u0394-XAI\uff09\uff0c\u5305\u542b\u4e00\u7cfb\u5217\u8bbe\u8ba1\u539f\u5219\uff0c\u5173\u6ce8\u53c2\u8003\u6a21\u578b\u4e0e\u5e72\u9884\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4ecb\u7ecd\u4e86\u53ef\u80fd\u7684\u5206\u6790\u6d41\u7a0b\uff0c\u5e76\u5c06\u5176\u4e0e\u8bbe\u8ba1\u539f\u5219\u5173\u8054\uff0c\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u0394-XAI\u5b9e\u9a8c\u3002", "result": "\u5efa\u7acb\u4e86\u0394-XAI\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u4e86\u6bd4\u8f83\u6027\u89e3\u91ca\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "conclusion": "\u884c\u4e3a\u53d8\u5316\u5e94\u8be5\u901a\u8fc7\u6bd4\u8f83\u6027\u65b9\u6cd5\u6765\u89e3\u91ca\uff0c\u5173\u6ce8\u5e72\u9884\u5f15\u8d77\u7684\u6a21\u578b\u95f4\u5dee\u5f02\u3002\u0394-XAI\u6846\u67b6\u4e3a\u5f00\u53d1\u5408\u9002\u7684\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.00827", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00827", "abs": "https://arxiv.org/abs/2602.00827", "authors": ["Taesun Yeom", "Taehyeok Ha", "Jaeho Lee"], "title": "Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization", "comment": null, "summary": "Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\\textit{over-fitting}$.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5b58\u5728\u6700\u4f18\u503c\uff0c\u65e2\u4e0d\u80fd\u592a\u5c0f\u4e5f\u4e0d\u80fd\u592a\u5927\uff0c\u8fd9\u4e0e\u666e\u904d\u8ba4\u4e3a\"\u7279\u5f81\u5b66\u4e60\u8d8a\u5f3a\u6cdb\u5316\u8d8a\u597d\"\u7684\u76f4\u89c9\u76f8\u53cd\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u4e3b\u8981\u7814\u7a76\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5728\u6e10\u8fd1\u6761\u4ef6\u4e0b\u7684\u5f71\u54cd\uff0c\u4f46\u5bf9\u5b9e\u9645\u8bad\u7ec3\u4e2d\uff08\u5982\u8fbe\u5230\u76ee\u6807\u8bad\u7ec3\u98ce\u9669\u65f6\u505c\u6b62\uff09\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u9700\u8981\u63a2\u7a76\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5728\u5b9e\u9645\u8bad\u7ec3\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u5f71\u54cd\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u89c2\u5bdf\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u7136\u540e\u9488\u5bf9\u4e24\u5c42ReLU\u7f51\u7edc\u4f7f\u7528\u903b\u8f91\u635f\u5931\u8bad\u7ec3\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u5c3a\u5ea6\u63a7\u5236\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\uff0c\u8fdb\u884c\u68af\u5ea6\u6d41\u52a8\u529b\u5b66\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u6700\u4f18\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\uff0c\u80fd\u5e26\u6765\u663e\u8457\u6cdb\u5316\u63d0\u5347\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u6700\u4f18\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u6e90\u4e8e\u4e24\u79cd\u7ade\u4e89\u6548\u5e94\u7684\u6743\u8861\uff1a\u8fc7\u5927\u7684\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5bfc\u81f4\"\u8fc7\u5ea6\u5bf9\u9f50\"\u73b0\u8c61\u635f\u5bb3\u6cdb\u5316\uff0c\u800c\u8fc7\u5c0f\u7684\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5219\u5bfc\u81f4\u8fc7\u62df\u5408\u3002", "conclusion": "\u7279\u5f81\u5b66\u4e60\u5f3a\u5ea6\u5bf9\u6cdb\u5316\u5b58\u5728\u975e\u5355\u8c03\u5f71\u54cd\uff0c\u5b58\u5728\u6700\u4f18\u503c\u5e73\u8861\u8fc7\u5ea6\u5bf9\u9f50\u548c\u8fc7\u62df\u5408\u4e24\u79cd\u6548\u5e94\uff0c\u8fd9\u6311\u6218\u4e86\"\u7279\u5f81\u5b66\u4e60\u8d8a\u5f3a\u6cdb\u5316\u8d8a\u597d\"\u7684\u4f20\u7edf\u89c2\u70b9\u3002"}}
{"id": "2602.02099", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02099", "abs": "https://arxiv.org/abs/2602.02099", "authors": ["Keqin Peng", "Yuanxin Ouyang", "Xuebo Liu", "Zhiliang Tian", "Ruijian Han", "Yancheng Yuan", "Liang Ding"], "title": "Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.", "AI": {"tldr": "DDCA\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u6027\u548c\u6548\u7387\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u957f\u5ea6\u60e9\u7f5a\u6765\u89e3\u51b3RLVR\u4e2d\u8fc7\u5ea6\u5197\u957f\u548c\u957f\u5ea6\u60e9\u7f5a\u635f\u5bb3\u51c6\u786e\u6027\u7684\u95ee\u9898\u3002", "motivation": "RLVR\u867d\u7136\u80fd\u6fc0\u53d1\u591a\u6b65\u63a8\u7406\uff0c\u4f46\u5e38\u5bfc\u81f4\u8fc7\u5ea6\u5197\u957f\u7684\u8f68\u8ff9\u3002\u7b80\u5355\u7684\u957f\u5ea6\u60e9\u7f5a\u5728\u7fa4\u4f53\u76f8\u5bf9\u4f18\u5316\u4e2d\u4f1a\u4e25\u91cd\u635f\u5bb3\u51c6\u786e\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u957f\u5ea6\u57fa\u7ebf\u7a00\u91ca\uff08\u9519\u8bef\u56de\u7b54\u964d\u4f4e\u7fa4\u4f53\u57fa\u7ebf\uff0c\u8fc7\u5ea6\u60e9\u7f5a\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff09\u548c\u96be\u5ea6-\u60e9\u7f5a\u4e0d\u5339\u914d\uff08\u9759\u6001\u60e9\u7f5a\u65e0\u6cd5\u9002\u5e94\u95ee\u9898\u96be\u5ea6\uff09\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u89e3\u8026\u6761\u4ef6\u4f18\u52bf\uff08DDCA\uff09\uff1a1\uff09\u5728\u6b63\u786e\u56de\u7b54\u7c07\u5185\u6761\u4ef6\u8ba1\u7b97\u957f\u5ea6\u4f18\u52bf\uff0c\u6d88\u9664\u57fa\u7ebf\u7a00\u91ca\uff1b2\uff09\u4f7f\u7528\u7fa4\u4f53\u901a\u8fc7\u7387\u4f5c\u4e3a\u96be\u5ea6\u4ee3\u7406\uff0c\u52a8\u6001\u8c03\u6574\u60e9\u7f5a\u5f3a\u5ea6\u3002", "result": "\u5728GSM8K\u3001MATH500\u3001AMC23\u548cAIME25\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDDCA\u76f8\u6bd4\u81ea\u9002\u5e94\u57fa\u7ebf\u6301\u7eed\u6539\u8fdb\u4e86\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\uff1a\u5728\u7b80\u5355\u4efb\u52a1\uff08\u5982GSM8K\uff09\u4e0a\u51cf\u5c11\u7ea660%\u751f\u6210token\uff0c\u5728\u66f4\u96be\u57fa\u51c6\uff08\u5982AIME25\uff09\u4e0a\u51cf\u5c11\u8d85\u8fc720%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "DDCA\u901a\u8fc7\u89e3\u8026\u6b63\u786e\u6027\u548c\u6548\u7387\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u957f\u5ea6\u60e9\u7f5a\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u5197\u957f\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.01173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01173", "abs": "https://arxiv.org/abs/2602.01173", "authors": ["Lancheng Gao", "Ziheng Jia", "Zixuan Xing", "Wei Sun", "Huiyu Duan", "Guangtao Zhai", "Xiongkuo Min"], "title": "EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment", "comment": null, "summary": "Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.", "AI": {"tldr": "EEmoDB\u662f\u6700\u5927\u7684\u56fe\u50cf\u8bf1\u53d1\u60c5\u611f\u7406\u89e3\u6570\u636e\u96c6\uff0c\u5305\u542b5\u4e2a\u5206\u6790\u7ef4\u5ea6\u548c5\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5e76\u63d0\u51faEEmo-Logic\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548cGRPO\u4f18\u5316\u5b9e\u73b0\u5f3a\u5927\u7684\u60c5\u611f\u7406\u89e3\u548c\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u56fe\u50cf\u8bf1\u53d1\u60c5\u611f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u8981\u4e48\u53ea\u80fd\u8fdb\u884c\u7c97\u7c92\u5ea6\u60c5\u611f\u611f\u77e5\uff0c\u8981\u4e48\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6765\u7406\u89e3\u56fe\u50cf\u7684\u591a\u7ef4\u60c5\u611f\u5c5e\u6027\u548c\u5f3a\u5ea6\u7ec6\u5fae\u5dee\u522b\u3002", "method": "1) \u6784\u5efaEEmoDB\u6570\u636e\u96c6\uff1a\u5305\u542b125k\u56fe\u50cf\u7684120\u4e07QA\u5bf9(EEmoDB-QA)\u548c25k\u56fe\u50cf\u768436k\u8bc4\u4f30\u6570\u636e\u96c6(EEmoDB-Assess)\uff0c\u6db5\u76d65\u4e2a\u5206\u6790\u7ef4\u5ea6\u548c5\u4e2a\u4efb\u52a1\u7c7b\u522b\uff1b2) \u63d0\u51faEEmo-Logic\u6a21\u578b\uff1a\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u548c\u4efb\u52a1\u5b9a\u5236\u7684\u7ec4\u76f8\u5bf9\u504f\u597d\u4f18\u5316(GRPO)\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5956\u52b1\u8bbe\u8ba1\u3002", "result": "EEmo-Logic\u5728\u9886\u57df\u5185\u548c\u8de8\u9886\u57df\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728\u60c5\u611fQA\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002EEmoDB\u662f\u76ee\u524d\u6700\u5927\u7684\u56fe\u50cf\u8bf1\u53d1\u60c5\u611f\u7406\u89e3\u6570\u636e\u96c6\u3002", "conclusion": "EEmoDB\u6570\u636e\u96c6\u548cEEmo-Logic\u6a21\u578b\u4e3a\u56fe\u50cf\u8bf1\u53d1\u60c5\u611f\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5171\u60c5\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u8d4b\u80fd\u591a\u6837\u5316\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u3002"}}
{"id": "2602.02313", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02313", "abs": "https://arxiv.org/abs/2602.02313", "authors": ["Changming Li", "Kaixing Zhang", "Haoyun Xu", "Yingdong Shi", "Zheng Zhang", "Kaitao Song", "Kan Ren"], "title": "Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient", "comment": null, "summary": "Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.", "AI": {"tldr": "IPG\u6846\u67b6\u901a\u8fc7\u4f20\u64ad\u57fa\u4e8e\u7ed3\u679c\u7684\u4fe1\u53f7\u6765\u5b9a\u4f4d\u548c\u8c03\u5236\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7ec4\u4ef6\u5b9a\u4f4d\u548c\u53ef\u9760\u7684\u884c\u4e3a\u8c03\u5236\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u6027\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u590d\u6742\u63a8\u7406\u673a\u5236\u6216\u6355\u6349\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u5bf9\u63a8\u7406\u8f93\u51fa\u7684\u987a\u5e8f\u5f71\u54cd\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5bf9\u63a8\u7406\u884c\u4e3a\u6709\u987a\u5e8f\u8d21\u732e\u7684\u7ec4\u4ef6\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u7b56\u7565\u68af\u5ea6\uff08IPG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u57fa\u4e8e\u7ed3\u679c\u7684\u4fe1\u53f7\uff08\u5982\u63a8\u7406\u540e\u51c6\u786e\u6027\uff09\u5411\u540e\u4f20\u64ad\u901a\u8fc7\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\uff0c\u5c06\u63a8\u7406\u884c\u4e3a\u5f52\u56e0\u4e8e\u6a21\u578b\u5185\u90e8\u7ec4\u4ef6\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u5e76\u80fd\u5728\u4e0d\u540c\u63a8\u7406\u6a21\u578b\u4e2d\u53ef\u9760\u5730\u8c03\u5236\u63a8\u7406\u884c\u4e3a\uff08\u5982\u63a8\u7406\u80fd\u529b\u3001\u63a8\u7406\u5f3a\u5ea6\uff09\u3002", "conclusion": "IPG\u6846\u67b6\u57fa\u4e8e\u7ed3\u679c\u5bfc\u5411\u548c\u987a\u5e8f\u5f71\u54cd\u611f\u77e5\u539f\u5219\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5bf9\u63a8\u7406\u884c\u4e3a\u6709\u987a\u5e8f\u8d21\u732e\u7684\u7ec4\u4ef6\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.00834", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00834", "abs": "https://arxiv.org/abs/2602.00834", "authors": ["Wei Chen", "Jiacheng Li", "Shigui Li", "Zhiqi Lin", "Junmei Yang", "John Paisley", "Delu Zeng"], "title": "Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation", "comment": null, "summary": "Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\\textbf{Min}imum \\textbf{P}ath \\textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.", "AI": {"tldr": "\u57fa\u4e8e\u5206\u6570\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u8def\u5f84\u4f9d\u8d56\u6096\u8bba\uff0c\u4f5c\u8005\u63d0\u51fa\u6700\u5c0f\u8def\u5f84\u65b9\u5dee\u539f\u5219\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8def\u5f84\u5b66\u4e60\u6570\u636e\u81ea\u9002\u5e94\u4f4e\u65b9\u5dee\u8def\u5f84\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u4e00\u4e2a\u91cd\u8981\u6096\u8bba\uff1a\u867d\u7136\u7406\u8bba\u4e0a\u8def\u5f84\u72ec\u7acb\uff0c\u4f46\u5b9e\u9645\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u9009\u62e9\u7684\u8def\u5f84\u8c03\u5ea6\u3002\u8fd9\u662f\u56e0\u4e3a\u53ef\u8bad\u7ec3\u7684\u4f18\u5316\u76ee\u6807\u4e0e\u7406\u60f3\u76ee\u6807\u4e4b\u95f4\u5ffd\u7565\u4e86\u4e00\u4e2a\u5173\u952e\u9879\u2014\u2014\u65f6\u95f4\u5206\u6570\u7684\u8def\u5f84\u65b9\u5dee\u3002", "method": "\u63d0\u51fa\u6700\u5c0f\u8def\u5f84\u65b9\u5dee\u539f\u5219\uff0c\u63a8\u5bfc\u51fa\u8def\u5f84\u65b9\u5dee\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5c06\u96be\u89e3\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u4f18\u5316\u95ee\u9898\u3002\u4f7f\u7528\u7075\u6d3b\u7684Kumaraswamy\u6df7\u5408\u6a21\u578b\u53c2\u6570\u5316\u8def\u5f84\uff0c\u5b66\u4e60\u6570\u636e\u81ea\u9002\u5e94\u7684\u4f4e\u65b9\u5dee\u8def\u5f84\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u9009\u62e9\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u5b8c\u6574\u76ee\u6807\u4ea7\u751f\u4e86\u66f4\u51c6\u786e\u548c\u7a33\u5b9a\u7684\u4f30\u8ba1\u5668\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u6700\u5c0f\u8def\u5f84\u65b9\u5dee\u539f\u5219\u89e3\u51b3\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u4e2d\u7684\u8def\u5f84\u4f9d\u8d56\u6096\u8bba\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u53c2\u6570\u5316\u8def\u5f84\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e3a\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.02104", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02104", "abs": "https://arxiv.org/abs/2602.02104", "authors": ["Shaltiel Shmidman", "Avi Shmidman", "Amir DN Cohen", "Moshe Koppel"], "title": "Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs", "comment": null, "summary": "Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.", "AI": {"tldr": "Dicta-LM 3.0\uff1a\u4e00\u4e2a\u9488\u5bf9\u5e0c\u4f2f\u6765\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7cfb\u5217\uff0c\u5305\u542b24B\u300112B\u548c1.7B\u4e09\u79cd\u89c4\u6a21\uff0c\u652f\u630165k\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5e76\u5f15\u5165\u4e86\u5e0c\u4f2f\u6765\u8bed\u804a\u5929LLM\u8bc4\u4f30\u57fa\u51c6\u5957\u4ef6\u3002", "motivation": "\u867d\u7136\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u4e86\u5f00\u6e90\u6743\u91cdLLM\uff0c\u4f46\u9488\u5bf9\u82f1\u8bed\u4ee5\u5916\u8bed\u8a00\u7684\u4e3b\u6743\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u4f9b\u5e94\u4e0d\u8db3\u800c\u9700\u6c42\u65fa\u76db\u3002\u8bad\u7ec3\u5e0c\u4f2f\u6765\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff1a24B\u6a21\u578b\u57fa\u4e8eMistral-Small-3.1\uff0c12B\u57fa\u4e8eNVIDIA Nemotron Nano V2\uff0c1.7B\u57fa\u4e8eQwen3-1.7B\u3002\u4f7f\u7528\u5927\u91cf\u5e0c\u4f2f\u6765\u8bed\u548c\u82f1\u8bed\u6587\u672c\u8bed\u6599\u8fdb\u884c\u8bad\u7ec3\uff0c\u53d1\u5e03\u57fa\u7840\u6a21\u578b\u548c\u5e26\u5de5\u5177\u8c03\u7528\u652f\u6301\u7684\u804a\u5929\u6a21\u578b\u53d8\u4f53\uff0c\u5747\u652f\u630165k\u539f\u751f\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "\u53d1\u5e03\u4e86Dicta-LM 3.0\u6a21\u578b\u7cfb\u5217\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u5e0c\u4f2f\u6765\u8bed\u804a\u5929LLM\u8bc4\u4f30\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d6\u7ffb\u8bd1\u3001\u6458\u8981\u3001Winograd\u3001\u4ee5\u8272\u5217\u5e38\u8bc6\u3001\u5e0c\u4f2f\u6765\u8bed\u5143\u97f3\u6807\u6ce8\u7b49\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3LLM\u7684\u590d\u6742\u6027\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u975e\u82f1\u8bed\u8bed\u8a00\u7684LLM\u9002\u914d\u6846\u67b6\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2602.01183", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01183", "abs": "https://arxiv.org/abs/2602.01183", "authors": ["Chunming He", "Rihan Zhang", "Fengyang Xiao", "Dingming Zhang", "Zhiwen Cao", "Sina Farsiu"], "title": "Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion", "comment": "8 figures, 11 tables", "summary": "Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.", "AI": {"tldr": "CurriSeg\uff1a\u4e00\u79cd\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u53cd\u8bfe\u7a0b\u5b66\u4e60\u7684\u53cc\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0a\u4e0b\u6587\u7ea0\u7f20\u5185\u5bb9\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6837\u672c\u9009\u62e9\u548c\u9891\u8c31\u76f2\u5fae\u8c03\u63d0\u5347\u5206\u5272\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u751f\u7269\u5b66\u4e60\u4ece\u6613\u5230\u96be\u8fc7\u7a0b\u7684\u542f\u53d1\uff0c\u9488\u5bf9\u4e0a\u4e0b\u6587\u7ea0\u7f20\u5185\u5bb9\u5206\u5272\uff08CECS\uff09\u8fd9\u4e00\u6311\u6218\u6027\u4efb\u52a1\uff0c\u4f20\u7edf\u5206\u5272\u7f51\u7edc\u4e3b\u8981\u4f9d\u8d56\u67b6\u6784\u6539\u8fdb\u800c\u5ffd\u89c6\u4e86\u5b66\u4e60\u52a8\u6001\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faCurriSeg\u53cc\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u8bfe\u7a0b\u9009\u62e9\u9636\u6bb5\uff1a\u57fa\u4e8e\u6837\u672c\u635f\u5931\u7684\u65f6\u95f4\u7edf\u8ba1\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u533a\u5206\u56f0\u96be\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u6837\u672c\u4e0e\u566a\u58f0/\u6a21\u7cca\u6837\u672c\uff1b2\uff09\u53cd\u8bfe\u7a0b\u4fc3\u8fdb\u9636\u6bb5\uff1a\u8bbe\u8ba1\u9891\u8c31\u76f2\u5fae\u8c03\uff0c\u6291\u5236\u9ad8\u9891\u6210\u5206\u4ee5\u5f3a\u5236\u4f9d\u8d56\u4f4e\u9891\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "result": "\u5728\u591a\u4e2aCECS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u6216\u603b\u8bad\u7ec3\u65f6\u95f4\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6e10\u8fdb\u548c\u6311\u6218\u7684\u76f8\u4e92\u4f5c\u7528\u57f9\u517b\u9c81\u68d2\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5272\u80fd\u529b\u3002", "conclusion": "CurriSeg\u4e3a\u4e0a\u4e0b\u6587\u7ea0\u7f20\u5185\u5bb9\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u8bfe\u7a0b\u548c\u53cd\u8bfe\u7a0b\u5b66\u4e60\u539f\u7406\uff0c\u589e\u5f3a\u4e86\u8868\u793a\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u9c81\u68d2\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.02350", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02350", "abs": "https://arxiv.org/abs/2602.02350", "authors": ["Xingyuan Hua", "Sheng Yue", "Xinyi Li", "Yizhe Zhao", "Jinrui Zhang", "Ju Ren"], "title": "Context Learning for Multi-Agent Discussion", "comment": null, "summary": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.", "AI": {"tldr": "M2CL\u63d0\u51fa\u4e86\u4e00\u79cd\u591aLLM\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6bcf\u4e2a\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff0c\u52a8\u6001\u751f\u6210\u6bcf\u8f6e\u8ba8\u8bba\u7684\u4e0a\u4e0b\u6587\u6307\u4ee4\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd20%-50%\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u65b9\u6cd5\u5bb9\u6613\u906d\u53d7\u8ba8\u8bba\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u7531\u4e8e\u4e2a\u4f53\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4LLM\u65e0\u6cd5\u8fbe\u6210\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "M2CL\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff0c\u901a\u8fc7\u81ea\u52a8\u4fe1\u606f\u7ec4\u7ec7\u548c\u7cbe\u70bc\u52a8\u6001\u751f\u6210\u6bcf\u8f6e\u8ba8\u8bba\u7684\u4e0a\u4e0b\u6587\u6307\u4ee4\uff0c\u91c7\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u81ea\u9002\u5e94\u673a\u5236\u63a7\u5236\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u8f93\u51fa\u5dee\u5f02\u3002", "result": "\u5728\u5b66\u672f\u63a8\u7406\u3001\u5177\u8eab\u4efb\u52a1\u548c\u79fb\u52a8\u63a7\u5236\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\uff0cM2CL\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd520%-50%\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "M2CL\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f7fLLM\u80fd\u591f\u907f\u514d\u8fc7\u65e9\u6536\u655b\u4e8e\u591a\u6570\u566a\u58f0\uff0c\u9010\u6b65\u8fbe\u6210\u6b63\u786e\u5171\u8bc6\u3002"}}
{"id": "2602.02108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02108", "abs": "https://arxiv.org/abs/2602.02108", "authors": ["Wenhao Li", "Daohai Yu", "Gen Luo", "Yuxin Zhang", "Fei Chao", "Rongrong Ji", "Yifan Wu", "Jiaxin Liu", "Ziyang Gong", "Zimu Liao"], "title": "Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts", "comment": null, "summary": "Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.", "AI": {"tldr": "OOMB\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5185\u5b58\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5757\u5faa\u73af\u8bad\u7ec3\u548c\u5373\u65f6\u6fc0\u6d3b\u91cd\u8ba1\u7b97\u5b9e\u73b0O(1)\u6fc0\u6d3b\u5185\u5b58\u5360\u7528\uff0c\u7ed3\u5408KV\u7f13\u5b58\u4f18\u5316\u6280\u672f\uff0c\u4f7fQwen2.5-7B\u6a21\u578b\u6bcf\u589e\u52a01\u4e07token\u4e0a\u4e0b\u6587\u4ec5\u970010MB\u5185\u5b58\uff0c\u53ef\u5728\u5355\u5f20H200 GPU\u4e0a\u8bad\u7ec3400\u4e07token\u4e0a\u4e0b\u6587\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\uff0cGPU\u5185\u5b58\u5f00\u9500\uff08\u7279\u522b\u662f\u6fc0\u6d3b\u503c\u7684\u5185\u5b58\u5360\u7528\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff09\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u5757\u5faa\u73af\u8bad\u7ec3\u6846\u67b6\u548c\u5373\u65f6\u6fc0\u6d3b\u91cd\u8ba1\u7b97\u5b9e\u73b0\u6052\u5b9a\u6fc0\u6d3b\u5185\u5b58\u5360\u7528\uff1b\u901a\u8fc7\u5206\u9875\u5185\u5b58\u7ba1\u7406\u5668\u6d88\u9664KV\u7f13\u5b58\u788e\u7247\u5316\uff1b\u4f7f\u7528\u5f02\u6b65CPU\u5378\u8f7d\u9690\u85cf\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\uff1b\u91c7\u7528\u9875\u9762\u7ea7\u7a00\u758f\u6ce8\u610f\u529b\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "result": "Qwen2.5-7B\u6a21\u578b\u6bcf\u589e\u52a01\u4e07token\u4e0a\u4e0b\u6587\u4ec5\u970010MB\u5185\u5b58\u5f00\u9500\uff0c\u53ef\u5728\u5355\u5f20H200 GPU\u4e0a\u8bad\u7ec3400\u4e07token\u4e0a\u4e0b\u6587\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u578b\u96c6\u7fa4\u4f7f\u7528\u4e0a\u4e0b\u6587\u5e76\u884c\u3002", "conclusion": "OOMB\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u8bad\u7ec3\u7684\u8d44\u6e90\u6548\u7387\uff0c\u4e3a\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0b\u8bad\u7ec3\u8d85\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.01194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01194", "abs": "https://arxiv.org/abs/2602.01194", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Fenghua Ling", "Lei Bai"], "title": "EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting", "comment": null, "summary": "Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.", "AI": {"tldr": "\u63d0\u51faEMFormer\u67b6\u6784\u548c\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u957f\u671f\u5929\u6c14\u9884\u62a5\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u5929\u6c14\u9884\u6d4b\u548c\u89c6\u89c9\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u9ad8\u8bad\u7ec3\u5f00\u9500\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u957f\u671f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "1) \u63d0\u51fa\u9ad8\u6548\u591a\u5c3a\u5ea6Transformer\uff08EMFormer\uff09\uff0c\u901a\u8fc7\u5355\u5377\u79ef\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b2) \u91c7\u7528\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5fae\u8c03\u63d0\u5347\u65f6\u95f4\u4e00\u81f4\u6027\uff1b3) \u8bbe\u8ba1\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6b63\u5f26\u52a0\u6743\u52a8\u6001\u5e73\u8861\u4e0d\u540c\u635f\u5931\u9879\u3002", "result": "\u5728\u5929\u6c14\u9884\u62a5\u548c\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u53d6\u5f97\u5f3a\u52b2\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff1b\u5728\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\uff08ImageNet-1K\u548cADE20K\uff09\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edf\u591a\u5c3a\u5ea6\u6a21\u5757\u5b9e\u73b05.69\u500d\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684EMFormer\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u77ed\u671f\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2602.02369", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02369", "abs": "https://arxiv.org/abs/2602.02369", "authors": ["Yaolun Zhang", "Yiran Wu", "Yijiong Yu", "Qingyun Wu", "Huazheng Wang"], "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback", "comment": "13 pages", "summary": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.", "AI": {"tldr": "Live-Evo\u662f\u4e00\u4e2a\u5728\u7ebf\u81ea\u6f14\u8fdb\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ecf\u9a8c\u94f6\u884c\u548c\u5143\u6307\u5bfc\u94f6\u884c\u5206\u79bb\"\u53d1\u751f\u4e86\u4ec0\u4e48\"\u548c\"\u5982\u4f55\u4f7f\u7528\"\uff0c\u80fd\u591f\u6839\u636e\u8fde\u7eed\u53cd\u9988\u52a8\u6001\u66f4\u65b0\u8bb0\u5fc6\u6743\u91cd\uff0c\u5728\u771f\u5b9e\u5206\u5e03\u6f02\u79fb\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u6f14\u8fdb\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6298\u53e0\u9759\u6001\u57fa\u51c6\u6765\u8fd1\u4f3c\u5728\u7ebf\u5b66\u4e60\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u771f\u5b9e\u5206\u5e03\u6f02\u79fb\u548c\u8fde\u7eed\u53cd\u9988\u4e0b\u8868\u73b0\u8106\u5f31\u3002\u9700\u8981\u771f\u6b63\u7684\u5728\u7ebf\u81ea\u6f14\u8fdb\u8bb0\u5fc6\u7cfb\u7edf\u6765\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u3002", "method": "Live-Evo\u91c7\u7528\u53cc\u94f6\u884c\u67b6\u6784\uff1a\u7ecf\u9a8c\u94f6\u884c\u5b58\u50a8\u539f\u59cb\u7ecf\u9a8c\uff0c\u5143\u6307\u5bfc\u94f6\u884c\u5b58\u50a8\u5982\u4f55\u4f7f\u7528\u7ecf\u9a8c\u7684\u6307\u5bfc\u3002\u7cfb\u7edf\u7ef4\u62a4\u7ecf\u9a8c\u6743\u91cd\uff0c\u6839\u636e\u53cd\u9988\u52a8\u6001\u66f4\u65b0\uff1a\u6709\u5e2e\u52a9\u7684\u7ecf\u9a8c\u88ab\u5f3a\u5316\u548c\u66f4\u9891\u7e41\u68c0\u7d22\uff0c\u8bef\u5bfc\u6027\u6216\u8fc7\u65f6\u7684\u7ecf\u9a8c\u88ab\u964d\u6743\u548c\u9010\u6e10\u9057\u5fd8\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5f3a\u5316\u548c\u8870\u51cf\u673a\u5236\u3002", "result": "\u572810\u5468\u65f6\u95f4\u8de8\u5ea6\u7684Prophet Arena\u5b9e\u65f6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLive-Evo\u5c06Brier\u5206\u6570\u63d0\u9ad8\u4e8620.8%\uff0c\u5e02\u573a\u56de\u62a5\u589e\u52a0\u4e8612.9%\u3002\u5728\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u7684\u6301\u7eed\u589e\u76ca\u3002", "conclusion": "Live-Evo\u901a\u8fc7\u5728\u7ebf\u81ea\u6f14\u8fdb\u8bb0\u5fc6\u7cfb\u7edf\u6709\u6548\u5e94\u5bf9\u5206\u5e03\u6f02\u79fb\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6301\u7eed\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8bb0\u5fc6\u7ba1\u7406\u673a\u5236\u3002"}}
{"id": "2602.00852", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00852", "abs": "https://arxiv.org/abs/2602.00852", "authors": ["Pattarawat Chormai", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Investigating the Robustness of Subtask Distillation under Spurious Correlation", "comment": "7 pages, 3 figures", "summary": "Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u5b50\u4efb\u52a1\u84b8\u998f\u65f6\u4e0d\u540c\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0SubDistill\u7b49\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u5b50\u4efb\u52a1\u84b8\u998f\u4f9d\u8d56\u4e8e\u6709\u9650\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u6216\u4e0d\u5177\u4ee3\u8868\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5728\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u84b8\u998f\u65f6\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u4e86\u5df2\u5efa\u7acb\u7684\u84b8\u998f\u65b9\u6cd5\u4ee5\u53ca\u6700\u8fd1\u7684SubDistill\u65b9\u6cd5\uff0c\u5728\u5177\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u84b8\u998f\u5b9e\u9a8c\uff0c\u5e76\u89c2\u5bdf\u968f\u7740\u76f8\u5173\u6027\u5f3a\u5ea6\u589e\u52a0\u65f6\u5404\u65b9\u6cd5\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u968f\u7740\u865a\u5047\u76f8\u5173\u6027\u5f3a\u5ea6\u589e\u52a0\uff0cSubDistill\u7b49\u5148\u8fdb\u65b9\u6cd5\u4fdd\u6301\u76f8\u5bf9\u7a33\u5065\uff0c\u800c\u4e00\u4e9b\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u5230\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u4e24\u8005\u4e4b\u95f4\u5dee\u8ddd\u9010\u6e10\u6269\u5927\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5177\u6709\u865a\u5047\u76f8\u5173\u6027\u7684\u4e0d\u5b8c\u7f8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u6311\u6218\uff0c\u5e76\u8868\u660e\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.02132", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02132", "abs": "https://arxiv.org/abs/2602.02132", "authors": ["Faaiz Joad", "Majd Hawasly", "Sabri Boughorbel", "Nadir Durrani", "Husrev Taha Sencar"], "title": "There Is More to Refusal in Large Language Models than a Single Direction", "comment": null, "summary": "Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u7684\u62d2\u7edd\u884c\u4e3a\u7531\u591a\u4e2a\u51e0\u4f55\u4e0a\u4e0d\u540c\u7684\u6fc0\u6d3b\u65b9\u5411\u63a7\u5236\uff0c\u800c\u975e\u5355\u4e00\u65b9\u5411\uff0c\u4f46\u7ebf\u6027\u8c03\u63a7\u8fd9\u4e9b\u65b9\u5411\u4f1a\u4ea7\u751f\u76f8\u4f3c\u7684\u62d2\u7edd-\u8fc7\u5ea6\u62d2\u7edd\u6743\u8861\uff0c\u5f62\u6210\u5171\u4eab\u7684\u4e00\u7ef4\u63a7\u5236\u65cb\u94ae\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u7531\u5355\u4e00\u6fc0\u6d3b\u7a7a\u95f4\u65b9\u5411\u4ecb\u5bfc\uff0c\u4f46\u672c\u6587\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e00\u89e3\u91ca\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u66f4\u5168\u9762\u5730\u7406\u89e3\u62d2\u7edd\u884c\u4e3a\u7684\u673a\u5236\u3002", "method": "\u572811\u4e2a\u62d2\u7edd\u548c\u4e0d\u670d\u4ece\u7c7b\u522b\uff08\u5305\u62ec\u5b89\u5168\u6027\u3001\u4e0d\u5b8c\u6574\u6216\u4e0d\u652f\u6301\u7684\u8bf7\u6c42\u3001\u62df\u4eba\u5316\u3001\u8fc7\u5ea6\u62d2\u7edd\u7b49\uff09\u4e0a\uff0c\u5206\u6790\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u65b9\u5411\uff0c\u7814\u7a76\u7ebf\u6027\u8c03\u63a7\u8fd9\u4e9b\u65b9\u5411\u7684\u6548\u679c\u3002", "result": "\u62d2\u7edd\u884c\u4e3a\u5bf9\u5e94\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u51e0\u4f55\u4e0a\u4e0d\u540c\u7684\u65b9\u5411\uff0c\u4f46\u7ebf\u6027\u8c03\u63a7\u4efb\u4f55\u62d2\u7edd\u76f8\u5173\u65b9\u5411\u90fd\u4f1a\u4ea7\u751f\u51e0\u4e4e\u76f8\u540c\u7684\u62d2\u7edd-\u8fc7\u5ea6\u62d2\u7edd\u6743\u8861\uff1b\u4e0d\u540c\u65b9\u5411\u7684\u4e3b\u8981\u5f71\u54cd\u4e0d\u662f\u6a21\u578b\u662f\u5426\u62d2\u7edd\uff0c\u800c\u662f\u5982\u4f55\u62d2\u7edd\u3002", "conclusion": "LLM\u7684\u62d2\u7edd\u673a\u5236\u6bd4\u5355\u4e00\u65b9\u5411\u5047\u8bbe\u66f4\u590d\u6742\uff0c\u5b58\u5728\u591a\u4e2a\u51e0\u4f55\u4e0a\u4e0d\u540c\u7684\u62d2\u7edd\u65b9\u5411\uff0c\u4f46\u5b83\u4eec\u901a\u8fc7\u5171\u4eab\u7684\u4e00\u7ef4\u63a7\u5236\u673a\u5236\u4ea7\u751f\u76f8\u4f3c\u7684\u62d2\u7edd\u884c\u4e3a\u6a21\u5f0f\u3002"}}
{"id": "2602.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01200", "abs": "https://arxiv.org/abs/2602.01200", "authors": ["Haoran Lai", "Zihang Jiang", "Kun Zhang", "Qingsong Yao", "Rongsheng Wang", "Zhiyang He", "Xiaodong Tao", "Wei Wei", "Shaohua Kevin Zhou"], "title": "Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis", "comment": null, "summary": "Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\\% on CT-RATE and 44.99\\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.", "AI": {"tldr": "Med3D-R1\uff1a\u4e00\u4e2a\u7528\u4e8e3D\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u63d0\u5347\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u5728CT-RATE\u548cRAD-ChestCT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u7a33\u5065\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u76843D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u6311\u6218\uff1a\u4f53\u7d20\u533b\u5b66\u5f71\u50cf\u7684\u590d\u6742\u6027\u3001\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u62a5\u544a\u8868\u9762\u6a21\u5f0f\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u5956\u52b1\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faMed3D-R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff1a\u5f15\u5165\u6b8b\u5dee\u5bf9\u9f50\u673a\u5236\u8fde\u63a53D\u7279\u5f81\u4e0e\u6587\u672c\u5d4c\u5165\uff0c\u4f7f\u7528\u5f02\u5e38\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u5f3a\u8c03\u4e34\u5e8a\u4fe1\u606f\u6807\u8bb0\uff1b2\uff09\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u91cd\u65b0\u8bbe\u8ba1\u4e00\u81f4\u6027\u5956\u52b1\u4ee5\u4fc3\u8fdb\u8fde\u8d2f\u7684\u9010\u6b65\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a3D\u8bca\u65ad\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff1aCT-RATE\u4e0a41.92%\uff0cRAD-ChestCT\u4e0a44.99%\uff0c\u8868\u660e\u5f02\u5e38\u8bca\u65ad\u548c\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u901a\u8fc7\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u900f\u660e\u76843D\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u6765\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u7684\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2602.02386", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02386", "abs": "https://arxiv.org/abs/2602.02386", "authors": ["Mika Okamoto", "Ansel Kaplan Erol", "Glenn Matlin"], "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing", "comment": "Appeared at MLSys YPS 2025", "summary": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.", "AI": {"tldr": "BELLA\u662f\u4e00\u4e2a\u9884\u7b97\u9ad8\u6548\u7684LLM\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u6280\u80fd\u5206\u6790\u81ea\u52a8\u63a8\u8350\u6700\u4f18\u6a21\u578b\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u63a7\u5236\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a\u805a\u5408\u6307\u6807\uff0c\u63a9\u76d6\u4e86\u4efb\u52a1\u6240\u9700\u7684\u5177\u4f53\u80fd\u529b\uff0c\u65e0\u6cd5\u5224\u65ad\u66f4\u4fbf\u5b9c\u7684\u6a21\u578b\u662f\u5426\u8db3\u591f\u3002LLM\u4ece\u4e1a\u8005\u9700\u8981\u5728\u4e0d\u6d6a\u8d39\u8d44\u91d1\u7684\u60c5\u51b5\u4e0b\u4e3a\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6a21\u578b\u3002", "method": "BELLA\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u901a\u8fc7\u6279\u8bc4\u8005\u5206\u6790\u5206\u89e3LLM\u8f93\u51fa\u5e76\u63d0\u53d6\u6240\u9700\u7ec6\u7c92\u5ea6\u6280\u80fd\uff1b2) \u5c06\u6280\u80fd\u805a\u7c7b\u4e3a\u7ed3\u6784\u5316\u80fd\u529b\u77e9\u9635\uff1b3) \u591a\u76ee\u6807\u4f18\u5316\u9009\u62e9\u6a21\u578b\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u6027\u80fd\u3002", "result": "BELLA\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u63a8\u8350\uff0c\u63d0\u4f9b\u5f53\u524d\u9ed1\u76d2\u8def\u7531\u7cfb\u7edf\u7f3a\u4e4f\u7684\u900f\u660e\u5ea6\u3002\u8be5\u6846\u67b6\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u4e3a\u90e8\u7f72LLM\u505a\u51fa\u539f\u5219\u6027\u7684\u6210\u672c-\u6027\u80fd\u6743\u8861\u3002", "conclusion": "BELLA\u6846\u67b6\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u6280\u80fd\u7684\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9884\u7b97\u9ad8\u6548\u7684LLM\u9009\u62e9\uff0c\u7279\u522b\u9002\u7528\u4e8e\u91d1\u878d\u63a8\u7406\u7b49\u5177\u6709\u591a\u6837\u5316\u6280\u80fd\u9700\u6c42\u548c\u6a21\u578b\u6210\u672c\u53d8\u5316\u7684\u9886\u57df\u3002"}}
{"id": "2602.02140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02140", "abs": "https://arxiv.org/abs/2602.02140", "authors": ["Chenlong Wang", "Yuhang Chen", "Zhihan Hu", "Dongping Chen", "Wenhu Chen", "Sarah Wiegreffe", "Tianyi Zhou"], "title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models", "comment": null, "summary": "Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.", "AI": {"tldr": "GapEval\u662f\u4e00\u4e2a\u53cc\u5411\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u91cf\u5316\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u4ec5\u5b9e\u73b0\u8868\u9762\u7edf\u4e00\u800c\u975e\u6df1\u5ea6\u8ba4\u77e5\u878d\u5408\u3002", "motivation": "\u5c3d\u7ba1\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e24\u9879\u80fd\u529b\u662f\u5426\u771f\u6b63\u5bf9\u9f50\u548c\u6574\u5408\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u4ecd\u4e0d\u660e\u786e\u3002\u9700\u8981\u91cf\u5316\u8bc4\u4f30\u8fd9\u79cd\"\u7edf\u4e00\"\u662f\u5426\u53ea\u662f\u8868\u9762\u73b0\u8c61\u3002", "method": "\u63d0\u51faGapEval\u53cc\u5411\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u4e2a\u95ee\u9898\u53ef\u5728\u56fe\u50cf\u548c\u6587\u672c\u4e24\u79cd\u6a21\u6001\u4e2d\u53cc\u5411\u56de\u7b54\uff0c\u5bf9\u79f0\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002\u4ece\u77e5\u8bc6\u64cd\u7eb5\u89d2\u5ea6\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u5e95\u5c42\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5404\u79cd\u67b6\u6784\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u90fd\u5b58\u5728\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u4ec5\u5b9e\u73b0\u8868\u9762\u7edf\u4e00\u800c\u975e\u6df1\u5ea6\u8ba4\u77e5\u878d\u5408\u3002\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u4fdd\u6301\u5206\u79bb\uff0c\u8de8\u6a21\u6001\u80fd\u529b\u6d8c\u73b0\u548c\u77e5\u8bc6\u540c\u6b65\u4e0d\u8db3\u3002", "conclusion": "\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u7684\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u5c1a\u672a\u771f\u6b63\u5bf9\u9f50\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5b9e\u73b0\u6df1\u5ea6\u8ba4\u77e5\u878d\u5408\u7684\u673a\u5236\u3002\u77e5\u8bc6\u5728\u6a21\u578b\u5185\u90e8\u4fdd\u6301\u5206\u79bb\u72b6\u6001\uff0c\u80fd\u529b\u6d8c\u73b0\u4e0e\u8de8\u6a21\u6001\u77e5\u8bc6\u540c\u6b65\u4e0d\u8db3\u3002"}}
{"id": "2602.01257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01257", "abs": "https://arxiv.org/abs/2602.01257", "authors": ["Yunchuan Ma", "Laiyun Qing", "Guorong Li", "Yuqing Liu", "Yuankai Qi", "Qingming Huang"], "title": "Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment", "comment": null, "summary": "Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.", "AI": {"tldr": "\u63d0\u51faTRA\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u7cbe\u70bc\u548c\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u89c6\u9891\u63cf\u8ff0\u6587\u672c\u7279\u5f81\u589e\u5f3a\u70b9\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u70b9\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u4ec5\u8003\u8651\u89c6\u89c9\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u4fa7\u7684\u8bed\u4e49\u4fe1\u606f\u3002\u6587\u672c\u7279\u5f81\u8bed\u4e49\u4e30\u5bcc\uff0c\u53ef\u4ee5\u8865\u5145\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u6587\u672c\u7cbe\u70bc\u4e0e\u5bf9\u9f50\uff08TRA\uff09\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u89c6\u9891\u5e27\u63cf\u8ff0\uff1b2\uff09\u70b9\u57fa\u6587\u672c\u7cbe\u70bc\u6a21\u5757\uff08PTR\uff09\u5229\u7528\u70b9\u6807\u6ce8\u548c\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7cbe\u70bc\u521d\u59cb\u63cf\u8ff0\uff1b3\uff09\u70b9\u57fa\u591a\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff08PMA\uff09\u5c06\u6240\u6709\u7279\u5f81\u6295\u5f71\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u70b9\u7ea7\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\u51cf\u5c11\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u95f4\u7684\u5dee\u8ddd\uff1b4\uff09\u589e\u5f3a\u7684\u591a\u6a21\u6001\u7279\u5f81\u8f93\u5165\u52a8\u4f5c\u68c0\u6d4b\u5668\u8fdb\u884c\u7cbe\u786e\u5b9a\u4f4d\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u591a\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u8ba1\u7b97\u5f00\u9500\u5206\u6790\u663e\u793a\u6846\u67b6\u53ef\u5728\u5355\u5f2024GB RTX 3090 GPU\u4e0a\u8fd0\u884c\uff0c\u8868\u660e\u5176\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "TRA\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u6587\u672c\u7279\u5f81\u8865\u5145\u89c6\u89c9\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u70b9\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.02416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02416", "abs": "https://arxiv.org/abs/2602.02416", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Ayush Jain", "Kavosh Asadi", "Youliang Yu", "Daniel Jiang", "Boris Vidolov", "Kaveh Hassani", "Paul Sajda", "Jalaj Bhandari", "Yonathan Efroni"], "title": "Structure Enables Effective Self-Localization of Errors in LLMs", "comment": null, "summary": "Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faThought-ICS\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u63a8\u7406\u6b65\u9aa4\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u9519\u8bef\u5b9a\u4f4d\u4e0e\u4fee\u6b63\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u5927\u8111\u4e00\u6837\uff0c\u5728\u79bb\u6563\u51b3\u7b56\u70b9\u76d1\u63a7\u9519\u8bef\u5e76\u91cd\u65b0\u91c7\u6837\u66ff\u4ee3\u65b9\u6848\uff0c\u4ece\u800c\u6784\u5efa\u80fd\u6709\u6548\u81ea\u6211\u4fee\u6b63\u7684AI\u7cfb\u7edf\u3002", "method": "\u63d0\u51faThought-ICS\uff08\u8fed\u4ee3\u4fee\u6b63\u91c7\u6837\u601d\u60f3\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u79bb\u6563\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u601d\u60f3\u6b65\u9aa4\uff1b2\uff09\u6bcf\u6b21\u53ea\u751f\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u601d\u60f3\u6b65\u9aa4\uff08\u4ee3\u8868\u6a21\u578b\u7684\u523b\u610f\u51b3\u7b56\uff09\uff1b3\uff09\u521b\u5efa\u81ea\u7136\u8fb9\u754c\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u9519\u8bef\uff1b4\uff09\u9a8c\u8bc1\u540e\u5b9a\u4f4d\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\uff1b5\uff09\u56de\u6eaf\u5230\u6700\u540e\u4e00\u4e2a\u6b63\u786e\u70b9\u751f\u6210\u66ff\u4ee3\u63a8\u7406\u3002", "result": "1\uff09\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4e2d\uff0c\u6a21\u578b\u80fd\u53ef\u9760\u5b9a\u4f4d\u9519\u8bef\uff0c\u800c\u5728\u4f20\u7edf\u975e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u4e2d\u5219\u5931\u8d25\uff1b2\uff09\u5728\u9700\u8981\u4fee\u6b63\u7531oracle\u9a8c\u8bc1\u7684\u9519\u8bef\u63a8\u7406\u65f6\uff0cThought-ICS\u5b9e\u73b020-40%\u7684\u81ea\u6211\u4fee\u6b63\u63d0\u5347\uff1b3\uff09\u5728\u5b8c\u5168\u81ea\u4e3b\u65e0\u5916\u90e8\u9a8c\u8bc1\u7684\u8bbe\u7f6e\u4e0b\uff0c\u4f18\u4e8e\u5f53\u4ee3\u81ea\u6211\u4fee\u6b63\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u79bb\u6563\u601d\u60f3\u6b65\u9aa4\uff0c\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u5b9a\u4f4d\u548c\u4fee\u6b63\u81ea\u8eab\u9519\u8bef\u3002Thought-ICS\u6846\u67b6\u6a21\u4eff\u4eba\u7c7b\u5927\u8111\u7684\u9519\u8bef\u76d1\u63a7\u673a\u5236\uff0c\u4e3a\u6784\u5efa\u80fd\u81ea\u6211\u4fee\u6b63\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2602.02159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02159", "abs": "https://arxiv.org/abs/2602.02159", "authors": ["Lingkun Long", "Yushi Huang", "Shihao Bai", "Ruihao Gong", "Jun Zhang", "Ao Zhou", "Jianlei Yang"], "title": "Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM", "AI": {"tldr": "Focus-dLLM\uff1a\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u6ce8\u610f\u529b\u7a00\u758f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u53bb\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6307\u793a\u5668\u548csink\u611f\u77e5\u526a\u679d\u7b56\u7565\uff0c\u572832K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b0\u8d85\u8fc729\u500d\u7684\u65e0\u635f\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53cc\u5411\u5168\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u63a8\u7406\u6548\u7387\u3002\u73b0\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u9700\u8981\u9884\u6d4b\u5c1a\u672a\u89e3\u7801\u7684token\u7684\u6ce8\u610f\u529b\u91cd\u8981\u6027\uff0c\u800c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u672a\u63a9\u7801\u7684token\u4f4d\u7f6e\u662f\u672a\u77e5\u7684\u3002", "method": "1. \u57fa\u4e8etoken\u7f6e\u4fe1\u5ea6\u5728\u76f8\u90bb\u6b65\u9aa4\u95f4\u5f3a\u76f8\u5173\u7684\u53d1\u73b0\uff0c\u8bbe\u8ba1\u8fc7\u53bb\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6307\u793a\u5668\u6765\u9884\u6d4b\u672a\u63a9\u7801\u533a\u57df\uff1b2. \u63d0\u51fasink\u611f\u77e5\u526a\u679d\u7b56\u7565\uff0c\u51c6\u786e\u4f30\u8ba1\u5e76\u79fb\u9664\u5197\u4f59\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u7559\u9ad8\u5f71\u54cd\u529b\u7684\u6ce8\u610f\u529bsink\uff1b3. \u5229\u7528\u89c2\u5bdf\u5230\u7684\u8de8\u5c42\u4e00\u81f4\u6027\uff0c\u5728\u4e0d\u540c\u5c42\u95f4\u91cd\u7528\u5df2\u8bc6\u522b\u7684sink\u4f4d\u7f6e\u4ee5\u51cf\u5c11\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u572832K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u63d0\u4f9b\u8d85\u8fc729\u500d\u7684\u65e0\u635f\u52a0\u901f\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "Focus-dLLM\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u6ce8\u610f\u529b\u7a00\u758f\u5316\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2602.01268", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01268", "abs": "https://arxiv.org/abs/2602.01268", "authors": ["Jaehyeon Cho", "Jhonghyun An"], "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth", "comment": "Accepted to ICRA 2026", "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5355\u76ee\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u6df1\u5ea6\u4f30\u8ba1\u8f6c\u6362\u4e3a\u5ea6\u91cf\u6df1\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u6d4b\u8ddd\u6d4b\u91cf\u6821\u51c6\u76f8\u5bf9\u6df1\u5ea6\u4f5c\u4e3a\u5148\u9a8c\uff0c\u8bbe\u8ba1\u7ec6\u5316\u7f51\u7edc\u5b9e\u73b0\u5c11\u6837\u672c\u4e0b\u7684\u51c6\u786e\u5ea6\u91cf\u6df1\u5ea6\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u5355\u76ee\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8f93\u51fa\u7684\u662f\u76f8\u5bf9\u6df1\u5ea6\u800c\u975e\u5ea6\u91cf\u6df1\u5ea6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\u3002\u9700\u8981\u89e3\u51b3\u4ece\u76f8\u5bf9\u6df1\u5ea6\u5230\u5ea6\u91cf\u6df1\u5ea6\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u6807\u7b7e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528\u76f8\u5bf9\u6df1\u5ea6\u4fdd\u6301\u5168\u5c40\u5e03\u5c40\u548c\u8fb9\u754c\u7684\u7279\u70b9\uff0c\u901a\u8fc7\u7a00\u758f\u6d4b\u8ddd\u6d4b\u91cf\u6821\u51c6\u76f8\u5bf9\u6df1\u5ea6\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u4f2a\u5ea6\u91cf\u6df1\u5ea6\u5148\u9a8c\u3002\u57fa\u4e8e\u6b64\u5148\u9a8c\u8bbe\u8ba1\u7ec6\u5316\u7f51\u7edc\uff0c\u5728\u53ef\u9760\u533a\u57df\u9075\u5faa\u5148\u9a8c\uff0c\u5728\u5fc5\u8981\u533a\u57df\u8fdb\u884c\u4fee\u6b63\uff0c\u5b9e\u73b0\u4ece\u6781\u5c11\u6807\u8bb0\u6837\u672c\u4e2d\u9884\u6d4b\u51c6\u786e\u5ea6\u91cf\u6df1\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7f3a\u4e4f\u7cbe\u9009\u9a8c\u8bc1\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u7279\u522b\u6709\u6548\uff0c\u80fd\u591f\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u5c3a\u5ea6\u548c\u9510\u5229\u8fb9\u7f18\u3002\u7cfb\u7edf\u8868\u660e\u5c06\u57fa\u7840\u5148\u9a8c\u4e0e\u7a00\u758f\u951a\u70b9\u7ed3\u5408\u662f\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u90e8\u7f72\u6df1\u5ea6\u8865\u5168\u7684\u5b9e\u7528\u9014\u5f84\u3002", "conclusion": "\u5c06\u57fa\u7840\u5148\u9a8c\u4e0e\u7a00\u758f\u951a\u70b9\u76f8\u7ed3\u5408\u662f\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6807\u7b7e\u7a00\u7f3a\u4e0b\u7a33\u5065\u6df1\u5ea6\u8865\u5168\u95ee\u9898\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u90e8\u7f72\u5c31\u7eea\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02419", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02419", "abs": "https://arxiv.org/abs/2602.02419", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration", "comment": null, "summary": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.", "AI": {"tldr": "SafeGround\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684GUI grounding\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u548c\u7edf\u8ba1\u4fdd\u8bc1\u7684FDR\u63a7\u5236\uff0c\u5b9e\u73b0\u98ce\u9669\u611f\u77e5\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8GUI\u4ea4\u4e92\u7684\u53ef\u9760\u6027\u3002", "motivation": "GUI grounding\u4e2d\u7684\u9519\u8bef\u9884\u6d4b\u53ef\u80fd\u5bfc\u81f4\u4ee3\u4ef7\u9ad8\u6602\u4e14\u96be\u4ee5\u9006\u8f6c\u7684\u64cd\u4f5c\uff08\u5982\u9519\u8bef\u652f\u4ed8\u6279\u51c6\uff09\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u5e76\u63a7\u5236\u98ce\u9669\u3002", "method": "1. \u4f7f\u7528\u5206\u5e03\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u6355\u6349\u6a21\u578b\u8f93\u51fa\u7684\u7a7a\u95f4\u5206\u6563\u6027\uff1b2. \u901a\u8fc7\u6821\u51c6\u8fc7\u7a0b\u83b7\u5f97\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684FDR\u63a7\u5236\u7684\u6d4b\u8bd5\u65f6\u51b3\u7b56\u9608\u503c\u3002", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSafeGround\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u80fd\u66f4\u597d\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u9884\u6d4b\uff1b\u6821\u51c6\u9608\u503c\u80fd\u53ef\u9760\u5b9e\u73b0\u4e25\u683c\u7684\u98ce\u9669\u63a7\u5236\uff0c\u7cfb\u7edf\u7ea7\u51c6\u786e\u7387\u76f8\u6bd4Gemini-only\u63a8\u7406\u6700\u9ad8\u63d0\u53475.38\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SafeGround\u4e3aGUI grounding\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u98ce\u9669\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u7edf\u8ba1\u6821\u51c6\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.00872", "categories": ["cs.LG", "math-ph"], "pdf": "https://arxiv.org/pdf/2602.00872", "abs": "https://arxiv.org/abs/2602.00872", "authors": ["Shihao Wang", "Qipeng Qian", "Jingquan Wang"], "title": "Learning Heat-based Equations in Self-similar variables", "comment": null, "summary": "We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76f8\u4f3c\u53d8\u91cf\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u70ed\u57fa\u65b9\u7a0b\u7684\u957f\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u76f8\u6bd4\u4f20\u7edf\u7269\u7406\u5750\u6807\u8bad\u7ec3\uff0c\u5728\u4e8c\u7ef4\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u548c\u4e00\u7ef4\u7c98\u6027Burgers\u65b9\u7a0b\u4e0a\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u7a33\u5b9a\u7684\u957f\u65f6\u95f4\u5916\u63a8\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u8bad\u7ec3\u5728\u7269\u7406\u5750\u6807\u4e2d\u5b66\u4e60\u70ed\u57fa\u65b9\u7a0b\u7684\u957f\u65f6\u95f4\u52a8\u529b\u5b66\u65f6\uff0c\u53ef\u80fd\u96be\u4ee5\u51c6\u786e\u6355\u6349\u957f\u671f\u8d8b\u52bf\u548c\u7a33\u5b9a\u5916\u63a8\u3002\u81ea\u76f8\u4f3c\u53d8\u91cf\u4e3a\u8fd9\u7c7b\u65b9\u7a0b\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u66f4\u81ea\u7136\u7684\u8868\u793a\uff0c\u53ef\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u5f00\u53d1\u4e86\u4e0e\u6807\u51c6\u795e\u7ecf\u7b97\u5b50\u8bad\u7ec3\u517c\u5bb9\u7684\u81ea\u76f8\u4f3c\u53d8\u91cf\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u4e8c\u7ef4\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u548c\u4e00\u7ef4\u7c98\u6027Burgers\u65b9\u7a0b\u4e0a\u5b9e\u4f8b\u5316\u3002\u4f7f\u7528\u4e24\u79cd\u7b80\u5355\u5168\u8fde\u63a5\u67b6\u6784\uff08\u6807\u51c6\u591a\u5c42\u611f\u77e5\u673a\u548c\u56e0\u5b50\u5316\u5168\u8fde\u63a5\u7f51\u7edc\uff09\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u7269\u7406\u5750\u6807\u548c\u81ea\u76f8\u4f3c\u5750\u6807\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u5728\u4e24\u4e2a\u7cfb\u7edf\u548c\u4e24\u79cd\u67b6\u6784\u4e2d\uff0c\u81ea\u76f8\u4f3c\u53d8\u91cf\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u59cb\u7ec8\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u7a33\u5b9a\u7684\u8bad\u7ec3\u7a97\u53e3\u5916\u63a8\uff0c\u5e76\u66f4\u597d\u5730\u6355\u6349\u5b9a\u6027\u957f\u671f\u8d8b\u52bf\u3002\u81ea\u76f8\u4f3c\u5750\u6807\u8bad\u7ec3\u5728\u6240\u6709\u5bf9\u6bd4\u4e2d\u90fd\u4f18\u4e8e\u7269\u7406\u5750\u6807\u8bad\u7ec3\u3002", "conclusion": "\u81ea\u76f8\u4f3c\u5750\u6807\u4e3a\u5b66\u4e60\u70ed\u57fa\u65b9\u7a0b\u7684\u957f\u65f6\u95f4\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u5408\u7406\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u795e\u7ecf\u7b97\u5b50\u6a21\u578b\u7684\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.02160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02160", "abs": "https://arxiv.org/abs/2602.02160", "authors": ["Bowen Xu", "Shaoyu Wu", "Hao Jiang", "Kai Liu", "Xin Chen", "Lulu Hu", "Bin Yang"], "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use", "comment": null, "summary": "Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5$\\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.", "AI": {"tldr": "D-CORE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u63a8\u7406\u8fc7\u7a0b\u7ec4\u5408\u6765\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u61d2\u60f0\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7f3a\u4e4f\u5b50\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\"\u61d2\u60f0\u63a8\u7406\"\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u3002", "method": "\u63d0\u51faD-CORE\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u81ea\u84b8\u998f\u6fc0\u52b1\u6a21\u578b\u7684\u4efb\u52a1\u5206\u89e3\u63a8\u7406\u80fd\u529b\uff1b2\uff09\u4f7f\u7528\u591a\u6837\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6062\u590d\u6a21\u578b\u7684\u53cd\u601d\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728BFCLv3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cD-CORE-8B\u8fbe\u523077.7%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u6700\u4f738B\u6a21\u578b5.7%\uff1bD-CORE-14B\u4ee579.3%\u51c6\u786e\u7387\u521b\u4e0b\u65b0SOTA\uff0c\u6027\u80fd\u8d85\u8d8a70B\u6a21\u578b\u4f46\u53c2\u6570\u91cf\u4ec5\u4e3a\u51761/5\u3002", "conclusion": "D-CORE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u4efb\u52a1\u5206\u89e3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.01273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01273", "abs": "https://arxiv.org/abs/2602.01273", "authors": ["Xun Zhang", "Kaicheng Yang", "Hongliang Lu", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution", "comment": "Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR", "summary": "Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\\times$ and computational operations by over 60$\\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.", "AI": {"tldr": "Q-DiT4SR\uff1a\u9996\u4e2a\u9488\u5bf9DiT\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684PTQ\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7H-SVD\u5206\u5c42\u5206\u89e3\u548c\u65b9\u5dee\u611f\u77e5\u65f6\u7a7a\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\uff0c\u5728W4A4\u914d\u7f6e\u4e0b\u5b9e\u73b05.8\u500d\u6a21\u578b\u538b\u7f29\u548c60\u500d\u8ba1\u7b97\u52a0\u901f", "motivation": "DiT\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7eb9\u7406\uff0c\u4f46\u63a8\u7406\u8d1f\u62c5\u91cd\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709PTQ\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9U-Net\u67b6\u6784\u6216\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\uff0c\u76f4\u63a5\u5e94\u7528\u4e8eDiT\u8d85\u5206\u8fa8\u7387\u4f1a\u5bfc\u81f4\u5c40\u90e8\u7eb9\u7406\u4e25\u91cd\u9000\u5316", "method": "1. H-SVD\uff1a\u5206\u5c42SVD\u5206\u89e3\uff0c\u7ed3\u5408\u5168\u5c40\u4f4e\u79e9\u5206\u652f\u548c\u5c40\u90e8\u5757\u72b6\u79e91\u5206\u652f\uff1b2. VaSMP\uff1a\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u7684\u6570\u636e\u65e0\u5173\u8de8\u5c42\u6743\u91cd\u6bd4\u7279\u5206\u914d\uff1b3. VaTMP\uff1a\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5728\u6269\u6563\u65f6\u95f4\u6b65\u4e0a\u8c03\u5ea6\u5c42\u5185\u6fc0\u6d3b\u7cbe\u5ea6", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cQ-DiT4SR\u5728W4A6\u548cW4A4\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002W4A4\u914d\u7f6e\u5b9e\u73b05.8\u500d\u6a21\u578b\u538b\u7f29\u548c\u8d85\u8fc760\u500d\u8ba1\u7b97\u64cd\u4f5c\u51cf\u5c11", "conclusion": "Q-DiT4SR\u662f\u9996\u4e2a\u4e13\u95e8\u4e3aDiT\u8d85\u5206\u8fa8\u7387\u8bbe\u8ba1\u7684PTQ\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5c42\u5206\u89e3\u548c\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7eb9\u7406\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\uff0c\u4e3aDiT\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02453", "abs": "https://arxiv.org/abs/2602.02453", "authors": ["Andong Chen", "Wenxin Zhu", "Qiuyu Ding", "Yuchen Song", "Muyun Yang", "Tiejun Zhao"], "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling", "comment": "Working paper", "summary": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.", "AI": {"tldr": "\u63d0\u51fa\"Thinking with Comics\"\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\uff0c\u4f7f\u7528\u6f2b\u753b\u4f5c\u4e3a\u56fe\u50cf\u548c\u89c6\u9891\u4e4b\u95f4\u7684\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u5a92\u4ecb\uff0c\u5728\u4fdd\u6301\u65f6\u95f4\u7ed3\u6784\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u9759\u6001\u56fe\u50cf\u96be\u4ee5\u8868\u793a\u65f6\u95f4\u7ed3\u6784\uff0c\u800c\u89c6\u9891\u5219\u5197\u4f59\u5ea6\u9ad8\u3001\u8ba1\u7b97\u6210\u672c\u5927\u3002\u9700\u8981\u5bfb\u627e\u4e00\u79cd\u65e2\u80fd\u4fdd\u7559\u65f6\u95f4\u4fe1\u606f\u53c8\u9ad8\u6548\u7684\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4ee5\u6f2b\u753b\u4e3a\u5a92\u4ecb\u7684\u89c6\u89c9\u63a8\u7406\u8303\u5f0f\uff0c\u7cfb\u7edf\u7814\u7a76\u57fa\u4e8e\u6f2b\u753b\u7684\u4e24\u79cd\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cThinking with Comics\u5728\u591a\u6b65\u65f6\u95f4\u548c\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8eThinking with Images\uff0c\u540c\u65f6\u6bd4Thinking with Video\u663e\u8457\u66f4\u9ad8\u6548\u3002\u4e0d\u540c\u6f2b\u753b\u53d9\u4e8b\u7ed3\u6784\u548c\u98ce\u683c\u5bf9\u4efb\u52a1\u6027\u80fd\u6709\u7a33\u5b9a\u5f71\u54cd\u3002", "conclusion": "\u6f2b\u753b\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\uff0c\u80fd\u591f\u6539\u5584\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5728\u4fe1\u606f\u5bc6\u5ea6\u3001\u65f6\u95f4\u7ed3\u6784\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.00879", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00879", "abs": "https://arxiv.org/abs/2602.00879", "authors": ["Hao Mark Chen", "Zhiwen Mo", "Royson Lee", "Qianzhou Wang", "Da Li", "Shell Xu Hu", "Wayne Luk", "Timothy Hospedales", "Hongxiang Fan"], "title": "Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs", "comment": null, "summary": "Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.", "AI": {"tldr": "DES\u6280\u672f\u901a\u8fc7\u5e8f\u5217\u7ea7\u4e13\u5bb6\u5171\u4eab\u89e3\u51b3MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u7206\u70b8\u95ee\u9898\uff0c\u51cf\u5c1155%\u4e13\u5bb6\u6fc0\u6d3b\u548c38%\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u630199%\u51c6\u786e\u7387", "motivation": "MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u89e3\u7801\u65f6\u9762\u4e34\u4e13\u5bb6\u7206\u70b8\u95ee\u9898\uff1a\u5e76\u884c\u751f\u6210\u7684token\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u51e0\u4e4e\u7ebf\u6027\u589e\u957f\uff0c\u5bfc\u81f4\u5185\u5b58\u6d41\u91cf\u6fc0\u589e\uff0c\u4f7f\u63a8\u7406\u8fdb\u5165\u5185\u5b58\u53d7\u9650\u72b6\u6001\uff0c\u62b5\u6d88\u4e86MoE\u548c\u5e76\u884c\u89e3\u7801\u7684\u6548\u7387\u4f18\u52bf", "method": "\u63d0\u51fa\u52a8\u6001\u4e13\u5bb6\u5171\u4eab\uff08DES\uff09\u6280\u672f\uff0c\u5c06MoE\u4f18\u5316\u4ecetoken\u7ea7\u526a\u679d\u548c\u4f20\u7edf\u4e13\u5bb6\u8df3\u8fc7\u65b9\u6cd5\u8f6c\u5411\u5e8f\u5217\u7ea7\u6838\u5fc3\u96c6\u9009\u62e9\u3002\u5305\u62ec\u4e24\u79cd\u521b\u65b0\u9009\u62e9\u7b56\u7565\uff1a1\uff09\u5e8f\u5217\u5185\u5171\u4eab\uff08DES-Seq\uff09\uff0c\u5728\u5e8f\u5217\u7ea7\u522b\u9002\u5e94\u6700\u4f18\u5206\u914d\uff1b2\uff09\u663e\u8457\u6027\u611f\u77e5\u6295\u7968\uff08DES-Vote\uff09\uff0c\u8ba9token\u57fa\u4e8e\u805a\u5408\u8def\u7531\u5668\u6743\u91cd\u5171\u540c\u9009\u4e3e\u6838\u5fc3\u96c6", "result": "\u5728MoE dLLMs\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDES\u51cf\u5c11\u4e86\u8d85\u8fc755%\u7684\u552f\u4e00\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u5ef6\u8fdf\u964d\u4f4e\u8fbe38%\uff0c\u540c\u65f6\u4fdd\u630199%\u7684\u539f\u59cb\u51c6\u786e\u7387\uff0c\u6709\u6548\u89e3\u8026\u4e86\u5185\u5b58\u5f00\u9500\u4e0e\u5e76\u884c\u5ea6", "conclusion": "DES\u901a\u8fc7\u5e8f\u5217\u7ea7\u4e13\u5bb6\u5171\u4eab\u6709\u6548\u89e3\u51b3\u4e86MoE\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u7206\u70b8\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u5f00\u9500\u4e0e\u5e76\u884c\u5ea6\u7684\u89e3\u8026"}}
{"id": "2602.02178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02178", "abs": "https://arxiv.org/abs/2602.02178", "authors": ["Liang Lin", "Feng Xiong", "Zengbin Wang", "Kun Wang", "Junhao Dong", "Xuecai Hu", "Yong Wang", "Xiangxiang Chu"], "title": "AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.", "AI": {"tldr": "AR-MAP\uff1a\u4e00\u79cd\u5229\u7528\u5bf9\u9f50\u597d\u7684\u81ea\u56de\u5f52LLM\u4f5c\u4e3a\u9690\u5f0f\u6559\u5e08\u6765\u5bf9\u9f50\u6269\u6563LLM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u91cd\u7f29\u653e\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u5bf9\u9f50\u7684\u9ad8\u65b9\u5dee\u95ee\u9898", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u4f5c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u5e76\u884ctoken\u751f\u6210\uff0c\u4f46\u5176\u504f\u597d\u5bf9\u9f50\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u57fa\u4e8eELBO\u7684\u4f3c\u7136\u4f30\u8ba1\u4f1a\u5f15\u5165\u9ad8\u65b9\u5dee", "method": "\u63d0\u51faAR-MAP\u6846\u67b6\uff0c\u5229\u7528\u5df2\u5bf9\u9f50\u7684\u81ea\u56de\u5f52LLM\u4f5c\u4e3a\u9690\u5f0f\u6559\u5e08\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u6743\u91cd\u7f29\u653e\u8ba9DLLM\u5438\u6536\u5bf9\u9f50\u77e5\u8bc6\uff0c\u5229\u7528\u4e24\u79cd\u6a21\u578b\u67b6\u6784\u7684\u5171\u4eab\u7ed3\u6784", "result": "\u5728\u591a\u79cd\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cAR-MAP\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u4f18\u4e8e\u73b0\u6709DLLM\u4e13\u7528\u5bf9\u9f50\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u5e73\u5747\u5f97\u520669.08%", "conclusion": "AR-MAP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u65b9\u5dee\u7684DLLM\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u81ea\u56de\u5f52LLM\u7684\u5bf9\u9f50\u77e5\u8bc6\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u5bf9\u9f50\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u9ad8\u65b9\u5dee\u95ee\u9898"}}
{"id": "2602.01277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01277", "abs": "https://arxiv.org/abs/2602.01277", "authors": ["Yihan Xie", "Han Xia", "Zhen Yang"], "title": "TF-Lane: Traffic Flow Module for Robust Lane Perception", "comment": "9 pages, 7 figures, 7 tables", "summary": "Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.", "AI": {"tldr": "\u63d0\u51faTFM\u6a21\u5757\uff0c\u5229\u7528\u5b9e\u65f6\u4ea4\u901a\u6d41\u4fe1\u606f\u589e\u5f3a\u8f66\u9053\u611f\u77e5\uff0c\u89e3\u51b3\u906e\u6321\u6216\u8f66\u9053\u7f3a\u5931\u573a\u666f\u4e0b\u89c6\u89c9\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u7684\u8f66\u9053\u611f\u77e5\u65b9\u6cd5\u5728\u906e\u6321\u6216\u8f66\u9053\u7f3a\u5931\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u4f7f\u7528\u9ad8\u7cbe\u5730\u56fe\u4f5c\u4e3a\u8865\u5145\u4fe1\u606f\u5b58\u5728\u8ba2\u9605\u6210\u672c\u9ad8\u548c\u5b9e\u65f6\u6027\u53d7\u9650\u7684\u95ee\u9898\u3002\u4ea4\u901a\u6d41\u4f5c\u4e3a\u521b\u65b0\u4fe1\u606f\u6e90\uff0c\u5177\u6709\u5b9e\u65f6\u6027\u597d\u4e14\u65e0\u9700\u989d\u5916\u6210\u672c\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faTrafficFlow-aware Lane perception Module (TFM)\uff0c\u6709\u6548\u63d0\u53d6\u5b9e\u65f6\u4ea4\u901a\u6d41\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u8f66\u9053\u611f\u77e5\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002\u8be5\u65b9\u6cd5\u6e90\u4e8e\u771f\u5b9e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u5e76\u5728\u5f00\u6e90\u7b97\u6cd5\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u6a21\u578b\u548c\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08Nuscenes\u548cOpenLaneV2\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTFM\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u83b7\u5f97+4.1% mAP\u589e\u76ca\u3002", "conclusion": "TFM\u901a\u8fc7\u5229\u7528\u5b9e\u65f6\u4ea4\u901a\u6d41\u4fe1\u606f\u6709\u6548\u589e\u5f3a\u4e86\u8f66\u9053\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u4f20\u611f\u5668\u5728\u906e\u6321\u6216\u8f66\u9053\u7f3a\u5931\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8f66\u9053\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02455", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02455", "abs": "https://arxiv.org/abs/2602.02455", "authors": ["Han Bao", "Zheyuan Zhang", "Pengcheng Jing", "Zhengqing Yuan", "Kaiwen Shi", "Yanfang Ye"], "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction", "comment": "65 pages, 40 figures", "summary": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.", "AI": {"tldr": "Drift-Bench\u662f\u9996\u4e2a\u8bc4\u4f30\u81ea\u4e3b\u4ee3\u7406\u5728\u8f93\u5165\u6545\u969c\u4e0b\u591a\u8f6e\u6f84\u6e05\u80fd\u529b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u72b6\u6001\u5bfc\u5411\u548c\u670d\u52a1\u5bfc\u5411\u6267\u884c\u73af\u5883\u8861\u91cf\u4ee3\u7406\u7684\u8bed\u7528\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u4ee3\u7406\u8fc7\u6e21\uff0c\u7528\u6237\u8f93\u5165\u7ecf\u5e38\u8fdd\u53cd\u5408\u4f5c\u5047\u8bbe\uff08\u5982\u9690\u542b\u610f\u56fe\u3001\u7f3a\u5931\u53c2\u6570\u3001\u9519\u8bef\u9884\u8bbe\u6216\u6a21\u7cca\u8868\u8fbe\uff09\uff0c\u4ea7\u751f\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u7684\u6267\u884c\u98ce\u9669\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5047\u8bbe\u6307\u4ee4\u660e\u786e\u6216\u4ec5\u9650\u4e8e\u6587\u672c\u5355\u8f6e\u6f84\u6e05\uff0c\u65e0\u6cd5\u8861\u91cf\u5728\u63a5\u5730\u6267\u884c\u98ce\u9669\u4e0b\u7684\u591a\u8f6e\u6d88\u6b67\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u901a\u4fe1\u7406\u8bba\uff0cDrift-Bench\u63d0\u4f9b\u7edf\u4e00\u7684\u5408\u4f5c\u6545\u969c\u5206\u7c7b\u6cd5\uff0c\u91c7\u7528\u89d2\u8272\u9a71\u52a8\u7684\u7528\u6237\u6a21\u62df\u5668\u548cRise\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u72b6\u6001\u5bfc\u5411\u548c\u670d\u52a1\u5bfc\u5411\u6267\u884c\u73af\u5883\u4e2d\u8bc4\u4f30\u591a\u8f6e\u6f84\u6e05\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u8fd9\u4e9b\u6545\u969c\u4e0b\u4ee3\u7406\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u6f84\u6e05\u6548\u679c\u56e0\u7528\u6237\u89d2\u8272\u548c\u6545\u969c\u7c7b\u578b\u800c\u5f02\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7406\u5728\u5e94\u5bf9\u8f93\u5165\u6545\u969c\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Drift-Bench\u8fde\u63a5\u4e86\u6f84\u6e05\u7814\u7a76\u548c\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\uff0c\u80fd\u591f\u7cfb\u7edf\u8bca\u65ad\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6267\u884c\u7684\u6545\u969c\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.00884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00884", "abs": "https://arxiv.org/abs/2602.00884", "authors": ["Louis Serrano", "Jiequn Han", "Edouard Oyallon", "Shirley Ho", "Rudy Morel"], "title": "Test-time Generalization for Physics through Neural Operator Splitting", "comment": null, "summary": "Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6d4b\u8bd5\u65f6\u795e\u7ecf\u7b97\u5b50\u5206\u88c2\u7b56\u7565\uff0c\u901a\u8fc7\u7ec4\u5408\u8bad\u7ec3\u7b97\u5b50\u6765\u8fd1\u4f3c\u672a\u89c1\u8fc7\u7684\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u5728\u5904\u7406\u8bad\u7ec3\u5206\u5e03\u5916\u7684\u6d4b\u8bd5\u8f93\u5165\uff08\u5982\u65b0\u521d\u59cb\u6761\u4ef6\u3001\u672a\u89c1PDE\u7cfb\u6570\u6216\u7269\u7406\u73b0\u8c61\uff09\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u65b0\u52a8\u529b\u5b66\u7684\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u6837\u672c\u6cdb\u5316", "method": "\u57fa\u4e8eDISCO\uff08\u8de8\u4e0d\u540c\u52a8\u529b\u5b66\u8bad\u7ec3\u7684\u795e\u7ecf\u7b97\u5b50\u5b57\u5178\uff09\uff0c\u63d0\u51fa\u6d4b\u8bd5\u65f6\u795e\u7ecf\u7b97\u5b50\u5206\u88c2\u7b56\u7565\uff0c\u901a\u8fc7\u641c\u7d22\u8bad\u7ec3\u7b97\u5b50\u7684\u7ec4\u5408\u6765\u8fd1\u4f3c\u672a\u89c1\u8fc7\u7684\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6743\u91cd", "result": "\u5728\u53c2\u6570\u5916\u63a8\u548c\u7269\u7406\u73b0\u8c61\u65b0\u7ec4\u5408\u7b49\u6311\u6218\u6027\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6cdb\u5316\u7ed3\u679c\uff0c\u5e76\u80fd\u6062\u590d\u5e95\u5c42PDE\u53c2\u6570", "conclusion": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u662f\u6784\u5efa\u7075\u6d3b\u3001\u53ef\u7ec4\u5408\u548c\u53ef\u6cdb\u5316\u795e\u7ecf\u7b97\u5b50\u7684\u5173\u952e\u9014\u5f84"}}
{"id": "2602.02182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02182", "abs": "https://arxiv.org/abs/2602.02182", "authors": ["Tja\u0161a Ar\u010don", "Matej Klemen", "Marko Robnik-\u0160ikonja", "Kaja Dobrovoljc"], "title": "Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages", "comment": null, "summary": "Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.", "AI": {"tldr": "LLMs\u5728\u5143\u8bed\u8a00\u77e5\u8bc6\uff08\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u663e\u5f0f\u63a8\u7406\uff09\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0cGPT-4o\u8868\u73b0\u6700\u4f73\u4f46\u51c6\u786e\u7387\u4ec50.367\uff0c\u6240\u6709\u6a21\u578b\u90fd\u65e0\u6cd5\u8d85\u8d8a\u591a\u6570\u7c7b\u57fa\u7ebf\uff0c\u4e14\u6027\u80fd\u53d7\u8bed\u8a00\u6570\u5b57\u8d44\u6e90\u53ef\u7528\u6027\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dLLMs\u4e3b\u8981\u5728\u8bed\u8a00\u4f7f\u7528\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u4f46\u5176\u5bf9\u8bed\u8a00\u7ed3\u6784\u7684\u7406\u89e3\uff08\u5143\u8bed\u8a00\u77e5\u8bc6\uff09\u4ecd\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u8bed\u8a00\u57fa\u51c6\u901a\u5e38\u5173\u6ce8\u72ed\u7a84\u73b0\u8c61\u3001\u5f3a\u8c03\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5f88\u5c11\u8bc4\u4f30\u5143\u8bed\u8a00\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u51c6\u786e\u7387\u548c\u5b8f\u89c2F1\u5206\u6570\uff0c\u7ed3\u5408\u591a\u6570\u7c7b\u548c\u968f\u673a\u57fa\u7ebf\uff0c\u5206\u6790\u6574\u4f53\u6027\u80fd\u5e76\u8003\u5bdf\u8bed\u8a00\u9886\u57df\u548c\u8bed\u8a00\u76f8\u5173\u56e0\u7d20\u7684\u53d8\u5316\u3002\u521b\u5efa\u5305\u542b\u591a\u79cd\u8bed\u8a00\u548c\u8bed\u8a00\u73b0\u8c61\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "LLMs\u7684\u5143\u8bed\u8a00\u77e5\u8bc6\u6709\u9650\uff1aGPT-4o\u8868\u73b0\u6700\u597d\u4f46\u51c6\u786e\u7387\u4ec50.367\uff0c\u5f00\u6e90\u6a21\u578b\u843d\u540e\u3002\u6240\u6709\u6a21\u578b\u90fd\u9ad8\u4e8e\u968f\u673a\u57fa\u7ebf\u4f46\u65e0\u6cd5\u8d85\u8d8a\u591a\u6570\u7c7b\u57fa\u7ebf\uff0c\u8868\u660e\u5b83\u4eec\u80fd\u6355\u6349\u8de8\u8bed\u8a00\u6a21\u5f0f\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u6cd5\u533a\u5206\u3002\u6027\u80fd\u56e0\u8bed\u8a00\u9886\u57df\u800c\u5f02\uff0c\u8bcd\u6c47\u7279\u5f81\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u97f3\u7cfb\u7279\u5f81\u6700\u4f4e\u3002\u8bed\u8a00\u5c42\u9762\uff0c\u51c6\u786e\u7387\u4e0e\u6570\u5b57\u8bed\u8a00\u72b6\u6001\u5f3a\u76f8\u5173\uff1a\u6570\u5b57\u5b58\u5728\u5ea6\u9ad8\u3001\u8d44\u6e90\u53ef\u7528\u7684\u8bed\u8a00\u8bc4\u4f30\u66f4\u51c6\u786e\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u663e\u8457\u8f83\u4f4e\u3002", "conclusion": "LLMs\u7684\u5143\u8bed\u8a00\u77e5\u8bc6\u662f\u788e\u7247\u5316\u7684\uff0c\u7531\u6570\u636e\u53ef\u7528\u6027\u5851\u9020\u800c\u975e\u8de8\u4e16\u754c\u8bed\u8a00\u7684\u53ef\u6cdb\u5316\u8bed\u6cd5\u80fd\u529b\u3002\u4f5c\u8005\u53d1\u5e03\u5f00\u6e90\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u9f13\u52b1\u672a\u6765LLMs\u4e2d\u66f4\u5927\u7684\u5168\u7403\u8bed\u8a00\u591a\u6837\u6027\u3002"}}
{"id": "2602.01278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01278", "abs": "https://arxiv.org/abs/2602.01278", "authors": ["Zhengbo Zhang", "Yihe Tian", "Wanke Xia", "Lin Chen", "Yue Sun", "Kun Ding", "Ying Wang", "Bing Xu", "Shiming Xiang"], "title": "DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction", "comment": null, "summary": "Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.", "AI": {"tldr": "DSFC-Net\uff1a\u4e00\u79cd\u53cc\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u4fe1\u606f\u6765\u51c6\u786e\u63d0\u53d6\u519c\u6751\u9053\u8def\uff0c\u89e3\u51b3\u4e86\u519c\u6751\u73af\u5883\u4e2d\u9053\u8def\u68c0\u6d4b\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u519c\u6751\u9053\u8def\u63d0\u53d6\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u8def\u9762\u6750\u6599\u591a\u6837\u6027\u5bfc\u81f4\u7c7b\u5185\u5dee\u5f02\u5927\u3001\u7c7b\u95f4\u533a\u5206\u5ea6\u4f4e\uff1b\u690d\u88ab\u906e\u6321\u7834\u574f\u7a7a\u95f4\u8fde\u7eed\u6027\uff1b\u9053\u8def\u72ed\u7a84\u52a0\u5267\u68c0\u6d4b\u96be\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7ed3\u6784\u5316\u57ce\u5e02\u73af\u5883\uff0c\u5728\u519c\u6751\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDSFC-Net\u53cc\u7f16\u7801\u5668\u6846\u67b6\uff1aCNN\u5206\u652f\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u9053\u8def\u8fb9\u754c\u548c\u77ed\u7a0b\u8fde\u7eed\u6027\uff1b\u7a7a\u95f4-\u9891\u7387\u6df7\u5408\u53d8\u6362\u5668\uff08SFT\uff09\u5efa\u6a21\u5168\u5c40\u62d3\u6251\u4f9d\u8d56\u5173\u7cfb\u4ee5\u5e94\u5bf9\u690d\u88ab\u906e\u6321\u3002SFT\u5305\u542b\u8de8\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\u7b56\u7565\u89e3\u8026\u9ad8\u4f4e\u9891\u4fe1\u606f\u3002\u8fd8\u63d0\u51fa\u901a\u9053\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u901a\u9053\u7279\u5f81\u54cd\u5e94\uff0c\u6574\u5408\u5c40\u90e8\u7eb9\u7406\u548c\u5168\u5c40\u8bed\u4e49\u3002", "result": "\u5728WHU-RuR+\u3001DeepGlobe\u548cMassachusetts\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DSFC-Net\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DSFC-Net\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u548c\u9891\u57df\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u519c\u6751\u9053\u8def\u63d0\u53d6\u7684\u72ec\u7279\u6311\u6218\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.02465", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02465", "abs": "https://arxiv.org/abs/2602.02465", "authors": ["Jana Zeller", "Thadd\u00e4us Wiedemer", "Fanfei Li", "Thomas Klein", "Prasanna Mayilvahanan", "Matthias Bethge", "Felix Wichmann", "Ryan Cotterell", "Wieland Brendel"], "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery", "comment": "9 pages, 8 figures", "summary": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86MentisOculi\u8bc4\u4f30\u5957\u4ef6\uff0c\u53d1\u73b0\u5f53\u524d\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u867d\u7136\u5177\u5907\u6587\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u89c6\u89c9\u601d\u7ef4\u7b56\u7565\u672a\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u524d\u6cbf\u6a21\u578b\u4ece\u4ec5\u80fd\u5904\u7406\u89c6\u89c9\u4fe1\u606f\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5411\u80fd\u591f\u539f\u751f\u4ea4\u9519\u751f\u6210\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8f6c\u53d8\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u4f7f\u7528\u4e2d\u95f4\u53ef\u89c6\u5316\u4f5c\u4e3a\u63a8\u7406\u8f85\u52a9\u5de5\u5177\u7684\u53ef\u80fd\u6027\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5fc3\u7406\u610f\u8c61\u3002", "method": "\u5f00\u53d1\u4e86MentisOculi\u2014\u2014\u4e00\u4e2a\u7a0b\u5e8f\u5316\u3001\u5206\u5c42\u5316\u7684\u591a\u6b65\u63a8\u7406\u95ee\u9898\u5957\u4ef6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u6311\u6218\u524d\u6cbf\u6a21\u578b\u3002\u8bc4\u4f30\u4e86\u4ece\u6f5c\u5728\u6807\u8bb0\u5230\u663e\u5f0f\u751f\u6210\u56fe\u50cf\u7b49\u591a\u79cd\u89c6\u89c9\u7b56\u7565\u3002", "result": "\u89c6\u89c9\u7b56\u7565\u901a\u5e38\u65e0\u6cd5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u867d\u7136\u5177\u5907\u89e3\u51b3\u4efb\u52a1\u7684\u6587\u672c\u63a8\u7406\u80fd\u529b\uff0c\u6709\u65f6\u4e5f\u80fd\u751f\u6210\u6b63\u786e\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u5b58\u5728\u751f\u6210\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u751a\u81f3\u65e0\u6cd5\u6709\u6548\u5229\u7528\u771f\u5b9e\u89c6\u89c9\u5316\u4fe1\u606f\u3002", "conclusion": "\u5c3d\u7ba1\u89c6\u89c9\u601d\u7ef4\u5177\u6709\u5185\u5728\u5438\u5f15\u529b\uff0c\u4f46\u76ee\u524d\u5c1a\u672a\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002MentisOculi\u4e3a\u5206\u6790\u548c\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u5960\u5b9a\u4e86\u5fc5\u8981\u57fa\u7840\u3002"}}
{"id": "2602.00885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00885", "abs": "https://arxiv.org/abs/2602.00885", "authors": ["Ahmad Sarlak", "Abolfazl Razi"], "title": "Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models", "comment": null, "summary": "Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.", "AI": {"tldr": "\u63d0\u51faProbDPP\u65b9\u6cd5\uff0c\u5728\u4f20\u7edfDPP\u6570\u636e\u9009\u62e9\u57fa\u7840\u4e0a\u8003\u8651\u6570\u636e\u8bbf\u95ee\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u9879\u5904\u7406\u6982\u7387\u6027\u6570\u636e\u8bbf\u95ee\uff0c\u5e76\u8bbe\u8ba1\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08\u5982DPP\uff09\u5047\u8bbe\u6570\u636e\u603b\u80fd\u53ef\u9760\u8bbf\u95ee\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u9762\u4e34\u5b58\u50a8\u6545\u969c\u3001\u901a\u4fe1\u95ee\u9898\u7b49\u5bfc\u81f4\u6570\u636e\u8bbf\u95ee\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u4f1a\u5931\u6548\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u6027\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u63d0\u51faProbDPP\u65b9\u6cd5\uff0c\u5c06k-DPP\u91cd\u65b0\u8868\u8ff0\u4e3a\u5305\u542b\u6b63\u5219\u5316\u9879\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5206\u89e3\u4e3a\u51e0\u4f55\u591a\u6837\u6027\u9879\u548c\u4e0d\u53ef\u9760\u6027\u6210\u672c\u3002\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7ec4\u5408\u534a\u8d4c\u535a\u95ee\u9898\uff0c\u8bbe\u8ba1UCB\u98ce\u683c\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u6765\u5b66\u4e60\u672a\u77e5\u7684\u53ef\u9760\u6027\u3002", "result": "ProbDPP\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u6837\u5316\u6570\u636e\u9009\u62e9\u3002\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u9057\u61be\u754c\u9650\uff0c\u786e\u4fdd\u4e86\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "ProbDPP\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5728\u8ba1\u7b97\u548c\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2602.02207", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02207", "abs": "https://arxiv.org/abs/2602.02207", "authors": ["Nisansa de Silva", "Surangika Ranathunga"], "title": "Sinhala Physical Common Sense Reasoning Dataset for Global PIQA", "comment": null, "summary": "This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.", "AI": {"tldr": "\u9996\u4e2a\u50e7\u4f3d\u7f57\u8bed\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b110\u4e2a\u4eba\u5de5\u521b\u5efa\u548c\u9a8c\u8bc1\u7684\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u63d0\u793a\u3001\u6b63\u786e\u7b54\u6848\u548c\u9519\u8bef\u7b54\u6848\uff0c\u4e3b\u8981\u6d89\u53ca\u65af\u91cc\u5170\u5361\u8bed\u5883\u3002", "motivation": "\u4e3a\u50e7\u4f3d\u7f57\u8bed\u521b\u5efa\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u586b\u8865\u8be5\u8bed\u8a00\u5728AI\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7a7a\u767d\uff0c\u652f\u6301\u65af\u91cc\u5170\u5361\u8bed\u5883\u4e0b\u7684AI\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u4eba\u5de5\u521b\u5efa\u548c\u9a8c\u8bc1110\u4e2a\u6570\u636e\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u63d0\u793a\u3001\u6b63\u786e\u7b54\u6848\u548c\u9519\u8bef\u7b54\u6848\uff0c\u95ee\u9898\u4e3b\u8981\u56f4\u7ed5\u65af\u91cc\u5170\u5361\u65e5\u5e38\u751f\u6d3b\u573a\u666f\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u9996\u4e2a\u50e7\u4f3d\u7f57\u8bed\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u6570\u636e\u96c6\uff0c\u4f5c\u4e3aGlobal PIQA\u9879\u76ee\u7684\u4e00\u90e8\u5206\uff0c\u4e3a\u50e7\u4f3d\u7f57\u8bedAI\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u50e7\u4f3d\u7f57\u8bed\u5728\u7269\u7406\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u65af\u91cc\u5170\u5361\u8bed\u5883\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01283", "abs": "https://arxiv.org/abs/2602.01283", "authors": ["Xianhui Zhang", "Chengyu Xie", "Linxia Zhu", "Yonghui Yang", "Weixiang Zhao", "Zifeng Cheng", "Cong Wang", "Fei Shen", "Tat-Seng Chua"], "title": "Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons", "comment": null, "summary": "Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.\n  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.\n  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.\n  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.\n  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.\n  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.\n  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8de8\u8bed\u8a00\u5171\u4eab\u5b89\u5168\u795e\u7ecf\u5143\uff08SS-Neurons\uff09\uff0c\u8fd9\u4e2a\u5c0f\u800c\u5173\u952e\u7684\u795e\u7ecf\u5143\u5b50\u96c6\u5171\u540c\u8c03\u8282\u8de8\u8bed\u8a00\u5b89\u5168\u884c\u4e3a\uff0c\u901a\u8fc7\u9488\u5bf9\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u591a\u8bed\u8a00\u5b89\u5168\u5b58\u5728\u663e\u8457\u4e0d\u5e73\u8861\uff0c\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u5b89\u5168\u6027\u80fd\u8fdc\u5f31\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u4e14\u795e\u7ecf\u673a\u5236\u9a71\u52a8\u5b89\u5168\u5bf9\u9f50\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5355\u8bed\u8a00\u5b89\u5168\u795e\u7ecf\u5143\u5e76\u9a8c\u8bc1\u5176\u56e0\u679c\u4f5c\u7528\uff0c\u7136\u540e\u8bc6\u522b\u8de8\u8bed\u8a00\u5171\u4eab\u5b89\u5168\u795e\u7ecf\u5143\u4f5c\u4e3aHR\u548cNHR\u8bed\u8a00\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u6700\u540e\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u8d44\u6e90\u5206\u5e03\u548c\u6a21\u578b\u67b6\u6784\u7684\u795e\u7ecf\u5143\u5bfc\u5411\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u6291\u5236SS-Neurons\u4f1a\u5bfc\u81f4NHR\u8bed\u8a00\u5b89\u5168\u6027\u80fd\u540c\u65f6\u4e0b\u964d\uff0c\u800c\u589e\u5f3a\u5b83\u4eec\u80fd\u63d0\u9ad8\u8de8\u8bed\u8a00\u9632\u5fa1\u4e00\u81f4\u6027\u3002\u9488\u5bf9\u8be5\u5c0f\u795e\u7ecf\u5143\u5b50\u96c6\u7684\u5fae\u8c03\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347NHR\u5b89\u5168\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002", "conclusion": "LLMs\u4e2d\u5b58\u5728\u8de8\u8bed\u8a00\u5171\u4eab\u5b89\u5168\u795e\u7ecf\u5143\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u662f\u8c03\u8282\u8de8\u8bed\u8a00\u5b89\u5168\u884c\u4e3a\u7684\u5173\u952e\uff0c\u9488\u5bf9\u8fd9\u4e9b\u795e\u7ecf\u5143\u7684\u8bad\u7ec3\u662f\u63d0\u5347\u975e\u9ad8\u8d44\u6e90\u8bed\u8a00\u5b89\u5168\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.02468", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02468", "abs": "https://arxiv.org/abs/2602.02468", "authors": ["Aiden Yiliu Li", "Xinyue Hao", "Shilong Liu", "Mengdi Wang"], "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts", "comment": null, "summary": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.", "AI": {"tldr": "Avenir-Web\u662f\u4e00\u4e2a\u65b0\u578b\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7\u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6\u3001\u7ecf\u9a8c\u6a21\u4eff\u89c4\u5212\u548c\u4efb\u52a1\u8ffd\u8e2a\u68c0\u67e5\u8868\u7b49\u6280\u672f\uff0c\u5728\u771f\u5b9e\u7f51\u9875\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u4e86\u5f00\u6e90SOTA\u6027\u80fd\uff0c\u8fbe\u5230\u4e0e\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406\u5728\u6267\u884c\u590d\u6742\u52a8\u6001\u7f51\u9875\u754c\u9762\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u4ecd\u4e0d\u53ef\u9760\uff0c\u5b58\u5728\u5143\u7d20\u5b9a\u4f4d\u4e0d\u51c6\u786e\u3001\u7f3a\u4e4f\u7ad9\u70b9\u7279\u5b9a\u7a0b\u5e8f\u77e5\u8bc6\u3001\u957f\u65f6\u4efb\u52a1\u8ffd\u8e2a\u548c\u8bb0\u5fc6\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "method": "Avenir-Web\u91c7\u7528\u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6\uff08Mixture of Grounding Experts\uff09\u8fdb\u884c\u51c6\u786e\u5143\u7d20\u5b9a\u4f4d\uff0c\u7ecf\u9a8c\u6a21\u4eff\u89c4\u5212\uff08Experience-Imitation Planning\uff09\u878d\u5165\u7a0b\u5e8f\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u53ca\u4efb\u52a1\u8ffd\u8e2a\u68c0\u67e5\u8868\u7ed3\u5408\u81ea\u9002\u5e94\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u7528\u6237\u754c\u9762\u8303\u5f0f\u7684\u9c81\u68d2\u4ea4\u4e92\u3002", "result": "\u5728Online-Mind2Web\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAvenir-Web\u663e\u8457\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\uff0c\u8fbe\u5230\u4e0e\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u5f00\u6e90\u7f51\u9875\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u65b0SOTA\u3002", "conclusion": "Avenir-Web\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u5b9a\u4f4d\u3001\u7a0b\u5e8f\u77e5\u8bc6\u6574\u5408\u548c\u81ea\u9002\u5e94\u8bb0\u5fc6\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f51\u9875\u4ee3\u7406\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u53ef\u9760\u7f51\u9875\u4ea4\u4e92\u5efa\u7acb\u4e86\u65b0\u7684\u5f00\u6e90\u6807\u51c6\u3002"}}
{"id": "2602.00888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00888", "abs": "https://arxiv.org/abs/2602.00888", "authors": ["Yingjie Niu", "Lanxin Lu", "Changhong Jin", "Ruihai Dong"], "title": "GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation", "comment": null, "summary": "The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.", "AI": {"tldr": "GAPNet\u662f\u4e00\u4e2a\u56fe\u81ea\u9002\u5e94\u63d2\u4ef6\u7f51\u7edc\uff0c\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u80a1\u7968\u5173\u7cfb\u62d3\u6251\u7ed3\u6784\uff0c\u63d0\u5347\u91d1\u878d\u9884\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u56fe\u7ed3\u6784\u6355\u6349\u80a1\u7968\u5173\u7cfb\uff0c\u4f46\u7f51\u7edc\u4fe1\u53f7\u566a\u58f0\u5927\u3001\u5f02\u6b65\u3001\u96be\u4ee5\u83b7\u53d6\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\u4e14\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d", "method": "\u63d0\u51faGAPNet\u63d2\u4ef6\u7f51\u7edc\uff0c\u5305\u542b\u7a7a\u95f4\u611f\u77e5\u5c42\uff08\u6355\u6349\u77ed\u671f\u8d44\u4ea7\u5171\u52a8\uff09\u548c\u65f6\u95f4\u611f\u77e5\u5c42\uff08\u7ef4\u6301\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u957f\u671f\u4f9d\u8d56\uff09\uff0c\u53ef\u9644\u52a0\u5230\u73b0\u6709\u56fe\u6216\u8d85\u56fe\u9aa8\u5e72\u6a21\u578b\u4e0a", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u80a1\u7968\u6570\u636e\u96c6\u4e0a\uff0cGAPNet\u663e\u8457\u63d0\u5347\u76c8\u5229\u6027\u548c\u7a33\u5b9a\u6027\uff0cRT-GCN\u5e74\u5316\u7d2f\u8ba1\u6536\u76ca\u8fbe0.47\uff0cCI-STHPAN\u8fbe0.63\uff0c\u590f\u666e\u6bd4\u7387\u5cf0\u503c\u5206\u522b\u4e3a2.20\u548c2.12", "conclusion": "\u8054\u5408\u5b66\u4e60\u56fe\u7ed3\u6784\u548c\u8868\u793a\u5bf9\u4e8e\u4efb\u52a1\u7279\u5b9a\u7684\u5173\u7cfb\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0cGAPNet\u7684\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u786e\u4fdd\u5176\u5e7f\u6cdb\u9002\u7528\u4e8e\u5404\u79cdGNN\u67b6\u6784"}}
{"id": "2602.02208", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02208", "abs": "https://arxiv.org/abs/2602.02208", "authors": ["Md. Toufique Hasan", "Ayman Asad Khan", "Mika Saari", "Vaishnavi Bankhele", "Pekka Abrahamsson"], "title": "Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study", "comment": "6 pages, 2 figures, submitted to MIPRO 2026", "summary": "Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.", "AI": {"tldr": "AgriHubi\u662f\u4e00\u4e2a\u9488\u5bf9\u82ac\u5170\u8bed\u519c\u4e1a\u51b3\u7b56\u652f\u6301\u7684\u9886\u57df\u81ea\u9002\u5e94\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u82ac\u5170\u519c\u4e1a\u6587\u6863\u548c\u5f00\u653e\u6a21\u578b\uff0c\u7ed3\u5408\u663e\u5f0f\u6765\u6e90\u6807\u6ce8\u548c\u7528\u6237\u53cd\u9988\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u56de\u7b54\u5b8c\u6574\u6027\u3001\u8bed\u8a00\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u53d7\u5230\u9650\u5236\uff1a\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u3001\u82f1\u8bed\u4e2d\u5fc3\u8bad\u7ec3\u6570\u636e\u3001\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e0d\u8db3\u3002\u8fd9\u4e9b\u95ee\u9898\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u66f4\u52a0\u4e25\u91cd\uff0c\u5c3d\u7ba1\u5b58\u5728\u9ad8\u8d28\u91cf\u7684\u9886\u57df\u6587\u6863\uff0c\u4f46\u901a\u7528\u6a21\u578b\u96be\u4ee5\u6709\u6548\u8bbf\u95ee\u3002", "method": "\u5f00\u53d1AgriHubi\u7cfb\u7edf\uff0c\u6574\u5408\u82ac\u5170\u519c\u4e1a\u6587\u6863\u4e0e\u5f00\u653ePORO\u7cfb\u5217\u6a21\u578b\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u67b6\u6784\uff0c\u7ed3\u5408\u663e\u5f0f\u6765\u6e90\u6807\u6ce8\u548c\u7528\u6237\u53cd\u9988\u673a\u5236\u652f\u6301\u8fed\u4ee3\u4f18\u5316\uff0c\u7ecf\u8fc78\u6b21\u8fed\u4ee3\u5f00\u53d1\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u7528\u6237\u7814\u7a76\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5728\u56de\u7b54\u5b8c\u6574\u6027\u3001\u8bed\u8a00\u51c6\u786e\u6027\u548c\u611f\u77e5\u53ef\u9760\u6027\u65b9\u9762\u6709\u660e\u663e\u63d0\u5347\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u5728\u90e8\u7f72\u66f4\u5927\u6a21\u578b\u65f6\u54cd\u5e94\u8d28\u91cf\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u7684\u5b9e\u9645\u6743\u8861\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u8bc4\u4f30\u9886\u57df\u7279\u5b9a\u7684RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u519c\u4e1a\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.01296", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01296", "abs": "https://arxiv.org/abs/2602.01296", "authors": ["Zeran Ke", "Bin Tan", "Gui-Song Xia", "Yujun Shen", "Nan Xue"], "title": "Interacted Planes Reveal 3D Line Mapping", "comment": "submitted to TPAMI", "summary": "3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.", "AI": {"tldr": "LiP-Map\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u7ebf\u548c\u5e73\u9762\u76843D\u7ebf\u56fe\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u53ef\u5b66\u4e60\u7684\u7ebf\u548c\u5e73\u9762\u57fa\u5143\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u9ad8\u6548\u76843D\u7ebf\u56fe\u91cd\u5efa\u3002", "motivation": "\u4ece\u7269\u7406\u548c\u62d3\u6251\u89d2\u5ea6\u51fa\u53d1\uff0c\u8ba4\u4e3a3D\u7ebf\u6700\u81ea\u7136\u5730\u4f5c\u4e3a\u6709\u96503D\u5e73\u9762\u5757\u7684\u8fb9\u7f18\u51fa\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7ebf\u548c\u5e73\u9762\u4e4b\u95f4\u5173\u7cfb\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5728\u4eba\u5de5\u73af\u5883\u4e2d\u7684\u7ed3\u6784\u5316\u91cd\u5efa\u80fd\u529b\u3002", "method": "\u63d0\u51faLiP-Map\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a21\u53ef\u5b66\u4e60\u7684\u7ebf\u548c\u5e73\u9762\u57fa\u5143\uff0c\u901a\u8fc7\u6784\u5efa\u5e73\u9762\u548c\u7ebf\u57fa\u5143\u4e4b\u95f4\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u5c06\u5e73\u9762\u62d3\u6251\u96c6\u6210\u52303D\u7ebf\u56fe\u6784\u5efa\u4e2d\uff0c\u800c\u4e0d\u662f\u65bd\u52a0\u6210\u5bf9\u5171\u9762\u7ea6\u675f\u3002", "result": "\u5728ScanNetV2\u3001ScanNet++\u3001Hypersim\u30017Scenes\u548cTanks&Temple\u7b49100\u591a\u4e2a\u573a\u666f\u4e0a\uff0cLiP-Map\u5728\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5728\u7ebf\u8f85\u52a9\u89c6\u89c9\u5b9a\u4f4d\u65b9\u9762\u4e5f\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u57287Scenes\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u91cd\u5efa\u6548\u7387\u9ad8\uff0c\u901a\u5e38\u6bcf\u4e2a\u573a\u666f\u53ea\u97003-5\u5206\u949f\u3002", "conclusion": "LiP-Map\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ebf\u548c\u5e73\u9762\u57fa\u5143\uff0c\u4e3a\u4eba\u5de5\u73af\u5883\u4e2d\u7684\u7ed3\u6784\u5316\u91cd\u5efa\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7ebf\u56fe\u6784\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5e76\u5728\u89c6\u89c9\u5b9a\u4f4d\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.02470", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02470", "abs": "https://arxiv.org/abs/2602.02470", "authors": ["Xutao Ma", "Yixiao Huang", "Hanlin Zhu", "Somayeh Sojoudi"], "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge", "comment": null, "summary": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form \"$A \\to A$\" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\"\u8eab\u4efd\u6865\u6881\"\uff08A\u2192A\uff09\u6570\u636e\u6b63\u5219\u5316\uff0c\u53ef\u4ee5\u663e\u8457\u7f13\u89e3\u81ea\u56de\u5f52LLM\u4e2d\u7684\"\u9006\u8f6c\u8bc5\u5492\"\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u9ad8\u5c42\u6b21\u7684\u89c4\u5219\u800c\u975e\u7b80\u5355\u8bb0\u5fc6\u4e8b\u5b9e\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b80\u5355\u903b\u8f91\u63a8\u7406\u5982\"\u9006\u8f6c\u8bc5\u5492\"\u4e0a\u5931\u8d25\u3002\u5148\u524d\u7814\u7a76\u8ba4\u4e3a\u8fd9\u662f\u81ea\u56de\u5f52\u56e0\u679cLLM\u7684\u56fa\u6709\u6839\u672c\u9650\u5236\uff0c\u8868\u660e\u6a21\u578b\u503e\u5411\u4e8e\u8bb0\u5fc6\u4e8b\u5b9e\u7ea7\u77e5\u8bc6\u800c\u975e\u6355\u6349\u66f4\u9ad8\u5c42\u6b21\u7684\u89c4\u5219\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u8bd5\u56fe\u8bc1\u660e\u901a\u8fc7\u7b80\u5355\u7684\u6570\u636e\u8c03\u6574\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u79f0\u4e3a\"\u8eab\u4efd\u6865\u6881\"\u7684\u7b80\u5355\u6b63\u5219\u5316\u6570\u636e\u914d\u65b9\uff0c\u5f62\u5f0f\u4e3a\"A\u2192A\"\uff08\u4f8b\u5982\"\u7231\u4e3d\u4e1d\u7684\u540d\u5b57\u662f\u7231\u4e3d\u4e1d\"\uff09\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u5728\u6b64\u914d\u65b9\u4e0b\uff0c\u5373\u4f7f\u5355\u5c42transformer\u4e5f\u80fd\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u5dee\u6765\u6253\u7834\u9006\u8f6c\u8bc5\u5492\u3002\u5b9e\u8bc1\u4e0a\uff0c\u57281B\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\u5fae\u8c03\u4f7f\u7528\u8be5\u6570\u636e\u914d\u65b9\u3002", "result": "\u4f7f\u7528\u63d0\u51fa\u7684\u6570\u636e\u914d\u65b9\u5fae\u8c03\u76841B\u8bed\u8a00\u6a21\u578b\u5728\u9006\u8f6c\u4efb\u52a1\u4e0a\u8fbe\u523040%\u7684\u6210\u529f\u7387\uff0c\u800c\u4ec5\u4f7f\u7528\u524d\u5411\u77e5\u8bc6\u6570\u636e\u8bad\u7ec3\u65f6\u6210\u529f\u7387\u63a5\u8fd1\u96f6\u3002\u8fd9\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u9ad8\u5c42\u6b21\u89c4\u5219\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u4e3a\u9006\u8f6c\u8bc5\u5492\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u4f4e\u6210\u672c\u8def\u5f84\u6765\u9f13\u52b1LLM\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u66f4\u9ad8\u5c42\u6b21\u7684\u89c4\u5219\uff0c\u6311\u6218\u4e86\u5148\u524d\u5173\u4e8e\u81ea\u56de\u5f52LLM\u56fa\u6709\u5c40\u9650\u6027\u7684\u89c2\u70b9\u3002"}}
{"id": "2602.00899", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00899", "abs": "https://arxiv.org/abs/2602.00899", "authors": ["Mritunjay Pandey"], "title": "Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation", "comment": "13 pages, 4 figures. Semantic dense retrieval for content-based recommendation on Amazon Reviews 2023 (Category - Fashion). Dataset statistics: 2.0M users; 825.9K items; 2.5M ratings; 94.9M review tokens; 510.5M metadata tokens. Timespan: May 1996 to September 2023. Metadata includes: user reviews (ratings, text, helpfulness votes, etc.); item metadata (descriptions, price, raw images, etc.)", "summary": "E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.\n  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.\n  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.\n  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7f16\u7801\u5668\u67b6\u6784\u7684\u7a20\u5bc6\u68c0\u7d22\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5546\u63a8\u8350\u4e2d\u7684\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5728\u4e9a\u9a6c\u900a\u8bc4\u8bba\u6570\u636e\u4e0a\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfBM25\u65b9\u6cd5\u66f4\u597d\u7684\u53ec\u56de\u6548\u679c\u3002", "motivation": "\u7535\u5546\u63a8\u8350\u548c\u641c\u7d22\u901a\u5e38\u4f9d\u8d56\u7a00\u758f\u5173\u952e\u8bcd\u5339\u914d\uff08\u5982BM25\uff09\uff0c\u5f53\u7528\u6237\u610f\u56fe\u4e0e\u4ea7\u54c1\u5143\u6570\u636e\u7684\u8bcd\u6c47\u91cd\u53e0\u6709\u9650\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u5931\u6548\u3002\u9700\u8981\u89e3\u51b3\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u5185\u5bb9\u63a8\u8350\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u7684\u7a20\u5bc6\u68c0\u7d22\u7cfb\u7edf\uff0c\u5728\u4e9a\u9a6c\u900a\u8bc4\u8bba\uff08\u65f6\u5c1a\u7c7b\uff09\u6570\u636e\u4e0a\u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u591a\u91cd\u8d1f\u6837\u672c\u6392\u5e8f\u635f\u5931\u8fdb\u884c\u5fae\u8c03\u3002\u8bad\u7ec3\u5bf9\u7531\u8bc4\u8bba\u6587\u672c\uff08\u4f5c\u4e3a\u67e5\u8be2\u4ee3\u7406\uff09\u548c\u7269\u54c1\u5143\u6570\u636e\uff08\u4f5c\u4e3a\u6b63\u6587\u6863\uff09\u6784\u6210\u3002\u4f7f\u7528FAISS HNSW\u7d22\u5f15\u548cONNX Runtime\u63a8\u7406\u7ba1\u9053\uff0c\u7ed3\u5408INT8\u52a8\u6001\u91cf\u5316\u5b9e\u73b0\u9ad8\u6548\u670d\u52a1\u3002", "result": "\u5728826,402\u4e2a\u76ee\u5f55\u7269\u54c1\u7684\u8bc4\u8bba\u5230\u6807\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRecall@10\u4eceBM25\u76840.26\u63d0\u5347\u52300.66\u3002\u540c\u65f6\u6ee1\u8db3\u5b9e\u9645\u5ef6\u8fdf\u548c\u6a21\u578b\u5927\u5c0f\u7ea6\u675f\uff1a\u4e2d\u4f4dCPU\u63a8\u7406\u5ef6\u8fdf\u4e3a6.1\u6beb\u79d2\uff08\u6279\u91cf\u5927\u5c0f\u4e3a1\uff09\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c114\u500d\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u590d\u73b0\u7684\u84dd\u56fe\uff0c\u7528\u4e8e\u5c06\u9886\u57df\u9002\u5e94\u7684\u7a20\u5bc6\u68c0\u7d22\u4ece\u79bb\u7ebf\u8bad\u7ec3\u6269\u5c55\u5230CPU\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u76ee\u5f55\u670d\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u7684\u8bed\u4e49\u5339\u914d\u80fd\u529b\u3002"}}
{"id": "2602.02219", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02219", "abs": "https://arxiv.org/abs/2602.02219", "authors": ["Yuzheng Xu", "Tosho Hirasawa", "Tadashi Kozuno", "Yoshitaka Ushiku"], "title": "Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u91cf\u89c4\u7684LLM\u8bc4\u4f30\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0c\u63d0\u51fa\u5e73\u8861\u6392\u5217\u7b56\u7565\u6765\u7f13\u89e3\u504f\u5dee\u5e76\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027", "motivation": "\u867d\u7136LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u4f30\uff08LLM\u4ece\u591a\u4e2a\u91cf\u89c4\u4e2d\u9009\u62e9\u5206\u6570\uff09\u8f83\u5c11\u88ab\u5206\u6790\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u53d7\u63a7\u5b9e\u9a8c\u8bc1\u660e\u4f4d\u7f6e\u504f\u5dee\u7684\u5b58\u5728\uff0c\u63d0\u51fa\u5e73\u8861\u6392\u5217\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u5206\u6570\u9009\u9879\u5747\u5300\u5206\u5e03\u5728\u5404\u4e2a\u4f4d\u7f6e\u4e0a\uff0c\u5e76\u901a\u8fc7\u805a\u5408\u5e73\u8861\u6392\u5217\u7684\u5206\u6570\u6765\u7f13\u89e3\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u57fa\u4e8e\u91cf\u89c4\u7684LLM\u8bc4\u4f30\u5b58\u5728\u4e00\u81f4\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u5e73\u8861\u6392\u5217\u7b56\u7565\u4e0d\u4ec5\u80fd\u63ed\u793a\u6f5c\u5728\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u9ad8LLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u57fa\u4e8e\u91cf\u89c4\u7684LLM\u8bc4\u4f30\u5e76\u975e\u672c\u8d28\u4e0a\u7684\u70b9\u5f0f\u8bc4\u4f30\uff0c\u7b80\u5355\u7684\u57fa\u4e8e\u6392\u5217\u7684\u6821\u51c6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5176\u53ef\u9760\u6027\uff0c\u4f4d\u7f6e\u504f\u5dee\u662f\u9700\u8981\u8003\u8651\u7684\u91cd\u8981\u95ee\u9898\u3002"}}
{"id": "2602.01298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01298", "abs": "https://arxiv.org/abs/2602.01298", "authors": ["Ching-Kai Huang", "Wen-Chieh Lin", "Yan-Cen Lee"], "title": "Interaction-Consistent Object Removal via MLLM-Based Reasoning", "comment": null, "summary": "Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.", "AI": {"tldr": "\u63d0\u51faICOR\u95ee\u9898\uff08\u4ea4\u4e92\u4e00\u81f4\u7684\u5bf9\u8c61\u79fb\u9664\uff09\uff0c\u8981\u6c42\u79fb\u9664\u76ee\u6807\u5bf9\u8c61\u53ca\u5176\u76f8\u5173\u7684\u4ea4\u4e92\u5143\u7d20\uff0c\u5e76\u5f00\u53d1REORM\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9700\u8981\u8054\u5408\u79fb\u9664\u7684\u5143\u7d20\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u5bf9\u8c61\u79fb\u9664\u65b9\u6cd5\u901a\u5e38\u53ea\u79fb\u9664\u547d\u540d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u4f46\u5ffd\u7565\u4e86\u4e0e\u4e4b\u76f8\u5173\u7684\u4ea4\u4e92\u8bc1\u636e\uff08\u5982\u5149\u7167\u6548\u679c\u3001\u7269\u7406\u8fde\u63a5\u5bf9\u8c61\u3001\u76ee\u6807\u4ea7\u751f\u7684\u5143\u7d20\u7b49\uff09\uff0c\u5bfc\u81f4\u7ed3\u679c\u5728\u8bed\u4e49\u4e0a\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faREORM\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff1a1\uff09MLLM\u9a71\u52a8\u5206\u6790\u63a8\u65ad\u9700\u8981\u8054\u5408\u79fb\u9664\u7684\u5143\u7d20\uff1b2\uff09\u63a9\u7801\u5f15\u5bfc\u79fb\u9664\uff1b3\uff09\u81ea\u6821\u6b63\u673a\u5236\uff1b4\uff09\u652f\u6301\u6709\u9650\u8d44\u6e90\u7684\u672c\u5730\u90e8\u7f72\u53d8\u4f53\u3002\u540c\u65f6\u6784\u5efaICOREval\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728ICOREval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREORM\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u4ea7\u751f\u4ea4\u4e92\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "conclusion": "ICOR\u95ee\u9898\u5f62\u5f0f\u5316\u4e86\u5bf9\u8c61\u79fb\u9664\u4e2d\u7684\u4ea4\u4e92\u4e00\u81f4\u6027\u95ee\u9898\uff0cREORM\u6846\u67b6\u901a\u8fc7MLLM\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.02475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02475", "abs": "https://arxiv.org/abs/2602.02475", "authors": ["Shraddha Barke", "Arnav Goyal", "Alind Khare", "Avaljot Singh", "Suman Nath", "Chetan Bansal"], "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories", "comment": null, "summary": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.", "AI": {"tldr": "\u63d0\u51faAGENTRX\u6846\u67b6\uff0c\u81ea\u52a8\u8bca\u65adAI\u4ee3\u7406\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684\u5173\u952e\u5931\u8d25\u6b65\u9aa4\u548c\u7c7b\u522b\uff0c\u901a\u8fc7\u7ea6\u675f\u5408\u6210\u548c\u9a8c\u8bc1\u65e5\u5fd7\u5b9e\u73b0", "motivation": "AI\u4ee3\u7406\u5931\u8d25\u96be\u4ee5\u5b9a\u4f4d\uff0c\u56e0\u4e3a\u6267\u884c\u5177\u6709\u6982\u7387\u6027\u3001\u957f\u65f6\u7a0b\u3001\u591a\u4ee3\u7406\u548c\u566a\u58f0\u5de5\u5177\u8f93\u51fa\u7684\u7279\u70b9\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6cd5", "method": "AGENTRX\u6846\u67b6\uff1a\u5408\u6210\u7ea6\u675f\u6761\u4ef6\uff0c\u9010\u6b65\u8bc4\u4f30\uff0c\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u7ea6\u675f\u8fdd\u53cd\u9a8c\u8bc1\u65e5\u5fd7\uff0c\u4f7f\u7528LLM\u6cd5\u5b98\u5b9a\u4f4d\u5173\u952e\u6b65\u9aa4\u548c\u5931\u8d25\u7c7b\u522b", "result": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u7ed3\u6784\u5316API\u5de5\u4f5c\u6d41\u3001\u4e8b\u4ef6\u7ba1\u7406\u3001\u5f00\u653e\u5f0fWeb/\u6587\u4ef6\u4efb\u52a1\uff09\u4e2d\uff0c\u6846\u67b6\u5728\u6b65\u9aa4\u5b9a\u4f4d\u548c\u5931\u8d25\u5f52\u56e0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "conclusion": "AGENTRX\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u9886\u57df\u65e0\u5173\u7684\u8bca\u65ad\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3AI\u4ee3\u7406\u5931\u8d25\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b115\u4e2a\u5931\u8d25\u8f68\u8ff9\u7684\u65b0\u57fa\u51c6"}}
{"id": "2602.00906", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DS", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.00906", "abs": "https://arxiv.org/abs/2602.00906", "authors": ["Anxin Guo", "Jingwei Li"], "title": "Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing", "comment": null, "summary": "Large language models often hallucinate with high confidence on \"random facts\" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified \"closed world\" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5c06LLM\u8bb0\u5fc6\u968f\u673a\u4e8b\u5b9e\u5efa\u6a21\u4e3a\u6210\u5458\u6d4b\u8bd5\u95ee\u9898\uff0c\u8bc1\u660e\u5373\u4f7f\u5728\u6700\u4f18\u8bad\u7ec3\u548c\u5b8c\u7f8e\u6570\u636e\u4e0b\uff0c\u6709\u9650\u5bb9\u91cf\u4e0b\u7684\u6700\u4f18\u7b56\u7565\u662f\u7ed9\u67d0\u4e9b\u975e\u4e8b\u5b9e\u5206\u914d\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u5e7b\u89c9\u4e0d\u53ef\u907f\u514d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u5bf9\u7f3a\u4e4f\u53ef\u63a8\u65ad\u6a21\u5f0f\u7684\"\u968f\u673a\u4e8b\u5b9e\"\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5e7b\u89c9\u3002\u4f5c\u8005\u5e0c\u671b\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u5f62\u5f0f\u5316\u5206\u6790\u8fd9\u79cd\u73b0\u8c61\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5373\u4f7f\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u5e7b\u89c9\u4ecd\u7136\u5b58\u5728\u3002", "method": "\u5c06\u968f\u673a\u4e8b\u5b9e\u7684\u8bb0\u5fc6\u5efa\u6a21\u4e3a\u6210\u5458\u6d4b\u8bd5\u95ee\u9898\uff0c\u7edf\u4e00Bloom\u6ee4\u6ce2\u5668\u7684\u79bb\u6563\u8bef\u5dee\u5ea6\u91cf\u4e0eLLM\u7684\u8fde\u7eed\u5bf9\u6570\u635f\u5931\u3002\u5728\u4e8b\u5b9e\u7a00\u758f\u7684\u5047\u8bbe\u4e0b\uff0c\u5efa\u7acb\u7387\u5931\u771f\u5b9a\u7406\uff0c\u7528\u4e8b\u5b9e\u4e0e\u975e\u4e8b\u5b9e\u5f97\u5206\u5206\u5e03\u4e4b\u95f4\u7684\u6700\u5c0fKL\u6563\u5ea6\u8868\u5f81\u6700\u4f18\u8bb0\u5fc6\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6700\u4f18\u8bad\u7ec3\u3001\u5b8c\u7f8e\u6570\u636e\u548c\u7b80\u5316\"\u5c01\u95ed\u4e16\u754c\"\u8bbe\u7f6e\u4e0b\uff0c\u6709\u9650\u5bb9\u91cf\u4e0b\u7684\u4fe1\u606f\u8bba\u6700\u4f18\u7b56\u7565\u4e0d\u662f\u5f03\u6743\u6216\u9057\u5fd8\uff0c\u800c\u662f\u7ed9\u67d0\u4e9b\u975e\u4e8b\u5b9e\u5206\u914d\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u5bfc\u81f4\u5e7b\u89c9\u3002\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\u3002", "conclusion": "\u5e7b\u89c9\u662f\u635f\u5931\u538b\u7f29\u7684\u81ea\u7136\u7ed3\u679c\uff0c\u5373\u4f7f\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u4e5f\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u3002\u8fd9\u4e3a\u7406\u89e3LLM\u5e7b\u89c9\u63d0\u4f9b\u4e86\u65b0\u7684\u4fe1\u606f\u8bba\u89c6\u89d2\uff0c\u8868\u660e\u6709\u9650\u5bb9\u91cf\u4e0b\u7684\u6700\u4f18\u7b56\u7565\u5fc5\u7136\u5305\u542b\u4e00\u5b9a\u7a0b\u5ea6\u7684\u5e7b\u89c9\u3002"}}
{"id": "2602.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02221", "abs": "https://arxiv.org/abs/2602.02221", "authors": ["Frederic Blum", "Johann-Mattis List"], "title": "Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation", "comment": "Accepted for the L'Change workshop @ EACL 2026", "summary": "Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u5e73\u8861\u5e73\u5747\u91cd\u590d\u7387\u6765\u8861\u91cf\u8bed\u97f3\u5bf9\u5e94\u6a21\u5f0f\u7684\u89c4\u5f8b\u6027\uff0c\u5e76\u5f00\u53d1\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc6\u522b\u7f3a\u4e4f\u89c4\u5f8b\u6027\u7684\u540c\u6e90\u8bcd\u96c6\u3002", "motivation": "\u5386\u53f2\u8bed\u8a00\u6bd4\u8f83\u4e2d\uff0c\u8bed\u97f3\u5bf9\u5e94\u89c4\u5f8b\u6027\u662f\u4e3b\u8981\u8bc1\u636e\uff0c\u4f46\u4f20\u7edf\u4e0a\u66f4\u591a\u4f9d\u8d56\u76f4\u89c9\u5224\u65ad\u800c\u975e\u91cf\u5316\u8bc4\u4f30\uff0c\u4e14\u4e0d\u89c4\u5219\u73b0\u8c61\u6bd4\u65b0\u8bed\u6cd5\u5b66\u6d3e\u6a21\u578b\u9884\u671f\u7684\u66f4\u5e38\u89c1\u3002\u968f\u7740\u8ba1\u7b97\u5386\u53f2\u8bed\u8a00\u5b66\u7684\u53d1\u5c55\u548c\u6807\u51c6\u5316\u8bcd\u6c47\u6570\u636e\u7684\u589e\u52a0\uff0c\u9700\u8981\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u5e76\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5e73\u8861\u5e73\u5747\u91cd\u590d\u7387\u4f5c\u4e3a\u65b0\u7684\u89c4\u5f8b\u6027\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6b64\u5ea6\u91cf\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc6\u522b\u5bf9\u5e94\u6a21\u5f0f\u7f3a\u4e4f\u89c4\u5f8b\u6027\u7684\u540c\u6e90\u8bcd\u96c6\u3002\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u91c7\u7528\u7559\u4e00\u9a8c\u8bc1\u6cd5\u6d4b\u91cf\u540c\u6e90\u8bcd\u96c6\u7684\u89c4\u5f8b\u6027\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523085%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u5c55\u793a\u4e86\u4f7f\u7528\u5927\u6570\u636e\u96c6\u5b50\u6837\u672c\u7684\u597d\u5904\uff0c\u4ee5\u53ca\u6570\u636e\u4e2d\u4e0d\u89c4\u5219\u6027\u589e\u52a0\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u65b0\u7684\u89c4\u5f8b\u6027\u5ea6\u91cf\u548c\u57fa\u4e8e\u6b64\u7684\u4e0d\u89c4\u5219\u540c\u6e90\u8bcd\u8bc6\u522b\u65b9\u6cd5\u5728\u63d0\u9ad8\u73b0\u6709\u548c\u672a\u6765\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u6bd4\u8f83\u6570\u636e\u96c6\u8d28\u91cf\u65b9\u9762\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2602.01303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01303", "abs": "https://arxiv.org/abs/2602.01303", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Chu Chen", "Wei Tang", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation", "comment": null, "summary": "Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory", "AI": {"tldr": "ReDiStory\uff1a\u901a\u8fc7\u63a8\u7406\u65f6\u63d0\u793a\u5d4c\u5165\u91cd\u7ec4\u6539\u8fdb\u591a\u5e27\u6545\u4e8b\u751f\u6210\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u51cf\u5c11\u5e27\u95f4\u8bed\u4e49\u5e72\u6270\uff0c\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027", "motivation": "\u751f\u6210\u8fde\u8d2f\u89c6\u89c9\u6545\u4e8b\u9700\u8981\u5728\u591a\u5f20\u56fe\u50cf\u4e2d\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u7684\u540c\u65f6\u4fdd\u7559\u5e27\u7279\u5b9a\u8bed\u4e49\u3002\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\u5c06\u8eab\u4efd\u548c\u5e27\u63d0\u793a\u62fc\u63a5\u4e3a\u7edf\u4e00\u8868\u793a\uff0c\u4f46\u5728\u590d\u6742\u6545\u4e8b\u4e2d\u5e38\u5f15\u5165\u5e27\u95f4\u8bed\u4e49\u5e72\u6270\uff0c\u524a\u5f31\u8eab\u4efd\u4fdd\u6301\u80fd\u529b", "method": "\u63d0\u51faReDiStory\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u5bf9\u63d0\u793a\u5d4c\u5165\u8fdb\u884c\u91cd\u7ec4\uff1a1\uff09\u5c06\u6587\u672c\u5d4c\u5165\u663e\u5f0f\u5206\u89e3\u4e3a\u8eab\u4efd\u76f8\u5173\u548c\u5e27\u7279\u5b9a\u7ec4\u4ef6\uff1b2\uff09\u901a\u8fc7\u6291\u5236\u8de8\u5e27\u5171\u4eab\u65b9\u5411\u6765\u53bb\u76f8\u5173\u5e27\u5d4c\u5165\uff0c\u51cf\u5c11\u4ea4\u53c9\u5e72\u6270", "result": "\u5728\u76f8\u540c\u6269\u6563\u4e3b\u5e72\u548c\u63a8\u7406\u8bbe\u7f6e\u4e0b\uff0cReDiStory\u5728ConsiStory+\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd41Prompt1Story\u5728\u591a\u4e2a\u8eab\u4efd\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u6539\u5584\u8eab\u4efd\u4e00\u81f4\u6027\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u4fdd\u771f\u5ea6", "conclusion": "ReDiStory\u901a\u8fc7\u63a8\u7406\u65f6\u63d0\u793a\u5d4c\u5165\u91cd\u7ec4\u6709\u6548\u51cf\u5c11\u591a\u5e27\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u8de8\u5e27\u8bed\u4e49\u5e72\u6270\uff0c\u65e0\u9700\u4fee\u6539\u6269\u6563\u53c2\u6570\u6216\u989d\u5916\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u8eab\u4efd\u4e00\u81f4\u6027"}}
{"id": "2211.11434", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2211.11434", "abs": "https://arxiv.org/abs/2211.11434", "authors": ["Lucas Lange", "Maja Schneider", "Peter Christen", "Erhard Rahm"], "title": "Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)", "comment": "Extended version of the paper accepted at the 20th International Conference on Security and Cryptography SECRYPT 2023. This version is more detailed and includes additional content: a longer results chapter and an appendix containing a proof", "summary": "Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728COVID-19\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e94\u7528\u5dee\u5206\u9690\u79c1(DP)\u7684\u5b9e\u9645\u6548\u679c\uff0c\u901a\u8fc7\u6210\u5458\u63a8\u7406\u653b\u51fb(MIA)\u5b9e\u8bc1\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4\u6548\u679c\uff0c\u53d1\u73b0DP\u5bf9\u5b9e\u9645MIA\u9632\u5fa1\u7684\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684COVID-19\u7b5b\u67e5\u7814\u7a76\u5b58\u5728\u6570\u636e\u96c6\u5c0f\u3001\u9690\u79c1\u4fdd\u8bc1\u5f31\u6216\u4e0d\u660e\u786e\u3001\u672a\u7814\u7a76\u5b9e\u9645\u9690\u79c1\u6548\u679c\u7b49\u95ee\u9898\u3002\u9700\u8981\u6539\u8fdb\u5dee\u5206\u9690\u79c1\u5728\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8003\u8651\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u66f4\u4e25\u683c\u7684\u9690\u79c1\u9884\u7b97\u4e0b\u8bc4\u4f30\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002\u901a\u8fc7\u9ed1\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb(MIA)\u5b9e\u8bc1\u4f30\u8ba1\u5b9e\u9645\u9690\u79c1\u6cc4\u6f0f\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u4e0d\u540c\u4efb\u52a1\u6240\u9700\u7684\u9690\u79c1\u7ea7\u522b\u53ef\u80fd\u56e0MIA\u7684\u5b9e\u9645\u5a01\u80c1\u800c\u5f02\uff1b2) \u968f\u7740DP\u4fdd\u8bc1\u589e\u5f3a\uff0c\u5b9e\u9645\u9690\u79c1\u6cc4\u6f0f\u4ec5\u8fb9\u9645\u6539\u5584\uff0cDP\u5bf9MIA\u9632\u5fa1\u7684\u5b9e\u9645\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u5dee\u5206\u9690\u79c1\u5728COVID-19\u5206\u7c7b\u4efb\u52a1\u4e2d\u5bf9\u5b9e\u9645\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u9632\u5fa1\u6548\u679c\u6709\u9650\u3002\u57fa\u4e8e\u653b\u51fb\u7279\u5b9a\u7684\u5b9e\u8bc1\u9690\u79c1\u8bc4\u4f30\u53ef\u4ee5\u66f4\u597d\u5730\u8c03\u6574\u5b9e\u9645\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002"}}
{"id": "2602.00907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00907", "abs": "https://arxiv.org/abs/2602.00907", "authors": ["Pingping Wang", "Yihong Yuan", "Lingcheng Li", "Yongmei Lu"], "title": "PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning", "comment": null, "summary": "PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.", "AI": {"tldr": "PyGALAX\u662f\u4e00\u4e2a\u96c6\u6210\u4e86AutoML\u548cXAI\u7684Python\u5730\u7406\u7a7a\u95f4\u5206\u6790\u5305\uff0c\u7528\u4e8e\u5904\u7406\u7a7a\u95f4\u5f02\u8d28\u6027\u7684\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u76f8\u6bd4\u4f20\u7edfGWR\u65b9\u6cd5\u6709\u66f4\u597d\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5730\u7406\u7a7a\u95f4\u5206\u6790\u4e2d\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u80fd\u591f\u66f4\u4fbf\u6377\u5730\u4f7f\u7528\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8eGALAX\u6846\u67b6\u6539\u8fdb\uff0c\u96c6\u6210AutoML\u81ea\u52a8\u9009\u62e9\u548c\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528SHAP\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u65b0\u589e\u81ea\u52a8\u5e26\u5bbd\u9009\u62e9\u548c\u7075\u6d3b\u6838\u51fd\u6570\u9009\u62e9\u529f\u80fd\u3002", "result": "PyGALAX\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5730\u7406\u52a0\u6743\u56de\u5f52\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u7a7a\u95f4\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u5168\u5c40\u548c\u5c40\u90e8\u5c3a\u5ea6\u7684\u900f\u660e\u6d1e\u5bdf\u3002", "conclusion": "PyGALAX\u5c06\u5148\u8fdb\u7684\u5730\u7406\u7a7a\u95f4\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6253\u5305\u6210\u53ef\u8bbf\u95ee\u3001\u53ef\u590d\u73b0\u3001\u6613\u90e8\u7f72\u7684Python\u5de5\u5177\u5305\uff0c\u4e3a\u5730\u7406\u5b66\u3001\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883\u79d1\u5b66\u7b49\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5f3a\u5927\u652f\u6301\u3002"}}
{"id": "2602.02266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02266", "abs": "https://arxiv.org/abs/2602.02266", "authors": ["Tan Sang Nguyen", "Muhammad Reza Qorib", "Hwee Tou Ng"], "title": "OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data", "comment": null, "summary": "Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.", "AI": {"tldr": "OpenSeal\u662f\u9996\u4e2a\u771f\u6b63\u5f00\u6e90\u7684\u4e1c\u5357\u4e9a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u4ec5\u4f7f\u752834.7B\u5e73\u884c\u6570\u636e\u57288x H200 GPU\u4e0a\u8bad\u7ec3180\u5c0f\u65f6\uff0c\u6027\u80fd\u5ab2\u7f8e\u540c\u7c7b\u6a21\u578b", "motivation": "\u73b0\u6709LLM\u591a\u4e3a\u82f1\u8bed\u4e2d\u5fc3\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u4e1c\u5357\u4e9a\u8bed\u8a00\u6a21\u578b\u867d\u5df2\u51fa\u73b0\uff0c\u4f46\u90fd\u4e0d\u662f\u771f\u6b63\u5f00\u6e90\uff08\u672a\u516c\u5f00\u8bad\u7ec3\u6570\u636e\uff09\u3002\u771f\u6b63\u5f00\u6e90\u6a21\u578b\u5bf9\u900f\u660e\u5ea6\u3001\u7406\u89e3\u6a21\u578b\u5185\u90e8\u673a\u5236\uff08\u504f\u89c1\u3001\u6cdb\u5316\u3001\u591a\u8bed\u8a00\u6027\uff09\u81f3\u5173\u91cd\u8981", "method": "\u901a\u8fc7\u53d7\u63a7\u7efc\u5408\u5b9e\u9a8c\u7814\u7a76\u5e73\u884c\u6570\u636e\u5728LLM\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u4ec5\u4f7f\u7528\u5e73\u884c\u6570\u636e\u662f\u6269\u5c55LLM\u5230\u65b0\u8bed\u8a00\u7684\u6700\u6709\u6548\u65b9\u6cd5", "result": "\u4ec5\u4f7f\u752834.7B tokens\u5e73\u884c\u6570\u636e\u548c180\u5c0f\u65f68x H200 GPU\u8bad\u7ec3\uff0c\u6784\u5efa\u4e86OpenSeal\uff0c\u8fd9\u662f\u9996\u4e2a\u771f\u6b63\u5f00\u6e90\u7684\u4e1c\u5357\u4e9aLLM\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u540c\u7c7b\u6a21\u578b\u76f8\u5f53", "conclusion": "\u5e73\u884c\u6570\u636e\u5728\u6269\u5c55LLM\u5230\u65b0\u8bed\u8a00\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0cOpenSeal\u7684\u6210\u529f\u8bc1\u660e\u4e86\u901a\u8fc7\u6709\u9650\u5e73\u884c\u6570\u636e\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u771f\u6b63\u5f00\u6e90\u591a\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u8303\u4f8b"}}
{"id": "2602.01305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01305", "abs": "https://arxiv.org/abs/2602.01305", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Wei Tang", "Chu Chen", "Kangning Cui", "Mohd Yamani Idna Idris"], "title": "StoryState: Agent-Based State Control for Consistent and Editable Storybooks", "comment": null, "summary": "Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState", "AI": {"tldr": "StoryState\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u7f16\u6392\u5c42\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u53ef\u7f16\u8f91\u7684\u6545\u4e8b\u72b6\u6001\u6765\u6539\u8fdb\u591a\u6a21\u6001\u6545\u4e8b\u4e66\u751f\u6210\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7f16\u8f91\u5e76\u4fdd\u6301\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u4e00\u952e\u5f0f\u6545\u4e8b\u4e66\u751f\u6210\u65b9\u6cd5\u4e2d\uff0c\u6545\u4e8b\u72b6\u6001\uff08\u89d2\u8272\u3001\u4e16\u754c\u8bbe\u5b9a\u3001\u9875\u9762\u5bf9\u8c61\uff09\u662f\u9690\u5f0f\u7684\uff0c\u5bfc\u81f4\u7f16\u8f91\u7c97\u7cd9\u4e14\u5bb9\u6613\u7834\u574f\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "method": "StoryState\u5c06\u6545\u4e8b\u8868\u793a\u4e3a\u7ed3\u6784\u5316\u5bf9\u8c61\uff08\u89d2\u8272\u8868\u3001\u5168\u5c40\u8bbe\u7f6e\u3001\u9875\u9762\u573a\u666f\u7ea6\u675f\uff09\uff0c\u4f7f\u7528\u5c11\u91cfLLM\u4ee3\u7406\u7ef4\u62a4\u72b6\u6001\u5e76\u751f\u62101Prompt1Story\u98ce\u683c\u7684\u63d0\u793a\uff0c\u7eaf\u63d0\u793a\u64cd\u4f5c\u4f7f\u5176\u4e0e\u591a\u79cd\u751f\u6210\u540e\u7aef\u517c\u5bb9\u3002", "result": "\u5728\u591a\u9875\u9762\u7f16\u8f91\u4efb\u52a1\u4e2d\uff0cStoryState\u5b9e\u73b0\u4e86\u5c40\u90e8\u9875\u9762\u7f16\u8f91\uff0c\u63d0\u9ad8\u4e86\u8de8\u9875\u9762\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e86\u610f\u5916\u66f4\u6539\u3001\u4ea4\u4e92\u8f6e\u6b21\u548c\u7f16\u8f91\u65f6\u95f4\uff0c\u63a5\u8fd1Gemini Storybook\u7684\u4e00\u6b21\u6027\u4e00\u81f4\u6027\u3002", "conclusion": "StoryState\u901a\u8fc7\u663e\u5f0f\u6545\u4e8b\u72b6\u6001\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6545\u4e8b\u751f\u6210\u7684\u7f16\u8f91\u548c\u4e00\u81f4\u6027\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2401.13327", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2401.13327", "abs": "https://arxiv.org/abs/2401.13327", "authors": ["Lucas Lange", "Nils Wenzlitschke", "Erhard Rahm"], "title": "Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection", "comment": "Published in the MDPI Sensors Journal", "summary": "Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u6765\u5408\u6210\u667a\u80fd\u624b\u8868\u5065\u5eb7\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u538b\u529b\u68c0\u6d4b\u7814\u7a76\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u6570\u636e\u53ef\u7528\u6027\u3002", "motivation": "\u667a\u80fd\u624b\u8868\u5065\u5eb7\u4f20\u611f\u5668\u6570\u636e\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u538b\u529b\u68c0\u6d4b\u7b49\u533b\u7597\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u63d0\u4f9b\u8db3\u591f\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u5408\u6210\u591a\u4f20\u611f\u5668\u667a\u80fd\u624b\u8868\u5065\u5eb7\u8bfb\u6570\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u6d4b\u8bd5\u591a\u79cdGAN\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5728\u5b9e\u9645\u538b\u529b\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u57fa\u4e8eGAN\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff1a\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u573a\u666f\u4e0bF1\u5206\u6570\u63d0\u534711.90-15.48%\uff0c\u975e\u9690\u79c1\u8bad\u7ec3\u573a\u666f\u4e0b\u4ecd\u67090.45%\u63d0\u5347\u3002\u4f46\u9690\u79c1\u8981\u6c42\u589e\u5f3a\u4f1a\u663e\u8457\u5f71\u54cd\u5408\u6210\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u5728\u4f18\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u8bad\u7ec3\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4f46\u9700\u8981\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2602.00910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00910", "abs": "https://arxiv.org/abs/2602.00910", "authors": ["Cuong Manh Nguyen", "Truong-Son Hy"], "title": "Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment", "comment": null, "summary": "Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u533b\u7597\u56fe\u50cf\u5206\u6790\u4e2d\u9ad8\u6548\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u91cd\u70b9\u5173\u6ce8CNN\u3001\u8f7b\u91cf\u7ea7Transformer\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u538b\u7f29\u7b56\u7565\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u56fe\u50cf\u5206\u6790\u4e2d\u53d1\u6325\u4e86\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u90e8\u7f72\u4e2d\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u9650\u5236\u548c\u60a3\u8005\u6570\u636e\u9690\u79c1\u7b49\u6311\u6218\u3002\u4e91\u5904\u7406\u65b9\u5f0f\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u800c\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u5c06\u73b0\u4ee3\u9ad8\u6548\u6a21\u578b\u5206\u4e3a\u4e09\u7c7b\uff1a\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNNs)\u3001\u8f7b\u91cf\u7ea7Transformer\u548c\u65b0\u5174\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u578b\u3002\u540c\u65f6\u7cfb\u7edf\u5206\u6790\u4e86\u6a21\u578b\u538b\u7f29\u7b56\u7565\uff0c\u5305\u62ec\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u4f4e\u79e9\u5206\u89e3\u7b49\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u533b\u7597\u9886\u57df\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u5168\u9762\u5206\u7c7b\u548c\u5206\u6790\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u538b\u7f29\u7b56\u7565\u5728\u4fdd\u6301\u8bca\u65ad\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u9700\u6c42\u7684\u6548\u80fd\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4ece\u9ad8\u6027\u80fdAI\u5230\u8d44\u6e90\u53d7\u9650\u4e34\u5e8a\u73af\u5883\u7684\u8fc7\u6e21\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u8bbe\u5907\u7aef\u667a\u80fd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2602.02270", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02270", "abs": "https://arxiv.org/abs/2602.02270", "authors": ["El Batoul Bechiri", "Dihia Lanasri"], "title": "dziribot: rag based intelligent conversational agent for algerian arabic dialect", "comment": null, "summary": "The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.", "AI": {"tldr": "DziriBOT\u662f\u4e00\u4e2a\u9488\u5bf9\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00Darja\u8bbe\u8ba1\u7684\u6df7\u5408\u667a\u80fd\u5bf9\u8bdd\u4ee3\u7406\uff0c\u91c7\u7528\u591a\u5c42\u67b6\u6784\u6574\u5408NLU\u548cRAG\u6280\u672f\uff0c\u901a\u8fc7\u5fae\u8c03DziriBERT\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u65b9\u8a00\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5ba2\u6237\u670d\u52a1\u6570\u5b57\u5316\u9700\u6c42\u589e\u957f\uff0c\u4f46\u963f\u5c14\u53ca\u5229\u4e9a\u65b9\u8a00Darja\u5b58\u5728\u975e\u6807\u51c6\u5316\u62fc\u5199\u3001\u4e0e\u6cd5\u8bed\u4ee3\u7801\u8f6c\u6362\u3001\u963f\u62c9\u4f2f\u8bed\u548c\u62c9\u4e01\u5b57\u6bcd\u6df7\u7528\u7b49\u590d\u6742\u8bed\u8a00\u7279\u5f81\uff0c\u4f20\u7edf\u5bf9\u8bdd\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u5c42\u67b6\u6784\u6574\u5408\u4e13\u7528\u81ea\u7136\u8bed\u8a00\u7406\u89e3(NLU)\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u652f\u6301\u7ed3\u6784\u5316\u670d\u52a1\u6d41\u7a0b\u548c\u57fa\u4e8e\u4f01\u4e1a\u6587\u6863\u7684\u52a8\u6001\u77e5\u8bc6\u5bc6\u96c6\u578b\u54cd\u5e94\u3002\u9488\u5bf9Darja\u4f4e\u8d44\u6e90\u7279\u6027\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u65b9\u6cd5\uff1a\u7a00\u758f\u7279\u5f81Rasa\u7ba1\u9053\u3001\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u3001\u57fa\u4e8eTransformer\u7684\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u7684DziriBERT\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u62fc\u5199\u566a\u58f0\u548c\u7f55\u89c1\u610f\u56fe\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DziriBOT\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u6b63\u5f0f\u8bed\u8a00\u6a21\u578b\u4e0e\u963f\u5c14\u53ca\u5229\u4e9a\u7528\u6237\u8bed\u8a00\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u533a\u57df\u5e02\u573a\u7684\u65b9\u8a00\u611f\u77e5\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2602.01306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01306", "abs": "https://arxiv.org/abs/2602.01306", "authors": ["Ayushman Sarkar", "Zhenyu Yu", "Mohd Yamani Idna Idris"], "title": "DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling", "comment": null, "summary": "Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory", "AI": {"tldr": "DeCorStory\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u63a8\u7406\u65f6\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u5316\u63d0\u793a\u5d4c\u5165\u6765\u51cf\u5c11\u6587\u672c\u5230\u56fe\u50cf\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u5e27\u95f4\u8bed\u4e49\u5e72\u6270\uff0c\u89e3\u51b3\u989c\u8272\u6cc4\u6f0f\u3001\u80cc\u666f\u6df7\u5408\u548c\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08\u5982One-Prompt-One-Story\uff09\u5c06\u6240\u6709\u63d0\u793a\u8fde\u63a5\u6210\u5355\u4e00\u5e8f\u5217\uff0c\u5bfc\u81f4\u5f3a\u5d4c\u5165\u76f8\u5173\u6027\uff0c\u5f15\u8d77\u989c\u8272\u6cc4\u6f0f\u3001\u80cc\u666f\u6df7\u5408\u548c\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u5e27\u89c6\u89c9\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528Gram-Schmidt\u63d0\u793a\u5d4c\u5165\u53bb\u76f8\u5173\u6765\u6b63\u4ea4\u5316\u5e27\u7ea7\u8bed\u4e49\uff0c\u7136\u540e\u901a\u8fc7\u5947\u5f02\u503c\u91cd\u52a0\u6743\u589e\u5f3a\u63d0\u793a\u7279\u5b9a\u4fe1\u606f\uff0c\u4ee5\u53ca\u8eab\u4efd\u4fdd\u6301\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u7a33\u5b9a\u89d2\u8272\u8eab\u4efd\u3002\u65e0\u9700\u6a21\u578b\u4fee\u6539\u6216\u5fae\u8c03\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u7ba1\u9053\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u63d0\u793a-\u56fe\u50cf\u5bf9\u9f50\u3001\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\u65b9\u9762\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DeCorStory\u901a\u8fc7\u51cf\u5c11\u5e27\u95f4\u8bed\u4e49\u5e72\u6270\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u53ef\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2409.01329", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.DB"], "pdf": "https://arxiv.org/pdf/2409.01329", "abs": "https://arxiv.org/abs/2409.01329", "authors": ["Lucas Lange", "Maurice-Maximilian Heykeroth", "Erhard Rahm"], "title": "Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning", "comment": "Accepted at 21st Conference on Database Systems for Business, Technology and Web (BTW 2025)", "summary": "Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u56fe\u50cf\u6570\u636e\u96c6\u7279\u5f81\u5982\u4f55\u5f71\u54cdCNN\u6a21\u578b\u7684\u6548\u7528-\u9690\u79c1\u6743\u8861\uff0c\u53d1\u73b0\u7c7b\u522b\u4e0d\u5e73\u8861\u4f1a\u589e\u52a0\u5c11\u6570\u7c7b\u522b\u7684\u8106\u5f31\u6027\uff0c\u800c\u5dee\u5206\u9690\u79c1\u53ef\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\u3002\u6570\u636e\u96c6\u7c7b\u522b\u8f83\u5c11\u80fd\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u800c\u9ad8\u71b5\u6216\u4f4eFDR\u7684\u6570\u636e\u96c6\u5219\u4f1a\u6076\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u9762\u4e34\u5b89\u5168\u6311\u6218\uff0c\u53ef\u80fd\u906d\u53d7\u653b\u51fb\u5e76\u6cc4\u9732\u4fe1\u606f\u3002\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u6765\u5e73\u8861\u6548\u7528\u548c\u9690\u79c1\uff0c\u4f46\u9700\u8981\u4e86\u89e3\u6570\u636e\u96c6\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u6743\u8861\uff0c\u4ee5\u4fbf\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u4e2a\u56fe\u50cf\u6570\u636e\u96c6\u548c\u4e0d\u540c\u7684\u9690\u79c1\u9884\u7b97\uff0c\u7814\u7a76\u8bc6\u522b\u4e86\u5f71\u54cd\u79c1\u6709\u548c\u975e\u79c1\u6709CNN\u6a21\u578b\u6548\u7528\u548c\u8106\u5f31\u6027\u7684\u6570\u636e\u96c6\u7279\u5f81\u3002\u5177\u4f53\u5206\u6790\u4e86\u6570\u636e\u96c6\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u7c7b\u522b\u6570\u91cf\u3001\u71b5\u503c\u548cFisher\u5224\u522b\u6bd4\u7b49\u7279\u5f81\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4f1a\u589e\u52a0\u5c11\u6570\u7c7b\u522b\u7684\u8106\u5f31\u6027\uff0c\u4f46\u5dee\u5206\u9690\u79c1\u53ef\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\uff1b2\uff09\u7c7b\u522b\u8f83\u5c11\u7684\u6570\u636e\u96c6\u80fd\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\uff1b3\uff09\u9ad8\u71b5\u6216\u4f4eFisher\u5224\u522b\u6bd4\u7684\u6570\u636e\u96c6\u4f1a\u6076\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6570\u636e\u96c6\u7279\u5f81\u4f30\u8ba1\u548c\u4f18\u5316\u6548\u7528-\u9690\u79c1\u6743\u8861\u7684\u5b9d\u8d35\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u8fdb\u884c\u6570\u636e\u548c\u9690\u79c1\u4fee\u6539\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2602.00918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00918", "abs": "https://arxiv.org/abs/2602.00918", "authors": ["Aur\u00e9lien Renault", "Alexis Bondu", "Antoine Cornu\u00e9jols", "Vincent Lemaire"], "title": "Early Classification of Time Series in Non-Stationary Cost Regimes", "comment": null, "summary": "Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u65f6\u95f4\u5e8f\u5217\u65e9\u671f\u5206\u7c7b\u5728\u6210\u672c\u975e\u5e73\u7a33\u6027\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u5e94\u5bf9\u6210\u672c\u6f02\u79fb\u548c\u968f\u673a\u53d8\u5316", "motivation": "\u73b0\u6709ECTS\u65b9\u6cd5\u5047\u8bbe\u51b3\u7b56\u6210\u672c\u5df2\u77e5\u3001\u56fa\u5b9a\u4e14\u6b63\u786e\u6307\u5b9a\uff0c\u4f46\u5b9e\u8df5\u4e2d\u6210\u672c\u5f80\u5f80\u4e0d\u786e\u5b9a\u4e14\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u5bfc\u81f4\u8bad\u7ec3\u76ee\u6807\u4e0e\u90e8\u7f72\u76ee\u6807\u4e0d\u5339\u914d", "method": "\u5c06\u4ee3\u8868\u6027ECTS\u65b9\u6cd5\u9002\u5e94\u5230\u5728\u7ebf\u5b66\u4e60\u8bbe\u7f6e\uff0c\u9488\u5bf9\u53ef\u5206\u79bb\u65b9\u6cd5\u4ec5\u66f4\u65b0\u89e6\u53d1\u6a21\u578b\u800c\u4fdd\u6301\u5206\u7c7b\u5668\u56fa\u5b9a\uff0c\u63d0\u51fa\u57fa\u4e8e\u591a\u81c2\u8d4c\u535a\u673a\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u9002\u5e94\u65b9\u6cd5", "result": "\u5728\u7ebf\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u9ad8ECTS\u65b9\u6cd5\u5bf9\u6210\u672c\u6f02\u79fb\u7684\u9c81\u68d2\u6027\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b56\u7565\u5728\u4e0d\u540c\u6210\u672c\u673a\u5236\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u6027\u80fd", "conclusion": "\u5728\u7ebf\u5b66\u4e60\u662f\u5e94\u5bf9ECTS\u4e2d\u6210\u672c\u975e\u5e73\u7a33\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0cRL\u7b56\u7565\u5728\u6210\u672c\u6f02\u79fb\u548c\u968f\u673a\u53d8\u5316\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02276", "abs": "https://arxiv.org/abs/2602.02276", "authors": ["Kimi Team", "Tongtong Bai", "Yifan Bai", "Yiping Bao", "S. H. Cai", "Yuan Cao", "Y. Charles", "H. S. Che", "Cheng Chen", "Guanduo Chen", "Huarong Chen", "Jia Chen", "Jiahao Chen", "Jianlong Chen", "Jun Chen", "Kefan Chen", "Liang Chen", "Ruijue Chen", "Xinhao Chen", "Yanru Chen", "Yanxu Chen", "Yicun Chen", "Yimin Chen", "Yingjiang Chen", "Yuankun Chen", "Yujie Chen", "Yutian Chen", "Zhirong Chen", "Ziwei Chen", "Dazhi Cheng", "Minghan Chu", "Jialei Cui", "Jiaqi Deng", "Muxi Diao", "Hao Ding", "Mengfan Dong", "Mengnan Dong", "Yuxin Dong", "Yuhao Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Lingxiao Du", "Yulun Du", "Yu Fan", "Shengjun Fang", "Qiulin Feng", "Yichen Feng", "Garimugai Fu", "Kelin Fu", "Hongcheng Gao", "Tong Gao", "Yuyao Ge", "Shangyi Geng", "Chengyang Gong", "Xiaochen Gong", "Zhuoma Gongque", "Qizheng Gu", "Xinran Gu", "Yicheng Gu", "Longyu Guan", "Yuanying Guo", "Xiaoru Hao", "Weiran He", "Wenyang He", "Yunjia He", "Chao Hong", "Hao Hu", "Jiaxi Hu", "Yangyang Hu", "Zhenxing Hu", "Ke Huang", "Ruiyuan Huang", "Weixiao Huang", "Zhiqi Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yu Jing", "Guokun Lai", "Aidi Li", "C. Li", "Cheng Li", "Fang Li", "Guanghe Li", "Guanyu Li", "Haitao Li", "Haoyang Li", "Jia Li", "Jingwei Li", "Junxiong Li", "Lincan Li", "Mo Li", "Weihong Li", "Wentao Li", "Xinhang Li", "Xinhao Li", "Yang Li", "Yanhao Li", "Yiwei Li", "Yuxiao Li", "Zhaowei Li", "Zheming Li", "Weilong Liao", "Jiawei Lin", "Xiaohan Lin", "Zhishan Lin", "Zichao Lin", "Cheng Liu", "Chenyu Liu", "Hongzhang Liu", "Liang Liu", "Shaowei Liu", "Shudong Liu", "Shuran Liu", "Tianwei Liu", "Tianyu Liu", "Weizhou Liu", "Xiangyan Liu", "Yangyang Liu", "Yanming Liu", "Yibo Liu", "Yuanxin Liu", "Yue Liu", "Zhengying Liu", "Zhongnuo Liu", "Enzhe Lu", "Haoyu Lu", "Zhiyuan Lu", "Junyu Luo", "Tongxu Luo", "Yashuo Luo", "Long Ma", "Yingwei Ma", "Shaoguang Mao", "Yuan Mei", "Xin Men", "Fanqing Meng", "Zhiyong Meng", "Yibo Miao", "Minqing Ni", "Kun Ouyang", "Siyuan Pan", "Bo Pang", "Yuchao Qian", "Ruoyu Qin", "Zeyu Qin", "Jiezhong Qiu", "Bowen Qu", "Zeyu Shang", "Youbo Shao", "Tianxiao Shen", "Zhennan Shen", "Juanfeng Shi", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Pengwei Song", "Tianhui Song", "Xiaoxi Song", "Hongjin Su", "Jianlin Su", "Zhaochen Su", "Lin Sui", "Jinsong Sun", "Junyao Sun", "Tongyu Sun", "Flood Sung", "Yunpeng Tai", "Chuning Tang", "Heyi Tang", "Xiaojuan Tang", "Zhengyang Tang", "Jiawen Tao", "Shiyuan Teng", "Chaoran Tian", "Pengfei Tian", "Ao Wang", "Bowen Wang", "Chensi Wang", "Chuang Wang", "Congcong Wang", "Dingkun Wang", "Dinglu Wang", "Dongliang Wang", "Feng Wang", "Hailong Wang", "Haiming Wang", "Hengzhi Wang", "Huaqing Wang", "Hui Wang", "Jiahao Wang", "Jinhong Wang", "Jiuzheng Wang", "Kaixin Wang", "Linian Wang", "Qibin Wang", "Shengjie Wang", "Shuyi Wang", "Si Wang", "Wei Wang", "Xiaochen Wang", "Xinyuan Wang", "Yao Wang", "Yejie Wang", "Yipu Wang", "Yiqin Wang", "Yucheng Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhaowei Wang", "Zhengtao Wang", "Zhexu Wang", "Zihan Wang", "Zizhe Wang", "Chu Wei", "Ming Wei", "Chuan Wen", "Zichen Wen", "Chengjie Wu", "Haoning Wu", "Junyan Wu", "Rucong Wu", "Wenhao Wu", "Yuefeng Wu", "Yuhao Wu", "Yuxin Wu", "Zijian Wu", "Chenjun Xiao", "Jin Xie", "Xiaotong Xie", "Yuchong Xie", "Yifei Xin", "Bowei Xing", "Boyu Xu", "Jianfan Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinbo Xu", "Xinran Xu", "Yangchuan Xu", "Yichang Xu", "Yuemeng Xu", "Zelai Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Guangyao Yang", "Hao Yang", "Junwei Yang", "Kai Yang", "Ningyuan Yang", "Ruihan Yang", "Xiaofei Yang", "Xinlong Yang", "Ying Yang", "Yi Yang", "Yi Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Dan Ye", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Chengzhen Yu", "Longhui Yu", "Tao Yu", "Tianxiang Yu", "Enming Yuan", "Mengjie Yuan", "Xiaokun Yuan", "Yang Yue", "Weihao Zeng", "Dunyuan Zha", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Jin Zhang", "Puqi Zhang", "Qiao Zhang", "Rui Zhang", "Xiaobin Zhang", "Y. Zhang", "Yadong Zhang", "Yangkun Zhang", "Yichi Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yushun Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Chenguang Zhao", "Feifan Zhao", "Jinxiang Zhao", "Shuai Zhao", "Xiangyu Zhao", "Yikai Zhao", "Zijia Zhao", "Huabin Zheng", "Ruihan Zheng", "Shaojie Zheng", "Tengyang Zheng", "Junfeng Zhong", "Longguang Zhong", "Weiming Zhong", "M. Zhou", "Runjie Zhou", "Xinyu Zhou", "Zaida Zhou", "Jinguo Zhu", "Liya Zhu", "Xinhao Zhu", "Yuxuan Zhu", "Zhen Zhu", "Jingze Zhuang", "Weiyu Zhuang", "Ying Zou", "Xinxing Zu"], "title": "Kimi K2.5: Visual Agentic Intelligence", "comment": "Kimi K2.5 tech report", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "AI": {"tldr": "Kimi K2.5\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c-\u89c6\u89c9\u8054\u5408\u4f18\u5316\u548cAgent Swarm\u5e76\u884c\u6846\u67b6\uff0c\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u5ef6\u8fdf\u964d\u4f4e4.5\u500d\u3002", "motivation": "\u63a8\u52a8\u901a\u7528\u667a\u80fd\u4f53\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u548c\u5e76\u884c\u6267\u884c\u6846\u67b6\u63d0\u5347\u667a\u80fd\u4f53\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6587\u672c-\u89c6\u89c9\u8054\u5408\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9SFT\u3001\u6587\u672c-\u89c6\u89c9\u8054\u5408\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u6a21\u6001\u4f18\u5316\u6280\u672f\uff0c\u5e76\u5f15\u5165Agent Swarm\u81ea\u5bfc\u5411\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u52a8\u6001\u5206\u89e3\u590d\u6742\u4efb\u52a1\u4e3a\u5f02\u6784\u5b50\u95ee\u9898\u5e76\u884c\u6267\u884c\u3002", "result": "\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0cAgent Swarm\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe4.5\u500d\u3002", "conclusion": "Kimi K2.5\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u4f18\u5316\u548c\u5e76\u884c\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u667a\u80fd\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5f00\u6e90\u6a21\u578b\u68c0\u67e5\u70b9\u5c06\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.01329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01329", "abs": "https://arxiv.org/abs/2602.01329", "authors": ["Divya Jyoti Bajpai", "Shubham Agarwal", "Apoorv Saxena", "Kuldeep Kulkarni", "Subrata Mitra", "Manjesh Kumar Hanawal"], "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching", "comment": "Accepted at International Conference on Learning Representations (ICLR 2026)", "summary": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.", "AI": {"tldr": "FlowCast\uff1a\u57fa\u4e8e\u6052\u5b9a\u901f\u5ea6\u9884\u6d4b\u7684\u8bad\u7ec3\u514d\u8d39\u63a8\u6d4b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8df3\u8fc7\u7a33\u5b9a\u533a\u57df\u7684\u5197\u4f59\u6b65\u9aa4\u52a0\u901fFlow Matching\u6a21\u578b\u63a8\u7406\uff0c\u5b9e\u73b02.5\u500d\u4ee5\u4e0a\u52a0\u901f\u4e14\u65e0\u8d28\u91cf\u635f\u5931", "motivation": "Flow Matching\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u8fc7\u6162\uff08\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\uff09\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6216\u4ea4\u4e92\u5e94\u7528\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u5982\u84b8\u998f\u3001\u622a\u65ad\u6216\u4e00\u81f4\u6027\u8bad\u7ec3\uff09\u8981\u4e48\u964d\u4f4e\u8d28\u91cf\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u91cd\u8bad\u7ec3\uff0c\u8981\u4e48\u7f3a\u4e4f\u6cdb\u5316\u6027\u3002", "method": "FlowCast\u5229\u7528FM\u6a21\u578b\u8bad\u7ec3\u65f6\u4fdd\u6301\u6052\u5b9a\u901f\u5ea6\u7684\u7279\u6027\uff0c\u901a\u8fc7\u5916\u63a8\u5f53\u524d\u901f\u5ea6\u6765\u63a8\u6d4b\u672a\u6765\u901f\u5ea6\uff08\u65e0\u989d\u5916\u65f6\u95f4\u6210\u672c\uff09\uff0c\u5982\u679c\u5747\u65b9\u8bef\u5dee\u5728\u9608\u503c\u5185\u5219\u63a5\u53d7\u8be5\u63a8\u6d4b\u3002\u8fd9\u79cd\u6052\u5b9a\u901f\u5ea6\u9884\u6d4b\u5141\u8bb8\u5728\u7a33\u5b9a\u533a\u57df\u6fc0\u8fdb\u8df3\u8fc7\u5197\u4f59\u6b65\u9aa4\uff0c\u540c\u65f6\u5728\u590d\u6742\u533a\u57df\u4fdd\u6301\u7cbe\u5ea6\u3002\u65e0\u9700\u8f85\u52a9\u7f51\u7edc\uff0c\u5373\u63d2\u5373\u7528\u3002", "result": "FlowCast\u5728\u56fe\u50cf\u751f\u6210\u3001\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0>2.5\u500d\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u4e0e\u6807\u51c6\u5b8c\u6574\u751f\u6210\u76f8\u6bd4\u65e0\u8d28\u91cf\u635f\u5931\u3002\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u754c\u5b9a\u4e86\u63a8\u6d4b\u8f68\u8ff9\u4e0e\u5b8c\u6574FM\u8f68\u8ff9\u4e4b\u95f4\u7684\u6700\u574f\u504f\u5dee\u3002", "conclusion": "FlowCast\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8bad\u7ec3\u514d\u8d39\u3001\u5373\u63d2\u5373\u7528\u7684FM\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6052\u5b9a\u901f\u5ea6\u7279\u6027\u8fdb\u884c\u63a8\u6d4b\u751f\u6210\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2501.17634", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2501.17634", "abs": "https://arxiv.org/abs/2501.17634", "authors": ["Lucas Lange", "Ole Borchardt", "Erhard Rahm"], "title": "Federated Learning With Individualized Privacy Through Client Sampling", "comment": "Accepted at 10th International Conference on Machine Learning Technologies (ICMLT 2025)", "summary": "With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e2a\u4f53\u5316\u5dee\u5206\u9690\u79c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u7528\u6237\u4e2a\u4eba\u9690\u79c1\u504f\u597d\u8c03\u6574\u5ba2\u6237\u7aef\u91c7\u6837\u7387\uff0c\u76f8\u6bd4\u7edf\u4e00\u9690\u79c1\u4fdd\u62a4\u57fa\u7ebf\u6709\u660e\u663e\u6539\u8fdb", "motivation": "\u968f\u7740\u7528\u6237\u6570\u636e\u6536\u96c6\u95ee\u9898\u65e5\u76ca\u53d7\u5173\u6ce8\uff0c\u4e2a\u4f53\u5316\u9690\u79c1\u6210\u4e3a\u5e73\u8861\u4fdd\u62a4\u4e0e\u6548\u7528\u7684\u6709\u524d\u666f\u65b9\u6848\u3002\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6240\u6709\u7528\u6237\u5b9e\u65bd\u7edf\u4e00\u533f\u540d\u5316\u7ea7\u522b\uff0c\u800c\u4e2a\u4f53\u5316\u65b9\u6cd5\u5141\u8bb8\u7528\u6237\u6839\u636e\u81ea\u8eab\u8212\u9002\u5ea6\u9009\u62e9\u9690\u79c1\u8bbe\u7f6e", "method": "\u5c06SAMPLE\u7b97\u6cd5\u4ece\u96c6\u4e2d\u5f0f\u8bbe\u7f6e\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\uff0c\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u6784\u9690\u79c1\u9884\u7b97\u8ba1\u7b97\u5ba2\u6237\u7aef\u7279\u5b9a\u91c7\u6837\u7387\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6539\u8fdb\u7684IDP-FedAvg\u7b97\u6cd5\u4e2d", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73b0\u5b9e\u9690\u79c1\u5206\u5e03\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u7edf\u4e00\u5dee\u5206\u9690\u79c1\u57fa\u7ebf\u6709\u660e\u663e\u6539\u8fdb\uff0c\u964d\u4f4e\u4e86\u9690\u79c1\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002\u4e0e\u76f8\u5173\u5de5\u4f5c\u4e2d\u5206\u914d\u4e0d\u540c\u566a\u58f0\u5c3a\u5ea6\u7684SCALE\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u663e\u8457\u66f4\u597d", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u6210\u529f\u5b9e\u73b0\u4e86\u4e2a\u4f53\u5316\u5dee\u5206\u9690\u79c1\uff0c\u4f46\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u7684\u7ea6\u675f"}}
{"id": "2602.00927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00927", "abs": "https://arxiv.org/abs/2602.00927", "authors": ["Yihao Xue", "Allan Zhang", "Jianhao Huang", "Amit Sahai", "Baharan Mirzasoleiman"], "title": "Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision", "comment": null, "summary": "Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728\u4ec5\u7ed3\u679c\u76d1\u7763\u4e0b\uff0c\u589e\u52a0\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\uff08\u5982RL token\u9884\u7b97\u6216\u5faa\u73afTransformer\u5faa\u73af\u6b21\u6570\uff09\u80fd\u6301\u7eed\u63d0\u5347\u5206\u5e03\u5916\u6027\u80fd\uff0c\u5373\u4f7f\u5206\u5e03\u5185\u6027\u80fd\u5df2\u9971\u548c\uff0c\u8868\u660e\u9c81\u68d2\u6027\u9700\u8981\u6bd4ID\u9a8c\u8bc1\u66f4\u5927\u7684\u63a8\u7406\u9884\u7b97\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3LLM\u8fdb\u884c\u957f\u63a8\u7406\u5df2\u6210\u4e3a\u6784\u5efa\u5148\u8fdb\u6a21\u578b\u7684\u5173\u952e\uff0c\u4f46\u63a8\u7406\u957f\u5ea6\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5728\u5206\u5e03\u5916\u6837\u672c\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff1a1\uff09\u7406\u8bba\u5c42\u9762\u63d0\u51fa\u4e24\u79cd\u673a\u5236\u89e3\u91ca\uff1a\u81ea\u8fed\u4ee3\u589e\u5f3a\u5047\u8bbe\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\u3001\u6b63\u5219\u5316\u51cf\u5c11\u5bf9\u6377\u5f84\u89e3\u7684\u4f9d\u8d56\uff1b2\uff09\u5b9e\u8bc1\u5c42\u9762\u901a\u8fc7\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\u9a8c\u8bc1\uff1a\u5728\u5408\u6210\u4efb\u52a1\u4e0a\u589e\u52a0\u5faa\u73afTransformer\u5faa\u73af\u6b21\u6570\u3001\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u589e\u52a0RL\u5fae\u8c03\u7684token\u9884\u7b97\u3002", "result": "\u53d1\u73b0\u65b0\u9896\u73b0\u8c61\uff1a\u5728\u4ec5\u7ed3\u679c\u76d1\u7763\u4e0b\uff0c\u5206\u5e03\u5916\u6027\u80fd\u968f\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\u589e\u52a0\u800c\u6301\u7eed\u6539\u5584\uff0c\u5373\u4f7f\u5206\u5e03\u5185\u6027\u80fd\u5df2\u9971\u548c\u3002\u7406\u8bba\u673a\u5236\u5f97\u5230\u5b9e\u8bc1\u652f\u6301\uff0c\u8868\u660e\u9c81\u68d2\u6027\u9700\u8981\u6bd4ID\u9a8c\u8bc1\u66f4\u5927\u7684\u63a8\u7406\u9884\u7b97\u3002", "conclusion": "\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\u662f\u5f71\u54cd\u6a21\u578b\u9c81\u68d2\u6027\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4ec5\u4f9d\u8d56\u5206\u5e03\u5185\u9a8c\u8bc1\u53ef\u80fd\u4f4e\u4f30\u6240\u9700\u63a8\u7406\u9884\u7b97\u3002\u8fd9\u4e00\u53d1\u73b0\u5bf9\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684LLM\u8bad\u7ec3\u65b9\u6cd5\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.02287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02287", "abs": "https://arxiv.org/abs/2602.02287", "authors": ["Isaac Chung", "Linda Freienthal"], "title": "Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages", "comment": "First Workshop on Multilingual Multicultural Evaluation, co-located with EACL 2026", "summary": "Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.\n  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8de8\u8bed\u8a00\u8bc4\u4f30LLM\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u6392\u540d\u4e0d\u7a33\u5b9a\u6027\uff1a\u8868\u9762\u6307\u6807\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u8bed\u7528\u5224\u65ad\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u51fa\u73b0\u6392\u540d\u53cd\u8f6c\u548c\u63a5\u8fd1\u96f6\u76f8\u5173\u6027\uff0c\u8868\u660e\u96f6-shot\u8bc4\u5224\u5668\u8fc1\u79fb\u4e0d\u53ef\u9760\u3002", "motivation": "\u8de8\u8bed\u8a00\u8bc4\u4f30LLM\u901a\u5e38\u6df7\u6dc6\u4e86\u4e24\u4e2a\u65b9\u5dee\u6765\u6e90\uff1a\u771f\u5b9e\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u548c\u6d4b\u91cf\u4e0d\u7a33\u5b9a\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63a7\u5236\u751f\u6210\u6761\u4ef6\u6765\u5206\u79bb\u8fd9\u4e9b\u56e0\u7d20\uff0c\u63a2\u7a76\u8bc4\u4f30\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u7231\u6c99\u5c3c\u4e9a\u8bed\u3001\u82ac\u5170\u8bed\u548c\u5308\u7259\u5229\u8bed\u4e09\u79cd\u5f62\u6001\u4e30\u5bcc\u7684\u82ac\u5170-\u4e4c\u6208\u5c14\u8bed\u7cfb\u8bed\u8a00\uff0c\u5728\u76f8\u540c\u53c2\u6570\u4e0b\u751f\u6210\u5408\u6210\u5ba2\u6237\u652f\u6301\u5bf9\u8bdd\u3002\u6bd4\u8f83\u81ea\u52a8\u6307\u6807\u548cLLM-as-a-judge\u8bc4\u5206\uff0c\u5e76\u4ee5\u5c11\u91cf\u7231\u6c99\u5c3c\u4e9a\u8bed\u6bcd\u8bed\u8005\u6807\u6ce8\u4f5c\u4e3a\u53c2\u8003\u70b9\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u6392\u540d\u4e0d\u7a33\u5b9a\u6027\uff1a\u8868\u9762\u6307\u6807\uff08\u8bcd\u6c47\u591a\u6837\u6027\u3001\u8868\u9762\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u4fdd\u6301\u8de8\u8bed\u8a00\u7a33\u5b9a\u6027\uff0c\u4f46\u8bed\u7528\u5224\u65ad\uff08\u8fde\u8d2f\u6027\u3001\u6307\u4ee4\u9075\u5faa\uff09\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u51fa\u73b0\u6392\u540d\u53cd\u8f6c\u548c\u63a5\u8fd1\u96f6\u76f8\u5173\u6027\u3002", "conclusion": "\u96f6-shot\u8bc4\u5224\u5668\u8fc1\u79fb\u5728\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684\u8bed\u7bc7\u7ea7\u8bc4\u4f30\u4e2d\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u8fdb\u884c\u6821\u51c6\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u8bca\u65ad\u63a2\u9488\uff1a\u5728\u76f8\u540c\u751f\u6210\u6761\u4ef6\u4e0b\u4e0d\u7a33\u5b9a\u7684\u8bc4\u4f30\u65b9\u6cd5\u8868\u660e\u8fc1\u79fb\u5931\u8d25\u3002"}}
{"id": "2602.01334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01334", "abs": "https://arxiv.org/abs/2602.01334", "authors": ["Yan Ma", "Weiyu Zhang", "Tianle Li", "Linge Du", "Xuyang Shen", "Pengfei Liu"], "title": "What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom", "comment": "code: https://github.com/GAIR-NLP/Med", "summary": "Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MED\u6846\u67b6\u6765\u533a\u5206\u89c6\u89c9\u5de5\u5177\u4f7f\u7528RL\u4e2d\u5185\u5728\u80fd\u529b\u63d0\u5347\u4e0e\u5de5\u5177\u8bf1\u5bfc\u6548\u679c\u7684\u8d21\u732e\uff0c\u53d1\u73b0\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u662f\u5b66\u4e60\u4e0e\u5de5\u5177\u5b89\u5168\u5171\u5b58\u800c\u975e\u771f\u6b63\u638c\u63e1\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u5de5\u5177\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u4e0d\u6e05\u695a\u8fd9\u4e9b\u63d0\u5347\u662f\u6765\u81ea\u5de5\u5177\u4f7f\u7528\u7684\u6539\u8fdb\u8fd8\u662f\u6a21\u578b\u5185\u5728\u80fd\u529b\u7684\u6f14\u5316\u3002\u9700\u8981\u533a\u5206\u8fd9\u4e24\u79cd\u56e0\u7d20\u6765\u7406\u89e3\u5f53\u524d\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u63d0\u51faMED\uff08\u6d4b\u91cf-\u89e3\u91ca-\u8bca\u65ad\uff09\u6846\u67b6\uff1a1\uff09\u7c97\u7c92\u5ea6\u533a\u5206\u5185\u5728\u80fd\u529b\u53d8\u5316\u4e0e\u5de5\u5177\u8bf1\u5bfc\u6548\u679c\uff1b2\uff09\u5c06\u5de5\u5177\u8bf1\u5bfc\u6027\u80fd\u5dee\u5f02\u5206\u89e3\u4e3a\u589e\u76ca\u548c\u635f\u5bb3\u9879\uff1b3\uff09\u63a2\u7a76\u9a71\u52a8\u8fd9\u4e9b\u53d8\u5316\u7684\u673a\u5236\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u5de5\u5177\u5148\u9a8c\u7684VLM\u548c\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u7531\u5185\u5728\u5b66\u4e60\u4e3b\u5bfc\uff0c\u5de5\u5177\u4f7f\u7528RL\u4e3b\u8981\u51cf\u5c11\u5de5\u5177\u8bf1\u5bfc\u7684\u635f\u5bb3\uff08\u5982\u8c03\u7528\u9519\u8bef\u548c\u5de5\u5177\u6a21\u5f0f\u5e72\u6270\uff09\uff0c\u5728\u57fa\u4e8e\u5de5\u5177\u7ea0\u6b63\u5185\u5728\u5931\u8d25\u65b9\u9762\u8fdb\u5c55\u6709\u9650\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u5de5\u5177\u4f7f\u7528RL\u4e3b\u8981\u662f\u5b66\u4e60\u4e0e\u5de5\u5177\u5b89\u5168\u5171\u5b58\u800c\u975e\u771f\u6b63\u638c\u63e1\u5de5\u5177\uff0c\u5de5\u5177\u4e3b\u8981\u5e2e\u52a9\u51cf\u5c11\u9519\u8bef\u800c\u975e\u663e\u8457\u63d0\u5347\u80fd\u529b\u3002"}}
{"id": "2507.18992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18992", "abs": "https://arxiv.org/abs/2507.18992", "authors": ["Jongsoo Lee", "Jangwon Kim", "Jiseok Jeong", "Soohee Han"], "title": "Reinforcement Learning via Conservative Agent for Environments with Random Delays", "comment": null, "summary": "Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4fdd\u5b88\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u8f6c\u6362\u4e3a\u6052\u5b9a\u5ef6\u8fdf\u7b49\u4ef7\u5f62\u5f0f\uff0c\u4f7f\u73b0\u6709\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u65e0\u9700\u7b97\u6cd5\u4fee\u6539\u4e14\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u5e38\u53d7\u73af\u5883\u5ef6\u8fdf\u53cd\u9988\u56f0\u6270\uff0c\u8fdd\u53cd\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6052\u5b9a\u5ef6\u8fdf\u73af\u5883\uff0c\u800c\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u56e0\u53ef\u53d8\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4fdd\u5b88\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u91cd\u65b0\u8868\u8ff0\u4e3a\u5176\u6052\u5b9a\u5ef6\u8fdf\u7b49\u4ef7\u5f62\u5f0f\u3002\u8fd9\u79cd\u8f6c\u6362\u4f7f\u4efb\u4f55\u6700\u5148\u8fdb\u7684\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u90fd\u80fd\u76f4\u63a5\u6269\u5c55\u5230\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u65e0\u9700\u4fee\u6539\u7b97\u6cd5\u7ed3\u6784\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4fdd\u5b88\u667a\u80fd\u4f53\u7684\u7b97\u6cd5\u5728\u6e10\u8fdb\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "\u4fdd\u5b88\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u4f7f\u73b0\u6709\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u5e94\u7528\uff0c\u4e3a\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00931", "abs": "https://arxiv.org/abs/2602.00931", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zihao He", "Muhammad Usman Rafique", "Asad Aali", "Muhammad Ali Jamshed", "John M. Cioffi", "Emily Fox"], "title": "Continuous-Utility Direct Preference Optimization", "comment": "Submitted to ICML 2026", "summary": "Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.", "AI": {"tldr": "CU-DPO\u6846\u67b6\u7528\u8fde\u7eed\u5206\u6570\u66ff\u4ee3\u4e8c\u5143\u6807\u7b7e\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u63a8\u7406\u8d28\u91cf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7b56\u7565\u9009\u62e9\u548c\u6267\u884c\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901a\u5e38\u88ab\u89c6\u4e3a\u5355\u4e00\u80fd\u529b\uff0c\u4f9d\u8d56\u4e8c\u5143\u504f\u597d\u76d1\u7763\u65e0\u6cd5\u6355\u6349\u90e8\u5206\u8fdb\u5c55\u6216\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u8d28\u91cf\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u76d1\u7763\u4fe1\u53f7\u6765\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u6548\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u7528\u8fde\u7eed\u5206\u6570\u66ff\u4ee3\u4e8c\u5143\u6807\u7b7e\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u7b56\u7565\u9009\u62e9\u9636\u6bb5\uff0c\u901a\u8fc7\u6700\u4f73vs\u6240\u6709\u6bd4\u8f83\u4f18\u5316\u6a21\u578b\u9009\u62e9\u6700\u4f73\u7b56\u7565\uff1b2) \u6267\u884c\u7cbe\u70bc\u9636\u6bb5\uff0c\u4f7f\u7528\u8fb9\u7f18\u5206\u5c42\u5bf9\u8bad\u7ec3\u6a21\u578b\u6b63\u786e\u6267\u884c\u9009\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cCU-DPO\u5c06\u7b56\u7565\u9009\u62e9\u51c6\u786e\u7387\u4ece35-46%\u63d0\u5347\u523068-78%\uff0c\u5728\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e2d\u83b7\u5f97\u9ad8\u8fbe6.6\u5206\u7684\u63d0\u5347\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5206\u5e03\u5916\u4efb\u52a1\u3002", "conclusion": "CU-DPO\u901a\u8fc7\u8fde\u7eed\u5206\u6570\u76d1\u7763\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u7ec6\u7c92\u5ea6\u76d1\u7763\u4fe1\u53f7\u6bd4\u4f20\u7edf\u4e8c\u5143\u504f\u597d\u66f4\u6709\u6548\uff0c\u4e3a\u6a21\u578b\u63a8\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2602.02290", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02290", "abs": "https://arxiv.org/abs/2602.02290", "authors": ["Alex Argese", "Pasquale Lisena", "Rapha\u00ebl Troncy"], "title": "Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?", "comment": null, "summary": "Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.", "AI": {"tldr": "\u63d0\u51faStoryScore\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u7684\u79d1\u5b66\u6545\u4e8b\uff0c\u6574\u5408\u8bed\u4e49\u5bf9\u9f50\u3001\u8bcd\u6c47\u57fa\u7840\u3001\u53d9\u4e8b\u63a7\u5236\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u5197\u4f59\u907f\u514d\u548c\u5b9e\u4f53\u7ea7\u5e7b\u89c9\u68c0\u6d4b\uff0c\u89e3\u51b3\u73b0\u6709\u6307\u6807\u65e0\u6cd5\u533a\u5206\u6559\u5b66\u521b\u9020\u6027\u548c\u4e8b\u5b9e\u9519\u8bef\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u80fd\u5c06\u79d1\u5b66\u6587\u7ae0\u8f6c\u5316\u4e3a\u9002\u5408\u4e0d\u540c\u53d7\u4f17\u7684\u53d9\u4e8b\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u6545\u4e8b\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u603b\u7ed3\u6307\u6807\u65e0\u6cd5\u6355\u6349\u53d9\u4e8b\u6240\u9700\u7684\u62bd\u8c61\u3001\u7b80\u5316\u548c\u6559\u5b66\u521b\u9020\u6027\uff0c\u540c\u65f6\u5e7b\u89c9\u68c0\u6d4b\u5668\u5728\u79d1\u5b66\u80cc\u666f\u4e0b\u7ecf\u5e38\u8bef\u5224\u5408\u6cd5\u53d9\u4e8b\u91cd\u6784\u6216\u5728\u521b\u9020\u6027\u4ecb\u5165\u65f6\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faStoryScore\u590d\u5408\u6307\u6807\uff0c\u6574\u5408\u516d\u4e2a\u7ef4\u5ea6\uff1a\u8bed\u4e49\u5bf9\u9f50\uff08\u4e0e\u539f\u6587\u5185\u5bb9\u4e00\u81f4\u6027\uff09\u3001\u8bcd\u6c47\u57fa\u7840\uff08\u672f\u8bed\u51c6\u786e\u6027\uff09\u3001\u53d9\u4e8b\u63a7\u5236\uff08\u6545\u4e8b\u8bb2\u8ff0\u8d28\u91cf\uff09\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\uff08\u903b\u8f91\u7ec4\u7ec7\uff09\u3001\u5197\u4f59\u907f\u514d\uff08\u4fe1\u606f\u5bc6\u5ea6\uff09\u3001\u5b9e\u4f53\u7ea7\u5e7b\u89c9\u68c0\u6d4b\uff08\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u3002", "result": "\u5206\u6790\u663e\u793a\u8bb8\u591a\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u6559\u5b66\u521b\u9020\u6027\u548c\u4e8b\u5b9e\u9519\u8bef\uff0c\u81ea\u52a8\u6307\u6807\u80fd\u6709\u6548\u8bc4\u4f30\u4e0e\u539f\u6587\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4f46\u96be\u4ee5\u8bc4\u4f30\u53d9\u4e8b\u65b9\u5f0f\u548c\u63a7\u5236\u8d28\u91cf\u3002", "conclusion": "StoryScore\u4e3aAI\u751f\u6210\u7684\u79d1\u5b66\u6545\u4e8b\u63d0\u4f9b\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6307\u6807\u5728\u8bc4\u4f30\u53d9\u4e8b\u8d28\u91cf\u548c\u533a\u5206\u521b\u9020\u6027\u91cd\u6784\u4e0e\u4e8b\u5b9e\u9519\u8bef\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5bf9\u79d1\u5b66\u4f20\u64ad\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.01335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01335", "abs": "https://arxiv.org/abs/2602.01335", "authors": ["Yu Xu", "Yuxin Zhang", "Juan Cao", "Lin Gao", "Chunyu Wang", "Oliver Deussen", "Tong-Yee Lee", "Fan Tang"], "title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning", "comment": "11 pages, 10 figures", "summary": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u89c9\u9690\u55bb\u8fc1\u79fb\u4efb\u52a1\uff0c\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u8de8\u57df\u8bed\u4e49\u878d\u5408\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u751f\u6210AI\u6a21\u578b\u5c40\u9650\u4e8e\u50cf\u7d20\u7ea7\u6307\u4ee4\u5bf9\u9f50\u548c\u8868\u9762\u5916\u89c2\u4fdd\u6301\uff0c\u65e0\u6cd5\u6355\u6349\u9690\u55bb\u751f\u6210\u6240\u9700\u7684\u62bd\u8c61\u903b\u8f91\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u9690\u55bb\u7684\u521b\u9020\u6027\u672c\u8d28\u8fc1\u79fb\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8e\u6982\u5ff5\u878d\u5408\u7406\u8bba\u7684\u8ba4\u77e5\u542f\u53d1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u5f0f\u8bed\u6cd5\u89e3\u8026\u5173\u7cfb\u4e0d\u53d8\u6027\uff0c\u901a\u8fc7\u611f\u77e5\u3001\u8fc1\u79fb\u3001\u751f\u6210\u548c\u8bca\u65ad\u56db\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u89c6\u89c9\u9690\u55bb\u8fc1\u79fb", "result": "\u5728\u9690\u55bb\u4e00\u81f4\u6027\u3001\u7c7b\u6bd4\u6070\u5f53\u6027\u548c\u89c6\u89c9\u521b\u9020\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u5e7f\u544a\u548c\u5a92\u4f53\u4e2d\u7684\u81ea\u52a8\u5316\u9ad8\u5f71\u54cd\u529b\u521b\u610f\u5e94\u7528\u94fa\u5e73\u9053\u8def", "conclusion": "\u89c6\u89c9\u9690\u55bb\u8fc1\u79fb\u4efb\u52a1\u548c\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u548c\u8f6c\u79fb\u62bd\u8c61\u903b\u8f91\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u9690\u55bb\u751f\u6210\uff0c\u63a8\u52a8\u4e86\u751f\u6210AI\u5728\u521b\u9020\u6027\u5e94\u7528\u4e2d\u7684\u53d1\u5c55"}}
{"id": "2507.21934", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21934", "abs": "https://arxiv.org/abs/2507.21934", "authors": ["Tianyi Hu", "Andrea Morales-Garz\u00f3n", "Jingyi Zheng", "Maria Maistro", "Daniel Hershcovich"], "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation", "comment": null, "summary": "In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.", "AI": {"tldr": "CARRIAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u8de8\u6587\u5316\u98df\u8c31\u9002\u5e94\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u7ec4\u7ec7\u7684\u591a\u6837\u6027\u6765\u89e3\u51b3\u4f20\u7edfRAG\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u8f93\u51fa\u5355\u4e00\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u5728\u8de8\u6587\u5316\u98df\u8c31\u9002\u5e94\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6709\u9650\u7684\u4e0a\u4e0b\u6587\uff0c\u5373\u4f7f\u63d0\u4f9b\u591a\u6837\u5316\u7684\u8f93\u5165\u4e5f\u65e0\u6cd5\u4ea7\u751f\u591a\u6837\u5316\u7684\u8f93\u51fa\uff0c\u8fd9\u5728\u9700\u8981\u591a\u79cd\u6709\u6548\u7b54\u6848\u7684\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u662f\u4e00\u4e2a\u5173\u952e\u9650\u5236\u3002", "method": "\u63d0\u51faCARRIAGE\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u7ec4\u7ec7\u7684\u591a\u6837\u6027\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u98df\u8c31\u9002\u5e94\u7ed3\u679c\u3002", "result": "CARRIAGE\u5728\u98df\u8c31\u9002\u5e94\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u65b9\u9762\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6548\u7387\uff0c\u4f18\u4e8e\u5c01\u95ed\u5f0fLLM\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u660e\u786e\u65e8\u5728\u751f\u6210\u9ad8\u5ea6\u591a\u6837\u5316\u8f93\u51fa\u4ee5\u9002\u5e94\u591a\u79cd\u7528\u6237\u504f\u597d\u7684RAG\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86RAG\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u8f93\u51fa\u5355\u4e00\u7684\u95ee\u9898\u3002"}}
{"id": "2602.00942", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00942", "abs": "https://arxiv.org/abs/2602.00942", "authors": ["Hao Ma", "Melis Ilayda Bal", "Liang Zhang", "Bingcong Li", "Niao He", "Melanie Zeilinger", "Michael Muehlebach"], "title": "SALAAD: Sparse And Low-Rank Adaptation via ADMM", "comment": null, "summary": "Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.", "AI": {"tldr": "SALAAD\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u548c\u4f4e\u79e9\u7ed3\u6784\u52a8\u6001\u63a7\u5236\u6a21\u578b\u5bb9\u91cf\uff0c\u5b9e\u73b0\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u548c\u6027\u80fd\u4fdd\u6301\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u5185\u5b58\u9884\u7b97\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\uff0c\u9700\u8981\u7075\u6d3b\u63a7\u5236\u6a21\u578b\u5bb9\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u5ffd\u7565\u4e86\u5c42\u548c\u77e9\u9635\u7684\u5f02\u8d28\u6027\uff0c\u6216\u9700\u8981\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u4fee\u6539\u3002", "method": "\u63d0\u51faSALAAD\u6846\u67b6\uff0c\u5728\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4e0b\u5b66\u4e60\u7ed3\u6784\u5316\u6743\u91cd\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u63a7\u5236\u5668\u52a8\u6001\u5e73\u8861\u8bad\u7ec3\u635f\u5931\u548c\u7ed3\u6784\u7ea6\u675f\uff0c\u4fdd\u6301\u6807\u51c6\u8bad\u7ec3\u52a8\u6001\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u663e\u5f0f\u63a7\u5236\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u7684\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSALAAD\u663e\u8457\u51cf\u5c11\u90e8\u7f72\u65f6\u7684\u5185\u5b58\u6d88\u8017\uff0c\u6027\u80fd\u4e0e\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b9\u6cd5\u76f8\u5f53\u3002\u5355\u6b21\u8bad\u7ec3\u4ea7\u751f\u8fde\u7eed\u7684\u6a21\u578b\u5bb9\u91cf\u8c31\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5e73\u6ed1\u5f39\u6027\u90e8\u7f72\u5230\u4e0d\u540c\u5185\u5b58\u9884\u7b97\u3002", "conclusion": "SALAAD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u548c\u4f4e\u79e9\u7ed3\u6784\u5b9e\u73b0\u6a21\u578b\u5bb9\u91cf\u7684\u7075\u6d3b\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53d7\u9650\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u6311\u6218\u3002"}}
{"id": "2602.02301", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02301", "abs": "https://arxiv.org/abs/2602.02301", "authors": ["Min Cai", "Yu Liang", "Longzheng Wang", "Yan Wang", "Yueyang Zhang", "Long Xia", "Zhiyuan Sun", "Xi Ye", "Daiting Shi"], "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery", "comment": "Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io", "summary": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.", "AI": {"tldr": "\u63d0\u51faMGS\u65b9\u6cd5\u89e3\u51b3\u591a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u5206\u522b\u83b7\u5f974.3\u548c4.5\u4e2a\u767e\u5206\u70b9\u7684\u5e73\u5747\u63d0\u5347", "motivation": "\u8bad\u7ec3\u5355\u4e00\u901a\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b\u9762\u4e34\u9886\u57df\u5f02\u8d28\u6027\u6311\u6218\uff0c\u73b0\u6709Sequential RL\u548cMixed RL\u7b56\u7565\u5b58\u5728\u663e\u8457\u7684\u8de8\u9886\u57df\u5e72\u6270\uff0c\u5bfc\u81f4\u6574\u4f53\u6536\u76ca\u6709\u9650", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u68af\u5ea6\u624b\u672f(MGS)\u65b9\u6cd5\uff0c\u5728Transformer\u5185\u90e8\u6a21\u5757\u5c42\u9762\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\u95ee\u9898", "result": "\u5728\u6570\u5b66\u3001\u901a\u7528\u804a\u5929\u548c\u6307\u4ee4\u8ddf\u968f\u4e09\u4e2a\u4ee3\u8868\u6027\u9886\u57df\u4e0a\uff0cMGS\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u5206\u522b\u83b7\u5f974.3(16.6%)\u548c4.5(11.1%)\u4e2a\u767e\u5206\u70b9\u7684\u5e73\u5747\u63d0\u5347\uff0c\u4e14\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u4e2d\u4fdd\u6301\u6709\u6548", "conclusion": "\u9610\u660e\u4e86\u591a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e72\u6270\u7684\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u8bad\u7ec3\u901a\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01340", "abs": "https://arxiv.org/abs/2602.01340", "authors": ["Yubo Dong", "Linchao Zhu"], "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness", "comment": null, "summary": "Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u56fa\u5b9a\u538b\u7f29\u7387VAE\u8f6c\u6362\u4e3a\u652f\u6301\u591a\u7ea7\u65f6\u95f4\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u89e3\u51b3\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u4e0e\u6269\u6563\u6a21\u578b\u96c6\u6210\u9a8c\u8bc1\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u6709LVDM\u4f9d\u8d56VAE\u538b\u7f29\u89c6\u9891\uff0c\u4f46\u8fde\u7eedVAE\u5728\u63d0\u9ad8\u538b\u7f29\u7387\u65f6\uff0c\u589e\u52a0\u91c7\u6837\u5c42\u800c\u4e0d\u6269\u5c55\u9690\u85cf\u901a\u9053\u7ef4\u5ea6\u4f1a\u5bfc\u81f4\u6548\u7387\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5c06\u56fa\u5b9a\u538b\u7f29\u7387VAE\u8f6c\u6362\u4e3a\u652f\u6301\u591a\u7ea7\u65f6\u95f4\u538b\u7f29\u6a21\u578b\u7684\u6280\u672f\uff0c\u63d0\u4f9b\u7b80\u5355\u7684\u6700\u5c0f\u5316\u5fae\u8c03\u65b9\u6cd5\uff1b\u7814\u7a76\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u5bf9\u89c6\u9891\u7247\u6bb5\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u5c06\u591a\u7ea7\u65f6\u95f4\u538b\u7f29VAE\u4e0eDiT\u6269\u6563\u751f\u6210\u6a21\u578b\u96c6\u6210\u3002", "result": "\u63d0\u4f9b\u4e86\u591a\u7ea7\u65f6\u95f4\u538b\u7f29\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5b9e\u8bc1\u8bc1\u636e\uff1b\u5c55\u793a\u4e86\u4e0e\u6269\u6563\u6a21\u578b\u6846\u67b6\u7684\u6210\u529f\u5e76\u53d1\u8bad\u7ec3\u548c\u517c\u5bb9\u6027\uff1b\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86VAE\u7684\u591a\u7ea7\u65f6\u95f4\u538b\u7f29\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u9ad8\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u4e0e\u73b0\u6709\u6269\u6563\u6a21\u578b\u6846\u67b6\u517c\u5bb9\uff0c\u5c55\u793a\u4e86\u591a\u7ea7\u65f6\u95f4\u538b\u7f29\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.00943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00943", "abs": "https://arxiv.org/abs/2602.00943", "authors": ["Zhenyu Zhao", "David Zhang", "Ellie Zhao", "Ehsan Saberian"], "title": "Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems", "comment": null, "summary": "Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively \"no data,\" so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u5148\u9a8c\u6c64\u666e\u68ee\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u5148\u9a8c\u5206\u5e03\u63a7\u5236\u65b0\u7269\u54c1\u7684\u63a2\u7d22\u6982\u7387\uff0c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u51b7\u542f\u52a8\u63a2\u7d22\u8fc7\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6c64\u666e\u68ee\u91c7\u6837\u4f7f\u7528\u5747\u5300Beta(1,1)\u5148\u9a8c\uff0c\u5047\u8bbe\u65b0\u7269\u54c1\u670950%\u6210\u529f\u7387\uff0c\u5f53\u771f\u5b9e\u57fa\u7840\u7387\u8fdc\u4f4e\u4e8e\u6b64\u503c\u65f6\uff0c\u4f1a\u5bfc\u81f4\u5bf9\u5f31\u7269\u54c1\u7684\u8fc7\u5ea6\u63a2\u7d22\uff0c\u6d6a\u8d39\u66dd\u5149\u673a\u4f1a\u5e76\u635f\u5bb3\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5148\u9a8c\u6c64\u666e\u68ee\u91c7\u6837\uff0c\u901a\u8fc7\u95ed\u5f0f\u4e8c\u6b21\u89e3\u8ba1\u7b97\u5148\u9a8c\u5747\u503c\uff0c\u786e\u4fdd\u65b0\u7269\u54c1\u5728\u5f15\u5165\u65f6\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u7269\u54c1\u7684\u6982\u7387\u4e3a\u53ef\u8c03\u53c2\u6570epsilon\uff0c\u4ece\u800c\u7cbe\u786e\u63a7\u5236\u63a2\u7d22\u5f3a\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u8d1d\u53f6\u65af\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u8499\u7279\u5361\u6d1b\u9a8c\u8bc1\u3001\u79bb\u7ebf\u6279\u91cf\u6a21\u62df\u548c\u5927\u578b\u5728\u7ebf\u5b9e\u9a8c\u4e2d\uff0c\u52a8\u6001\u5148\u9a8c\u65b9\u6cd5\u76f8\u6bd4\u5747\u5300\u5148\u9a8c\u57fa\u7ebf\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63a2\u7d22\u63a7\u5236\u548c\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u5728\u670d\u52a1\u6570\u767e\u4e07\u7528\u6237\u7684\u7f29\u7565\u56fe\u4e2a\u6027\u5316\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u52a8\u6001\u5148\u9a8c\u6c64\u666e\u68ee\u91c7\u6837\u4e3a\u63a8\u8350\u7cfb\u7edf\u51b7\u542f\u52a8\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u3001\u53ef\u8c03\u8282\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\uff0c\u5728\u5b9e\u9645\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.02315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02315", "abs": "https://arxiv.org/abs/2602.02315", "authors": ["Rapha\u00ebl Sarfati", "Eric Bigelow", "Daniel Wurgaft", "Jack Merullo", "Atticus Geiger", "Owen Lewis", "Tom McGrath", "Ekdeep Singh Lubana"], "title": "The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors", "comment": null, "summary": "Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved \"belief manifolds\" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.", "AI": {"tldr": "LLMs\u7f16\u7801\u6982\u7387\u5206\u5e03\u4fe1\u5ff5\u5f62\u6210\u5f2f\u66f2\u7684\"\u4fe1\u5ff5\u6d41\u5f62\"\uff0c\u7ebf\u6027\u5e72\u9884\u4f1a\u504f\u79bb\u6d41\u5f62\uff0c\u800c\u51e0\u4f55\u611f\u77e5\u7684\u7ebf\u6027\u573a\u63a2\u6d4b\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u4fe1\u5ff5\u7ed3\u6784", "motivation": "\u7406\u89e3LLMs\u5982\u4f55\u7f16\u7801\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u4fe1\u5ff5\uff08\u7b54\u6848\u548c\u4e3b\u5f20\u7684\u540e\u9a8c\u5206\u5e03\uff09\uff0c\u8fd9\u4e9b\u4fe1\u5ff5\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5982\u4f55\u7f16\u7801\u3001\u5982\u4f55\u968f\u65b0\u8bc1\u636e\u66f4\u65b0\uff0c\u4ee5\u53ca\u5e72\u9884\u5982\u4f55\u91cd\u5851\u5b83\u4eec", "method": "\u5728\u53d7\u63a7\u8bbe\u7f6e\u4e2d\u8ba9Llama-3.2\u4ece\u6b63\u6001\u5206\u5e03\u751f\u6210\u6837\u672c\uff0c\u4ec5\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e2d\u7684\u6837\u672c\u6765\u9690\u5f0f\u63a8\u65ad\u5206\u5e03\u53c2\u6570\uff08\u5747\u503c\u548c\u6807\u51c6\u5dee\uff09\uff0c\u7814\u7a76\u4fe1\u5ff5\u6d41\u5f62\u7684\u5f62\u6210\u548c\u53d8\u5316", "result": "\u53d1\u73b0\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f1a\u5f62\u6210\u53c2\u6570\u5f2f\u66f2\u7684\"\u4fe1\u5ff5\u6d41\u5f62\"\u8868\u793a\uff1b\u6807\u51c6\u7ebf\u6027\u5e72\u9884\u5e38\u4f7f\u6a21\u578b\u504f\u79bb\u6d41\u5f62\u5e76\u5f15\u53d1\u8026\u5408\u7684\u5206\u5e03\u5916\u504f\u79fb\uff0c\u800c\u51e0\u4f55\u548c\u573a\u611f\u77e5\u7684\u5e72\u9884\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u76ee\u6807\u4fe1\u5ff5\u65cf", "conclusion": "LLMs\u4e2d\u81ea\u7136\u6d8c\u73b0\u4e30\u5bcc\u7ed3\u6784\uff0c\u7eaf\u7ebf\u6027\u6982\u5ff5\u8868\u793a\u901a\u5e38\u662f\u4e0d\u5145\u5206\u7684\u62bd\u8c61\uff1b\u7ebf\u6027\u573a\u63a2\u6d4b\uff08LFP\uff09\u53ef\u4f5c\u4e3a\u5e73\u94fa\u6570\u636e\u6d41\u5f62\u5e76\u5c0a\u91cd\u5e95\u5c42\u51e0\u4f55\u8fdb\u884c\u5e72\u9884\u7684\u7b80\u5355\u65b9\u6cd5"}}
{"id": "2602.01345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01345", "abs": "https://arxiv.org/abs/2602.01345", "authors": ["Yu Zhang", "Jingyi Liu", "Feng Liu", "Duoqian Miao", "Qi Zhang", "Kexue Fu", "Changwei Wang", "Longbing Cao"], "title": "Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis", "comment": "11 pages, 8 figures", "summary": "Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.", "AI": {"tldr": "NOVA\uff1a\u57fa\u4e8e\u71b5\u5206\u6790\u7684VAR\u6a21\u578b\u8bad\u7ec3\u514d\u8d39token\u7f29\u51cf\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8bc6\u522b\u5c3a\u5ea6\u71b5\u589e\u957f\u62d0\u70b9\uff0c\u52a8\u6001\u8ba1\u7b97\u5404\u5c3a\u5ea6\u548c\u5c42\u7684token\u7f29\u51cf\u6bd4\u4f8b\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u73b0\u6709VAR token\u7f29\u51cf\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u542f\u53d1\u5f0f\u9636\u6bb5\u5212\u5206\u3001\u975e\u81ea\u9002\u5e94\u8c03\u5ea6\u3001\u6709\u9650\u52a0\u901f\u8303\u56f4\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u52a0\u901f\u6f5c\u529b\u3002\u71b5\u53d8\u5316\u80fd\u53cd\u6620\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u8f6c\u53d8\uff0c\u4e3a\u6355\u6349\u5efa\u6a21\u52a8\u6001\u6f14\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5ea6\u91cf\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u8bc6\u522b\u5c3a\u5ea6\u71b5\u589e\u957f\u62d0\u70b9\u81ea\u9002\u5e94\u786e\u5b9a\u52a0\u901f\u6fc0\u6d3b\u5c3a\u5ea6\uff1b\u901a\u8fc7\u5c3a\u5ea6\u94fe\u63a5\u548c\u5c42\u94fe\u63a5\u6bd4\u4f8b\u8c03\u6574\uff0c\u52a8\u6001\u8ba1\u7b97\u6bcf\u4e2a\u5c3a\u5ea6\u548c\u5c42\u7684\u4e0d\u540ctoken\u7f29\u51cf\u6bd4\u4f8b\uff1b\u526a\u679d\u4f4e\u71b5token\u5e76\u91cd\u7528\u5148\u524d\u5c3a\u5ea6\u6b8b\u5dee\u7684\u7f13\u5b58\u6765\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u5206\u6790\u9a8c\u8bc1\u4e86NOVA\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "NOVA\u901a\u8fc7\u71b5\u5206\u6790\u5b9e\u73b0\u4e86VAR\u6a21\u578b\u7684\u81ea\u9002\u5e94token\u7f29\u51cf\u52a0\u901f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.00952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00952", "abs": "https://arxiv.org/abs/2602.00952", "authors": ["Jing Wang", "Jie Shen", "Dean Foster", "Zohar Karnin", "Jeremy C Weiss"], "title": "Optimal Budgeted Adaptation of Large Language Models", "comment": null, "summary": "The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \\emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\\tilde{O}(d\\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$ with $B=\u03b2T$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7b97\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u5c06LLM\u9002\u5e94\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587Stackelberg\u535a\u5f08\uff0c\u901a\u8fc7\u6807\u7b7e\u67e5\u8be2\u7b56\u7565\u548c\u7f6e\u4fe1\u95e8\u63a7\u5b9e\u73b0\u6807\u7b7e\u6548\u7387\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u6807\u6ce8\u6570\u636e\u53ef\u7528\u6027\u4e0e\u4e0b\u6e38\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u9ad8\u6548\u5229\u7528\u6807\u7b7e\u3002", "method": "\u5c06LLM\u9002\u5e94\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587Stackelberg\u535a\u5f08\uff0c\u5b66\u4e60\u8005\uff08\u9886\u5bfc\u8005\uff09\u627f\u8bfa\u8bc4\u5206\u7b56\u7565\u548c\u6807\u7b7e\u67e5\u8be2\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u73af\u5883\uff08\u8ddf\u968f\u8005\uff09\u9009\u62e9\u5177\u6709\u6311\u6218\u6027\u7684\u76d1\u7763\u66ff\u4ee3\u65b9\u6848\u3002\u5f15\u5165\u6709\u9650\u76d1\u7763\u9884\u7b97\u5230\u5b66\u4e60\u76ee\u6807\u4e2d\uff0c\u91c7\u7528\u5168\u53cd\u9988\u673a\u5236\uff0c\u5e76\u6269\u5c55\u4e86\u5e26\u6709\u6700\u5927\u5ef6\u8fdf\u4f18\u5148\u7f6e\u4fe1\u95e8\u63a7\u7684\u9009\u62e9\u6027\u6807\u7b7e\u67e5\u8be2\u6846\u67b6\u3002", "result": "\u5728\u6807\u51c6\u7ebf\u6027\u4e0a\u4e0b\u6587\u5047\u8bbe\u4e0b\u5b9e\u73b0$\\tilde{O}(d\\sqrt{T})$\u9057\u61be\u754c\uff0c\u901a\u8fc7LLF\u7f6e\u4fe1\u95e8\u63a7\u5b9e\u73b0$\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$\u7684\u9884\u7b97\u611f\u77e5\u9057\u61be\u754c\uff0c\u5176\u4e2d$B=\u03b2T$\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u9884\u7b97\u53d7\u9650\u7684LLM\u76d1\u7763\u5fae\u8c03\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u548c\u9009\u62e9\u6027\u6807\u7b7e\u67e5\u8be2\u7b56\u7565\u6709\u6548\u5e73\u8861\u4e86\u6807\u6ce8\u6210\u672c\u4e0e\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.02320", "categories": ["cs.CL", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2602.02320", "abs": "https://arxiv.org/abs/2602.02320", "authors": ["Feiyang Cai", "Guijuan He", "Yi Hu", "Jingjing Wang", "Joshua Luo", "Tianyu Zhu", "Srikanth Pilla", "Gang Li", "Ling Liu", "Feng Luo"], "title": "A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method", "comment": null, "summary": "Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.", "AI": {"tldr": "\u63d0\u51fa\u5168\u81ea\u52a8\u5206\u5b50\u7ed3\u6784\u63cf\u8ff0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790IUPAC\u540d\u79f0\u6784\u5efa\u7ed3\u6784\u5316XML\u5143\u6570\u636e\uff0c\u6307\u5bfcLLM\u751f\u6210\u51c6\u786e\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u521b\u5efa\u4e8616.3\u4e07\u5206\u5b50-\u63cf\u8ff0\u5bf9\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u7cbe\u5ea6\u8fbe98.6%\u3002", "motivation": "\u5206\u5b50\u529f\u80fd\u4e3b\u8981\u7531\u7ed3\u6784\u51b3\u5b9a\uff0c\u51c6\u786e\u5bf9\u9f50\u5206\u5b50\u7ed3\u6784\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u4e8eLLM\u8fdb\u884c\u5316\u5b66\u4efb\u52a1\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u7ed3\u6784\u63cf\u8ff0\u6570\u636e\u96c6\u3002", "method": "\u57fa\u4e8e\u89c4\u5219\u5316\u5b66\u547d\u540d\u6cd5\u89e3\u6790\u5668\u6269\u5c55\uff0c\u89e3\u91caIUPAC\u540d\u79f0\u5e76\u6784\u5efa\u4e30\u5bcc\u7684\u7ed3\u6784\u5316XML\u5143\u6570\u636e\uff0c\u660e\u786e\u7f16\u7801\u5206\u5b50\u7ed3\u6784\u4fe1\u606f\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u5143\u6570\u636e\u6307\u5bfcLLM\u751f\u6210\u51c6\u786e\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "\u6784\u5efa\u4e86\u7ea616.3\u4e07\u4e2a\u5206\u5b50-\u63cf\u8ff0\u5bf9\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM\u548c\u4e13\u5bb6\u4eba\u5de5\u8bc4\u4f302000\u4e2a\u5206\u5b50\u7684\u9a8c\u8bc1\u534f\u8bae\u663e\u793a\u63cf\u8ff0\u7cbe\u5ea6\u8fbe\u523098.6%\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u672a\u6765\u5206\u5b50-\u8bed\u8a00\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u6240\u63d0\u51fa\u7684\u6807\u6ce8\u65b9\u6cd5\u6613\u4e8e\u6269\u5c55\u5230\u66f4\u5927\u6570\u636e\u96c6\u548c\u4f9d\u8d56\u7ed3\u6784\u63cf\u8ff0\u7684\u66f4\u5e7f\u6cdb\u5316\u5b66\u4efb\u52a1\u3002"}}
{"id": "2602.01352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01352", "abs": "https://arxiv.org/abs/2602.01352", "authors": ["Xingzu Zhan", "Chen Xie", "Honghang Chen", "Yixun Lin", "Xiaochun Mai"], "title": "T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation", "comment": "8 pages,5 figures", "summary": "Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.", "AI": {"tldr": "\u63d0\u51faT2M Mamba\u6a21\u578b\uff0c\u901a\u8fc7\u5468\u671f\u6027-\u663e\u8457\u6027\u611f\u77e5Mamba\u548c\u5468\u671f\u6027\u5dee\u5206\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff0c\u89e3\u51b3\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u5468\u671f\u6027-\u663e\u8457\u6027\u8026\u5408\u548c\u8bed\u4e49\u7b49\u4ef7\u6027\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u9650\u5236\uff1a1) \u5c06\u52a8\u4f5c\u5468\u671f\u6027\u548c\u5173\u952e\u5e27\u663e\u8457\u6027\u89c6\u4e3a\u72ec\u7acb\u56e0\u7d20\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u8026\u5408\u5173\u7cfb\uff0c\u5bfc\u81f4\u957f\u5e8f\u5217\u751f\u6210\u6f02\u79fb\uff1b2) \u5bf9\u8bed\u4e49\u7b49\u4ef7\u6539\u5199\u8106\u5f31\uff0c\u5fae\u5c0f\u540c\u4e49\u8bcd\u66ff\u6362\u4f1a\u626d\u66f2\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u4f20\u64ad\u4ea7\u751f\u4e0d\u7a33\u5b9a\u6216\u9519\u8bef\u7684\u52a8\u4f5c\u3002", "method": "\u63d0\u51faT2M Mamba\u6a21\u578b\uff1a1) \u5468\u671f\u6027-\u663e\u8457\u6027\u611f\u77e5Mamba\uff0c\u901a\u8fc7\u589e\u5f3a\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b\u8fdb\u884c\u5173\u952e\u5e27\u6743\u91cd\u4f30\u8ba1\uff0c\u901a\u8fc7FFT\u52a0\u901f\u81ea\u76f8\u5173\u8fdb\u884c\u52a8\u4f5c\u5468\u671f\u6027\u4f30\u8ba1\uff0c\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u6355\u83b7\u8026\u5408\u52a8\u6001\uff1b2) \u5468\u671f\u6027\u5dee\u5206\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757(PDCAM)\uff0c\u589e\u5f3a\u6587\u672c\u548c\u52a8\u4f5c\u5d4c\u5165\u7684\u9c81\u68d2\u5bf9\u9f50\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cFID\u8fbe\u52300.068\uff0c\u5728\u6240\u6709\u5176\u4ed6\u6307\u6807\u4e0a\u53d6\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "T2M Mamba\u901a\u8fc7\u5efa\u6a21\u5468\u671f\u6027-\u663e\u8457\u6027\u8026\u5408\u548c\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u9f50\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u957f\u5e8f\u5217\u548c\u8bed\u4e49\u7b49\u4ef7\u6539\u5199\u573a\u666f\u4e0b\u3002"}}
{"id": "2602.00953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00953", "abs": "https://arxiv.org/abs/2602.00953", "authors": ["Sahar Almahfouz Nasser", "Juan Francisco Pesantez Borja", "Jincheng Liu", "Tanvir Hasan", "Zenghan Wang", "Suman Ghosh", "Sandeep Manandhar", "Shikhar Shiromani", "Twisha Shah", "Naoto Tokuyama", "Anant Madabhushi"], "title": "SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery", "comment": null, "summary": "Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684AI\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u75c5\u7406\u56fe\u50cf\u7279\u5f81\u4e0e\u5206\u5b50\u751f\u7269\u6807\u5fd7\u7269\u548c\u4e34\u5e8a\u7ed3\u679c\u76f8\u5173\u8054\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5de5\u7a0b\u5316\u75c5\u7406\u5b66\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684AI\u6a21\u578b\u591a\u4e3a\u9ed1\u76d2\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\u3002\u73b0\u6709\u7684\u5de5\u7a0b\u5316\u56fe\u50cf\u751f\u7269\u6807\u5fd7\u7269\u867d\u7136\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\uff0c\u4f46\u5f80\u5f80\u57fa\u4e8e\u8f76\u4e8b\u8bc1\u636e\u6216\u96f6\u6563\u7684\u6587\u732e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7684\u751f\u7269\u5b66\u9a8c\u8bc1\u3002", "method": "SAGE\u6574\u5408\u4e86\u6587\u732e\u951a\u5b9a\u63a8\u7406\u548c\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\uff0c\u901a\u8fc7\u534f\u8c03\u4e13\u95e8\u4ee3\u7406\u8fdb\u884c\u751f\u7269\u5b66\u60c5\u5883\u5316\u548c\u7ecf\u9a8c\u6027\u5047\u8bbe\u9a8c\u8bc1\uff0c\u5c06\u56fe\u50cf\u884d\u751f\u7279\u5f81\u4e0e\u57fa\u56e0\u8868\u8fbe\u7b49\u5206\u5b50\u751f\u7269\u6807\u5fd7\u7269\u53ca\u4e34\u5e8a\u7ed3\u679c\u76f8\u5173\u8054\u3002", "result": "SAGE\u80fd\u591f\u4f18\u5148\u8003\u8651\u900f\u660e\u4e14\u751f\u7269\u5b66\u652f\u6301\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63a8\u52a8\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u4e34\u5e8a\u8f6c\u5316\u3002", "conclusion": "SAGE\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u5316\u4ee3\u7406\u65b9\u6cd5\uff0c\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u751f\u7269\u5b66\u9a8c\u8bc1\u7684\u751f\u7269\u6807\u5fd7\u7269\u751f\u6210\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3AI\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7f3a\u4e4f\u900f\u660e\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2602.02326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02326", "abs": "https://arxiv.org/abs/2602.02326", "authors": ["Neeraja Kirtane", "Kuan-Hao Huang"], "title": "Language Steering for Multilingual In-Context Learning", "comment": null, "summary": "While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u5411\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5dee\u5f02\u5f15\u5bfc\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u63d0\u5347\u975e\u82f1\u8bed\u8bed\u8a00\u6027\u80fd", "motivation": "\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u82f1\u8bed\uff0c\u7279\u522b\u662f\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u82f1\u8bed\u6f14\u793a\u4f46\u6d4b\u8bd5\u975e\u82f1\u8bed\u8f93\u5165\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd", "method": "\u63d0\u51fa\u8bed\u8a00\u5411\u91cf\u65b9\u6cd5\uff0c\u5229\u7528\u6e90\u8bed\u8a00\u548c\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u7684\u6fc0\u6d3b\u5dee\u5f02\u6765\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c06\u5411\u91cf\u6dfb\u52a0\u5230\u4e2d\u95f4\u6a21\u578b\u6fc0\u6d3b\u4e2d\uff0c\u4f7f\u5185\u90e8\u8868\u793a\u5411\u76ee\u6807\u8bed\u8a00\u7a7a\u95f4\u8f6c\u79fb\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u300119\u79cd\u8bed\u8a00\u3001\u4e09\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u6240\u6709\u4efb\u52a1\u548c\u8bed\u8a00\u90fd\u663e\u793a\u51fa\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u7684\u6301\u7eed\u6539\u8fdb\u3002\u5c42\u6b21\u805a\u7c7b\u663e\u793a\u8bed\u8a00\u5411\u91cf\u4e0e\u8bed\u8a00\u5bb6\u65cf\u5bf9\u9f50\uff0c\u4e14\u8fd9\u4e9b\u8868\u793a\u5177\u6709\u4efb\u52a1\u65e0\u5173\u6027", "conclusion": "\u8bed\u8a00\u5411\u91cf\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u8bed\u8a00\u8868\u793a\u7684\u6709\u8da3\u8bed\u8a00\u5b66\u7ed3\u6784"}}
{"id": "2602.01369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01369", "abs": "https://arxiv.org/abs/2602.01369", "authors": ["Songping Wang", "Qinglong Liu", "Yueming Lyu", "Ning Li", "Ziwen He", "Caifeng Shan"], "title": "Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faTLGA\u653b\u51fb\u6846\u67b6\uff0c\u63ed\u793a\u89c6\u9891MoE\u6a21\u578b\u4e2d\u8def\u7531\u5668\u548c\u4e13\u5bb6\u6a21\u5757\u7684\u72ec\u7acb\u4e0e\u534f\u540c\u5f31\u70b9\uff0c\u5e76\u8bbe\u8ba1J-TLAT\u9632\u5fa1\u65b9\u6cd5\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u9488\u5bf9MoE\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u7edf\u4e00\u67b6\u6784\uff0c\u5ffd\u89c6\u4e86\u8def\u7531\u5668\u548c\u4e13\u5bb6\u6a21\u5757\u7b49\u5173\u952e\u7ec4\u4ef6\u7684\u72ec\u7acb\u548c\u534f\u540c\u5f31\u70b9\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u7ec4\u4ef6\u7ea7\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faTemporal Lipschitz-Guided Attacks (TLGA)\uff1a1) \u8bbe\u8ba1\u9488\u5bf9\u8def\u7531\u5668\u7684\u653b\u51fb\u63ed\u793a\u72ec\u7acb\u5f31\u70b9\uff1b2) \u63d0\u51faJoint Temporal Lipschitz-Guided Attacks (J-TLGA)\u534f\u540c\u6270\u52a8\u8def\u7531\u5668\u548c\u4e13\u5bb6\uff1b3) \u57fa\u4e8e\u6b64\u63d0\u51faJoint Temporal Lipschitz Adversarial Training (J-TLAT)\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u9632\u5fa1\u3002", "result": "TLGA\u6846\u67b6\u80fd\u6709\u6548\u63ed\u793aMoE\u67b6\u6784\u7684\u72ec\u7acb\u548c\u534f\u540c\u5f31\u70b9\uff0cJ-TLAT\u9632\u5fa1\u65b9\u6cd5\u80fd\u589e\u5f3a\u7ec4\u4ef6\u7ea7\u9c81\u68d2\u6027\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u4e00\u81f4\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u51cf\u5c1160%\u4ee5\u4e0a\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86MoE\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u653b\u51fb\u548c\u9632\u5fa1\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u548c\u7f13\u89e3MoE\u67b6\u6784\u7684\u7ec4\u4ef6\u7ea7\u5f31\u70b9\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.00957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00957", "abs": "https://arxiv.org/abs/2602.00957", "authors": ["Waqar Muhammad Ashraf", "Talha Ansar", "Fahad Ahmed", "Jawad Hussain", "Muhammad Mujtaba Abbas", "Vivek Dua"], "title": "From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps", "comment": "Corresponding author: v.dua@ucl.ac.uk", "summary": "Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u66f4\u65b0\u7b56\u7565\uff08ETL\u3001ALTL\u3001LLTL\uff09\u5728\u6570\u636e\u6f02\u79fb\u4e0b\u66f4\u65b0\u5931\u8d25ANN\u6a21\u578b\u7684\u6548\u679c\uff0c\u4ee5\u706b\u7535\u5382\u70df\u6c14\u5dee\u538b\u76d1\u6d4b\u4e3a\u6848\u4f8b\uff0c\u53d1\u73b0\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u4e0b\u5404\u7b56\u7565\u8868\u73b0\u4e0d\u540c\u3002", "motivation": "MLOps\u4e2d\u6a21\u578b\u9002\u5e94\u751f\u4ea7\u73af\u5883\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u6765\u66f4\u65b0\u6570\u636e\u6f02\u79fb\u4e0b\u5931\u8d25\u7684ML\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5de5\u4e1a\u8fc7\u7a0b\u76d1\u6d4b\u4e2d\u7684\u6a21\u578b\u66f4\u65b0\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u66f4\u65b0\u5931\u8d25\u7684ANN\u6a21\u578b\uff1a\u96c6\u6210\u8fc1\u79fb\u5b66\u4e60\uff08ETL\uff09\u3001\u5168\u5c42\u8fc1\u79fb\u5b66\u4e60\uff08ALTL\uff09\u548c\u6700\u540e\u4e00\u5c42\u8fc1\u79fb\u5b66\u4e60\uff08LLTL\uff09\u3002\u4ee5660MW\u706b\u7535\u5382\u7a7a\u6c14\u9884\u70ed\u5668\u70df\u6c14\u5dee\u538b\u76d1\u6d4b\u4e3a\u6848\u4f8b\uff0c\u6a21\u62df\u6279\u5904\u7406\u8fc7\u7a0b\uff0c\u5206\u6790\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u4e0b\u7684\u66f4\u65b0\u6548\u679c\u3002", "result": "\u5bf9\u4e8e5\u5929\u6279\u6b21\u5927\u5c0f\uff0cETL\u63d0\u4f9b\u76f8\u5bf9\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\uff1b\u5bf9\u4e8e8\u5929\u6279\u6b21\u5927\u5c0f\uff0cALTL\u66f4\u9002\u5408\u6709\u6548\u66f4\u65b0\u6a21\u578b\u3002\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u4e0b\uff0c\u6a21\u578b\u66f4\u65b0\u6280\u672f\u7684\u8ba1\u7b97\u9700\u6c42\uff08\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6a21\u578b\u8bad\u7ec3\uff09\u5448\u73b0\u6df7\u5408\u8d8b\u52bf\u3002", "conclusion": "\u4ece\u57fa\u4e8e\u6279\u5904\u7406\u7684\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u4e2d\u83b7\u5f97\u7684\u57fa\u7840\u548c\u5b9e\u8bc1\u89c1\u89e3\uff0c\u53ef\u4ee5\u5e2e\u52a9MLOps\u4ece\u4e1a\u8005\u5c06\u5931\u8d25\u6a21\u578b\u9002\u5e94\u6570\u636e\u6f02\u79fb\uff0c\u5b9e\u73b0\u5de5\u4e1a\u8fc7\u7a0b\u7684\u51c6\u786e\u76d1\u6d4b\u3002\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u4e0b\u9700\u8981\u9009\u62e9\u4e0d\u540c\u7684\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002"}}
{"id": "2602.02343", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02343", "abs": "https://arxiv.org/abs/2602.02343", "authors": ["Ziwen Xu", "Chenyan Wu", "Hengyu Sun", "Haiwen Hong", "Mengru Wang", "Yunzhi Yao", "Longtao Huang", "Hui Xue", "Shumin Deng", "Zhixuan Chu", "Huajun Chen", "Ningyu Zhang"], "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics", "comment": "Work in progress", "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u7684LLM\u63a7\u5236\u65b9\u6cd5\u89c6\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u8bf1\u5bfc\u7684\u52a8\u6001\u6743\u91cd\u66f4\u65b0\uff0c\u5e76\u5f15\u5165\u504f\u597d-\u6548\u7528\u5206\u6790\u6765\u91cf\u5316\u63a7\u5236\u6548\u679c\uff0c\u53d1\u73b0\u504f\u597d\u4e0e\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\uff0c\u6700\u540e\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u65b0\u7684SPLIT\u63a7\u5236\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u672c\u5730\u6743\u91cd\u5fae\u8c03\u3001LoRA\u9002\u914d\u3001\u57fa\u4e8e\u6fc0\u6d3b\u7684\u5e72\u9884\uff09\u901a\u5e38\u88ab\u5b64\u7acb\u7814\u7a76\uff0c\u96be\u4ee5\u6bd4\u8f83\u548c\u5efa\u7acb\u8054\u7cfb\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e9b\u65b9\u6cd5\u7684\u672c\u8d28\u8054\u7cfb\u3002", "method": "1. \u63d0\u51fa\u7edf\u4e00\u89c6\u56fe\uff1a\u5c06\u5404\u79cd\u5e72\u9884\u65b9\u6cd5\u6846\u67b6\u5316\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u8bf1\u5bfc\u7684\u52a8\u6001\u6743\u91cd\u66f4\u65b0\uff1b2. \u63d0\u51fa\u7edf\u4e00\u7684\u504f\u597d-\u6548\u7528\u5206\u6790\uff1a\u5c06\u63a7\u5236\u6548\u679c\u5206\u4e3a\u504f\u597d\uff08\u5bf9\u76ee\u6807\u6982\u5ff5\u7684\u503e\u5411\u6027\uff09\u548c\u6548\u7528\uff08\u8fde\u8d2f\u4e14\u4efb\u52a1\u6709\u6548\u7684\u751f\u6210\uff09\uff0c\u5e76\u4f7f\u7528\u6781\u6027\u914d\u5bf9\u7684\u5bf9\u6bd4\u793a\u4f8b\u5728\u5171\u4eab\u7684\u5bf9\u6570\u51e0\u7387\u5c3a\u5ea6\u4e0a\u6d4b\u91cf\u4e24\u8005\uff1b3. \u4ece\u6fc0\u6d3b\u6d41\u5f62\u89d2\u5ea6\u89e3\u91ca\u63a7\u5236\u884c\u4e3a\uff1b4. \u63d0\u51fa\u65b0\u7684SPLIT\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u89c2\u5bdf\u5230\u6240\u6709\u65b9\u6cd5\u4e2d\u90fd\u5b58\u5728\u4e00\u81f4\u7684\u504f\u597d-\u6548\u7528\u6743\u8861\uff1a\u66f4\u5f3a\u7684\u63a7\u5236\u4f1a\u589e\u52a0\u504f\u597d\uff0c\u4f46\u53ef\u9884\u6d4b\u5730\u964d\u4f4e\u6548\u7528\u3002\u4ece\u6fc0\u6d3b\u6d41\u5f62\u89d2\u5ea6\u770b\uff0c\u63a7\u5236\u5c06\u8868\u793a\u6cbf\u7740\u76ee\u6807\u6982\u5ff5\u65b9\u5411\u79fb\u52a8\u4ee5\u589e\u5f3a\u504f\u597d\uff0c\u800c\u5f53\u5e72\u9884\u5c06\u8868\u793a\u63a8\u79bb\u6a21\u578b\u7684\u6709\u6548\u751f\u6210\u6d41\u5f62\u65f6\uff0c\u6548\u7528\u4f1a\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLM\u63a7\u5236\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u504f\u597d\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u6790\u63d0\u51fa\u4e86SPLIT\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u63d0\u9ad8\u504f\u597d\u7684\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u6301\u6548\u7528\u3002"}}
{"id": "2602.01370", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01370", "abs": "https://arxiv.org/abs/2602.01370", "authors": ["Leonardo Brusini", "Cristian Sbrolli", "Eugenio Lomurno", "Toshihiko Yamasaki", "Matteo Matteucci"], "title": "PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles", "comment": null, "summary": "Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.", "AI": {"tldr": "PolyGen\u6846\u67b6\u901a\u8fc7\u591a\u751f\u6210\u5668\u5408\u6210\u6570\u636e\uff0c\u5f3a\u8c03\u7ed3\u6784\u591a\u6837\u6027\u800c\u975e\u5355\u7eaf\u6570\u636e\u91cf\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u5355\u6e90\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u5f53\u524d\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u751f\u6210\u5668\u6269\u5c55\uff0c\u8fd9\u5e26\u6765\u4e86\u7279\u5b9a\u9891\u8c31\u504f\u5dee\u5e76\u9650\u5236\u4e86\u7279\u5f81\u591a\u6837\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u8986\u76d6\u66f4\u591a\u6570\u636e\u6d41\u5f62\u3001\u5177\u6709\u7ec4\u5408\u4e25\u8c28\u6027\u7684\u65b9\u6cd5", "method": "\u91c7\u7528\u591a\u751f\u6210\u5668\uff08Polylithic\uff09\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u751f\u6210\u5668\u7684\u4ea4\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6d88\u9664\u6a21\u578b\u7279\u5b9a\u4f2a\u5f71\uff1b\u5f15\u5165\u7a0b\u5e8f\u5316\u786c\u8d1f\u6837\u672c\u8bfe\u7a0b\uff0c\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8bed\u6cd5\u7406\u89e3\uff1b\u5c06\u6570\u636e\u9884\u7b97\u4ece\u72ec\u7279\u63cf\u8ff0\u91cd\u65b0\u5206\u914d\u5230\u591a\u6e90\u53d8\u4f53", "result": "\u5728\u805a\u5408\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u9886\u5148\u7684\u5355\u6e90\u57fa\u7ebf\uff08SynthCLIP\uff09\u63d0\u534719.0%\uff1b\u5728SugarCrepe++\u7ec4\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u53479.1%\uff1b\u8bc1\u660e\u7ed3\u6784\u591a\u6837\u6027\u6bd4\u5355\u7eaf\u589e\u52a0\u5355\u6e90\u6837\u672c\u91cf\u66f4\u9ad8\u6548", "conclusion": "\u7ed3\u6784\u591a\u6837\u6027\u662f\u6bd4\u5355\u7eaf\u589e\u52a0\u5355\u6e90\u6837\u672c\u91cf\u66f4\u9ad8\u6548\u7684\u6570\u636e\u6269\u5c55\u89c4\u5f8b\uff0c\u591a\u751f\u6210\u5668\u5408\u6210\u6570\u636e\u65b9\u6cd5\u80fd\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u7a7a\u95f4"}}
{"id": "2602.00959", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00959", "abs": "https://arxiv.org/abs/2602.00959", "authors": ["Yuheng Yang", "Siqi Zhu", "Tao Feng", "Ge Liu", "Jiaxuan You"], "title": "Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction", "comment": "Homepage: https://ulab-uiuc.github.io/KnowledgeExtraction/", "summary": "Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u7cfb\u7edf\u63d0\u53d6\u548c\u91cf\u5316LLMs\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u53d1\u73b0\u9012\u5f52\u5206\u7c7b\u6cd5\u6700\u6709\u6548\uff0c\u89c2\u5bdf\u5230\u77e5\u8bc6\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u8bc6\u522b\u51faPass@1\u4e0ePass@k\u7684\u6743\u8861", "motivation": "LLMs\u88ab\u89c6\u4e3a\u538b\u7f29\u7684\u77e5\u8bc6\u5e93\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u5927\u591a\u662f\u9759\u6001\u7684\uff0c\u5bf9\u7cfb\u7edf\u77e5\u8bc6\u63a2\u6d4b\u652f\u6301\u6709\u9650\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u7406\u89e3LLMs\u771f\u6b63\u5305\u542b\u4ec0\u4e48\u77e5\u8bc6\u4ee5\u53ca\u77e5\u8bc6\u8fb9\u754c\u5728\u54ea\u91cc", "method": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cd\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u5728\u4e0d\u540c\u7c92\u5ea6\u4e0a\u63a2\u6d4b\u77e5\u8bc6\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u77e5\u8bc6\u5904\u7406\u6d41\u7a0b\uff1a\u5411\u91cf\u8fc7\u6ee4\u53bb\u9664\u91cd\u590d\u3001LLM\u88c1\u51b3\u89e3\u51b3\u8bed\u4e49\u91cd\u53e0\u3001\u9886\u57df\u76f8\u5173\u6027\u5ba1\u8ba1\u4fdd\u7559\u6709\u6548\u77e5\u8bc6\u5355\u5143", "result": "\u9012\u5f52\u5206\u7c7b\u6cd5\u662f\u6700\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\uff1b\u89c2\u5bdf\u5230\u6e05\u6670\u7684\u77e5\u8bc6\u7f29\u653e\u5b9a\u5f8b\uff08\u66f4\u5927\u6a21\u578b\u63d0\u53d6\u66f4\u591a\u77e5\u8bc6\uff09\uff1b\u8bc6\u522b\u51faPass@1\u4e0ePass@k\u6743\u8861\uff08\u9886\u57df\u4e13\u7528\u6a21\u578b\u521d\u59cb\u51c6\u786e\u7387\u9ad8\u4f46\u5feb\u901f\u9000\u5316\uff0c\u901a\u7528\u6a21\u578b\u6027\u80fd\u7a33\u5b9a\uff09\uff1b\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u5bfc\u81f4\u6a21\u578b\u5bb6\u65cf\u95f4\u53ef\u6d4b\u91cf\u7684\u77e5\u8bc6\u5dee\u5f02", "conclusion": "\u63d0\u51fa\u7684\u4ea4\u4e92\u5f0f\u4ee3\u7406\u6846\u67b6\u80fd\u7cfb\u7edf\u63d0\u53d6\u548c\u91cf\u5316LLMs\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3001\u77e5\u8bc6\u7f29\u653e\u89c4\u5f8b\u3001\u6a21\u578b\u7c7b\u578b\u95f4\u7684\u6027\u80fd\u6743\u8861\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u5bf9\u77e5\u8bc6\u5206\u5e03\u7684\u5f71\u54cd"}}
{"id": "2602.02360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02360", "abs": "https://arxiv.org/abs/2602.02360", "authors": ["Ryan Huynh", "Frank Guerin", "Alison Callwood"], "title": "Automated Multiple Mini Interview (MMI) Scoring", "comment": "18 pages, 2 figures", "summary": "Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30MMI\u4e2d\u7684\u8f6f\u6280\u80fd\uff0c\u901a\u8fc7\u5206\u89e3\u8bc4\u4f30\u8fc7\u7a0b\u4e3a\u8f6c\u5f55\u7cbe\u70bc\u548c\u6807\u51c6\u7279\u5b9a\u8bc4\u5206\uff0c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u8d85\u8d8a\u4e13\u95e8\u5fae\u8c03\u6a21\u578b\uff0c\u8fbe\u5230\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u7ade\u4e89\u6027\u9009\u62d4\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u8f6f\u6280\u80fd\uff08\u5982\u540c\u7406\u5fc3\u3001\u4f26\u7406\u5224\u65ad\u3001\u6c9f\u901a\u80fd\u529b\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bc4\u5206\u5f80\u5f80\u4e0d\u4e00\u81f4\u4e14\u5b58\u5728\u504f\u89c1\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u4e86\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u63a8\u7406\u7684\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u8ff7\u4f60\u9762\u8bd5\u7684\u62bd\u8c61\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7279\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5019\u9009\u4eba\u53d9\u8ff0\u4e2d\u7684\u9690\u542b\u4fe1\u53f7\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u5c06\u8bc4\u4f30\u8fc7\u7a0b\u5206\u89e3\u4e3a\u8f6c\u5f55\u7cbe\u70bc\u548c\u6807\u51c6\u7279\u5b9a\u8bc4\u5206\u4e24\u4e2a\u9636\u6bb5\u3002\u4f7f\u7528\u5927\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u8fdb\u884c3\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728MMI\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\uff08\u5e73\u5747QWK 0.62\uff09\u663e\u8457\u4f18\u4e8e\u4e13\u95e8\u5fae\u8c03\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5e73\u5747QWK 0.32\uff09\uff0c\u8fbe\u5230\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u53ef\u9760\u6027\u3002\u5728ASAP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5ab2\u7f8e\u9886\u57df\u7279\u5b9a\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u3001\u4e3b\u89c2\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u53ef\u80fd\u4e3a\u6570\u636e\u5bc6\u96c6\u578b\u5fae\u8c03\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6539\u53d8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u3002"}}
{"id": "2602.01382", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01382", "abs": "https://arxiv.org/abs/2602.01382", "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"], "title": "PromptRL: Prompt Matters in RL for Flow-Based Image Generation", "comment": null, "summary": "Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.\n  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptRL\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u7684\u63d0\u793a\u4f18\u5316\u4ee3\u7406\u96c6\u6210\u5230\u6d41\u5339\u914d\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RL\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u63d0\u793a\u8fc7\u62df\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u6d41\u5339\u914d\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\u5b58\u5728\u4e24\u4e2a\u88ab\u4f4e\u4f30\u4f46\u91cd\u8981\u7684\u9650\u5236\uff1a1\uff09\u7531\u4e8e\u751f\u6210\u591a\u6837\u6027\u4e0d\u8db3\u5bfc\u81f4\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff1b2\uff09\u660e\u663e\u7684\u63d0\u793a\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5373\u6a21\u578b\u4f1a\u8bb0\u5fc6\u7279\u5b9a\u7684\u8bad\u7ec3\u8868\u8ff0\uff0c\u5728\u8bed\u4e49\u76f8\u540c\u4f46\u98ce\u683c\u53d8\u5316\u7684\u63d0\u793a\u4e0a\u8868\u73b0\u6025\u5267\u4e0b\u964d\u3002", "method": "\u63d0\u51faPromptRL\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53ef\u8bad\u7ec3\u7684\u63d0\u793a\u4f18\u5316\u4ee3\u7406\u76f4\u63a5\u96c6\u6210\u5230\u57fa\u4e8e\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5faa\u73af\u4e2d\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5e26\u6765\u4e24\u4e2a\u4e92\u8865\u4f18\u52bf\uff1a\u5feb\u901f\u5f00\u53d1\u590d\u6742\u7684\u63d0\u793a\u91cd\u5199\u80fd\u529b\uff0c\u4ee5\u53ca\u91cd\u5851\u4f18\u5316\u52a8\u6001\u7684\u534f\u540c\u8bad\u7ec3\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1aGenEval\u5f97\u52060.97\uff0cOCR\u51c6\u786e\u73870.98\uff0cPickScore\u5f97\u520624.05\u3002\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e0a\uff0c\u5c06FLUX.1-Kontext\u7684EditReward\u4ece1.19\u63d0\u5347\u52301.43\uff0c\u4ec5\u97000.06\u767e\u4e07\u6b21rollout\uff0c\u8d85\u8d8a\u4e86Gemini 2.5 Flash Image\uff081.37\uff09\uff0c\u4e0e\u9700\u8981\u7ec6\u7c92\u5ea6\u6570\u636e\u6807\u6ce8\u548c\u590d\u6742\u591a\u9636\u6bb5\u8bad\u7ec3\u7684ReasonNet\uff081.44\uff09\u6027\u80fd\u76f8\u5f53\u3002\u5b9e\u9a8c\u8868\u660ePromptRL\u5728\u8fbe\u5230\u66f4\u9ad8\u6027\u80fd\u4e0a\u9650\u7684\u540c\u65f6\uff0c\u6bd4\u6734\u7d20\u6d41RL\u51cf\u5c11\u8d85\u8fc72\u500d\u7684rollout\u6b21\u6570\u3002", "conclusion": "PromptRL\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u6d41\u5339\u914d\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u63d0\u793a\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6548\u7387\u6539\u8fdb\u3002"}}
{"id": "2602.00960", "categories": ["cs.LG", "cs.AI", "cs.CE", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00960", "abs": "https://arxiv.org/abs/2602.00960", "authors": ["Leonardo Ferreira Guilhoto", "Akshat Kaushal", "Paris Perdikaris"], "title": "Multimodal Scientific Learning Beyond Diffusions and Flows", "comment": null, "summary": "Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.", "AI": {"tldr": "MDNs\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u76f8\u6bd4\u6269\u6563\u6a21\u578b\u548c\u6d41\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u9700\u8981\u5904\u7406\u591a\u6a21\u6001\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u3001\u591a\u7a33\u6001\u3001\u6df7\u6c8c\u52a8\u529b\u5b66\uff09\uff0c\u4f46\u5f53\u524d\u6d41\u884c\u7684\u9690\u5f0f\u751f\u6210\u6a21\u578b\uff08\u6269\u6563\u6a21\u578b\u3001\u6d41\u6a21\u578b\uff09\u5b58\u5728\u6570\u636e\u9700\u6c42\u5927\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4e0e\u79d1\u5b66\u95ee\u9898\u7ed3\u6784\u5316\u89e3\u7a7a\u95f4\u4e0d\u5339\u914d\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff08MDNs\uff09\u4f5c\u4e3a\u663e\u5f0f\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5176\u5f52\u7eb3\u504f\u7f6e\u9002\u5e94\u4f4e\u7ef4\u591a\u6a21\u6001\u7269\u7406\u95ee\u9898\uff0c\u76f4\u63a5\u5168\u5c40\u5206\u914d\u6982\u7387\u8d28\u91cf\u5230\u4e0d\u540c\u89e3\u5206\u652f", "result": "MDNs\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u80fd\u53ef\u9760\u6062\u590d\u5206\u79bb\u6a21\u6001\uff0c\u5728\u9006\u95ee\u9898\u3001\u591a\u7a33\u6001\u548c\u6df7\u6c8c\u79d1\u5b66\u56de\u5f52\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6837\u672c\u6548\u7387", "conclusion": "MDNs\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4e14\u88ab\u5ffd\u89c6\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6570\u636e\u7a00\u7f3a\u7684\u79d1\u5b66\u5e94\u7528\u573a\u666f"}}
{"id": "2602.02377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02377", "abs": "https://arxiv.org/abs/2602.02377", "authors": ["Haotong Yang", "Zitong Wang", "Shijia Kang", "Siqi Yang", "Wenkai Yu", "Xu Niu", "Yike Sun", "Yi Hu", "Zhouchen Lin", "Muhan Zhang"], "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof", "comment": "Under review", "summary": "While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality \"**question-proof-check**\" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u5229\u7528LLM\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\"\u95ee\u9898-\u8bc1\u660e-\u68c0\u67e5\"\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u8bad\u7ec3\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u589e\u5f3aLLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u867d\u7136LLM\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8bb8\u591a\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u662f\u57fa\u4e8e\u8bc1\u660e\u7684\uff0c\u65e0\u6cd5\u901a\u8fc7\u7b80\u5355\u7b54\u6848\u5339\u914d\u786e\u5b9a\u8bc1\u660e\u7684\u771f\u5b9e\u6027\u3002\u9700\u8981\u80fd\u591f\u53ef\u9760\u8bc4\u4f30\u5b8c\u6574\u8bc1\u660e\u8fc7\u7a0b\u7684\u5956\u52b1\u6a21\u578b\u6765\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u4ee5\u6700\u5c0f\u4eba\u5de5\u52aa\u529b\u5229\u7528LLM\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\"\u95ee\u9898-\u8bc1\u660e-\u68c0\u67e5\"\u4e09\u5143\u7ec4\u6570\u636e\uff1b\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u95ee\u9898\u6765\u6e90\u3001\u751f\u6210\u65b9\u6cd5\u548c\u6a21\u578b\u914d\u7f6e\u521b\u5efa\u591a\u6837\u5316\u7684\u8bc1\u660e\u5bf9\uff1b\u91c7\u7528\u5206\u5c42\u4eba\u5de5\u5ba1\u67e5\u8fdb\u884c\u6807\u7b7e\u5bf9\u9f50\uff1b\u8bad\u7ec3\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u52a0\u5165\u8fc7\u7a0b\u5956\u52b1\u548c\u4ee4\u724c\u6743\u91cd\u5e73\u8861\u6765\u7a33\u5b9aRL\u8fc7\u7a0b", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5f3a\u5927\u6027\u80fd\uff0c\u5305\u62ec\u5956\u52b1\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u6307\u5bfc\uff0c\u4e3a\u589e\u5f3aLLM\u6570\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u7528\u65b9\u6cd5\u548c\u5de5\u5177", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u548c\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86LLM\u5728\u6570\u5b66\u8bc1\u660e\u9a8c\u8bc1\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01391", "abs": "https://arxiv.org/abs/2602.01391", "authors": ["Xiaoyan Xing", "Xiao Zhang", "Sezer Karaoglu", "Theo Gevers", "Anand Bhattad"], "title": "Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics", "comment": "Project page: https:\\\\augmented-latent-intrinsics.github.io", "summary": "Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\\\augmented-latent-intrinsics.github.io", "AI": {"tldr": "ALI\u901a\u8fc7\u878d\u5408\u50cf\u7d20\u5bf9\u9f50\u7684\u89c6\u89c9\u7279\u5f81\u5230\u6f5c\u5728\u5185\u5728\u8868\u793a\u6846\u67b6\u4e2d\uff0c\u5e73\u8861\u8bed\u4e49\u62bd\u8c61\u548c\u5149\u5ea6\u4fdd\u771f\u5ea6\uff0c\u5728\u56fe\u50cf\u91cd\u7167\u660e\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5c24\u5176\u5728\u590d\u6742\u955c\u9762\u6750\u6599\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6f5c\u5728\u5185\u5728\u8868\u793a\u7684\u56fe\u50cf\u91cd\u7167\u660e\u65b9\u6cd5\u5728\u5904\u7406\u91d1\u5c5e\u548c\u73bb\u7483\u7b49\u6311\u6218\u6027\u6750\u6599\u65f6\u5e38\u5e38\u5931\u8d25\uff0c\u7814\u7a76\u53d1\u73b0\u9876\u7ea7\u8bed\u4e49\u7f16\u7801\u5668\u7684\u7279\u5f81\u53cd\u800c\u4f1a\u964d\u4f4e\u91cd\u7167\u660e\u8d28\u91cf\uff0c\u63ed\u793a\u4e86\u8bed\u4e49\u62bd\u8c61\u4e0e\u5149\u5ea6\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u6f5c\u5728\u5185\u5728\u8868\u793a\uff08ALI\uff09\uff0c\u901a\u8fc7\u5c06\u50cf\u7d20\u5bf9\u9f50\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7279\u5f81\u878d\u5408\u5230\u6f5c\u5728\u5185\u5728\u6846\u67b6\u4e2d\uff0c\u5e73\u8861\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u5bc6\u96c6\u5149\u5ea6\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u81ea\u76d1\u7763\u7ec6\u5316\u7b56\u7565\u7f13\u89e3\u771f\u5b9e\u4e16\u754c\u914d\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u4ec5\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0cALI\u5728\u91cd\u7167\u660e\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5728\u590d\u6742\u955c\u9762\u6750\u6599\u4e0a\u83b7\u5f97\u6700\u5927\u589e\u76ca\u3002", "conclusion": "ALI\u901a\u8fc7\u5e73\u8861\u8bed\u4e49\u62bd\u8c61\u548c\u5149\u5ea6\u4fdd\u771f\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u91cd\u7167\u660e\u4e2d\u7684\u6750\u6599\u6311\u6218\uff0c\u4e3a\u5904\u7406\u590d\u6742\u955c\u9762\u6750\u6599\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00969", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00969", "abs": "https://arxiv.org/abs/2602.00969", "authors": ["Junlin Huang", "Wenyi Fang", "Zhenheng Tang", "Yuxin Wang", "Xueze Kang", "Yang Zheng", "Bo Li", "Xiaowen Chu"], "title": "On the Spectral Flattening of Quantized Embeddings", "comment": null, "summary": "Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u8bed\u8a00\u6570\u636e\u7684\u5e42\u5f8b\u8c31\u7279\u6027\u662f\u8bed\u4e49\u7f16\u7801\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5747\u5300\u91cf\u5316\u4f1a\u622a\u65ad\u8c31\u5c3e\u5bfc\u81f4\u8c31\u5e73\u5766\u5316\u548c\u7a33\u5b9a\u79e9\u589e\u52a0\uff0c\u9020\u6210\u8868\u793a\u5d29\u6e83\uff0c\u963b\u788dLLMs\u5728\u8d85\u4f4e\u7cbe\u5ea6\u4e0b\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u65f6\u9762\u4e34\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u79bb\u6563\u91cf\u5316\u7ea6\u675f\u4e0e\u8bed\u8a00\u6570\u636e\u56fa\u6709\u7684\u91cd\u5c3e\u8c31\u7279\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002\u9700\u8981\u7406\u89e3\u91cf\u5316\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u8868\u793a\u7684\u8c31\u7ed3\u6784\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u4e3a\u4f55\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u5931\u8d25\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316Zipf\u7edf\u8ba1\u4e0e\u968f\u673a\u77e9\u9635\u7406\u8bba\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u8bc1\u660e\u5d4c\u5165\u5947\u5f02\u503c\u8c31\u7684\u5e42\u5f8b\u8870\u51cf\u662f\u8bed\u4e49\u7f16\u7801\u7684\u57fa\u672c\u8981\u6c42\u3002\u63a8\u5bfc\u7406\u8bba\u754c\u9650\u663e\u793a\u5747\u5300\u91cf\u5316\u5f15\u5165\u566a\u58f0\u57fa\u5e95\uff0c\u4e0d\u6210\u6bd4\u4f8b\u5730\u622a\u65ad\u8c31\u5c3e\uff0c\u5bfc\u81f4\u8c31\u5e73\u5766\u5316\u548c\u7a33\u5b9a\u79e9\u589e\u52a0\u3002\u5728GPT-2\u548cTinyLlama\u7b49\u4e0d\u540c\u67b6\u6784\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5747\u5300\u91cf\u5316\u4f1a\u622a\u65ad\u8c31\u5c3e\uff0c\u5bfc\u81f4\u8c31\u5e73\u5766\u5316\u548c\u7a33\u5b9a\u79e9\u4e25\u683c\u53ef\u8bc1\u660e\u7684\u589e\u52a0\u3002\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793a\u8fd9\u79cd\u51e0\u4f55\u9000\u5316\u4f1a\u5f15\u53d1\u8868\u793a\u5d29\u6e83\u3002\u91cf\u5316\u4e86LLMs\u7684\u8c31\u654f\u611f\u6027\uff0c\u5e76\u786e\u7acb\u8c31\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u7a33\u5b9a\u4f4e\u6bd4\u7279\u4f18\u5316\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u8bed\u8a00\u6570\u636e\u7684\u5e42\u5f8b\u8c31\u7279\u6027\u5bf9\u8bed\u4e49\u7f16\u7801\u81f3\u5173\u91cd\u8981\uff0c\u5747\u5300\u91cf\u5316\u4f1a\u7834\u574f\u8fd9\u79cd\u8c31\u7ed3\u6784\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u3002\u8c31\u4fdd\u771f\u5ea6\u662fLLMs\u5728\u8d85\u4f4e\u7cbe\u5ea6\u4e0b\u7a33\u5b9a\u8bad\u7ec3\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02378", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02378", "abs": "https://arxiv.org/abs/2602.02378", "authors": ["Raunak Jain", "Mudita Khurana", "John Stephens", "Srinivas Dharmasanam", "Shankar Venkataraman"], "title": "From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making", "comment": null, "summary": "As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524dLLM\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u5b58\u5728\"\u8c04\u5a9a\u5f0f\u540c\u610f\"\u95ee\u9898\uff0c\u63d0\u51fa\u9700\u8981\u4ece\u7b54\u6848\u751f\u6210\u8f6c\u5411\u534f\u4f5c\u524d\u63d0\u6cbb\u7406\uff0c\u5efa\u7acb\u57fa\u4e8e\u77e5\u8bc6\u57fa\u8d28\u7684\u5dee\u5f02\u9a71\u52a8\u63a7\u5236\u5faa\u73af\u3002", "motivation": "\u968f\u7740LLM\u4ece\u8f85\u52a9\u5de5\u5177\u6269\u5c55\u5230\u51b3\u7b56\u652f\u6301\uff0c\u51fa\u73b0\u4e86\u4e00\u4e2a\u5371\u9669\u6a21\u5f0f\uff1a\u6d41\u7545\u7684\u540c\u610f\u4f46\u6ca1\u6709\u6821\u51c6\u7684\u5224\u65ad\u3002\u4f4e\u6469\u64e6\u7684\u52a9\u624b\u53ef\u80fd\u53d8\u5f97\u8c04\u5a9a\uff0c\u9690\u542b\u5047\u8bbe\u88ab\u56fa\u5316\uff0c\u9a8c\u8bc1\u6210\u672c\u8f6c\u5ac1\u7ed9\u4e13\u5bb6\uff0c\u800c\u7ed3\u679c\u53cd\u9988\u592a\u665a\u65e0\u6cd5\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u5728\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4e2d\uff0c\u6269\u5c55\u6d41\u7545\u540c\u610f\u4f1a\u653e\u5927\u7cdf\u7cd5\u627f\u8bfa\u7684\u901f\u5ea6\u5feb\u4e8e\u5efa\u7acb\u4e13\u4e1a\u77e5\u8bc6\u7684\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u4ece\u7b54\u6848\u751f\u6210\u8f6c\u5411\u534f\u4f5c\u524d\u63d0\u6cbb\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5efa\u7acb\u57fa\u4e8e\u77e5\u8bc6\u57fa\u8d28\u7684\u5dee\u5f02\u9a71\u52a8\u63a7\u5236\u5faa\u73af\uff1a\u68c0\u6d4b\u51b2\u7a81\uff0c\u901a\u8fc7\u7c7b\u578b\u5316\u5dee\u5f02\uff08\u76ee\u7684\u8bba\u3001\u8ba4\u77e5\u8bba\u3001\u7a0b\u5e8f\u6027\uff09\u5b9a\u4f4d\u9519\u4f4d\uff0c\u901a\u8fc7\u51b3\u7b56\u5207\u7247\u89e6\u53d1\u6709\u754c\u534f\u5546\u3002\u627f\u8bfa\u95e8\u63a7\u963b\u6b62\u5bf9\u672a\u627f\u8bfa\u7684\u627f\u8f7d\u524d\u63d0\u91c7\u53d6\u884c\u52a8\uff0c\u4ef7\u503c\u95e8\u63a7\u6311\u6218\u5728\u4ea4\u4e92\u6210\u672c\u4e0b\u5206\u914d\u63a2\u6d4b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u4fe1\u4efb\u4ece\u5bf9\u8bdd\u6d41\u7545\u6027\u8f6c\u79fb\u5230\u53ef\u5ba1\u8ba1\u7684\u524d\u63d0\u548c\u8bc1\u636e\u6807\u51c6\u4e0a\u3002\u901a\u8fc7\u8f85\u5bfc\u6848\u4f8b\u8fdb\u884c\u8bf4\u660e\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u8bc1\u4f2a\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u53ef\u9760\u7684\u4eba\u673a\u4f19\u4f34\u5173\u7cfb\u9700\u8981\u6839\u672c\u6027\u8f6c\u53d8\uff1a\u4ece\u7b54\u6848\u751f\u6210\u8f6c\u5411\u534f\u4f5c\u524d\u63d0\u6cbb\u7406\uff0c\u5efa\u7acb\u57fa\u4e8e\u77e5\u8bc6\u57fa\u8d28\u7684\u5dee\u5f02\u9a71\u52a8\u63a7\u5236\u5faa\u73af\uff0c\u5c06\u4fe1\u4efb\u951a\u5b9a\u5728\u53ef\u5ba1\u8ba1\u7684\u524d\u63d0\u548c\u8bc1\u636e\u6807\u51c6\u800c\u975e\u5bf9\u8bdd\u6d41\u7545\u6027\u4e0a\u3002"}}
{"id": "2602.01418", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01418", "abs": "https://arxiv.org/abs/2602.01418", "authors": ["Christoffer Koo \u00d8hrstr\u00f8m", "Rafael I. Cabral Muchacho", "Yifei Dong", "Filippos Moumtzidellis", "Ronja G\u00fcldenring", "Florian T. Pokorny", "Lazaros Nalpantidis"], "title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas", "comment": null, "summary": "We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.", "AI": {"tldr": "\u63d0\u51fa\u629b\u7269\u7ebf\u4f4d\u7f6e\u7f16\u7801\uff08PaPE\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u629b\u7269\u7ebf\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u4e13\u95e8\u4e3a\u57fa\u4e8e\u6ce8\u610f\u529b\u67b6\u6784\u7684\u89c6\u89c9\u6a21\u6001\u8bbe\u8ba1\uff0c\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u4e3b\u8981\u4ece\u8bed\u8a00\u5904\u7406\u76841D\u5e8f\u5217\u6269\u5c55\u5230\u89c6\u89c9\u7684nD\u7ed3\u6784\uff0c\u4f46\u672a\u80fd\u5145\u5206\u8003\u8651\u89c6\u89c9\u6a21\u6001\u7684\u7279\u6027\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u7b26\u5408\u89c6\u89c9\u7279\u6027\u7684\u4f4d\u7f6e\u7f16\u7801", "method": "\u57fa\u4e8e\u629b\u7269\u7ebf\u8bbe\u8ba1\u4f4d\u7f6e\u7f16\u7801\uff0c\u9075\u5faa\u4e94\u4e2a\u539f\u5219\uff1a\u5e73\u79fb\u4e0d\u53d8\u6027\u3001\u65cb\u8f6c\u4e0d\u53d8\u6027\uff08PaPE-RI\uff09\u3001\u8ddd\u79bb\u8870\u51cf\u3001\u65b9\u5411\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5", "result": "\u5728\u6db5\u76d64\u79cd\u6a21\u6001\u76848\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cPaPE\u6216PaPE-RI\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff1b\u5728ImageNet-1K\u5916\u63a8\u5b9e\u9a8c\u4e2d\uff0cPaPE\u6bd4\u6b21\u4f18\u4f4d\u7f6e\u7f16\u7801\u7edd\u5bf9\u63d0\u5347\u9ad8\u8fbe10.5%", "conclusion": "PaPE\u662f\u4e00\u79cd\u6709\u6548\u7684\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6355\u6349\u89c6\u89c9\u6a21\u6001\u7279\u6027\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u5916\u63a8\u80fd\u529b"}}
{"id": "2602.00974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00974", "abs": "https://arxiv.org/abs/2602.00974", "authors": ["Adrien Aumon", "Myriam Lizotte", "Guy Wolf", "Kevin R. Moon", "Jake S. Rhodes"], "title": "Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment", "comment": null, "summary": "Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.", "AI": {"tldr": "FoSTA\u5229\u7528\u68ee\u6797\u8bf1\u5bfc\u7684\u51e0\u4f55\u7ed3\u6784\u53bb\u566a\u5e76\u6062\u590d\u4efb\u52a1\u76f8\u5173\u6d41\u5f62\uff0c\u901a\u8fc7\u5c42\u6b21\u8bed\u4e49\u4f20\u8f93\u5bf9\u9f50\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728\u5408\u6210\u57fa\u51c6\u548c\u5355\u7ec6\u80de\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u6807\u7b7e\u76d1\u7763\u6d41\u5f62\u5bf9\u9f50\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u5efa\u6a21\u57df\u5185\u5173\u7cfb\uff0c\u5f53\u7279\u5f81\u4e0e\u4efb\u52a1\u5174\u8da3\u5f31\u76f8\u5173\u65f6\u4f1a\u4ea7\u751f\u566a\u58f0\u548c\u8bed\u4e49\u8bef\u5bfc\u7ed3\u6784\uff0c\u5bfc\u81f4\u5bf9\u9f50\u8d28\u91cf\u4e0b\u964d\u3002", "method": "FoSTA\u5229\u7528\u68ee\u6797\u8bf1\u5bfc\u7684\u51e0\u4f55\u7ed3\u6784\u53bb\u566a\u57df\u5185\u7ed3\u6784\uff0c\u4ece\u6807\u7b7e\u4fe1\u606f\u68ee\u6797\u4eb2\u548c\u529b\u76f4\u63a5\u6784\u5efa\u8bed\u4e49\u8868\u793a\uff0c\u901a\u8fc7\u5feb\u901f\u5c42\u6b21\u8bed\u4e49\u4f20\u8f93\u5bf9\u9f50\uff0c\u6355\u6349\u6709\u610f\u4e49\u7684\u8de8\u57df\u5173\u7cfb\u3002", "result": "\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0cFoSTA\u5728\u5408\u6210\u57fa\u51c6\u4e0a\u6539\u8fdb\u4e86\u5bf9\u5e94\u6062\u590d\u548c\u6807\u7b7e\u8f6c\u79fb\uff0c\u5728\u5355\u7ec6\u80de\u5e94\u7528\uff08\u5305\u62ec\u6279\u6b21\u6821\u6b63\u548c\u751f\u7269\u4fdd\u5b88\u6027\uff09\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "FoSTA\u901a\u8fc7\u68ee\u6797\u5f15\u5bfc\u7684\u8bed\u4e49\u4f20\u8f93\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02382", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02382", "abs": "https://arxiv.org/abs/2602.02382", "authors": ["Ziyan Zhang", "Chao Wang", "Zhuo Chen", "Chiyi Li", "Kai Song"], "title": "ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs", "comment": null, "summary": "Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.", "AI": {"tldr": "ROG\uff1a\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u4e0eLLM\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u56de\u7b54\u590d\u6742\u4e00\u9636\u903b\u8f91\u67e5\u8be2\uff0c\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u7684\u90bb\u57df\u68c0\u7d22\u548c\u9010\u6b65\u63a8\u7406\u63d0\u9ad8\u590d\u6742\u67e5\u8be2\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u56de\u7b54\u590d\u6742\u7684\u4e00\u9636\u903b\u8f91\u67e5\u8be2\uff08\u5305\u542b\u6295\u5f71\u3001\u4ea4\u96c6\u3001\u5e76\u96c6\u3001\u5426\u5b9a\u7b49\u64cd\u4f5c\uff09\u975e\u5e38\u56f0\u96be\uff0c\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u7ed3\u6784\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faROG\u6846\u67b6\uff0c\u5c06\u591a\u64cd\u4f5c\u7b26\u67e5\u8be2\u5206\u89e3\u4e3a\u5355\u64cd\u4f5c\u7b26\u5b50\u67e5\u8be2\u5e8f\u5217\uff0c\u7ed3\u5408\u67e5\u8be2\u611f\u77e5\u7684\u90bb\u57df\u68c0\u7d22\u548cLLM\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u6bcf\u4e00\u6b65\u90fd\u57fa\u4e8e\u7d27\u51d1\u7684\u67e5\u8be2\u76f8\u5173\u90bb\u57df\u8bc1\u636e\uff0c\u5e76\u7f13\u5b58\u4e2d\u95f4\u7b54\u6848\u96c6\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u91cd\u7528\u3002", "result": "\u5728\u6807\u51c6\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROG\u76f8\u6bd4\u57fa\u4e8e\u5d4c\u5165\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\uff0c\u5728\u9ad8\u5ea6\u590d\u6742\u548c\u5426\u5b9a\u5bc6\u96c6\u7684\u67e5\u8be2\u7c7b\u578b\u4e0a\u6539\u8fdb\u6700\u5927\u3002", "conclusion": "ROG\u901a\u8fc7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u9010\u6b65\u63a8\u7406\u66ff\u4ee3\u5b66\u4e60\u578b\u64cd\u4f5c\u7b26\uff0c\u4e3a\u57fa\u4e8e\u5d4c\u5165\u7684\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u590d\u5408\u9519\u8bef\uff0c\u5728\u590d\u6742\u548c\u5426\u5b9a\u5bc6\u96c6\u67e5\u8be2\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u3002"}}
{"id": "2602.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01435", "abs": "https://arxiv.org/abs/2602.01435", "authors": ["Soumyaroop Nandi", "Prem Natarajan"], "title": "BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images", "comment": null, "summary": "We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet", "AI": {"tldr": "BioTamperNet\u662f\u4e00\u4e2a\u68c0\u6d4b\u751f\u7269\u533b\u5b66\u56fe\u50cf\u91cd\u590d\u533a\u57df\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fd1\u4f3c\u7684\u4eb2\u548c\u529b\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u53d6\u8bc1\u6a21\u578b\u4e3b\u8981\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u7ec6\u5fae\u7be1\u6539\u53ef\u80fd\u635f\u5bb3\u5b9e\u9a8c\u6709\u6548\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7684\u7be1\u6539\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBioTamperNet\u6846\u67b6\uff0c\u5305\u542b\u4eb2\u548c\u529b\u5f15\u5bfc\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6355\u83b7\u56fe\u50cf\u5185\u76f8\u4f3c\u6027\uff0c\u4ee5\u53ca\u4eb2\u548c\u529b\u5f15\u5bfc\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5efa\u6a21\u56fe\u50cf\u95f4\u5bf9\u5e94\u5173\u7cfb\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7SSM\u542f\u53d1\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u540c\u65f6\u8bc6\u522b\u7be1\u6539\u533a\u57df\u53ca\u5176\u6765\u6e90\u5bf9\u5e94\u533a\u57df\u3002", "result": "\u5728\u57fa\u51c6\u751f\u7269\u53d6\u8bc1\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBioTamperNet\u5728\u51c6\u786e\u68c0\u6d4b\u91cd\u590d\u533a\u57df\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BioTamperNet\u901a\u8fc7\u4eb2\u548c\u529b\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u548cSSM\u542f\u53d1\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u7be1\u6539\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u56fe\u50cf\u53d6\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.00987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00987", "abs": "https://arxiv.org/abs/2602.00987", "authors": ["Sawan Kumar", "Souvik Chakraborty"], "title": "Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees", "comment": "Accepted at ICLR 2026", "summary": "Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.", "AI": {"tldr": "\u63d0\u51faRandom Wavelet Features (RWF)\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6ce2\u91c7\u6837\u6784\u5efa\u53ef\u6269\u5c55\u7684\u975e\u5e73\u7a33\u6838\u8fd1\u4f3c\uff0c\u586b\u8865\u4e86\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u8ba1\u7b97\u590d\u6742\u7684\u6a21\u578b\u4e0e\u53ef\u6269\u5c55\u4f46\u6709\u9650\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u8fc7\u7a0b\u662f\u975e\u5e73\u7a33\u7684\uff0c\u7edf\u8ba1\u7279\u6027\u968f\u8f93\u5165\u57df\u53d8\u5316\uff0c\u4f46\u73b0\u6709\u53ef\u6269\u5c55\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5e73\u7a33\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u56f0\u96be\u6743\u8861\u3002", "method": "\u5f15\u5165Random Wavelet Features (RWF)\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5c0f\u6ce2\u65cf\u4e2d\u91c7\u6837\u6765\u6784\u5efa\u53ef\u6269\u5c55\u7684\u975e\u5e73\u7a33\u6838\u8fd1\u4f3c\uff0c\u5229\u7528\u5c0f\u6ce2\u7684\u56fa\u6709\u5c40\u90e8\u5316\u548c\u591a\u5206\u8fa8\u7387\u7ed3\u6784\u751f\u6210\u663e\u5f0f\u7279\u5f81\u6620\u5c04\u3002", "result": "RWF\u5728\u7406\u8bba\u4e0a\u6709\u6b63\u5b9a\u6027\u3001\u65e0\u504f\u6027\u548c\u4e00\u81f4\u6536\u655b\u4fdd\u8bc1\uff1b\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u8bc1\u8868\u660e\uff0cRWF\u4f18\u4e8e\u5e73\u7a33\u968f\u673a\u7279\u5f81\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6743\u8861\u3002", "conclusion": "RWF\u586b\u8865\u4e86\u8868\u8fbe\u6027\u6a21\u578b\u548c\u53ef\u6269\u5c55\u65b9\u6cd5\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u975e\u5e73\u7a33\u95ee\u9898\u89e3\u9501\u4e86\u53ef\u6269\u5c55\u4e14\u8868\u8fbe\u6027\u5f3a\u7684\u6838\u65b9\u6cd5\u3002"}}
{"id": "2602.02414", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02414", "abs": "https://arxiv.org/abs/2602.02414", "authors": ["Joshua Mitton", "Prarthana Bhattacharyya", "Digory Smith", "Thomas Christie", "Ralph Abboud", "Simon Woodhead"], "title": "Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank", "comment": "21 pages, 8 figures, 8 tables. Joshua Mitton and Prarthana Bhattacharyya contributed equally to this paper", "summary": "Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.", "AI": {"tldr": "\u4f7f\u7528LLM\u4ece\u5e08\u751f\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u5b66\u751f\u8bef\u89e3\uff1a\u9996\u5148\u751f\u6210\u53ef\u80fd\u7684\u8bef\u89e3\uff0c\u7136\u540e\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u5ea6\u68c0\u7d22\uff0c\u6700\u540e\u7528\u53e6\u4e00\u4e2aLLM\u91cd\u65b0\u6392\u5e8f\u4ee5\u63d0\u9ad8\u76f8\u5173\u6027\u3002", "motivation": "\u53ca\u65f6\u51c6\u786e\u8bc6\u522b\u5b66\u751f\u8bef\u89e3\u5bf9\u6539\u5584\u5b66\u4e60\u6210\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u9ad8\u5ea6\u4f9d\u8d56\u6559\u5e08\u7684\u52aa\u529b\u548c\u76f4\u89c9\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5fae\u8c03LLM\u751f\u6210\u53ef\u80fd\u7684\u8bef\u89e3\uff1b2) \u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u5ea6\u68c0\u7d22\u6700\u6709\u5e0c\u671b\u7684\u5019\u9009\uff1b3) \u7528\u53e6\u4e00\u4e2a\u5fae\u8c03LLM\u8bc4\u4f30\u548c\u91cd\u65b0\u6392\u5e8f\u4ee5\u63d0\u9ad8\u76f8\u5173\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6559\u80b2\u5bf9\u8bdd\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528LLaMA\u3001Qwen\u548cClaude\u7b49\u6a21\u578b\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5fae\u8c03\u80fd\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u751a\u81f3\u53ef\u4ee5\u8d85\u8d8a\u66f4\u5927\u7684\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684LLM\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u5b66\u751f\u8bef\u89e3\uff0c\u751f\u6210\u548c\u91cd\u65b0\u6392\u5e8f\u6b65\u9aa4\u5bf9\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u5fae\u8c03\u5c0f\u6a21\u578b\u53ef\u4ee5\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01452", "abs": "https://arxiv.org/abs/2602.01452", "authors": ["Penghao Deng", "Jidong J. Yang", "Jiachen Bian"], "title": "Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles", "comment": "21 pages, 15 figures, 3 tables", "summary": "Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a \"part-versus-whole\" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u89c6\u89c9\u65b9\u6cd5\uff08\u76f4\u63a5\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u8f85\u52a9\u5206\u7c7b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u7528\u4e8e\u4ece\u9a7e\u9a76\u573a\u666f\u4e2d\u8bc6\u522b\u9a7e\u9a76\u5458\u6ce8\u89c6\u70b9\u7684\u8bed\u4e49\u5bf9\u8c61\uff0c\u53d1\u73b0YOLOv13\u548cQwen2.5-VL-32b\u8868\u73b0\u6700\u4f73\uff0c\u5927\u578bVLM\u5728\u591c\u95f4\u8bc6\u522b\u5c0f\u76ee\u6807\u65f6\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u7406\u89e3\u9a7e\u9a76\u5458\u5728\u9a7e\u9a76\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u5e03\u5bf9\u5f00\u53d1\u4e0b\u4e00\u4ee3\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u548c\u63d0\u9ad8\u9053\u8def\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8f66\u8f86\u524d\u89c6\u6444\u50cf\u5934\u6355\u83b7\u7684\u9053\u8def\u573a\u666f\uff0c\u5c06\u6ce8\u89c6\u70b9\u4e0e\u5bf9\u8c61\u8bed\u4e49\u5173\u8054\u8d77\u6765\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u89c6\u89c9\u65b9\u6cd5\uff1a1\uff09\u76f4\u63a5\u76ee\u6807\u68c0\u6d4b\uff08YOLOv13\uff09\uff1b2\uff09\u5206\u5272\u8f85\u52a9\u5206\u7c7b\uff08SAM2+EfficientNetV2 vs YOLOv13\uff09\uff1b3\uff09\u57fa\u4e8e\u67e5\u8be2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-7b vs Qwen2.5-VL-32b\uff09\u3002\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8bc6\u522b\u6ce8\u89c6\u70b9\u5bf9\u5e94\u8bed\u4e49\u5bf9\u8c61\u65f6\u7684\u6027\u80fd\u3002", "result": "\u76f4\u63a5\u76ee\u6807\u68c0\u6d4b\uff08YOLOv13\uff09\u548cQwen2.5-VL-32b\u8868\u73b0\u6700\u4f73\uff0cMacro F1-Score\u8d85\u8fc70.84\u3002\u5927\u578bVLM\uff08Qwen2.5-VL-32b\uff09\u5728\u8bc6\u522b\u4ea4\u901a\u706f\u7b49\u5c0f\u578b\u5b89\u5168\u5173\u952e\u5bf9\u8c61\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u591c\u95f4\u6076\u52a3\u6761\u4ef6\u4e0b\u3002\u5206\u5272\u8f85\u52a9\u65b9\u6cd5\u56e0\"\u90e8\u5206vs\u6574\u4f53\"\u8bed\u4e49\u5dee\u8ddd\u5bfc\u81f4\u53ec\u56de\u7387\u5927\u5e45\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4f20\u7edf\u68c0\u6d4b\u5668\u7684\u5b9e\u65f6\u6548\u7387\u4e0e\u5927\u578bVLM\u63d0\u4f9b\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u4eba\u7c7b\u611f\u77e5\u667a\u80fd\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.01003", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01003", "abs": "https://arxiv.org/abs/2602.01003", "authors": ["Zhishen Sun", "Sizhe Dang", "Guang Dai", "Haishan Ye"], "title": "ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning", "comment": null, "summary": "Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\\% and is comparable to GRPO with an accuracy of 78.34\\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\\times$ compared to PPO and by $10\\times$ compared to GRPO, achieving an extremely low GPU memory usage.", "AI": {"tldr": "ESSAM\u7ed3\u5408\u8fdb\u5316\u7b56\u7565\u548c\u9510\u5ea6\u611f\u77e5\u6700\u5927\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4eGPU\u5185\u5b58\u4f7f\u7528\uff08\u76f8\u6bd4PPO\u51cf\u5c1118\u500d\uff0c\u76f8\u6bd4GRPO\u51cf\u5c1110\u500d\uff09\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u65f6GPU\u5185\u5b58\u4f7f\u7528\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5168\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\u3002", "method": "\u63d0\u51faESSAM\u6846\u67b6\uff0c\u5c06\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u7684\u53c2\u6570\u7a7a\u95f4\u96f6\u9636\u641c\u7d22\u4e0e\u9510\u5ea6\u611f\u77e5\u6700\u5927\u5316\uff08SAM\uff09\u7d27\u5bc6\u7ed3\u5408\uff0c\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03\u3002", "result": "ESSAM\u5728GSM8K\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe78.27%\uff0c\u6027\u80fd\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\uff08PPO 77.72%\uff0cGRPO 78.34%\uff09\uff0cGPU\u5185\u5b58\u4f7f\u7528\u5e73\u5747\u6bd4PPO\u51cf\u5c1118\u500d\uff0c\u6bd4GRPO\u51cf\u5c1110\u500d\u3002", "conclusion": "ESSAM\u5728\u4fdd\u6301\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86GPU\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6570\u5b66\u63a8\u7406\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02440", "abs": "https://arxiv.org/abs/2602.02440", "authors": ["Nishat Raihan", "Sadiya Sayara Chowdhury Puspo", "Ana-Maria Bucur", "Stevie Chancellor", "Marcos Zampieri"], "title": "Large Language Models for Mental Health: A Multilingual Evaluation", "comment": null, "summary": "Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u548c\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u4e14\u4e0b\u964d\u7a0b\u5ea6\u56e0\u8bed\u8a00\u548c\u7c7b\u578b\u800c\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\uff0c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u4e86\u89e3\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728\u516b\u4e2a\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\u53ca\u5176\u673a\u5668\u7ffb\u8bd1\u7248\u672c\u4e0a\u8bc4\u4f30\u4e13\u6709\u548c\u5f00\u6e90LLM\u3002\u6bd4\u8f83\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u4e0e\u4f20\u7edfNLP\u57fa\u7ebf\u5bf9\u6bd4\u3002\u540c\u65f6\u8bc4\u4f30\u4e0d\u540c\u8bed\u8a00\u5bb6\u65cf\u548c\u7c7b\u578b\u5b66\u7684\u7ffb\u8bd1\u8d28\u91cf\u53ca\u5176\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u4e13\u6709LLM\u548c\u5fae\u8c03\u5f00\u6e90LLM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684F1\u5206\u6570\uff0c\u7ecf\u5e38\u8d85\u8d8a\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u4f46\u5728\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u4e0a\u6027\u80fd\u666e\u904d\u8f83\u4f4e\uff0c\u4e14\u4e0b\u964d\u7a0b\u5ea6\u56e0\u8bed\u8a00\u548c\u7c7b\u578b\u5b66\u800c\u5f02\u3002\u7ffb\u8bd1\u8d28\u91cf\u5f15\u5165\u7684\u7ed3\u6784\u6216\u8bcd\u6c47\u4e0d\u5339\u914d\u4f1a\u5f71\u54cdLLM\u8868\u73b0\u3002", "conclusion": "LLM\u5728\u5904\u7406\u975e\u82f1\u8bed\u5fc3\u7406\u5065\u5eb7\u4efb\u52a1\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5f53\u7ffb\u8bd1\u8d28\u91cf\u5f15\u5165\u7ed3\u6784\u6216\u8bcd\u6c47\u4e0d\u5339\u914d\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u548c\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u6570\u636e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.01459", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01459", "abs": "https://arxiv.org/abs/2602.01459", "authors": ["Joey Kuang", "Alexander Wong"], "title": "Understanding vision transformer robustness through the lens of out-of-distribution detection", "comment": "Accepted to JCVIS 2025", "summary": "Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u53ef\u80fd\u635f\u5bb3\u89c6\u89c9Transformer\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728OOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u800c\u6570\u636e\u589e\u5f3a\u53ef\u80fd\u662f\u66f4\u597d\u7684\u9009\u62e9\u3002", "motivation": "\u89c6\u89c9Transformer\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b9e\u73b0\u53ef\u8bbf\u95ee\u548c\u5b9e\u65f6\u4f7f\u7528\u4ecd\u5177\u6311\u6218\u3002\u91cf\u5316\u53ef\u964d\u4f4e\u5185\u5b58\u548c\u63a8\u7406\u6210\u672c\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u635f\u5931\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u5e03\u5185\u4efb\u52a1\u884c\u4e3a\uff0c\u800c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u80fd\u901a\u8fc7\u63a2\u7d22\u5206\u5e03\u5916\u60c5\u51b5\u63d0\u4f9b\u91cf\u5316\u7279\u6027\u7684\u6d1e\u5bdf\u3002", "method": "\u7814\u7a76\u91cf\u5316\u7684\u5c0f\u578b\u53d8\u4f53\u6d41\u884c\u89c6\u89c9Transformer\uff08DeiT\u3001DeiT3\u548cViT\uff09\u5728\u5e38\u89c1OOD\u6570\u636e\u96c6\u4e0a\u7684\u884c\u4e3a\u3002\u5206\u6790\u5305\u62ecID\u4efb\u52a1\u8868\u73b0\u548cOOD\u68c0\u6d4b\uff0c\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u89c4\u6a21\uff08ImageNet-22k vs ImageNet-1k\uff09\u5bf9\u91cf\u5316\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "ID\u5206\u6790\u663e\u793a4\u4f4d\u6a21\u578b\u521d\u59cb\u4e0d\u7a33\u5b9a\uff0c\u7279\u522b\u662fImageNet-22k\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u6700\u5f3a\u7684FP32\u6a21\u578bDeiT3\u5728\u91cf\u5316\u540e\u6027\u80fd\u4e0b\u964d17%\uff0c\u6210\u4e3a\u6700\u5f31\u76844\u4f4d\u6a21\u578b\u4e4b\u4e00\u3002OOD\u68c0\u6d4b\u63ed\u793a\uff1aImageNet-22k\u9884\u8bad\u7ec3\u7684ViT\u548cDeiT3\u5728AUPR-out\u6307\u6807\u4e0a\u5206\u522b\u7ecf\u538615.0%\u548c19.2%\u7684\u5e73\u5747\u91cf\u5316\u635f\u5931\uff0c\u800c\u4ec5ImageNet-1k\u9884\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u4ec5\u7ecf\u53869.5%\u548c12.0%\u7684\u635f\u5931\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u53ef\u80fd\u635f\u5bb3\u4f4e\u6bd4\u7279\u91cf\u5316\u5728OOD\u68c0\u6d4b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u6570\u636e\u589e\u5f3a\u53ef\u80fd\u662f\u66f4\u6709\u76ca\u7684\u9009\u62e9\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u91cf\u5316\u6a21\u578b\u65f6\u8003\u8651OOD\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.01005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01005", "abs": "https://arxiv.org/abs/2602.01005", "authors": ["Deepak Bastola", "Pitambar Acharya", "Dipak Dulal", "Rabina Dhakal", "Yang Li"], "title": "Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning", "comment": "13 pages and submission to Public Health Nutrition is in progress", "summary": "Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \\emph{anemic} versus \\emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u5c3c\u6cca\u5c14\u513f\u7ae5\u8d2b\u8840\u72b6\u51b5\uff0c\u901a\u8fc7\u591a\u79cd\u7279\u5f81\u9009\u62e9\u6280\u672f\u8bc6\u522b\u5173\u952e\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u513f\u7ae5\u8d2b\u8840\u662f\u5c3c\u6cca\u5c14\u4e3b\u8981\u7684\u516c\u5171\u536b\u751f\u6311\u6218\uff0c\u4e0e\u751f\u957f\u53d1\u80b2\u53d7\u635f\u3001\u8ba4\u77e5\u80fd\u529b\u4e0b\u964d\u548c\u53d1\u75c5\u7387\u589e\u52a0\u76f8\u5173\u3002\u9700\u8981\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u513f\u7ae5\uff0c\u4ee5\u4fbf\u8fdb\u884c\u9488\u5bf9\u6027\u5e72\u9884\u3002", "method": "\u4f7f\u7528\u5c3c\u6cca\u5c14\u4eba\u53e3\u4e0e\u5065\u5eb7\u8c03\u67e5(NDHS 2022)\u76841,855\u540d\u513f\u7ae5\u6570\u636e\uff0c\u5c06\u8d2b\u8840\u72b6\u6001\u5b9a\u4e49\u4e3a\u4e8c\u5206\u7c7b\u4efb\u52a1\u3002\u5e94\u7528\u56db\u79cd\u7279\u5f81\u9009\u62e9\u65b9\u6cd5(\u5361\u65b9\u68c0\u9a8c\u3001\u4e92\u4fe1\u606f\u3001\u70b9\u4e8c\u5217\u76f8\u5173\u3001Boruta)\u8bc6\u522b\u5173\u952e\u7279\u5f81\u3002\u6bd4\u8f83\u4e868\u79cd\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b(LR\u3001KNN\u3001DT\u3001RF\u3001XGBoost\u3001SVM\u3001NB\u3001LDA)\u548c2\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b(DNN\u3001TabNet)\u7684\u6027\u80fd\u3002", "result": "\u4e94\u79cd\u7279\u5f81\u88ab\u6240\u6709\u65b9\u6cd5\u4e00\u81f4\u9009\u62e9\uff1a\u513f\u7ae5\u5e74\u9f84\u3001\u8fd1\u671f\u53d1\u70ed\u3001\u5bb6\u5ead\u89c4\u6a21\u3001\u6bcd\u4eb2\u8d2b\u8840\u72b6\u51b5\u548c\u5bc4\u751f\u866b\u9a71\u866b\u6cbb\u7597\u3002\u903b\u8f91\u56de\u5f52\u83b7\u5f97\u6700\u4f73\u53ec\u56de\u7387(0.701)\u548cF1\u5206\u6570(0.649)\uff0cDNN\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387(0.709)\uff0cSVM\u83b7\u5f97\u6700\u9ad8AUC(0.736)\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u80fd\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u8d2b\u8840\u9884\u6d4b\u80fd\u529b\u3002\u513f\u7ae5\u5e74\u9f84\u3001\u611f\u67d3\u6307\u6807\u3001\u6bcd\u4eb2\u8d2b\u8840\u72b6\u51b5\u548c\u9a71\u866b\u53f2\u7b49\u53ef\u89e3\u91ca\u7279\u5f81\u662f\u5c3c\u6cca\u5c14\u516c\u5171\u536b\u751f\u7b5b\u67e5\u548c\u98ce\u9669\u5206\u5c42\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2602.02462", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02462", "abs": "https://arxiv.org/abs/2602.02462", "authors": ["Gabriele Maraia", "Marco Valentino", "Fabio Massimo Zanzotto", "Leonardo Ranaldi"], "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u5f15\u5bfc\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7ed3\u6784\u63a8\u7406\u4e0e\u8bcd\u6c47\u8bed\u4e49\uff0c\u51cf\u5c11LLMs\u5728\u6f14\u7ece\u63a8\u7406\u4e2d\u7684\u8bed\u4e49\u504f\u89c1\uff0c\u63d0\u5347\u5f62\u5f0f\u63a8\u7406\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u4e2d\u5b58\u5728\u5185\u5bb9\u6548\u5e94\uff0c\u5373\u6df7\u6dc6\u8bed\u4e49\u5408\u7406\u6027\u4e0e\u5f62\u5f0f\u6709\u6548\u6027\u3002\u5373\u4f7f\u6a21\u578b\u751f\u6210\u9010\u6b65\u89e3\u91ca\uff0c\u4e2d\u95f4\u63a8\u7406\u4ecd\u7ee7\u627f\u76f8\u540c\u7684\u8bed\u4e49\u6377\u5f84\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u65f6\u7ed3\u6784\u7ea6\u675f\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u53ef\u9760\u6291\u5236\u8bed\u4e49\u5e72\u6270\u4ecd\u662f\u6311\u6218\u3002", "method": "\u6784\u5efa\u6210\u5bf9\u7684\u5185\u5bb9\u4e30\u5bcc\u548c\u62bd\u8c61\u4e09\u6bb5\u8bba\uff0c\u4f7f\u7528\u6a21\u578b\u5728\u62bd\u8c61\u8f93\u5165\u4e0a\u7684\u6fc0\u6d3b\u5b9a\u4e49\u62bd\u8c61\u63a8\u7406\u7a7a\u95f4\u3002\u5b66\u4e60\u8f7b\u91cf\u7ea7\u62bd\u8c61\u5668\uff0c\u4ece\u5185\u5bb9\u6761\u4ef6\u6b8b\u5dee\u6d41\u72b6\u6001\u9884\u6d4b\u4e0e\u8be5\u7a7a\u95f4\u5bf9\u9f50\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u5e72\u9884\u5728\u524d\u5411\u4f20\u64ad\u4e2d\u96c6\u6210\u8fd9\u4e9b\u9884\u6d4b\u3002", "result": "\u4f7f\u7528\u8de8\u8bed\u8a00\u8fc1\u79fb\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u663e\u793a\u62bd\u8c61\u5bf9\u9f50\u7684\u5f15\u5bfc\u51cf\u5c11\u4e86\u5185\u5bb9\u9a71\u52a8\u7684\u9519\u8bef\uff0c\u5e76\u63d0\u9ad8\u4e86\u6709\u6548\u6027\u654f\u611f\u6027\u80fd\u3002", "conclusion": "\u6fc0\u6d3b\u7ea7\u62bd\u8c61\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u673a\u5236\uff0c\u80fd\u591f\u589e\u5f3aLLMs\u4e2d\u5f62\u5f0f\u63a8\u7406\u5bf9\u8bed\u4e49\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.01530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01530", "abs": "https://arxiv.org/abs/2602.01530", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Preserving Localized Patch Semantics in VLMs", "comment": null, "summary": "Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word \"cat\"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.", "AI": {"tldr": "\u63d0\u51faLogit Lens Loss (LLL)\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2dLogit Lens\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u8865\u5145\u635f\u5931\u51fd\u6570\u9632\u6b62\u89c6\u89c9token\u4e22\u5931\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\uff0c\u4ece\u800c\u751f\u6210\u6709\u610f\u4e49\u7684\u5bf9\u8c61\u7f6e\u4fe1\u5ea6\u70ed\u56fe\u3002", "motivation": "Logit Lens\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5e94\u7528\u65f6\uff0c\u89c6\u89c9token\u7684\u5c40\u90e8\u4fe1\u606f\u4f1a\u6269\u6563\u5230\u8bed\u8a00token\u4e2d\uff0c\u5bfc\u81f4\u53ef\u89c6\u5316\u6548\u679c\u4e0d\u53ef\u7528\uff0c\u65e0\u6cd5\u7528\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "method": "\u63d0\u51faLogit Lens Loss (LLL)\uff0c\u4f5c\u4e3anext-token prediction\u7684\u8865\u5145\u635f\u5931\uff0c\u4f7f\u89c6\u89c9token\u5d4c\u5165\u4e0e\u63cf\u8ff0\u5176\u56fe\u50cf\u533a\u57df\u7684\u6587\u672c\u6982\u5ff5\u66f4\u8bed\u4e49\u5bf9\u9f50\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "result": "LLL\u4e0d\u4ec5\u4f7fLogit Lens\u4ea7\u751f\u6709\u610f\u4e49\u7684\u5bf9\u8c61\u7f6e\u4fe1\u5ea6\u70ed\u56fe\uff0c\u8fd8\u63d0\u9ad8\u4e86\u5206\u5272\u7b49\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u65e0\u9700\u9644\u52a0\u7279\u6b8a\u5934\u90e8\u3002", "conclusion": "LLL\u901a\u8fc7\u7ea6\u675f\u56fe\u50cf\u548c\u6587\u672ctoken\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u6df7\u5408\uff0c\u6709\u6548\u4fdd\u6301\u4e86\u89c6\u89c9token\u7684\u5c40\u90e8\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.01009", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01009", "abs": "https://arxiv.org/abs/2602.01009", "authors": ["Haoran Li", "Chenhan Xiao", "Lihao Mai", "Yang Weng", "Erik Blasch"], "title": "LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems", "comment": null, "summary": "Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\\underline{LA}rge-\\underline{S}cale \\underline{S}mall \\underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.", "AI": {"tldr": "LASS-ODE\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21ODE\u7cfb\u7edf\u52a8\u6001\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027ODE\u8868\u793a\u548c\u8de8\u7cfb\u7edf\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u7269\u7406\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u5171\u4eab\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u7269\u7406\u7cfb\u7edf\u52a8\u6001\u9884\u6d4b\u65b9\u9762\u8fdb\u5c55\u6709\u9650\u3002\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1) \u7269\u7406\u8ba1\u7b97\u53ef\u6269\u5c55\u6027 - \u7269\u7406\u7ea6\u675f\u5b66\u4e60\u9700\u8981ODE\u79ef\u5206\u7b49\u8ba1\u7b97\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u7cfb\u7edf\uff1b2) \u77e5\u8bc6\u5171\u4eab\u6548\u7387 - \u6ce8\u610f\u529b\u673a\u5236\u4e3b\u8981\u5728\u5355\u4e2a\u7cfb\u7edf\u5185\u8ba1\u7b97\uff0c\u9650\u5236\u4e86\u8de8\u7cfb\u7edf\u5171\u4eabODE\u7ed3\u6784\u7684\u63d0\u53d6\u3002", "method": "1) \u63d0\u51fa\u5c40\u90e8\u7ebf\u6027ODE\u8868\u793a\uff1a\u901a\u8fc7token-wise\u5c40\u90e8\u7ebf\u6027ODE\u8868\u793a\u4fdd\u6301\u7269\u7406\u4fdd\u771f\u5ea6\uff0c\u907f\u514d\u6602\u8d35\u7684\u975e\u7ebf\u6027\u79ef\u5206\uff1b2) \u5f15\u5165\u8de8\u7cfb\u7edf\u6ce8\u610f\u529b\u673a\u5236\uff1a\u901a\u8fc7\u516c\u5171\u7ed3\u6784\u4e2d\u5fc3(CSH)\u5b58\u50a8\u5171\u4eabtoken\uff0c\u805a\u5408\u8de8\u7cfb\u7edf\u77e5\u8bc6\uff1b3) \u572840GB ODE\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3LASS-ODE\u6a21\u578b\u3002", "result": "LASS-ODE\u5728\u9886\u57df\u5185\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684ODE\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u83b7\u5f97\u989d\u5916\u6539\u8fdb\u3002\u5c40\u90e8\u7ebf\u6027\u8868\u793a\u663e\u8457\u52a0\u901f\u4e86\u79ef\u5206\u8fc7\u7a0b\uff0c\u540c\u65f6\u51c6\u786e\u8fd1\u4f3c\u5c40\u90e8\u6570\u636e\u6d41\u5f62\u3002", "conclusion": "\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027ODE\u8868\u793a\u548c\u8de8\u7cfb\u7edf\u6ce8\u610f\u529b\u673a\u5236\uff0cLASS-ODE\u6210\u529f\u89e3\u51b3\u4e86\u7269\u7406\u7cfb\u7edf\u52a8\u6001\u9884\u6d4b\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u5171\u4eab\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21ODE\u7cfb\u7edf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2602.02464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02464", "abs": "https://arxiv.org/abs/2602.02464", "authors": ["Or Shafran", "Shaked Ronen", "Omri Fahn", "Shauli Ravfogel", "Atticus Geiger", "Mor Geva"], "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry", "comment": null, "summary": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6df7\u5408\u56e0\u5b50\u5206\u6790\u5668(MFA)\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u6fc0\u6d3b\u7a7a\u95f4\u7684\u5c40\u90e8\u9ad8\u65af\u533a\u57df\u6765\u6355\u6349\u8bed\u8a00\u6a21\u578b\u4e2d\u975e\u7ebf\u6027\u3001\u591a\u7ef4\u5ea6\u7684\u6982\u5ff5\u7ed3\u6784\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u7ebf\u6027\u53ef\u5206\u5047\u8bbe\u7684\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u5206\u89e3\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6982\u5ff5\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7ebf\u6027\u53ef\u5206\uff0c\u53ea\u5bfb\u627e\u5355\u4e2a\u5168\u5c40\u65b9\u5411\uff0c\u8fd9\u5ffd\u7565\u4e86\u5177\u6709\u975e\u7ebf\u6027\u6216\u591a\u7ef4\u7ed3\u6784\u7684\u590d\u6742\u6982\u5ff5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u8fd9\u4e9b\u590d\u6742\u7ed3\u6784\u7684\u53ef\u6269\u5c55\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408\u56e0\u5b50\u5206\u6790\u5668(MFA)\u4f5c\u4e3a\u6838\u5fc3\u65b9\u6cd5\uff0c\u5c06\u6fc0\u6d3b\u7a7a\u95f4\u5efa\u6a21\u4e3a\u4e00\u7ec4\u5177\u6709\u5c40\u90e8\u534f\u65b9\u5dee\u7ed3\u6784\u7684\u9ad8\u65af\u533a\u57df\u3002MFA\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u4e24\u4e2a\u7ec4\u5408\u51e0\u4f55\u5bf9\u8c61\uff1a\u533a\u57df\u8d28\u5fc3\u548c\u5c40\u90e8\u53d8\u5f02\u3002\u5728Llama-3.1-8B\u548cGemma-2-2B\u4e0a\u8bad\u7ec3\u5927\u89c4\u6a21MFA\u6a21\u578b\u3002", "result": "MFA\u6210\u529f\u6355\u6349\u4e86\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u590d\u6742\u975e\u7ebf\u6027\u7ed3\u6784\u3002\u5728\u5b9a\u4f4d\u548c\u64cd\u63a7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMFA\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u6709\u76d1\u7763\u5b9a\u4f4d\u65b9\u6cd5\u7ade\u4e89\u529b\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u64cd\u63a7\u6027\u80fd\u4e0a\u901a\u5e38\u4f18\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u3002", "conclusion": "\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff08\u901a\u8fc7\u5b50\u7a7a\u95f4\u8868\u8fbe\uff09\u662f\u7528\u4e8e\u53ef\u6269\u5c55\u6982\u5ff5\u53d1\u73b0\u548c\u6a21\u578b\u63a7\u5236\u7684\u6709\u524d\u666f\u5206\u6790\u5355\u5143\uff0c\u80fd\u591f\u6355\u6349\u5b64\u7acb\u65b9\u5411\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742\u7ed3\u6784\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.01533", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01533", "abs": "https://arxiv.org/abs/2602.01533", "authors": ["Zhe Ling", "Sicheng Yu", "Danyu Yang"], "title": "Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units", "comment": null, "summary": "Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\\pm 180^{\\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\\%$, $96.67\\%$, and $94.33\\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u8def\u5f84\u7b7e\u540d(SW-PS)\u548c\u7ebf\u6027\u5faa\u73af\u5355\u5143(LRU)\u7684\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u65cb\u8f6c\u53d8\u5f62\u95ee\u9898\uff0c\u5728CASIA-OLHWDB1.1\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u867d\u7136\u53ef\u4ee5\u5229\u7528\u7b14\u753b\u987a\u5e8f\u548c\u52a8\u6001\u7279\u5f81\u83b7\u5f97\u6bd4\u79bb\u7ebf\u8bc6\u522b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u65cb\u8f6c\u53d8\u5f62\u4f1a\u7834\u574f\u7b14\u753b\u7684\u7a7a\u95f4\u5e03\u5c40\uff0c\u663e\u8457\u964d\u4f4e\u8bc6\u522b\u51c6\u786e\u7387\u3002\u63d0\u53d6\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5f00\u653e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u8def\u5f84\u7b7e\u540d(SW-PS)\u6765\u6355\u6349\u5b57\u7b26\u7684\u5c40\u90e8\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u7ebf\u6027\u5faa\u73af\u5355\u5143(LRU)\u4f5c\u4e3a\u5206\u7c7b\u5668\u3002LRU\u7ed3\u5408\u4e86\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(RNN)\u7684\u5feb\u901f\u589e\u91cf\u5904\u7406\u80fd\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u7684\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\u4f18\u52bf\uff0c\u540c\u65f6\u80fd\u591f\u53ef\u9760\u5730\u5efa\u6a21\u52a8\u6001\u7b14\u753b\u7279\u5f81\u3002", "result": "\u5728CASIA-OLHWDB1.1\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u5b50\u96c6\uff08\u6570\u5b57\u3001\u82f1\u6587\u5927\u5199\u5b57\u6bcd\u3001\u4e2d\u6587\u90e8\u9996\uff09\u4e0a\u8fdb\u884c\u4e86\u968f\u673a\u65cb\u8f6c\u89d2\u5ea6\u8fbe\u00b1180\u00b0\u7684\u8bc6\u522b\u5b9e\u9a8c\u3002\u7ecf\u8fc7\u96c6\u6210\u5b66\u4e60\u540e\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523099.62%\u300196.67%\u548c94.33%\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684SW-PS+LRU\u6846\u67b6\u5728\u6536\u655b\u901f\u5ea6\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u7ade\u4e89\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684SW-PS+LRU\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u5728\u7ebf\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u4e2d\u7684\u65cb\u8f6c\u53d8\u5f62\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5b57\u7b26\u7c7b\u522b\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u65cb\u8f6c\u4e0d\u53d8\u7279\u5f81\u63d0\u53d6\u8fd9\u4e00\u6311\u6218\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01017", "abs": "https://arxiv.org/abs/2602.01017", "authors": ["Fuxin Wang", "Amr Alazali", "Yiqiao Zhong"], "title": "How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments", "comment": "25 pages, 23 figures", "summary": "Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u53d7\u63a7\u5408\u6210\u5b9e\u9a8c\u53d1\u73b0\uff1a\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u53d6\u51b3\u4e8e\u8bad\u7ec3\u566a\u58f0\u6c34\u5e73\uff0c\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\u6a21\u578b\u80fd\u5b66\u4e60\u5fe0\u5b9e\u63a8\u7406\uff0c\u9ad8\u4e8e\u9608\u503c\u65f6\u5219\u8f6c\u5411\u4e0d\u5fe0\u5b9e\u7684\u8df3\u6b65\u63a8\u7406\uff0c\u4e2d\u95f4\u5b58\u5728\u6df7\u5408\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u81ea\u56de\u5f52\u8bad\u7ec3\u4e2d\u9690\u5f0f\u81ea\u6211\u9a8c\u8bc1\u7684\u51fa\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u89c2\u5bdf\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u7ecf\u5e38\u4e0d\u5fe0\u5b9e\uff08\u4e2d\u95f4\u6b65\u9aa4\u903b\u8f91\u4e0d\u4e00\u81f4\u6216\u672a\u80fd\u53cd\u6620\u56e0\u679c\u5173\u7cfb\uff09\uff0c\u4f46\u5bf9\u4ec0\u4e48\u662f\u5fe0\u5b9e\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u4ee5\u53ca\u4e0d\u5fe0\u5b9e\u6027\u5982\u4f55\u4ece\u81ea\u56de\u5f52\u8bad\u7ec3\u4e2d\u4ea7\u751f\u7f3a\u4e4f\u6839\u672c\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u53d7\u63a7\u5408\u6210\u5b9e\u9a8c\uff0c\u5728\u566a\u58f0\u6570\u636e\u4e0a\u8bad\u7ec3\u5c0f\u578bTransformer\u6765\u89e3\u51b3\u6a21\u7b97\u672f\u8868\u8fbe\u5f0f\uff0c\u5c06\u5176\u79f0\u4e3a\u7b97\u672f\u8868\u8fbe\u5f0f\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u52a8\u6001\u548c\u673a\u5236\u6765\u7814\u7a76\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u53ea\u6709\u5728\u8bad\u7ec3\u566a\u58f0\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\u624d\u80fd\u5b66\u4e60\u5fe0\u5b9e\u7684\u63a8\u7406\uff0c\u8fd9\u5f52\u56e0\u4e8e\u7b80\u5355\u6027\u504f\u5dee\uff1b\u5728\u8f83\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\uff0c\u8bad\u7ec3\u52a8\u6001\u8868\u73b0\u51fa\u4ece\u5fe0\u5b9e\u9010\u6b65\u63a8\u7406\u5230\u4e0d\u5fe0\u5b9e\u8df3\u6b65\u63a8\u7406\u7684\u8f6c\u53d8\uff0c\u4e2d\u95f4\u5b58\u5728\u9884\u6d4b\u71b5\u77ed\u6682\u589e\u52a0\u7684\u6df7\u5408\u6a21\u5f0f\uff1b\u673a\u5236\u5206\u6790\u663e\u793a\u6a21\u578b\u901a\u8fc7\u89e3\u51b3\u4e0d\u4e00\u81f4\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u7f16\u7801\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u53d7\u8bad\u7ec3\u566a\u58f0\u6c34\u5e73\u5f71\u54cd\uff0c\u6a21\u578b\u5728\u81ea\u56de\u5f52\u8bad\u7ec3\u4e2d\u80fd\u53d1\u5c55\u51fa\u9690\u5f0f\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\uff0c\u901a\u8fc7\u7f16\u7801\u5185\u90e8\u4e0d\u786e\u5b9a\u6027\u6765\u534f\u8c03\u4e0d\u4e00\u81f4\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u8fd9\u4e3a\u7406\u89e3\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02467", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02467", "abs": "https://arxiv.org/abs/2602.02467", "authors": ["Noam Steinmetz Yalon", "Ariel Goldstein", "Liad Mudrik", "Mor Geva"], "title": "Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models", "comment": null, "summary": "Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u662f\u5426\u5177\u6709\u57fa\u4e8e\u4fe1\u5ff5\u5f62\u6210\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u7ade\u4e89\u52a8\u6001\uff0c\u53d1\u73b0LLMs\u786e\u5b9e\u8868\u73b0\u51fa\u4fe1\u5ff5\u5f15\u5bfc\u7684\u4ee3\u7406\u884c\u4e3a\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4eba\u4eec\u5f00\u59cb\u8d28\u7591\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u5177\u6709\u67d0\u79cd\u5f62\u5f0f\u7684\u610f\u8bc6\u3002\u4e3a\u4e86\u63a2\u7d22\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u57fa\u4e8eButlin\u7b49\u4eba\u63d0\u51fa\u7684\u610f\u8bc6\u6307\u6807\u6846\u67b6\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86HOT-3\u6307\u6807\u2014\u2014\u5373\u6d4b\u8bd5\u7cfb\u7edf\u662f\u5426\u5177\u6709\u7531\u901a\u7528\u4fe1\u5ff5\u5f62\u6210\u548c\u884c\u52a8\u9009\u62e9\u7cfb\u7edf\u5f15\u5bfc\u7684\u4ee3\u7406\u80fd\u529b\u3002", "method": "\u5c06\u4fe1\u5ff5\u89c6\u4e3a\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u54cd\u5e94\u8f93\u5165\u800c\u4ea7\u751f\u7684\u8868\u5f81\uff0c\u5f15\u5165\u91cf\u5316\u4fe1\u5ff5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e3b\u5bfc\u6027\u7684\u6307\u6807\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u7ade\u4e89\u4fe1\u5ff5\u4e4b\u95f4\u7684\u52a8\u6001\u5173\u7cfb\uff0c\u7814\u7a76\u4fe1\u5ff5\u5f62\u6210\u4e0e\u884c\u52a8\u9009\u62e9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a(1)\u5916\u90e8\u64cd\u4f5c\u80fd\u7cfb\u7edf\u6027\u5730\u8c03\u8282\u5185\u90e8\u4fe1\u5ff5\u5f62\u6210\uff1b(2)\u4fe1\u5ff5\u5f62\u6210\u56e0\u679c\u6027\u5730\u9a71\u52a8\u6a21\u578b\u7684\u884c\u52a8\u9009\u62e9\uff1b(3)\u6a21\u578b\u80fd\u591f\u76d1\u63a7\u5e76\u62a5\u544a\u81ea\u8eab\u7684\u4fe1\u5ff5\u72b6\u6001\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3aLLMs\u4e2d\u5b58\u5728\u4fe1\u5ff5\u5f15\u5bfc\u7684\u4ee3\u7406\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a2\u7d22LLMs\u4e2d\u4ee3\u7406\u3001\u4fe1\u5ff5\u548c\u5143\u8ba4\u77e5\u7684\u6d8c\u73b0\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u8868\u73b0\u51fa\u57fa\u4e8e\u4fe1\u5ff5\u5f62\u6210\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\u7684\u4ee3\u7406\u884c\u4e3a\u7279\u5f81\u3002"}}
{"id": "2602.01538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01538", "abs": "https://arxiv.org/abs/2602.01538", "authors": ["Youliang Zhang", "Zhengguang Zhou", "Zhentao Yu", "Ziyao Huang", "Teng Hu", "Sen Liang", "Guozhen Zhang", "Ziqiao Peng", "Shunkai Li", "Yi Chen", "Zixiang Zhou", "Yuan Zhou", "Qinglin Lu", "Xiu Li"], "title": "Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars", "comment": null, "summary": "Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io", "AI": {"tldr": "InteractAvatar\uff1a\u4e00\u79cd\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u89c4\u5212\u548c\u89c6\u9891\u5408\u6210\uff0c\u751f\u6210\u5177\u6709\u73af\u5883\u611f\u77e5\u548c\u6587\u672c\u5bf9\u9f50\u4ea4\u4e92\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u7269\u4f53\u4ea4\u4e92\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\u751f\u6210\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u5177\u6709\u7b80\u5355\u4eba\u4f53\u52a8\u4f5c\u7684\u5168\u8eab\u8bf4\u8bdd\u865a\u62df\u4eba\uff0c\u4f46\u6269\u5c55\u5230\u57fa\u4e8e\u7269\u4f53\u4ea4\u4e92\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\uff08GHOI\uff09\u4ecd\u5177\u6311\u6218\uff0c\u9700\u8981\u865a\u62df\u4eba\u80fd\u591f\u611f\u77e5\u73af\u5883\u5e76\u4e0e\u5468\u56f4\u7269\u4f53\u8fdb\u884c\u6587\u672c\u5bf9\u9f50\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u6846\u67b6InteractAvatar\uff0c\u5305\u542b\u611f\u77e5\u4e0e\u4ea4\u4e92\u6a21\u5757\uff08PIM\uff09\u751f\u6210\u6587\u672c\u5bf9\u9f50\u7684\u4ea4\u4e92\u52a8\u4f5c\uff0c\u4ee5\u53ca\u97f3\u9891\u4ea4\u4e92\u611f\u77e5\u751f\u6210\u6a21\u5757\uff08AIM\uff09\u5408\u6210\u751f\u52a8\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\u3002\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8fd0\u52a8-\u89c6\u9891\u5bf9\u9f50\u5668\uff0c\u4e24\u4e2a\u6a21\u5757\u5171\u4eab\u76f8\u4f3c\u7f51\u7edc\u7ed3\u6784\uff0c\u5b9e\u73b0\u52a8\u4f5c\u4e0e\u89c6\u9891\u7684\u5e76\u884c\u534f\u540c\u751f\u6210\u3002", "result": "\u5efa\u7acb\u4e86GHOI\u89c6\u9891\u751f\u6210\u57fa\u51c6GroundedInter\uff0c\u5927\u91cf\u5b9e\u9a8c\u548c\u6bd4\u8f83\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u57fa\u4e8e\u7269\u4f53\u4ea4\u4e92\u7684\u8bf4\u8bdd\u865a\u62df\u4eba\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "InteractAvatar\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u89c4\u5212\u548c\u89c6\u9891\u5408\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86GHOI\u751f\u6210\u4e2d\u7684\u73af\u5883\u611f\u77e5\u548c\u63a7\u5236-\u8d28\u91cf\u56f0\u5883\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u6587\u672c\u5bf9\u9f50\u7269\u4f53\u4ea4\u4e92\u7684\u751f\u52a8\u8bf4\u8bdd\u865a\u62df\u4eba\u3002"}}
{"id": "2602.01025", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01025", "abs": "https://arxiv.org/abs/2602.01025", "authors": ["Kaiyuan Cui", "Yige Li", "Yutao Wu", "Xingjun Ma", "Sarah Erfani", "Christopher Leckie", "Hanxun Huang"], "title": "Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models", "comment": "ICLR 2026", "summary": "Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \\href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.", "AI": {"tldr": "\u63d0\u51faUltraBreak\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u7a7a\u95f4\u6b63\u5219\u5316\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u6587\u672c\u76d1\u7763\uff0c\u751f\u6210\u53ef\u8fc1\u79fb\u7684\u901a\u7528\u5bf9\u6297\u6a21\u5f0f\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u8d8a\u72f1\u65b9\u6cd5\u8fc7\u62df\u5408\u5355\u4e00\u6a21\u578b\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u56fe\u50cf\u8d8a\u72f1\u653b\u51fb\u7684\u98ce\u9669\u3002\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u8d8a\u72f1\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8fc1\u79fb\u5230\u9ed1\u76d2\u6a21\u578b\u3002", "method": "UltraBreak\u6846\u67b6\uff1a1) \u5728\u89c6\u89c9\u7a7a\u95f4\u901a\u8fc7\u53d8\u6362\u548c\u6b63\u5219\u5316\u7ea6\u675f\u5bf9\u6297\u6a21\u5f0f\uff1b2) \u901a\u8fc7\u8bed\u4e49\u76ee\u6807\u653e\u677e\u6587\u672c\u76ee\u6807\uff1b3) \u5728\u76ee\u6807LLM\u7684\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u53d1\u73b0\u901a\u7528\u5bf9\u6297\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eUltraBreak\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5206\u6790\u63ed\u793a\u901a\u8fc7\u8bed\u4e49\u76ee\u6807\u5e73\u6ed1\u635f\u5931\u666f\u89c2\u5bf9\u5b9e\u73b0\u901a\u7528\u53ef\u8fc1\u79fb\u8d8a\u72f1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "UltraBreak\u901a\u8fc7\u89c6\u89c9\u6b63\u5219\u5316\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u6587\u672c\u76d1\u7763\uff0c\u6709\u6548\u7f13\u89e3\u4ee3\u7406\u8fc7\u62df\u5408\uff0c\u5b9e\u73b0\u8de8\u6a21\u578b\u548c\u653b\u51fb\u76ee\u6807\u7684\u5f3a\u8fc1\u79fb\u6027\uff0c\u4e3a\u7406\u89e3\u8d8a\u72f1\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.02474", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02474", "abs": "https://arxiv.org/abs/2602.02474", "authors": ["Haozhen Zhang", "Quanyu Long", "Jianzhu Bao", "Tao Feng", "Weizhi Zhang", "Haodong Yue", "Wenya Wang"], "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents", "comment": "Code is available at https://github.com/ViktorAxelsen/MemSkill", "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.", "AI": {"tldr": "MemSkill\u5c06LLM\u4ee3\u7406\u5185\u5b58\u64cd\u4f5c\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u7684\u8bb0\u5fc6\u6280\u80fd\uff0c\u901a\u8fc7\u63a7\u5236\u5668\u9009\u62e9\u6280\u80fd\u3001\u6267\u884c\u5668\u751f\u6210\u8bb0\u5fc6\u3001\u8bbe\u8ba1\u5668\u8fdb\u5316\u6280\u80fd\u96c6\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u95ed\u73af\u7cfb\u7edf\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5185\u5b58\u7cfb\u7edf\u4f9d\u8d56\u5c11\u91cf\u9759\u6001\u624b\u5de5\u8bbe\u8ba1\u64cd\u4f5c\uff0c\u786c\u7f16\u7801\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u4e0b\u50f5\u5316\u4e14\u5bf9\u957f\u5386\u53f2\u4f4e\u6548\u3002\u9700\u8981\u66f4\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMemSkill\u6846\u67b6\uff1a1) \u63a7\u5236\u5668\u5b66\u4e60\u9009\u62e9\u76f8\u5173\u8bb0\u5fc6\u6280\u80fd\uff1b2) LLM\u6267\u884c\u5668\u6839\u636e\u6280\u80fd\u6307\u5bfc\u751f\u6210\u8bb0\u5fc6\uff1b3) \u8bbe\u8ba1\u5668\u5b9a\u671f\u5ba1\u67e5\u56f0\u96be\u6848\u4f8b\uff0c\u901a\u8fc7\u63d0\u51fa\u6539\u8fdb\u548c\u65b0\u6280\u80fd\u6765\u8fdb\u5316\u6280\u80fd\u96c6\u3002\u5f62\u6210\u95ed\u73af\u8fdb\u5316\u7cfb\u7edf\u3002", "result": "\u5728LoCoMo\u3001LongMemEval\u3001HotpotQA\u548cALFWorld\u7b49\u4efb\u52a1\u4e0a\uff0cMemSkill\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4efb\u52a1\u6027\u80fd\u548c\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002\u5206\u6790\u63ed\u793a\u4e86\u6280\u80fd\u8fdb\u5316\u8fc7\u7a0b\u3002", "conclusion": "MemSkill\u5c06\u5185\u5b58\u64cd\u4f5c\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u3001\u53ef\u8fdb\u5316\u7684\u6280\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u9002\u5e94\u3001\u81ea\u8fdb\u5316\u7684LLM\u4ee3\u7406\u5185\u5b58\u7ba1\u7406\uff0c\u4e3a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.01540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01540", "abs": "https://arxiv.org/abs/2602.01540", "authors": ["Yuehai Chen"], "title": "FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training", "comment": null, "summary": "Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.", "AI": {"tldr": "FSCA-Net\u901a\u8fc7\u7279\u5f81\u5206\u79bb\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709CNN\u548cTransformer\u6a21\u578b\u5728\u8de8\u57df\u4eba\u7fa4\u8ba1\u6570\u4e2d\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u56e0\u4e3a\u5171\u4eab\u7279\u5f81\u548c\u57df\u7279\u5b9a\u7279\u5f81\u7ea0\u7f20\u5728\u4e00\u8d77", "method": "\u63d0\u51faFSCA-Net\u6846\u67b6\uff0c\u5c06\u7279\u5f81\u663e\u5f0f\u89e3\u8026\u4e3a\u57df\u4e0d\u53d8\u548c\u57df\u7279\u5b9a\u7ec4\u4ef6\uff1b\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u5efa\u6a21\u7ec4\u4ef6\u95f4\u4ea4\u4e92\uff1b\u5f15\u5165\u4e92\u4fe1\u606f\u4f18\u5316\u76ee\u6807\u6700\u5927\u5316\u57df\u4e0d\u53d8\u7279\u5f81\u4e00\u81f4\u6027\u5e76\u6700\u5c0f\u5316\u57df\u7279\u5b9a\u7279\u5f81\u5197\u4f59", "result": "\u5728\u591a\u4e2a\u4eba\u7fa4\u8ba1\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFSCA-Net\u6709\u6548\u7f13\u89e3\u4e86\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6027\u80fd", "conclusion": "FSCA-Net\u4e3a\u5b9e\u9645\u4eba\u7fa4\u5206\u6790\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7279\u5f81\u5206\u79bb\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u63d0\u5347\u4e86\u8de8\u57df\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01027", "abs": "https://arxiv.org/abs/2602.01027", "authors": ["Xin Nie", "Haicheng Zhang", "Liang Dong", "Beining Feng", "Jinhong Weng", "Guiling Sun"], "title": "SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models", "comment": "24pages,17figures", "summary": "Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP", "AI": {"tldr": "SFMP\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u641c\u7d22\u3001\u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u4f4d\u5bbd\u3001\u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\u3001\u884c\u5217\u91cd\u6392\u5e8f\u548c\u7edf\u4e00GEMM\u5185\u6838\u56db\u4e2a\u521b\u65b0\u70b9\uff0c\u5728\u76f8\u540c\u5185\u5b58\u7ea6\u675f\u4e0b\u4f18\u4e8e\u73b0\u6709\u5c42\u7ea7\u6df7\u5408\u7cbe\u5ea6\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u79bb\u6563\u4f18\u5316\u6765\u786e\u5b9a\u7cbe\u5ea6\u5206\u914d\uff0c\u8981\u4e48\u7531\u4e8e\u4e0d\u89c4\u5219\u5185\u5b58\u5e03\u5c40\u5bfc\u81f4\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\u3002", "method": "1) \u5206\u6570\u4f4d\u5bbd\uff1a\u5c06\u6743\u91cd\u77e9\u9635\u7684\u6574\u6570\u4f4d\u5bbd\u6269\u5c55\u4e3a\u5206\u6570\u503c\uff0c\u5c06\u79bb\u6563\u7cbe\u5ea6\u5206\u914d\u8f6c\u5316\u4e3a\u8fde\u7eed\u95ee\u9898\uff1b2) \u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6\uff1a\u5728\u6743\u91cd\u77e9\u9635\u5185\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7cbe\u5ea6\u5206\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u53cb\u597d\u6027\uff1b3) \u884c\u5217\u6743\u91cd\u91cd\u6392\u5e8f\uff1a\u901a\u8fc7\u884c\u5217\u91cd\u6392\u5e8f\u805a\u5408\u91cd\u8981\u6743\u91cd\uff0c\u63a8\u7406\u65f6\u4ec5\u5f15\u5165\u5c0f\u7684\u6fc0\u6d3b\u91cd\u6392\u5e8f\u5f00\u9500\uff1b4) \u7edf\u4e00GEMM\u5185\u6838\uff1a\u652f\u6301\u4efb\u610f\u5e73\u5747\u4f4d\u5bbd\u7684\u6df7\u5408\u7cbe\u5ea6GEMM\u8fd0\u7b97\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSFMP\u5728\u76f8\u540c\u5185\u5b58\u7ea6\u675f\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5c42\u7ea7\u6df7\u5408\u7cbe\u5ea6\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u91cf\u5316\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "SFMP\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7684\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u641c\u7d22\u3001\u786c\u4ef6\u53cb\u597d\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e25\u683c\u5185\u5b58\u9884\u7b97\u4e0b\u7684\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.02477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02477", "abs": "https://arxiv.org/abs/2602.02477", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Zhenghao Lin", "Eric Hancheng Jiang", "Hengyuan Zhang", "Yelong Shen", "Kai-Wei Chang", "Ying Nian Wu", "Yeyun Gong", "Weizhu Chen"], "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u7ade\u8d5b\u7ea7\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3002", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5728\u6a21\u578b\u80fd\u529b\u6781\u9650\u65f6\u5f80\u5f80\u4e0d\u8db3\uff0c\u4e14\u5176\u4e25\u683c\u987a\u5e8f\u6027\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u7684\u53ef\u6269\u5c55\u6027\u3002\u5206\u6cbb\u7406\u5ff5\u867d\u6709\u671b\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u901a\u7528\u540e\u8bad\u7ec3\u4e0e\u5206\u6cbb\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5145\u5206\u5229\u7528\u8fd9\u79cd\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u7b56\u7565\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u7ec4\uff0c\u987a\u5e8f\u6c42\u89e3\uff0c\u7136\u540e\u57fa\u4e8e\u5b50\u95ee\u9898\u89e3\u89e3\u51b3\u539f\u95ee\u9898\u3002\u5206\u89e3\u548c\u6c42\u89e3\u90fd\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u53ef\u6bd4\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c\u5206\u6cbb\u6846\u67b6\u8d4b\u4e88\u6a21\u578b\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\u548c\u66f4\u5f3a\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\uff0c\u5728\u7ade\u8d5b\u7ea7\u57fa\u51c6\u4e0aPass@1\u8d85\u8d8a\u94fe\u5f0f\u601d\u7ef48.6%\uff0cPass@32\u8d85\u8d8a6.3%\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5206\u6cbb\u63a8\u7406\u6846\u67b6\u80fd\u6709\u6548\u89e3\u9501\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6700\u5177\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u6f5c\u529b\uff0c\u63d0\u4f9b\u6bd4\u94fe\u5f0f\u601d\u7ef4\u66f4\u4f18\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01541", "abs": "https://arxiv.org/abs/2602.01541", "authors": ["Boyi Li", "Yifan Shen", "Yuanzhe Liu", "Yifan Xu", "Jiateng Liu", "Xinzhuo Li", "Zhengyuan Li", "Jingyuan Zhu", "Yunhan Zhong", "Fangzhou Lan", "Jianguo Cao", "James M. Rehg", "Heng Ji", "Ismini Lourentzou", "Xu Cao"], "title": "Toward Cognitive Supersensing in Multimodal Large Language Model", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.", "AI": {"tldr": "\u63d0\u51faCognitive Supersensing\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9\u610f\u8c61\u9884\u6d4b\u5934\u8d4b\u4e88MLLMs\u4eba\u7c7b\u822c\u7684\u89c6\u89c9\u610f\u8c61\u80fd\u529b\uff0c\u63d0\u5347\u590d\u6742\u8ba4\u77e5\u95ee\u9898\u89e3\u51b3\u80fd\u529b", "motivation": "\u5f53\u524dMLLMs\u5728\u5f00\u653e\u8bcd\u6c47\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89e3\u51b3\u9700\u8981\u89c6\u89c9\u8bb0\u5fc6\u548c\u62bd\u8c61\u89c6\u89c9\u7ec6\u8282\u7684\u590d\u6742\u8ba4\u77e5\u95ee\u9898\u65f6\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5728\u6587\u672c\u7a7a\u95f4\u6269\u5c55\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5ffd\u89c6\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u89c6\u89c9\u7a7a\u95f4\u753b\u677f\u548c\u89c6\u89c9\u610f\u8c61\u7684\u89c6\u89c9\u63a8\u7406\u673a\u5236\u3002", "method": "1. \u5f15\u5165Cognitive Supersensing\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u542bLatent Visual Imagery Prediction (LVIP)\u5934\uff0c\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u8ba4\u77e5\u6f5c\u5728\u5d4c\u5165\u5e8f\u5217\u5e76\u4e0e\u7b54\u6848\u5bf9\u9f50\uff0c\u5f62\u6210\u57fa\u4e8e\u89c6\u89c9\u7684\u5185\u90e8\u63a8\u7406\u94fe\u30022. \u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u89c6\u89c9\u6f5c\u5728\u8868\u793a\u4f18\u5316\u6587\u672c\u63a8\u7406\u8def\u5f84\u30023. \u521b\u5efaCogSense-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e94\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u3002", "result": "\u91c7\u7528Cognitive Supersensing\u8bad\u7ec3\u7684MLLMs\u5728CogSense-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5e76\u5728\u9886\u57df\u5916\u6570\u5b66\u548c\u79d1\u5b66VQA\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u5185\u90e8\u89c6\u89c9\u610f\u8c61\u53ef\u80fd\u662f\u8fde\u63a5\u611f\u77e5\u8bc6\u522b\u548c\u8ba4\u77e5\u7406\u89e3\u7684\u5173\u952e\u3002", "conclusion": "\u5185\u90e8\u89c6\u89c9\u610f\u8c61\u5bf9\u4e8e\u63d0\u5347MLLMs\u7684\u8ba4\u77e5\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0cCognitive Supersensing\u8303\u5f0f\u6709\u6548\u5f25\u8865\u4e86\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u8fde\u63a5\u611f\u77e5\u548c\u8ba4\u77e5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.01039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01039", "abs": "https://arxiv.org/abs/2602.01039", "authors": ["Zhiwei Ling", "Hailiang Zhao", "Chao Zhang", "Xiang Ao", "Ziqi Wang", "Cheng Zhang", "Zhen Qin", "Xinkui Zhao", "Kingsum Chow", "Yuanqing Wu", "MengChu Zhou"], "title": "Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.", "AI": {"tldr": "FLood\uff1a\u4e00\u79cd\u57fa\u4e8eOOD\u68c0\u6d4b\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u52a0\u6743\u673a\u5236\u89e3\u51b3\u975eIID\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6536\u655b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u4e2d\uff0c\u7528\u6237\u3001\u8bbe\u5907\u548c\u5e94\u7528\u573a\u666f\u7684\u5f02\u8d28\u6027\u5bfc\u81f4\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u4e25\u91cd\u7834\u574f\u5168\u5c40\u6a21\u578b\u7684\u6536\u655b\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u670d\u52a1\u8d28\u91cf", "method": "\u63d0\u51faFLood\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u52a0\u6743\u673a\u5236\uff1a\u5ba2\u6237\u7aef\u5c42\u9762\u901a\u8fc7\u63d0\u5347\u4f2aOOD\u6837\u672c\u6743\u91cd\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u76d1\u7763\u635f\u5931\uff1b\u670d\u52a1\u5668\u5c42\u9762\u6839\u636e\u5ba2\u6237\u7aefOOD\u7f6e\u4fe1\u5ea6\u5206\u6570\u52a0\u6743\u805a\u5408\uff0c\u4f18\u5148\u8003\u8651\u5206\u5e03\u4e00\u81f4\u6027\u66f4\u9ad8\u7684\u5ba2\u6237\u7aef\u66f4\u65b0", "result": "\u5728\u591a\u79cd\u975eIID\u8bbe\u7f6e\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLood\u5728\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u53ef\u4f5c\u4e3a\u6b63\u4ea4\u63d2\u4ef6\u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7b97\u6cd5\u4e2d\u63d0\u5347\u6027\u80fd", "conclusion": "FLood\u4e3a\u73b0\u5b9e\u4e16\u754c\u8054\u90a6\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u90e8\u7f72\u53ef\u9760\u7684\u667a\u80fd\u670d\u52a1\uff0c\u6709\u6548\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218"}}
{"id": "2602.02486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02486", "abs": "https://arxiv.org/abs/2602.02486", "authors": ["Jialiang Zhu", "Gongrui Zhang", "Xiaolong Ma", "Lin Xu", "Miaosen Zhang", "Ruiqi Yang", "Song Wang", "Kai Qiu", "Zhirong Wu", "Qi Dai", "Ruichun Ma", "Bei Liu", "Yifan Yang", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Xin Geng", "Baining Guo"], "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents", "comment": null, "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.", "AI": {"tldr": "Re-TRAC\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u8f68\u8ff9\u63a2\u7d22\u548c\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\uff0c\u89e3\u51b3\u4e86ReAct\u6846\u67b6\u5728\u72b6\u6001\u56de\u6eaf\u3001\u5206\u652f\u641c\u7d22\u548c\u5168\u5c40\u610f\u8bc6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u6e10\u8fdb\u5f0f\u7814\u7a76\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e3b\u8981\u5efa\u7acb\u5728ReAct\u6846\u67b6\u4e0a\uff0c\u5176\u7ebf\u6027\u8bbe\u8ba1\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u96be\u4ee5\u56de\u6eaf\u65e9\u671f\u72b6\u6001\u3001\u65e0\u6cd5\u5206\u652f\u5230\u66ff\u4ee3\u641c\u7d22\u65b9\u5411\u3001\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7f3a\u4e4f\u5168\u5c40\u610f\u8bc6\uff0c\u5bfc\u81f4\u5c40\u90e8\u6700\u4f18\u3001\u5197\u4f59\u63a2\u7d22\u548c\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "Re-TRAC\u6846\u67b6\u901a\u8fc7\u5728\u6bcf\u4e2a\u8f68\u8ff9\u540e\u751f\u6210\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\u6765\u603b\u7ed3\u8bc1\u636e\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u5931\u8d25\u548c\u672a\u6765\u8ba1\u5212\uff0c\u5e76\u5c06\u540e\u7eed\u8f68\u8ff9\u5efa\u7acb\u5728\u8fd9\u4e2a\u72b6\u6001\u8868\u793a\u4e0a\uff0c\u5b9e\u73b0\u8de8\u8f68\u8ff9\u63a2\u7d22\u3002\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u8fd8\u5f15\u5165\u4e86Re-TRAC\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728BrowseComp\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRe-TRAC\u4f7f\u7528\u524d\u6cbfLLM\u65f6\u6bd4ReAct\u6027\u80fd\u63d0\u534715-20%\u3002\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u4e86\u53ef\u6bd4\u89c4\u6a21\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cRe-TRAC\u5728\u8f6e\u6b21\u95f4\u663e\u793a\u5de5\u5177\u8c03\u7528\u548c\u4ee4\u724c\u4f7f\u7528\u7684\u5355\u8c03\u51cf\u5c11\uff0c\u8868\u660e\u63a2\u7d22\u9010\u6e10\u53d8\u5f97\u66f4\u6709\u9488\u5bf9\u6027\u3002", "conclusion": "Re-TRAC\u901a\u8fc7\u8de8\u8f68\u8ff9\u53cd\u601d\u548c\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\uff0c\u5c06\u7814\u7a76\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u89c4\u5212\uff0c\u89e3\u51b3\u4e86ReAct\u6846\u67b6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2602.01559", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.01559", "abs": "https://arxiv.org/abs/2602.01559", "authors": ["Libo Zhu", "Zihan Zhou", "Zhiyi Zhou", "Yiyang Qu", "Weihang Zhang", "Keyu Shi", "Yifan Fu", "Yulun Zhang"], "title": "Combined Flicker-banding and Moire Removal for Screen-Captured Images", "comment": null, "summary": "Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moir\u00e9 patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moir\u00e9 patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moir\u00e9 patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.", "AI": {"tldr": "\u63d0\u51faCLEAR\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u5c4f\u5e55\u622a\u56fe\u56fe\u50cf\u4e2d\u6469\u5c14\u7eb9\u548c\u95ea\u70c1\u6761\u5e26\u7684\u8054\u5408\u53bb\u9664\uff0c\u901a\u8fc7\u9891\u7387\u57df\u5206\u89e3\u91cd\u7ec4\u6a21\u5757\u548c\u8f68\u8ff9\u5bf9\u9f50\u635f\u5931\u63d0\u5347\u590d\u5408\u4f2a\u5f71\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u62cd\u6444\u663e\u793a\u5c4f\u56fe\u50cf\u65f6\uff0c\u6469\u5c14\u7eb9\u548c\u95ea\u70c1\u6761\u5e26\u540c\u65f6\u5b58\u5728\u4e14\u5f3a\u8026\u5408\uff0c\u5bfc\u81f4\u4e25\u91cd\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002\u73b0\u6709\u5355\u4e00\u9000\u5316\u5904\u7406\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u8fd9\u79cd\u590d\u5408\u573a\u666f\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6062\u590d\u6846\u67b6CLEAR\uff1a1)\u6784\u5efa\u5305\u542b\u4e24\u79cd\u4f2a\u5f71\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b2)\u5f15\u5165\u57fa\u4e8eISP\u7684\u95ea\u70c1\u6a21\u62df\u6d41\u7a0b\u7a33\u5b9a\u8bad\u7ec3\u5e76\u6269\u5c55\u9000\u5316\u5206\u5e03\uff1b3)\u8bbe\u8ba1\u9891\u7387\u57df\u5206\u89e3\u91cd\u7ec4\u6a21\u5757\u548c\u8f68\u8ff9\u5bf9\u9f50\u635f\u5931\u589e\u5f3a\u590d\u5408\u4f2a\u5f71\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\uff0cCLEAR\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u89e3\u51b3\u4e86\u5c4f\u5e55\u622a\u56fe\u56fe\u50cf\u4e2d\u6469\u5c14\u7eb9\u548c\u95ea\u70c1\u6761\u5e26\u7684\u8054\u5408\u53bb\u9664\u95ee\u9898\uff0c\u63d0\u51fa\u7684CLEAR\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u9891\u7387\u57df\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u5904\u7406\u4e86\u590d\u5408\u4f2a\u5f71\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.01045", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01045", "abs": "https://arxiv.org/abs/2602.01045", "authors": ["Zixin Jessie Chen", "Hao Chen", "Yizhou Liu", "Jeff Gore"], "title": "Superposition unifies power-law training dynamics", "comment": "17 pages, 14 figures", "summary": "We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.", "AI": {"tldr": "\u7814\u7a76\u7279\u5f81\u53e0\u52a0\u5728\u5e42\u5f8b\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u53e0\u52a0\u74f6\u9888\u5bfc\u81f4\u666e\u9002\u76841/t\u8bad\u7ec3\u6307\u6570\uff0c\u76f8\u6bd4\u65e0\u53e0\u52a0\u7684\u5e8f\u5217\u5b66\u4e60\u52a0\u901f\u8fbe10\u500d", "motivation": "\u63a2\u7a76\u7279\u5f81\u53e0\u52a0\u5982\u4f55\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u5e42\u5f8b\u52a8\u6001\uff0c\u7279\u522b\u662f\u751f\u4ea7\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u7b49\u5e7f\u6cdb\u4f7f\u7528\u53e0\u52a0\u7684\u795e\u7ecf\u7f51\u7edc", "method": "\u4f7f\u7528\u6559\u5e08-\u5b66\u751f\u6846\u67b6\uff0c\u9996\u5148\u63a8\u5bfc\u65e0\u53e0\u52a0\u8bad\u7ec3\u7684\u5206\u6790\u7406\u8bba\uff0c\u7136\u540e\u7814\u7a76\u53e0\u52a0\u74f6\u9888\u5bf9\u8bad\u7ec3\u52a8\u6001\u7684\u5f71\u54cd", "result": "\u65e0\u53e0\u52a0\u65f6\u5e42\u5f8b\u6307\u6570\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7edf\u8ba1\u548c\u901a\u9053\u91cd\u8981\u6027\uff1b\u53e0\u52a0\u74f6\u9888\u5bfc\u81f4\u666e\u9002\u7684\u223c1\u6307\u6570\uff0c\u8bad\u7ec3\u52a0\u901f\u8fbe10\u500d", "conclusion": "\u53e0\u52a0\u5bfc\u81f4\u5feb\u901f\u8bad\u7ec3\u4e14\u5177\u6709\u6570\u636e\u65e0\u5173\u7684\u5e42\u5f8b\u6307\u6570\uff0c\u8fd9\u5bf9\u4f7f\u7528\u53e0\u52a0\u7684\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2602.02495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02495", "abs": "https://arxiv.org/abs/2602.02495", "authors": ["Peter Chen", "Xiaopeng Li", "Xi Chen", "Tianyi Lin"], "title": "Reward-free Alignment for Conflicting Objectives", "comment": "27 pages", "summary": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.", "AI": {"tldr": "\u63d0\u51faRACO\u6846\u67b6\uff0c\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u89e3\u51b3\u591a\u76ee\u6807\u51b2\u7a81\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u526a\u88c1\u7684\u51b2\u7a81\u89c4\u907f\u68af\u5ea6\u4e0b\u964d\u5b9e\u73b0\u6536\u655b\u4fdd\u8bc1\uff0c\u5728\u591a\u4e2aLLM\u4e0a\u5c55\u793a\u66f4\u597d\u7684\u5e15\u7d2f\u6258\u6743\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u5e38\u6d89\u53ca\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u52a0\u6743\u635f\u5931\u53ef\u80fd\u65e0\u6cd5\u540c\u65f6\u6539\u8fdb\u6240\u6709\u76ee\u6807\uff0c\u800c\u57fa\u4e8e\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\u589e\u52a0\u4e86\u590d\u6742\u6027\u5e76\u53ef\u80fd\u626d\u66f2\u7528\u6237\u504f\u597d\u3002", "method": "\u63d0\u51faRACO\u6846\u67b6\uff0c\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u901a\u8fc7\u526a\u88c1\u7684\u51b2\u7a81\u89c4\u907f\u68af\u5ea6\u4e0b\u964d\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u63d0\u4f9b\u6536\u655b\u5230\u5e15\u7d2f\u6258\u4e34\u754c\u70b9\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u52a0\u5165\u542f\u53d1\u5f0f\u6539\u8fdb\u3002", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\uff08Qwen 3\u3001Llama 3\u3001Gemma 3\uff09\u7684\u591a\u76ee\u6807\u6458\u8981\u548c\u5b89\u5168\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u5b9a\u6027\u5b9a\u91cf\u8bc4\u4f30\u5747\u663e\u793a\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u5e15\u7d2f\u6258\u6743\u8861\u3002", "conclusion": "RACO\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u76ee\u6807\u51b2\u7a81\u5bf9\u9f50\u95ee\u9898\uff0c\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u76f4\u63a5\u5229\u7528\u504f\u597d\u6570\u636e\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.01561", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01561", "abs": "https://arxiv.org/abs/2602.01561", "authors": ["Yejin Son", "Saejin Kim", "Dongjun Min", "Younjae Yu"], "title": "Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd", "comment": "24 pages", "summary": "Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.", "AI": {"tldr": "\u63d0\u51faMUN\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u975e\u5178\u578b\u89c6\u89c9\u573a\u666f\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1R-ICL\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u793a\u4f8b\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u4e0d\u5e38\u89c1\u573a\u666f\u4e0b\u7684\u63a8\u7406\u6027\u80fd", "motivation": "\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u5e38\u8bc6\u63a8\u7406\u662fAI\u57fa\u7840\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u504f\u79bb\u5178\u578b\u89c6\u89c9\u6216\u4e0a\u4e0b\u6587\u9884\u671f\u7684\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u975e\u5178\u578b\u3001\u6587\u5316\u591a\u6837\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027", "method": "\u63d0\u51faMUN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u89c6\u89c9\u573a\u666f\u4e0e\u610f\u5916\u7ed3\u679c\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b\u5f00\u53d1\u57fa\u4e8e\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(R-ICL)\u6846\u67b6\uff0c\u4f7f\u7528\u65b0\u578b\u591a\u6a21\u6001\u96c6\u6210\u68c0\u7d22\u5668(MER)\u8bc6\u522b\u8bed\u4e49\u76f8\u5173\u793a\u4f8b\uff0c\u5c06\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u5c0f\u6a21\u578b", "result": "R-ICL\u65b9\u6cd5\u5728MUN\u57fa\u51c6\u4e0a\u6bd4\u57fa\u7ebfICL\u65b9\u6cd5\u5e73\u5747\u63d0\u53478.3%\uff0c\u5728\u591a\u6a21\u6001\u96c6\u6210\u68c0\u7d22\u5668\u5e2e\u52a9\u4e0b\u80fd\u6709\u6548\u5904\u7406\u56fe\u50cf-\u6587\u672c\u5bf9\u6545\u610f\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5", "conclusion": "MUN\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u3001\u6587\u5316\u591a\u6837\u3001\u975e\u5178\u578b\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0cR-ICL\u6846\u67b6\u5728\u4f4e\u9891\u3001\u975e\u5178\u578b\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.01051", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01051", "abs": "https://arxiv.org/abs/2602.01051", "authors": ["Rong Fu", "Wenxin Zhang", "Muge Qi", "Yang Li", "Yabin Jin", "Jiekai Wu", "Jiaxuan Lu", "Chunlei Meng", "Youjin Wang", "Zeli Su", "Juntao Gao", "Li Bao", "Qi Zhao", "Wei Luo", "Simon Fong"], "title": "SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes", "comment": "19 pages, 8 figures, 8 tables", "summary": "Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4ece\u539f\u578b\u5b57\u5178\u5408\u6210\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4efb\u52a1\u63cf\u8ff0\u7b26\u548c\u9002\u914d\u5668\u6a21\u5757\uff0c\u5b9e\u73b0\u5c11\u91cf\u6837\u672c\u4e0b\u7684T\u7ec6\u80de\u53d7\u4f53\u5e93\u5206\u6790\uff0c\u65e0\u9700\u5b8c\u6574\u5fae\u8c03", "motivation": "T\u7ec6\u80de\u53d7\u4f53\u5e93\u5206\u6790\u4e3a\u75be\u75c5\u68c0\u6d4b\u548c\u514d\u75ab\u76d1\u6d4b\u63d0\u4f9b\u91cd\u8981\u4fe1\u53f7\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u6807\u7b7e\u7a00\u758f\u3001\u961f\u5217\u5f02\u8d28\u6027\u4ee5\u53ca\u5927\u578b\u7f16\u7801\u5668\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u8ba1\u7b97\u8d1f\u62c5\u7b49\u6311\u6218", "method": "1) \u4ece\u5b66\u4e60\u5230\u7684\u539f\u578b\u5b57\u5178\u5408\u6210\u7d27\u51d1\u7684\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u5316\uff1b2) \u4f7f\u7528\u57fa\u4e8e\u5e93\u63a2\u9488\u548c\u6c60\u5316\u5d4c\u5165\u7edf\u8ba1\u7684\u8f7b\u91cf\u7ea7\u4efb\u52a1\u63cf\u8ff0\u7b26\uff1b3) \u5e94\u7528\u5c0f\u578b\u9002\u914d\u5668\u6a21\u5757\u5230\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff1b4) \u901a\u8fc7\u57fa\u5e8f\u611f\u77e5\u63a2\u9488\u548c\u6821\u51c6\u7684\u57fa\u5e8f\u53d1\u73b0\u7ba1\u9053\u4fdd\u6301\u53ef\u89e3\u91ca\u6027", "result": "\u6846\u67b6\u80fd\u591f\u5728\u4ec5\u6709\u5c11\u91cf\u652f\u6301\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u7acb\u5373\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u65e0\u9700\u5b8c\u6574\u6a21\u578b\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u5c06\u9884\u6d4b\u51b3\u7b56\u4e0e\u5e8f\u5217\u7ea7\u4fe1\u53f7\u8054\u7cfb\u8d77\u6765", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u6807\u7b7e\u7a00\u7f3a\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u90e8\u7f72\u5e93\u4fe1\u606f\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u6837\u672c\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84"}}
{"id": "2602.01570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01570", "abs": "https://arxiv.org/abs/2602.01570", "authors": ["Yiwen Jia", "Hao Wei", "Yanhui Zhou", "Chenyang Ge"], "title": "One-Step Diffusion for Perceptual Image Compression", "comment": null, "summary": "Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.", "AI": {"tldr": "\u63d0\u51fa\u5355\u6b65\u6269\u6563\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5OSDiff\uff0c\u5728\u4fdd\u6301\u538b\u7f29\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b046\u500d\u63a8\u7406\u52a0\u901f", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u867d\u7136\u80fd\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u83b7\u5f97\u9ad8\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u7531\u4e8e\u89e3\u7801\u65f6\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72", "method": "\u63d0\u51fa\u4ec5\u9700\u5355\u6b65\u6269\u6563\u8fc7\u7a0b\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\uff1b\u5f15\u5165\u5728\u7d27\u51d1\u7279\u5f81\u8868\u793a\u4e0a\u64cd\u4f5c\u7684\u5224\u522b\u5668\uff08\u800c\u975e\u539f\u59cb\u50cf\u7d20\uff09\uff0c\u5229\u7528\u7279\u5f81\u66f4\u597d\u6355\u6349\u9ad8\u7ea7\u7eb9\u7406\u548c\u7ed3\u6784\u7ec6\u8282\u7684\u7279\u70b9\u6765\u63d0\u5347\u91cd\u5efa\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u538b\u7f29\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u6700\u8fd1\u7684\u6269\u6563\u65b9\u6cd5\u5b9e\u73b0\u4e8646\u500d\u7684\u63a8\u7406\u52a0\u901f", "conclusion": "\u63d0\u51fa\u7684\u5355\u6b65\u6269\u6563\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u7279\u5f81\u7ea7\u5224\u522b\u5668\u4fdd\u6301\u4e86\u611f\u77e5\u8d28\u91cf"}}
{"id": "2602.01053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01053", "abs": "https://arxiv.org/abs/2602.01053", "authors": ["Hyesung Jeon", "Hyeongju Ha", "Jae-Joon Kim"], "title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents", "comment": "23 pages, 9 figures, 19 tables", "summary": "Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.", "AI": {"tldr": "LRAgent\uff1a\u9488\u5bf9\u591aLoRA\u667a\u80fd\u4f53\u7cfb\u7edf\u7684KV\u7f13\u5b58\u5171\u4eab\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7f13\u5b58\u4e3a\u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u548c\u9002\u914d\u5668\u7ec4\u4ef6\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u591aLoRA\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5c3d\u7ba1\u5171\u4eab\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f46\u6bcf\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u6784\u5efa\u548c\u5b58\u50a8\u76f8\u540c\u957f\u5de5\u5177\u589e\u5f3a\u8f68\u8ff9\u7684KV\u7f13\u5b58\uff0c\u5bfc\u81f4\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709KV\u7f13\u5b58\u5171\u4eab\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u8fd9\u79cd\u591aLoRA\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faLRAgent\u6846\u67b6\uff1a1\uff09\u5c06KV\u7f13\u5b58\u5206\u89e3\u4e3a\u6765\u81ea\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u548c\u6765\u81eaLoRA\u6743\u91cd\u7684\u9002\u914d\u5668\u4f9d\u8d56\u7ec4\u4ef6\uff1b2\uff09\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u5e76\u4ee5\u56fa\u6709\u4f4e\u79e9\u5f62\u5f0f\u5b58\u50a8\u9002\u914d\u5668\u7ec4\u4ef6\u6765\u51cf\u5c11\u5185\u5b58\u5f00\u9500\uff1b3\uff09\u901a\u8fc7\u5171\u4eab\u4f4e\u79e9\u7f13\u5b58\u548c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff1b4\uff09\u5f15\u5165Flash-LoRA-Attention\u5185\u6838\uff0c\u91cd\u65b0\u6392\u5e8f\u6ce8\u610f\u529b\u8ba1\u7b97\u4ee5\u907f\u514d\u5c06\u4f4e\u79e9\u7f13\u5b58\u7269\u5316\u5230\u5b8c\u6574\u7ef4\u5ea6\u3002", "result": "LRAgent\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u5168\u5171\u4eab\u7f13\u5b58\u7684\u541e\u5410\u91cf\u548c\u9996\u4ee4\u724c\u5ef6\u8fdf\u65f6\u95f4\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u63a5\u8fd1\u975e\u5171\u4eab\u7f13\u5b58\u57fa\u7ebf\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LRAgent\u4e3a\u591aLoRA\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684KV\u7f13\u5b58\u5171\u4eab\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u667a\u80fd\u4f53\u95f4\u7f13\u5b58\u5dee\u5f02\u4e3b\u8981\u7531\u9002\u914d\u5668\u8f93\u51fa\u4e3b\u5bfc\u7684\u89c2\u5bdf\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.01574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01574", "abs": "https://arxiv.org/abs/2602.01574", "authors": ["Haobo Wang", "Weiqi Luo", "Xiaojun Jia", "Xiaochun Cao"], "title": "SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models", "comment": null, "summary": "Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.", "AI": {"tldr": "SGHA-Attack\uff1a\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u7684\u5206\u5c42\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u53c2\u8003\u951a\u70b9\u548c\u4e2d\u95f4\u5c42\u8bed\u4e49\u5bf9\u9f50\uff0c\u63d0\u5347\u5bf9\u6297\u6837\u672c\u5728\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u95f4\u7684\u8fc1\u79fb\u653b\u51fb\u80fd\u529b", "motivation": "\u73b0\u6709\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u53c2\u8003\u76ee\u6807\uff0c\u8fc7\u5ea6\u62df\u5408\u4ee3\u7406\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u5f3a\u8c03\u6700\u7ec8\u5c42\u5bf9\u9f50\u800c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e2d\u95f4\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u5728\u5f02\u6784\u6a21\u578b\u95f4\u7684\u8fc1\u79fb\u6548\u679c\u4e0d\u4f73", "method": "1) \u901a\u8fc7\u51bb\u7ed3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u89c6\u89c9\u57fa\u7840\u53c2\u8003\u6c60\uff0c\u9009\u62e9Top-K\u8bed\u4e49\u76f8\u5173\u951a\u70b9\u5f62\u6210\u52a0\u6743\u6df7\u5408\u6307\u5bfc\uff1b2) \u5728\u7279\u5f81\u5c42\u6b21\u4e2d\u6ce8\u5165\u76ee\u6807\u8bed\u4e49\uff0c\u5728\u591a\u4e2a\u6df1\u5ea6\u5bf9\u9f50\u4e2d\u95f4\u89c6\u89c9\u8868\u793a\uff08\u5168\u5c40\u548c\u7a7a\u95f4\u7c92\u5ea6\uff09\uff1b3) \u5728\u5171\u4eab\u6f5c\u5728\u5b50\u7a7a\u95f4\u4e2d\u540c\u6b65\u4e2d\u95f4\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u63d0\u4f9b\u6700\u7ec8\u6295\u5f71\u524d\u7684\u8de8\u6a21\u6001\u76d1\u7763", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1a\u9ed1\u76d2\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSGHA-Attack\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u76ee\u6807\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u9884\u5904\u7406\u548c\u51c0\u5316\u9632\u5fa1\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "SGHA-Attack\u901a\u8fc7\u591a\u53c2\u8003\u951a\u70b9\u548c\u5206\u5c42\u8bed\u4e49\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\u5728\u5f02\u6784\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u95f4\u8fc1\u79fb\u6548\u679c\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8de8\u6a21\u578b\u653b\u51fb\u80fd\u529b"}}
{"id": "2602.01058", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01058", "abs": "https://arxiv.org/abs/2602.01058", "authors": ["Dylan Zhang", "Yufeng Xu", "Haojin Wang", "Qingzhi Chen", "Hao Peng"], "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning", "comment": null, "summary": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.", "AI": {"tldr": "PEAR\u662f\u4e00\u79cdSFT\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u91cd\u65b0\u52a0\u6743SFT\u635f\u5931\uff0c\u89e3\u51b3SFT\u4e0eRL\u8bad\u7ec3\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u540e\u7eedRL\u8bad\u7ec3\u6548\u679c", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0cSFT\u9636\u6bb5\u901a\u5e38\u5b64\u7acb\u4f18\u5316\u4ee5\u6700\u5927\u5316SFT\u6027\u80fd\uff0c\u4f46\u66f4\u5f3a\u7684SFT\u68c0\u67e5\u70b9\u5728\u540e\u7eed\u76f8\u540cRL\u8bad\u7ec3\u540e\u53ef\u80fd\u8868\u73b0\u66f4\u5dee\uff0c\u539f\u56e0\u662f\u79bb\u7ebfSFT\u6570\u636e\u5206\u5e03\u4e0e\u5728\u7ebfRL\u7b56\u7565\u5206\u5e03\u5b58\u5728\u4e0d\u5339\u914d", "method": "\u63d0\u51faPEAR\u65b9\u6cd5\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u91cd\u65b0\u52a0\u6743SFT\u635f\u5931\uff0c\u5305\u542btoken\u7ea7\u3001block\u7ea7\u548c\u5e8f\u5217\u7ea7\u4e09\u79cd\u53d8\u4f53\uff0c\u53ef\u589e\u5f3a\u6807\u51c6SFT\u76ee\u6807\uff0c\u6536\u96c6\u79bb\u7ebf\u6570\u636e\u6982\u7387\u540e\u8bad\u7ec3\u5f00\u9500\u5f88\u5c0f", "result": "\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u6e38\u620f\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9Qwen 2.5/3\u548cDeepSeek-distilled\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cPEAR\u76f8\u6bd4\u6807\u51c6SFT\u6301\u7eed\u63d0\u5347\u540eRL\u6027\u80fd\uff0c\u5728AIME2025\u4e0apass@8\u63d0\u5347\u8fbe14.6%", "conclusion": "PEAR\u662f\u8fc8\u5411\u66f4\u6574\u4f53LLM\u540e\u8bad\u7ec3\u7684\u6709\u6548\u6b65\u9aa4\uff0c\u901a\u8fc7\u8003\u8651\u4e0b\u6e38RL\u6765\u8bbe\u8ba1\u548c\u8bc4\u4f30SFT\uff0c\u800c\u4e0d\u662f\u5b64\u7acb\u4f18\u5316SFT"}}
{"id": "2602.01586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01586", "abs": "https://arxiv.org/abs/2602.01586", "authors": ["Wencan Cheng", "Gim Hee Lee"], "title": "HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation", "comment": "AAAI accepted", "summary": "3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684HandMCM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4fe1\u606f\u6ce8\u5165/\u8fc7\u6ee4\u548c\u5bf9\u5e94\u5173\u7cfb\u5efa\u6a21\u6a21\u5757\uff0c\u6709\u6548\u5b66\u4e60\u5404\u79cd\u906e\u6321\u573a\u666f\u4e0b\u7684\u5173\u952e\u70b9\u52a8\u6001\u8fd0\u52a8\u5b66\u62d3\u6251\uff0c\u663e\u8457\u63d0\u53473D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6", "motivation": "3D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u5bf9\u4e8e\u589e\u5f3a\u73b0\u5b9e\u7b49\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u624b\u90e8\u81ea\u906e\u6321\u548c\u4e0e\u7269\u4f53\u4ea4\u4e92\u5bfc\u81f4\u7684\u906e\u6321\uff0c\u8be5\u4efb\u52a1\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHandMCM\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5f3a\u5927\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(Mamba)\u3002\u901a\u8fc7\u8bbe\u8ba1\u5c40\u90e8\u4fe1\u606f\u6ce8\u5165/\u8fc7\u6ee4\u6a21\u5757\u548c\u5bf9\u5e94\u5173\u7cfb\u5efa\u6a21\u6a21\u5757\uff0c\u4f7f\u5bf9\u5e94\u5173\u7cfbMamba\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5173\u952e\u70b9\u5728\u4e0d\u540c\u906e\u6321\u573a\u666f\u4e0b\u7684\u52a8\u6001\u8fd0\u52a8\u5b66\u62d3\u6251\u3002\u540c\u65f6\u96c6\u6210\u591a\u6a21\u6001\u56fe\u50cf\u7279\u5f81\u589e\u5f3a\u8f93\u5165\u9c81\u68d2\u6027\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4e25\u91cd\u906e\u6321\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u53473D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "HandMCM\u901a\u8fc7\u521b\u65b0\u7684Mamba\u67b6\u6784\u548c\u5c40\u90e8\u4fe1\u606f\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u906e\u6321\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u51c6\u786e\u53ef\u9760\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01083", "abs": "https://arxiv.org/abs/2602.01083", "authors": ["Adir Dayan", "Yam Eitan", "Haggai Maron"], "title": "On the Expressive Power of Permutation-Equivariant Weight-Space Networks", "comment": null, "summary": "Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u6743\u91cd\u7a7a\u95f4\u7f51\u7edc\u7684\u7cfb\u7edf\u6027\u8868\u8fbe\u80fd\u529b\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u4e3b\u6d41\u7f6e\u6362\u7b49\u53d8\u7f51\u7edc\u5177\u6709\u7b49\u6548\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u6743\u91cd\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u666e\u9002\u6027\u3002", "motivation": "\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u7814\u7a76\u76f4\u63a5\u64cd\u4f5c\u5176\u4ed6\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7684\u67b6\u6784\uff0c\u968f\u7740\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u666e\u53ca\uff0c\u8fd9\u7c7b\u7f51\u7edc\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u73b0\u6709SOTA\u7f51\u7edc\u4f9d\u8d56\u7f6e\u6362\u7b49\u53d8\u8bbe\u8ba1\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fd9\u53ef\u80fd\u5f71\u54cd\u8868\u8fbe\u80fd\u529b\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u3002\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u540c\u65f6\u6d89\u53ca\u6743\u91cd\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u6620\u5c04\uff0c\u4f7f\u5f97\u8868\u8fbe\u80fd\u529b\u5206\u6790\u7279\u522b\u5fae\u5999\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u7684\u8868\u5f81\u3002", "method": "\u5f00\u53d1\u4e86\u6743\u91cd\u7a7a\u95f4\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u7406\u8bba\uff0c\u9996\u5148\u8bc1\u660e\u4e86\u6240\u6709\u4e3b\u6d41\u7f6e\u6362\u7b49\u53d8\u7f51\u7edc\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u662f\u7b49\u6548\u7684\uff0c\u7136\u540e\u5728\u8f93\u5165\u6743\u91cd\u7684\u6e29\u548c\u81ea\u7136\u5047\u8bbe\u4e0b\uff0c\u5efa\u7acb\u4e86\u6743\u91cd\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u8bbe\u7f6e\u7684\u666e\u9002\u6027\uff0c\u5e76\u523b\u753b\u4e86\u666e\u9002\u6027\u4e0d\u518d\u6210\u7acb\u7684\u8fb9\u7f18\u60c5\u51b5\u3002", "result": "\u8bc1\u660e\u4e86\u7f6e\u6362\u7b49\u53d8\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u7b49\u6548\u6027\uff0c\u5efa\u7acb\u4e86\u6743\u91cd\u7a7a\u95f4\u548c\u51fd\u6570\u7a7a\u95f4\u7684\u666e\u9002\u6027\u7406\u8bba\uff0c\u5e76\u8bc6\u522b\u4e86\u666e\u9002\u6027\u5931\u6548\u7684\u8fb9\u7f18\u60c5\u51b5\uff0c\u4e3a\u6743\u91cd\u7a7a\u95f4\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6743\u91cd\u7a7a\u95f4\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7406\u8bba\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u7f6e\u6362\u7b49\u53d8\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u7b49\u6548\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u666e\u9002\u6027\u6761\u4ef6\uff0c\u4e3a\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.01591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01591", "abs": "https://arxiv.org/abs/2602.01591", "authors": ["Zhixiong Yue", "Zixuan Ni", "Feiyang Ye", "Jinshan Zhang", "Sheng Shen", "Zhenpeng Mi"], "title": "Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages", "comment": null, "summary": "Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.", "AI": {"tldr": "\u63d0\u51faTAFS-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u6e29\u5ea6\u9000\u706b\u91c7\u6837\u548c\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u3001\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u5c11\u6b65\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6d41\u5339\u914d\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u53bb\u566a\u6b65\u9aa4\uff0c\u4e14\u9762\u4e34\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u3001\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTAFS-GRPO\u6846\u67b6\uff1a1) \u6e29\u5ea6\u9000\u706b\u5c11\u6b65\u91c7\u6837\uff1a\u5728\u5355\u6b65\u91c7\u6837\u7ed3\u679c\u4e0a\u8fed\u4ee3\u6ce8\u5165\u81ea\u9002\u5e94\u65f6\u5e8f\u566a\u58f0\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5f15\u5165\u968f\u673a\u6027\uff1b2) \u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff1a\u7ed3\u5408\u6b65\u611f\u77e5\u4f18\u52bf\u96c6\u6210\u673a\u5236\uff0c\u65e0\u9700\u5956\u52b1\u51fd\u6570\u53ef\u5fae\uff0c\u63d0\u4f9b\u5bc6\u96c6\u4e14\u6b65\u7279\u5b9a\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTAFS-GRPO\u5728\u5c11\u6b65\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u50cf\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u5ea6\u3002", "conclusion": "TAFS-GRPO\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u5c11\u6b65\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2602.01105", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01105", "abs": "https://arxiv.org/abs/2602.01105", "authors": ["Zixiao Wang", "Yifei Shen", "Huishuai Zhang"], "title": "OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\\ell_{\\infty}$ Implicit Biases", "comment": "23 pages", "summary": "Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \\nameA{} (\\fullname{}), which combines spectral control from orthogonalized update directions with $\\ell_\\infty$-style coordinate control from sign updates. \\nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\\ell_\\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \\nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aA\u7684\u65b0\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u8c31\u63a7\u5236\u548c\u5750\u6807\u63a7\u5236\uff0c\u901a\u8fc7\u6b63\u4ea4\u5316\u52a8\u91cf\u65b9\u5411\u548c\u7b26\u53f7\u66f4\u65b0\uff0c\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u5339\u914d\u6216\u4f18\u4e8eAdamW\u548cMuon\uff0c\u540c\u65f6\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u8bb8\u591a\u4f18\u5316\u5668\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8303\u6570\u8bf1\u5bfc\u51e0\u4f55\u4e0b\u7684\u6700\u901f\u4e0b\u964d\u65b9\u6cd5\uff0c\u4ece\u800c\u7ee7\u627f\u76f8\u5e94\u7684\u9690\u5f0f\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u8c31\u63a7\u5236\uff08\u6765\u81ea\u6b63\u4ea4\u5316\u66f4\u65b0\u65b9\u5411\uff09\u548c\u5750\u6807\u63a7\u5236\uff08\u6765\u81ea\u7b26\u53f7\u66f4\u65b0\uff09\uff0c\u4ee5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8003\u8651\u8fd9\u4e24\u79cd\u51e0\u4f55\u7ea6\u675f\u3002", "method": "\u63d0\u51faA\u4f18\u5316\u5668\uff1a1\uff09\u5f62\u6210Lion\u98ce\u683c\u7684\u52a8\u91cf\u65b9\u5411\uff1b2\uff09\u901a\u8fc7\u5c11\u91cfNewton-Schulz\u8fed\u4ee3\u8fdb\u884c\u8fd1\u4f3c\u6b63\u4ea4\u5316\uff1b3\uff09\u5e94\u7528\u9010\u5143\u7d20\u7b26\u53f7\u64cd\u4f5c\u3002\u8fd9\u63d0\u4f9b\u4e86\u5bf9\u8c31\u7ea6\u675f\u548c\u2113\u221e\u7ea6\u675f\u96c6\u4ea4\u96c6\uff08\u7f29\u653eHadamard-like\u96c6\u5408\uff09\u4e0a\u6700\u5927\u6b65\u957f\u7684\u6709\u6548\u8fd1\u4f3c\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bad\u7ec3\u4e2d\uff0c\u5305\u62ecGPT-2\u548cLlama\u9884\u8bad\u7ec3\u3001SiT\u56fe\u50cf\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0cA\u4f18\u5316\u5668\u5728\u53ef\u6bd4\u8f83\u7684\u8c03\u4f18\u4e0b\u5339\u914d\u6216\u4f18\u4e8eAdamW\u548cMuon\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u52a8\u91cf\u7ea7\u522b\u7684\u4f18\u5316\u5668\u72b6\u6001\u3002\u5b83\u8fd8\u80fd\u7f13\u89e3AdamW\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\u5fae\u8c03\u65f6\u7684\u4f18\u5316\u5668\u4e0d\u5339\u914d\u95ee\u9898\u3002", "conclusion": "A\u4f18\u5316\u5668\u6210\u529f\u7ed3\u5408\u4e86\u8c31\u63a7\u5236\u548c\u5750\u6807\u63a7\u5236\uff0c\u901a\u8fc7\u6b63\u4ea4\u5316\u548c\u7b26\u53f7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2602.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01593", "abs": "https://arxiv.org/abs/2602.01593", "authors": ["Wenzhuo Zhao", "Keren Fu", "Jiahao He", "Xiaohong Liu", "Qijun Zhao", "Guangtao Zhai"], "title": "Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework", "comment": null, "summary": "Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the \"task-specific\" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.", "AI": {"tldr": "\u63d0\u51faSamba\u548cSamba+\u6a21\u578b\uff0c\u57fa\u4e8eMamba\u67b6\u6784\u89e3\u51b3\u591a\u79cd\u663e\u8457\u6027\u68c0\u6d4b\u4efb\u52a1\uff0c\u572822\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u68c0\u6d4b\u6a21\u578b\u53d7\u9650\u4e8eCNN\u7684\u6709\u9650\u611f\u53d7\u91ce\u548cTransformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9700\u8981\u5e73\u8861\u5168\u5c40\u611f\u53d7\u91ce\u548c\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u7eafMamba\u67b6\u6784Samba\uff0c\u5305\u542b\u663e\u8457\u6027\u5f15\u5bfc\u7684Mamba\u5757\uff08SGMB\uff09\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0a\u91c7\u6837\uff08CAU\uff09\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faSamba+\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u5b9e\u73b0\u7edf\u4e00\u6a21\u578b\uff0c\u5305\u542bHGA\u6a21\u5757\u548cMACL\u7b56\u7565", "result": "Samba\u5728\u516d\u79cd\u663e\u8457\u6027\u68c0\u6d4b\u4efb\u52a1\u768422\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff1bSamba+\u4f7f\u7528\u5355\u4e00\u8bad\u7ec3\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u53d6\u5f97\u66f4\u4f18\u7ed3\u679c", "conclusion": "Mamba\u67b6\u6784\u5728\u663e\u8457\u6027\u68c0\u6d4b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u63d0\u51fa\u7684Samba\u6846\u67b6\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7edf\u4e00\u7684\u663e\u8457\u6027\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.01113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01113", "abs": "https://arxiv.org/abs/2602.01113", "authors": ["Wenjie Liang", "Ranhui Yan", "Jia Cai", "You-Gan Wang"], "title": "Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems", "comment": null, "summary": "Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \\emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSEGIA\u5355\u8fb9\u56fe\u6ce8\u5165\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u5de5\u4e1aGNN\u7cfb\u7edf\u4e2d\u901a\u8fc7\u6ce8\u5165\u5355\u8fb9\u8fde\u63a5\u7684\u4f2a\u9020\u8282\u70b9\u6765\u5f71\u54cd\u4e0b\u6e38\u51b3\u7b56\uff0c\u653b\u51fb\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u9ad825%\u4ee5\u4e0a\u3002", "motivation": "\u5de5\u4e1aGNN\u76d1\u63a7\u7cfb\u7edf\uff08\u5982IIoT\u8bbe\u5907\u56fe\u3001\u7535\u7f51\u62d3\u6251\u6a21\u578b\uff09\u9762\u4e34\u5b89\u5168\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u6ce8\u5165\u4f2a\u9020\u8282\u70b9\uff08\u5982\u865a\u5047\u4f20\u611f\u5668\u3001\u865a\u62df\u7aef\u70b9\uff09\u6765\u504f\u7f6e\u4e0b\u6e38\u51b3\u7b56\uff0c\u540c\u65f6\u89c4\u907f\u57fa\u4e8e\u62d3\u6251\u548c\u540c\u8d28\u6027\u7684\u51c0\u5316\u673a\u5236\u3002", "method": "\u63d0\u51faSEGIA\uff08\u5355\u8fb9\u56fe\u6ce8\u5165\u653b\u51fb\uff09\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1\uff09\u526a\u679dSGC\u4ee3\u7406\u6a21\u578b\uff1b2\uff09\u591a\u8df3\u90bb\u57df\u91c7\u6837\uff1b3\uff09\u57fa\u4e8e\u53cd\u5411\u56fe\u5377\u79ef\u7684\u7279\u5f81\u5408\u6210\uff1b4\uff09\u76f8\u4f3c\u6027\u6b63\u5219\u5316\u76ee\u6807\u4ee5\u4fdd\u6301\u5c40\u90e8\u540c\u8d28\u6027\u5e76\u89c4\u907f\u8fb9\u526a\u679d\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u663e\u8457\u66f4\u5c0f\u7684\u8fb9\u9884\u7b97\u4e0b\uff0cSEGIA\u7684\u653b\u51fb\u6210\u529f\u7387\u6bd4\u4ee3\u8868\u6027\u57fa\u7ebf\u81f3\u5c11\u9ad825%\uff0c\u8868\u660e\u5de5\u4e1aGNN\u90e8\u7f72\u5b58\u5728\u7cfb\u7edf\u7ea7\u98ce\u9669\u3002", "conclusion": "\u5de5\u4e1aGNN\u90e8\u7f72\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u7684\u51c6\u5165\u9a8c\u8bc1\u548c\u90bb\u57df\u4e00\u81f4\u6027\u76d1\u63a7\u6765\u9632\u5fa1\u6b64\u7c7b\u8282\u70b9\u6ce8\u5165\u653b\u51fb\u3002"}}
{"id": "2602.01594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01594", "abs": "https://arxiv.org/abs/2602.01594", "authors": ["Wenzhuo Liu", "Qiannan Guo", "Zhen Wang", "Wenshuo Wang", "Lei Yang", "Yicheng Qiao", "Lening Wang", "Zhiwei Li", "Chen Lv", "Shanghang Zhang", "Junqiang Xi", "Huaping Liu"], "title": "UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception", "comment": null, "summary": "Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.", "AI": {"tldr": "\u63d0\u51faUV-M3TL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7a7a\u95f4\u901a\u9053\u591a\u6a21\u6001\u5d4c\u5165\u548c\u81ea\u9002\u5e94\u7279\u5f81\u89e3\u8026\u591a\u4efb\u52a1\u635f\u5931\uff0c\u540c\u65f6\u8bc6\u522b\u9a7e\u9a76\u5458\u884c\u4e3a\u3001\u60c5\u7eea\u3001\u8f66\u8f86\u884c\u4e3a\u548c\u4ea4\u901a\u73af\u5883\uff0c\u7f13\u89e3\u4efb\u52a1\u95f4\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u9700\u8981\u540c\u65f6\u7406\u89e3\u9a7e\u9a76\u5458\u884c\u4e3a\u548c\u611f\u77e5\u5bfc\u822a\u73af\u5883\uff0c\u4f46\u8054\u5408\u5b66\u4e60\u8fd9\u4e9b\u5f02\u8d28\u4efb\u52a1\u4f1a\u5bfc\u81f4\u4efb\u52a1\u95f4\u8d1f\u8fc1\u79fb\uff0c\u635f\u5bb3\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u591a\u529f\u80fd\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u53cc\u5206\u652f\u7a7a\u95f4\u901a\u9053\u591a\u6a21\u6001\u5d4c\u5165\uff08DB-SCME\uff09\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff1b\u81ea\u9002\u5e94\u7279\u5f81\u89e3\u8026\u591a\u4efb\u52a1\u635f\u5931\uff08AFD-Loss\uff09\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u548c\u7279\u5f81\u89e3\u8026\u7ea6\u675f\u7684\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u3002", "result": "\u5728AIDE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u56db\u4e2a\u4efb\u52a1\u7684SOTA\u6027\u80fd\uff1b\u5728\u591a\u4e2a\u516c\u5f00\u591a\u4efb\u52a1\u611f\u77e5\u57fa\u51c6\uff08BDD100K\u3001CityScapes\u3001NYUD-v2\u3001PASCAL-Context\uff09\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "UV-M3TL\u6846\u67b6\u80fd\u6709\u6548\u7f13\u89e3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5728\u9a7e\u9a76\u5458\u884c\u4e3a\u7406\u89e3\u548c\u73af\u5883\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2602.01120", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01120", "abs": "https://arxiv.org/abs/2602.01120", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Tianyi Zeng", "Xiao-Yong Wei", "Qing Li"], "title": "MarkovScale: Towards Optimal Sequential Scaling at Inference Time", "comment": "12 pages", "summary": "Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.", "AI": {"tldr": "\u63d0\u51faMarkovScale\u6846\u67b6\uff0c\u5c06\u987a\u5e8f\u7f29\u653e\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u83b7\u5f97\u7406\u8bba\u6700\u4f18\u8fb9\u754c\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u987a\u5e8f\u7f29\u653e\u662f\u91cd\u8981\u7684\u63a8\u7406\u65f6\u7f29\u653e\u8303\u5f0f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u4e14\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6709\u9650\u4e14\u96be\u4ee5\u7406\u89e3\u6700\u4f18\u8fb9\u754c\u3002", "method": "\u5c06\u987a\u5e8f\u7f29\u653e\u5efa\u6a21\u4e3a\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\uff0c\u786e\u5b9a\u7cbe\u5ea6\u63d0\u5347\u6761\u4ef6\u548c\u7406\u8bba\u6027\u80fd\u8fb9\u754c\uff0c\u5e76\u5b9e\u73b0MarkovScale\u7cfb\u7edf\u3002", "result": "\u57283\u4e2a\u9aa8\u5e72LLM\u30015\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c20+\u914d\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMarkovScale\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5e76\u884c\u548c\u987a\u5e8f\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "MarkovScale\u4e3a\u987a\u5e8f\u7f29\u653e\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u6700\u4f18\u8fb9\u754c\uff0c\u662f\u5b9e\u73b0LLM\u6700\u4f18\u4e14\u8d44\u6e90\u9ad8\u6548\u63a8\u7406\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.01609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01609", "abs": "https://arxiv.org/abs/2602.01609", "authors": ["Junqing Lin", "Xingyu Zheng", "Pei Cheng", "Bin Fu", "Jingwei Sun", "Guangzhong Sun"], "title": "Token Pruning for In-Context Generation in Diffusion Transformers", "comment": "20 pages", "summary": "In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.", "AI": {"tldr": "ToPi\u662f\u4e00\u4e2a\u9488\u5bf9DiTs\u4e2d\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\u7684\u8bad\u7ec3\u514d\u8d39\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u6821\u51c6\u9a71\u52a8\u7684\u654f\u611f\u6027\u5206\u6790\u8bc6\u522b\u5173\u952e\u6ce8\u610f\u529b\u5c42\uff0c\u9009\u62e9\u6027\u526a\u679d\u4e0a\u4e0b\u6587\u4ee4\u724c\uff0c\u5b9e\u73b0\u8d85\u8fc730%\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u4e2d\uff0c\u8f93\u5165\u62fc\u63a5\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u5927\u5e45\u589e\u52a0\uff0c\u9020\u6210\u663e\u8457\u8ba1\u7b97\u74f6\u9888\u3002\u73b0\u6709\u7684\u4ee4\u724c\u51cf\u5c11\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\uff0c\u91c7\u7528\u7edf\u4e00\u51cf\u5c11\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u53c2\u8003\u4e0a\u4e0b\u6587\u548c\u76ee\u6807\u6f5c\u5728\u8868\u793a\u5728\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u529f\u80fd\u7ef4\u5ea6\u4e0a\u7684\u89d2\u8272\u4e0d\u5bf9\u79f0\u6027\u3002", "method": "ToPi\u91c7\u7528\u79bb\u7ebf\u6821\u51c6\u9a71\u52a8\u7684\u654f\u611f\u6027\u5206\u6790\u8bc6\u522b\u5173\u952e\u6ce8\u610f\u529b\u5c42\uff0c\u4f5c\u4e3a\u5197\u4f59\u4f30\u8ba1\u7684\u7a33\u5065\u4ee3\u7406\u3002\u5229\u7528\u8fd9\u4e9b\u5c42\u63a8\u5bfc\u65b0\u9896\u7684\u5f71\u54cd\u5ea6\u91cf\u6765\u91cf\u5316\u6bcf\u4e2a\u4e0a\u4e0b\u6587\u4ee4\u724c\u7684\u8d21\u732e\uff0c\u8fdb\u884c\u9009\u62e9\u6027\u526a\u679d\uff0c\u5e76\u7ed3\u5408\u9002\u5e94\u6269\u6563\u8f68\u8ff9\u6f14\u5316\u7684\u65f6\u95f4\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cToPi\u5728\u590d\u6742\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc730%\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "ToPi\u4e3aDiTs\u4e2d\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u89d2\u8272\u4e0d\u5bf9\u79f0\u6027\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.01124", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01124", "abs": "https://arxiv.org/abs/2602.01124", "authors": ["Md Abrar Jahin", "Taufikur Rahman Fuad", "Jay Pujara", "Craig Knoblock"], "title": "ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs", "comment": null, "summary": "Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \\cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\\%$ Macro-F1 and $2.4\\%$ Micro-F1 while achieving $3-10\\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $\u03c1< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\\%$ sparsity.", "AI": {"tldr": "ChronoSpike\uff1a\u4e00\u79cd\u81ea\u9002\u5e94\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684LIF\u795e\u7ecf\u5143\u3001\u591a\u5934\u6ce8\u610f\u529b\u7a7a\u95f4\u805a\u5408\u548c\u8f7b\u91cf\u7ea7Transformer\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u7ebf\u6027\u5185\u5b58\u590d\u6742\u5ea6\u7684\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u57fa\u672c\u6743\u8861\uff1a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u590d\u6742\u5ea6\u9ad8\uff08O(T\u00b2)\uff09\uff0c\u5faa\u73af\u67b6\u6784\u5b58\u5728\u68af\u5ea6\u95ee\u9898\u548c\u5bc6\u96c6\u72b6\u6001\u5b58\u50a8\u95ee\u9898\u3002\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5177\u6709\u4e8b\u4ef6\u9a71\u52a8\u6548\u7387\uff0c\u4f46\u53d7\u9650\u4e8e\u987a\u5e8f\u4f20\u64ad\u3001\u4e8c\u8fdb\u5236\u4fe1\u606f\u4e22\u5931\u548c\u5c40\u90e8\u805a\u5408\u65e0\u6cd5\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faChronoSpike\u81ea\u9002\u5e94\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff1a1\uff09\u96c6\u6210\u53ef\u5b66\u4e60\u7684LIF\u795e\u7ecf\u5143\uff0c\u5177\u6709\u6bcf\u901a\u9053\u819c\u7535\u4f4d\u52a8\u6001\uff1b2\uff09\u5728\u8fde\u7eed\u7279\u5f81\u4e0a\u8fdb\u884c\u591a\u5934\u6ce8\u610f\u529b\u7a7a\u95f4\u805a\u5408\uff1b3\uff09\u8f7b\u91cf\u7ea7Transformer\u65f6\u95f4\u7f16\u7801\u5668\u3002\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5c40\u90e8\u5efa\u6a21\u548c\u957f\u7a0b\u4f9d\u8d56\u6355\u83b7\uff0c\u5185\u5b58\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027O(T\u00b7d)\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChronoSpike\u572812\u4e2a\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u63d0\u5347\u4e862.0% Macro-F1\u548c2.4% Micro-F1\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u5faa\u73af\u65b9\u6cd5\u5feb3-10\u500d\uff0c\u53c2\u6570\u9884\u7b97\u6052\u5b9a\u4e3a105K\uff08\u4e0e\u56fe\u5927\u5c0f\u65e0\u5173\uff09\u3002\u7406\u8bba\u4fdd\u8bc1\u5305\u62ec\u819c\u7535\u4f4d\u6709\u754c\u6027\u3001\u6536\u7f29\u56e0\u5b50\u03c1<1\u4e0b\u7684\u68af\u5ea6\u6d41\u7a33\u5b9a\u6027\u548cBIBO\u7a33\u5b9a\u6027\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u663e\u793a\u5f02\u8d28\u65f6\u95f4\u611f\u53d7\u91ce\u548c\u5b66\u4e60\u7684\u9996\u56e0\u6548\u5e94\uff0c\u7a00\u758f\u5ea6\u8fbe83-88%\u3002", "conclusion": "ChronoSpike\u901a\u8fc7\u7ed3\u5408\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u4e8b\u4ef6\u9a71\u52a8\u6548\u7387\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u8868\u8fbe\u529b\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u5185\u5b58\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2602.01623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01623", "abs": "https://arxiv.org/abs/2602.01623", "authors": ["Susan Liang", "Chao Huang", "Filippos Bellos", "Yolo Yunlong Tang", "Qianxiang Shen", "Jing Bi", "Luchuan Song", "Zeliang Zhang", "Jason Corso", "Chenliang Xu"], "title": "Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?", "comment": null, "summary": "State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.", "AI": {"tldr": "Omni-Judge\u7814\u7a76\u8bc4\u4f30\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u4f5c\u4e3a\u6587\u672c\u6761\u4ef6\u97f3\u9891\u89c6\u9891\u751f\u6210\u7684\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u5224\u8005\uff0c\u5728\u8bed\u4e49\u5bf9\u9f50\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u5e27\u7387\u611f\u77e5\u6307\u6807\u4e0a\u53d7\u9650\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u5982Sora 2\u548cVeo 3\uff09\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u548c\u540c\u6b65\u97f3\u9891\uff0c\u4f46\u8bc4\u4f30\u8fd9\u79cd\u4e09\u6a21\u6001\u8f93\u51fa\u4ecd\u5177\u6311\u6218\u6027\u3002\u4eba\u5de5\u8bc4\u4f30\u53ef\u9760\u4f46\u6210\u672c\u9ad8\u96be\u6269\u5c55\uff0c\u4f20\u7edf\u81ea\u52a8\u6307\u6807\uff08\u5982FVD\u3001CLAP\u3001ViCLIP\uff09\u4ec5\u5173\u6ce8\u5b64\u7acb\u6a21\u6001\u5bf9\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u63d0\u793a\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002", "method": "\u5f15\u5165Omni-Judge\u7814\u7a76\uff0c\u8bc4\u4f30\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6587\u672c\u6761\u4ef6\u97f3\u9891\u89c6\u9891\u751f\u6210\u8bc4\u5224\u8005\u7684\u80fd\u529b\u3002\u5168\u6a21\u6001LLM\u80fd\u81ea\u7136\u5904\u7406\u97f3\u9891\u3001\u89c6\u9891\u548c\u6587\u672c\uff0c\u652f\u6301\u4e30\u5bcc\u63a8\u7406\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u601d\u7ef4\u94fe\u53cd\u9988\u3002", "result": "\u5728\u4e5d\u4e2a\u611f\u77e5\u548c\u5bf9\u9f50\u6307\u6807\u4e0a\uff0cOmni-Judge\u8fbe\u5230\u4e0e\u4f20\u7edf\u6307\u6807\u76f8\u5f53\u7684\u5173\u8054\u6027\uff0c\u5728\u8bed\u4e49\u8981\u6c42\u9ad8\u7684\u4efb\u52a1\uff08\u5982\u97f3\u9891-\u6587\u672c\u5bf9\u9f50\u3001\u89c6\u9891-\u6587\u672c\u5bf9\u9f50\u3001\u97f3\u9891-\u89c6\u9891-\u6587\u672c\u4e00\u81f4\u6027\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4f46\u5728\u9ad8\u5e27\u7387\u611f\u77e5\u6307\u6807\uff08\u89c6\u9891\u8d28\u91cf\u548c\u97f3\u9891-\u89c6\u9891\u540c\u6b65\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u65f6\u95f4\u5206\u8fa8\u7387\u3002", "conclusion": "\u5168\u6a21\u6001LLM\u4f5c\u4e3a\u591a\u6a21\u6001\u751f\u6210\u7edf\u4e00\u8bc4\u4f30\u5668\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5f53\u524d\u5c40\u9650\u6027\u3002Omni-Judge\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u80fd\u66b4\u9732\u8bed\u4e49\u6216\u7269\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u652f\u6301\u57fa\u4e8e\u53cd\u9988\u7684\u4f18\u5316\u7b49\u5b9e\u9645\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2602.01126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01126", "abs": "https://arxiv.org/abs/2602.01126", "authors": ["Mengsha Kou", "Xiaoyu Xia", "Ziqi Wang", "Ibrahim Khalil", "Runkun Luo", "Jingwen Zhou", "Minhui Xue"], "title": "WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity", "comment": "12 pages", "summary": "Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.", "AI": {"tldr": "WinFLoRA\uff1a\u4e00\u79cd\u9690\u79c1\u5f02\u6784\u7684\u8054\u90a6LoRA\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u7684\u805a\u5408\u6743\u91cd\u4f5c\u4e3a\u6fc0\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u4e0d\u540c\u5ba2\u6237\u7aef\u5dee\u5206\u9690\u79c1\u566a\u58f0\u6c34\u5e73\u5dee\u5f02\u5bfc\u81f4\u7684\u9690\u79c1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u9690\u79c1\u654f\u611f\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u4e2d\uff0c\u4e0d\u540c\u5ba2\u6237\u7aef\u6ce8\u5165\u4e0d\u540c\u6c34\u5e73\u7684\u5dee\u5206\u9690\u79c1\u566a\u58f0\uff0c\u5bfc\u81f4\u9690\u79c1\u5f02\u8d28\u6027\u3002\u8fd9\u79cd\u5f02\u8d28\u6027\u4f7f\u4e2a\u4f53\u6fc0\u52b1\u4e0e\u5168\u5c40\u6027\u80fd\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u9002\u5e94\u5ba2\u6237\u7aef\u5f02\u6784\u9690\u79c1\u9700\u6c42\u5e76\u63d0\u5347\u5168\u5c40\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "WinFLoRA\u91c7\u7528\u566a\u58f0\u611f\u77e5\u7684\u805a\u5408\u6743\u91cd\u6fc0\u52b1\u673a\u5236\uff1a1\uff09\u57fa\u4e8e\u4e0a\u4f20\u7684LoRA\u9002\u914d\u5668\u4f30\u8ba1\u5ba2\u6237\u7aef\u566a\u58f0\u6c34\u5e73\uff1b2\uff09\u4e3a\u4f4e\u566a\u58f0\u8d21\u732e\u5206\u914d\u66f4\u5927\u6743\u91cd\uff0c\u4f7f\u5176\u5bf9\u5168\u5c40\u6a21\u578b\u6709\u66f4\u5927\u5f71\u54cd\u529b\uff1b3\uff09\u901a\u8fc7\u52a0\u6743\u805a\u5408\u63d0\u5347\u5168\u5c40\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6ee1\u8db3\u5ba2\u6237\u7aef\u5f02\u6784\u9690\u79c1\u9700\u6c42\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cWinFLoRA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u51c6\u65b9\u6cd5\uff1a1\uff09\u5168\u5c40\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe52.58%\uff1b2\uff09\u5ba2\u6237\u7aef\u6548\u7528\u63d0\u5347\u9ad8\u8fbe2.56\u500d\uff1b3\uff09\u65e0\u9700\u7b2c\u4e09\u65b9\u53c2\u4e0e\u5373\u53ef\u5b9e\u73b0\u9690\u79c1\u5f02\u8d28\u6027\u4e0b\u7684\u6fc0\u52b1\u5bf9\u9f50\u3002", "conclusion": "WinFLoRA\u6210\u529f\u89e3\u51b3\u4e86\u9690\u79c1\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6fc0\u52b1\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u7684\u6743\u91cd\u5206\u914d\u673a\u5236\uff0c\u5728\u6ee1\u8db3\u5ba2\u6237\u7aef\u4e0d\u540c\u9690\u79c1\u9700\u6c42\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5168\u5c40\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u667a\u80fdWeb\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01624", "abs": "https://arxiv.org/abs/2602.01624", "authors": ["Minh-Quan Le", "Gaurav Mittal", "Cheng Zhao", "David Gu", "Dimitris Samaras", "Mei Chen"], "title": "PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards", "comment": null, "summary": "Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.", "AI": {"tldr": "PISCES\uff1a\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u5956\u52b1\u7684\u65e0\u6807\u6ce8\u540e\u8bad\u7ec3\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u8bed\u4e49\u5bf9\u9f50", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5956\u52b1\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u504f\u597d\u6807\u6ce8\uff0c\u8981\u4e48\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u672a\u5bf9\u9f50\u5d4c\u5165\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6709\u9650\u6216\u76d1\u7763\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u65e0\u6807\u6ce8\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faPISCES\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u5956\u52b1\u6a21\u5757\uff1a1\uff09\u5206\u5e03\u7ea7OT\u5bf9\u9f50\u8d28\u91cf\u5956\u52b1\uff0c\u6355\u6349\u6574\u4f53\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff1b2\uff09\u79bb\u6563\u4ee4\u724c\u7ea7OT\u5bf9\u9f50\u8bed\u4e49\u5956\u52b1\uff0c\u5f3a\u5316\u6587\u672c\u548c\u89c6\u9891\u4ee4\u724c\u4e4b\u95f4\u7684\u8bed\u4e49\u65f6\u7a7a\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u77ed\u89c6\u9891\u548c\u957f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0cPISCES\u5728VBench\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u6807\u6ce8\u548c\u65e0\u6807\u6ce8\u65b9\u6cd5\uff0c\u4eba\u7c7b\u504f\u597d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u53cc\u91cdOT\u5bf9\u9f50\u5956\u52b1\u6a21\u5757\u517c\u5bb9\u591a\u79cd\u4f18\u5316\u8303\u5f0f\u3002", "conclusion": "PISCES\u9996\u6b21\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89c6\u89d2\u6539\u8fdb\u751f\u6210\u5f0f\u540e\u8bad\u7ec3\u4e2d\u7684\u65e0\u6807\u6ce8\u5956\u52b1\u76d1\u7763\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u5347\u65b9\u6848\u3002"}}
{"id": "2602.01128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01128", "abs": "https://arxiv.org/abs/2602.01128", "authors": ["Mete Erdogan"], "title": "Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models", "comment": null, "summary": "Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.", "AI": {"tldr": "TS-DPO\u5728\u5207\u7ebf\u7a7a\u95f4\u4e2d\u6267\u884cDPO\uff0c\u5b66\u4e60\u6bcf\u4e2a\u76ee\u6807\u7684\u66f4\u65b0\u65b9\u5411\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7ebf\u6027\u7ec4\u5408\u4ee5\u5b9e\u73b0\u53ef\u63a7\u7684\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u5c06\u53cd\u9988\u538b\u7f29\u4e3a\u5355\u4e00\u6807\u91cf\u5956\u52b1\uff0c\u56fa\u5b9a\u4e86\u76ee\u6807\u95f4\u7684\u5e73\u8861\uff0c\u65e0\u6cd5\u904d\u5386\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u9650\u5236\u4e86LLMs\u5728\u591a\u4e2a\u4eba\u7c7b\u504f\u597d\u7ef4\u5ea6\uff08\u5982\u5e2e\u52a9\u6027\u3001\u5b89\u5168\u6027\u3001\u5197\u957f\u5ea6\uff09\u4e0a\u7684\u53ef\u63a7\u5bf9\u9f50", "method": "\u57fa\u4e8e\u5207\u7ebf\u7a7a\u95f4\u7406\u8bba\uff0c\u5728\u5c40\u90e8\u7ebf\u6027\u533a\u57df\u6267\u884cDPO\uff0c\u5b66\u4e60\u6bcf\u4e2a\u76ee\u6807\u7684\u66f4\u65b0\u65b9\u5411\u3002\u8fd9\u4e9b\u65b9\u5411\u53ef\u5728\u63a8\u7406\u65f6\u7ebf\u6027\u7ec4\u5408\uff0c\u65e0\u9700\u989d\u5916\u4f18\u5316\u5373\u53ef\u751f\u6210\u7528\u6237\u6307\u5b9a\u7684\u884c\u4e3a", "result": "\u5728HelpSteer\u548cUltraFeedback\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5e2e\u52a9\u6027-\u5197\u957f\u5ea6\u6743\u8861\uff0cTS-DPO\u6bd4\u6807\u91cf\u5316DPO\u5b9e\u73b0\u4e86\u66f4\u5e7f\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u8986\u76d6\u548c\u66f4\u5e73\u6ed1\u7684\u504f\u597d\u63a7\u5236\u3002\u5178\u578b\u76f8\u5173\u5206\u6790\u663e\u793a\u5207\u7ebf\u7a7a\u95f4\u8bad\u7ec3\u653e\u5927\u4e86\u4e0e\u4e0d\u540c\u504f\u597d\u5bf9\u9f50\u7684\u5178\u578b\u65b9\u5411\uff0c\u6539\u5584\u4e86\u53ef\u5206\u79bb\u6027", "conclusion": "TS-DPO\u901a\u8fc7\u5207\u7ebf\u7a7a\u95f4\u4e2d\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u4f7fLLMs\u80fd\u591f\u4ee5\u539f\u5219\u6027\u548c\u53ef\u63a7\u7684\u65b9\u5f0f\u5e73\u8861\u591a\u4e2a\u4eba\u7c7b\u504f\u597d\u7ef4\u5ea6\uff0c\u4e3a\u53ef\u63a7\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.01630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01630", "abs": "https://arxiv.org/abs/2602.01630", "authors": ["Bohan Zeng", "Kaixin Zhu", "Daili Hua", "Bozhou Li", "Chengzhuo Tong", "Yuran Wang", "Xinyi Huang", "Yifan Dai", "Zixiang Zhang", "Yifan Yang", "Zhou Liu", "Hao Liang", "Xiaochen Ma", "Ruichuan An", "Tianyi Bai", "Hongcheng Gao", "Junbo Niu", "Yang Shi", "Xinlong Chen", "Yue Ding", "Minglei Shi", "Kai Zeng", "Yiwen Tang", "Yuanxing Zhang", "Pengfei Wan", "Xintao Wang", "Wentao Zhang"], "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks", "comment": "13 pages, 4 figures", "summary": "World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9700\u8981\u4e3a\u4e16\u754c\u6a21\u578b\u5efa\u7acb\u7edf\u4e00\u7684\u8bbe\u8ba1\u89c4\u8303\uff0c\u800c\u975e\u5f53\u524d\u96f6\u6563\u7684\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u66f4\u7cfb\u7edf\u3001\u666e\u9002\u7684\u4e16\u754c\u7406\u89e3\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u4e16\u754c\u6a21\u578b\u7814\u7a76\u8fc7\u4e8e\u788e\u7247\u5316\uff0c\u4e3b\u8981\u5173\u6ce8\u5c06\u4e16\u754c\u77e5\u8bc6\u6ce8\u5165\u5b64\u7acb\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u9884\u6d4b\u30013D\u4f30\u8ba1\u3001\u7b26\u53f7\u63a5\u5730\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u548c\u7cfb\u7edf\u6027\uff0c\u9650\u5236\u4e86\u6574\u4f53\u4e16\u754c\u7406\u89e3\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u5206\u6790\u73b0\u6709\u788e\u7247\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u4e16\u754c\u6a21\u578b\u8bbe\u8ba1\u89c4\u8303\uff0c\u5f3a\u8c03\u4e16\u754c\u6a21\u578b\u5e94\u662f\u6574\u5408\u4ea4\u4e92\u3001\u611f\u77e5\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7a7a\u95f4\u8868\u793a\u7684\u89c4\u8303\u6027\u6846\u67b6\uff0c\u800c\u975e\u677e\u6563\u7684\u80fd\u529b\u96c6\u5408\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u89c6\u89d2\u6765\u6307\u5bfc\u672a\u6765\u7814\u7a76\uff0c\u65e8\u5728\u63a8\u52a8\u6784\u5efa\u66f4\u901a\u7528\u3001\u9c81\u68d2\u548c\u539f\u5219\u6027\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4e3aAI\u667a\u80fd\u4f53\u7406\u89e3\u3001\u9884\u6d4b\u548c\u4ea4\u4e92\u590d\u6742\u73af\u5883\u63d0\u4f9b\u7cfb\u7edf\u6027\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u4ece\u4efb\u52a1\u7279\u5b9a\u7684\u96f6\u6563\u65b9\u6cd5\u8f6c\u5411\u7edf\u4e00\u7684\u8bbe\u8ba1\u89c4\u8303\uff0c\u5efa\u7acb\u6574\u5408\u4ea4\u4e92\u3001\u611f\u77e5\u3001\u63a8\u7406\u548c\u7a7a\u95f4\u8868\u793a\u7684\u7cfb\u7edf\u6027\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u4e16\u754c\u7406\u89e3\u548c\u667a\u80fd\u3002"}}
{"id": "2602.01135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01135", "abs": "https://arxiv.org/abs/2602.01135", "authors": ["Hugo Math", "Rainer Lienhart"], "title": "TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation", "comment": "8 pages, 6 figures,", "summary": "We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.", "AI": {"tldr": "TRACE\uff1a\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u4f5c\u4e3a\u9884\u8bad\u7ec3\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u4ece\u5355\u6761\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u63a8\u65ad\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u56e0\u679c\u56fe\uff0c\u652f\u6301\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\uff0c\u53ef\u7ebf\u6027\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bcd\u6c47\u8868", "motivation": "\u89e3\u51b3\u4ece\u5355\u6761\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\uff08\u5982\u8f66\u8f86\u65e5\u5fd7\u3001\u5236\u9020\u7cfb\u7edf\u3001\u60a3\u8005\u8f68\u8ff9\uff09\u4e2d\u53d1\u73b0\u56e0\u679c\u5173\u7cfb\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u573a\u666f\u7f3a\u4e4f\u91cd\u590d\u6837\u672c\u3001\u7ef4\u5ea6\u9ad8\u4e14\u5b58\u5728\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56", "method": "TRACE\u6846\u67b6\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u9884\u8bad\u7ec3\u5bc6\u5ea6\u4f30\u8ba1\u5668\u8fdb\u884c\u6761\u4ef6\u4e92\u4fe1\u606f\u4f30\u8ba1\uff0c\u63a8\u65ad\u4e8b\u4ef6\u7c7b\u578b\u95f4\u7684\u6458\u8981\u56e0\u679c\u56fe\uff0c\u652f\u6301\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\uff0c\u5728GPU\u4e0a\u5b8c\u5168\u5e76\u884c\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u4e8b\u4ef6\u8bcd\u6c47\u8868\u7ebf\u6027\u589e\u957f", "result": "\u5728\u7406\u8bba\u53ef\u8bc6\u522b\u6027\u65b9\u9762\u5efa\u7acb\u4e86\u4e0d\u5b8c\u7f8e\u81ea\u56de\u5f52\u6a21\u578b\u4e0b\u7684\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4e0d\u540c\u57fa\u7ebf\u548c\u8bcd\u6c47\u8868\u89c4\u6a21\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u80fd\uff0c\u5728\u8d85\u8fc729,100\u4e2a\u4e8b\u4ef6\u7c7b\u578b\u7684\u8f66\u8f86\u8bca\u65ad\u6839\u56e0\u5206\u6790\u4e2d\u6210\u529f\u5e94\u7528", "conclusion": "TRACE\u4e3a\u4ece\u5355\u6761\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u5927\u89c4\u6a21\u8bcd\u6c47\u8868\u548c\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.01633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01633", "abs": "https://arxiv.org/abs/2602.01633", "authors": ["Xinyuan Zhao", "Yihang Wu", "Ahmad Chaddad", "Tareef Daqqaq", "Reem Kateb"], "title": "Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification", "comment": "Accepted in Knowledge-Based Systems", "summary": "While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\\% to 41.69\\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u548c\u5ba2\u6237\u7aef\u611f\u77e5\u805a\u5408\u7b56\u7565\uff0c\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4f46\u533b\u7597\u56fe\u50cf\u53d7\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u96be\u4ee5\u83b7\u53d6\u3002\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u9762\u4e34\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u5f71\u54cd\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u8bbe\u8ba1\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931(DAFL)\uff0c\u6839\u636e\u5ba2\u6237\u7aef\u6837\u672c\u5206\u5e03\u548c\u7c7b\u522b\u6570\u636e\u5206\u5e03\u52a8\u6001\u8c03\u6574\u7c7b\u522b\u4e0d\u5e73\u8861\u7cfb\u6570\uff1b2) \u91c7\u7528\u52a0\u6743\u805a\u5408\u7b56\u7565\uff0c\u6839\u636e\u6570\u636e\u89c4\u6a21\u548c\u7279\u5f81\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\uff0c\u6355\u6349\u5ba2\u6237\u7aef\u95f4\u5dee\u5f02\u3002", "result": "\u5728ISIC\u3001Ocular Disease\u548cRSNA-ICH\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8eDenseNet121\u3001ResNet50\u3001ViT-S/16\u3001ViT-L/32\u3001FedCLIP\u3001Swin Transformer\u3001CoAtNet\u548cMixNet\uff0c\u51c6\u786e\u7387\u63d0\u53470.98%\u523041.69%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u548c\u5ba2\u6237\u7aef\u611f\u77e5\u805a\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.01136", "categories": ["cs.LG", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01136", "abs": "https://arxiv.org/abs/2602.01136", "authors": ["Ronald Katende"], "title": "A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning", "comment": "11 pages", "summary": "We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.\n  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.\n  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.\n  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u77e9\u9635\u8c31\u6846\u67b6\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5f15\u5165\u5168\u5c40\u77e9\u9635\u7a33\u5b9a\u6027\u6307\u6570\u548c\u8c31\u71b5\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u8c31\u6b63\u5219\u5316\u63d0\u5347\u5f52\u56e0\u7a33\u5b9a\u6027\u3002", "motivation": "\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7f51\u7edc\u654f\u611f\u6027\u3001\u6807\u7b7e\u566a\u58f0\u548c\u8bad\u7ec3\u52a8\u6001\u7684\u7cfb\u7edf\u6027\u8c31\u5206\u6790\u3002", "method": "\u5c06\u7f51\u7edc\u8868\u793a\u4e3a\u6570\u636e\u4f9d\u8d56\u7684\u7ebf\u6027\u7b97\u5b50\u4e58\u79ef\uff0c\u5f15\u5165\u5168\u5c40\u77e9\u9635\u7a33\u5b9a\u6027\u6307\u6570\u805a\u5408\u96c5\u53ef\u6bd4\u77e9\u9635\u3001\u53c2\u6570\u68af\u5ea6\u3001\u795e\u7ecf\u6b63\u5207\u6838\u7b97\u5b50\u548c\u635f\u5931\u6d77\u68ee\u77e9\u9635\u7684\u8c31\u4fe1\u606f\uff0c\u4f7f\u7528\u8c31\u71b5\u6539\u8fdb\u7ecf\u5178\u7b97\u5b50\u8303\u6570\u754c\u3002", "result": "\u5728MNIST\u3001CIFAR-10\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5ea6\u7684\u8c31\u6b63\u5219\u5316\u80fd\u663e\u8457\u6539\u5584\u5f52\u56e0\u7a33\u5b9a\u6027\uff0c\u5373\u4f7f\u5168\u5c40\u8c31\u6458\u8981\u53d8\u5316\u5f88\u5c0f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u8c31\u96c6\u4e2d\u5ea6\u4e0e\u5206\u6790\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u7cbe\u786e\u8054\u7cfb\uff0c\u4e3a\u9c81\u68d2\u6027\u611f\u77e5\u7684\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01639", "abs": "https://arxiv.org/abs/2602.01639", "authors": ["Tianyu Yang", "ChenWei He", "Xiangzhao Hao", "Tianyue Wang", "Jiarui Guo", "Haiyun Guo", "Leigang Qu", "Jinqiao Wang", "Tat-Seng Chua"], "title": "ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.", "AI": {"tldr": "ReCALL\u6846\u67b6\u89e3\u51b3\u751f\u6210\u5f0fMLLM\u9002\u914d\u4e3a\u68c0\u7d22\u5668\u65f6\u7684\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8bca\u65ad-\u751f\u6210-\u7cbe\u70bc\u6d41\u7a0b\u63d0\u5347\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u9002\u914d\u4e3a\u5355\u5d4c\u5165\u5224\u522b\u5f0f\u68c0\u7d22\u5668\u65f6\uff0c\u4f1a\u5f15\u53d1\u8303\u5f0f\u51b2\u7a81\uff0c\u5bfc\u81f4\u539f\u751f\u7ec6\u7c92\u5ea6\u63a8\u7406\u80fd\u529b\u9000\u5316\uff0c\u5f71\u54cd\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u6548\u679c", "method": "ReCALL\u6846\u67b6\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u901a\u8fc7\u81ea\u5f15\u5bfc\u4fe1\u606f\u5b9e\u4f8b\u6316\u6398\u8bca\u65ad\u68c0\u7d22\u5668\u7684\u8ba4\u77e5\u76f2\u70b9\uff1b2\uff09\u901a\u8fc7CoT\u63d0\u793a\u57fa\u7840MLLM\u751f\u6210\u7ea0\u6b63\u6307\u4ee4\u548c\u4e09\u5143\u7ec4\uff0c\u5e76\u7528VQA\u4e00\u81f4\u6027\u8fc7\u6ee4\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\uff1b3\uff09\u901a\u8fc7\u5206\u7ec4\u5bf9\u6bd4\u65b9\u6848\u5728\u751f\u6210\u7684\u4e09\u5143\u7ec4\u4e0a\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\uff0c\u5185\u5316\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u4e49\u533a\u5206", "result": "\u5728CIRR\u548cFashionIQ\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReCALL\u80fd\u6301\u7eed\u6821\u51c6\u9000\u5316\u80fd\u529b\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "ReCALL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0fMLLM\u9002\u914d\u4e3a\u68c0\u7d22\u5668\u65f6\u7684\u80fd\u529b\u9000\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8bca\u65ad-\u751f\u6210-\u7cbe\u70bc\u6d41\u7a0b\u91cd\u65b0\u6821\u51c6\u5224\u522b\u5f0f\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347"}}
{"id": "2602.01137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01137", "abs": "https://arxiv.org/abs/2602.01137", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "title": "Self-Generative Adversarial Fine-Tuning for Large Language Models", "comment": null, "summary": "Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.", "AI": {"tldr": "SGALM\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u5c06LLM\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u751f\u6210\u5bf9\u6297\u6e38\u620f\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\u5373\u53ef\u8054\u5408\u8fdb\u5316\u751f\u6210\u548c\u5224\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff09\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u7a00\u7f3a\u3002\u81ea\u535a\u5f08\u548c\u5408\u6210\u6570\u636e\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4f9d\u8d56\uff0c\u4f46\u4f9d\u8d56\u542f\u53d1\u5f0f\u5047\u8bbe\u6216\u65e0\u57fa\u7840\u7684\u81ea\u8bc4\u4f30\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u5dee\u7d2f\u79ef\u548c\u6027\u80fd\u6f02\u79fb\u3002", "method": "\u63d0\u51faSelf-Generative Adversarial LLM (SGALM)\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u751f\u6210\u5bf9\u6297\u6e38\u620f\uff0c\u5728\u5355\u4e2aLLM\u5185\u8054\u5408\u8fdb\u5316\u751f\u6210\u548c\u5224\u522b\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eSGALM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u65e2\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u9c81\u68d2\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5f15\u64ce\u3002", "conclusion": "SGALM\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u751f\u6210\u5bf9\u6297\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u81ea\u535a\u5f08\u65b9\u6cd5\u7684\u504f\u5dee\u7d2f\u79ef\u95ee\u9898\u3002"}}
{"id": "2602.01649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01649", "abs": "https://arxiv.org/abs/2602.01649", "authors": ["Yinchao Ma", "Qiang Zhou", "Zhibin Wang", "Xianing Chen", "Hanqing Yang", "Jun Song", "Bo Zheng"], "title": "Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning", "comment": "This paper is accepted by AAAI2026", "summary": "Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \\textbf{C}ontribution-\\textbf{a}ware token \\textbf{Co}mpression algorithm for \\textbf{VID}eo understanding (\\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.", "AI": {"tldr": "CaCoVID\u662f\u4e00\u79cd\u57fa\u4e8e\u8d21\u732e\u611f\u77e5\u7684\u89c6\u9891\u4ee4\u724c\u538b\u7f29\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u89c6\u9891\u4ee4\u724c\u7684\u5197\u4f59\u6027\u5bfc\u81f4\u63a8\u7406\u65f6\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u538b\u7f29\u7b97\u6cd5\u901a\u5e38\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u4fdd\u7559\u7279\u5f81\uff0c\u4f46\u6ce8\u610f\u529b\u5206\u6570\u4e0e\u5bf9\u6b63\u786e\u7b54\u6848\u7684\u5b9e\u9645\u8d21\u732e\u4e4b\u95f4\u7684\u5173\u7cfb\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f18\u5316\u7b56\u7565\u7f51\u7edc\u4ee5\u9009\u62e9\u5bf9\u6b63\u786e\u9884\u6d4b\u8d21\u732e\u6700\u5927\u7684\u89c6\u9891\u4ee4\u724c\u7ec4\u5408\uff1b\u63d0\u51fa\u5728\u7ebf\u7ec4\u5408\u7a7a\u95f4\u91c7\u6837\u7684\u7ec4\u5408\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5927\u5e45\u51cf\u5c11\u63a2\u7d22\u7a7a\u95f4\u5e76\u52a0\u901f\u7b56\u7565\u4f18\u5316\u6536\u655b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86CaCoVID\u7684\u6709\u6548\u6027\u3002", "conclusion": "CaCoVID\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u4ee4\u724c\u9009\u62e9\u7b56\u7565\uff0c\u4ece\u88ab\u52a8\u4fdd\u7559\u4ee4\u724c\u8f6c\u5411\u4e3b\u52a8\u53d1\u73b0\u6700\u4f18\u538b\u7f29\u4ee4\u724c\u7ec4\u5408\uff0c\u4e3a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01139", "abs": "https://arxiv.org/abs/2602.01139", "authors": ["Yassine Abbahaddou"], "title": "Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization", "comment": "PhD Thesis", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6cdb\u5316\u6027\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u8868\u793a\u5b66\u4e60\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u79fb\u4f4d\u7b97\u5b50\u7684\u8868\u793a\u5b66\u4e60\u3001\u56fe\u6570\u636e\u589e\u5f3a\u548c\u6b63\u4ea4\u5316\u566a\u58f0\u9632\u5fa1\u4e09\u5927\u8d21\u732e\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6cdb\u5316\u80fd\u529b\u3001\u5bf9\u6297\u6270\u52a8\u9c81\u68d2\u6027\u548c\u8868\u793a\u5b66\u4e60\u6548\u679c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1) \u57fa\u4e8e\u56fe\u79fb\u4f4d\u7b97\u5b50\u5f00\u53d1\u65b0\u7684\u8868\u793a\u5b66\u4e60\u6280\u672f\uff1b2) \u901a\u8fc7\u56fe\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff1b3) \u5229\u7528\u6b63\u4ea4\u5316\u6280\u672f\u548c\u57fa\u4e8e\u566a\u58f0\u7684\u9632\u5fa1\u673a\u5236\u589e\u5f3a\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\u5c40\u9650\u6027\u548c\u6f5c\u529b\u7684\u66f4\u539f\u5219\u6027\u7406\u89e3\uff0c\u63d0\u51fa\u4e86\u7cfb\u7edf\u6027\u7684\u6539\u8fdb\u6846\u67b6\uff0c\u6709\u671b\u5728\u5404\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u63d0\u5347GNN\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u8868\u793a\u5b66\u4e60\u3001\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff0c\u8be5\u7814\u7a76\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86GNN\u5728\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u4e2d\u7684\u8fdb\u6b65\u3002"}}
{"id": "2602.01661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01661", "abs": "https://arxiv.org/abs/2602.01661", "authors": ["Xingyu Miao", "Junting Dong", "Qin Zhao", "Yuhang Yang", "Junhao Chen", "Yang Long"], "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction", "comment": null, "summary": "In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u5e8f\u5217\u4e2d\u65f6\u95f4\u4e00\u81f4\u7684\u4eba\u4f53\u4e2d\u5fc3\u5bc6\u96c6\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u8fd0\u52a8\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7684\u95ea\u70c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5355\u5e27\u7cbe\u5ea6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8fd0\u52a8\u3001\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7ecf\u5e38\u51fa\u73b0\u95ea\u70c1\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u591a\u4e2a\u5bc6\u96c6\u4efb\u52a1\u7684\u914d\u5bf9\u4eba\u4f53\u89c6\u9891\u76d1\u7763\u6570\u636e\u3002", "method": "1) \u6784\u5efa\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u5e27\u548c\u8fd0\u52a8\u5bf9\u9f50\u5e8f\u5217\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u51c6\u786e\u7684\u6df1\u5ea6\u3001\u6cd5\u7ebf\u548c\u63a9\u7801\u6807\u7b7e\uff1b2) \u8bad\u7ec3\u7edf\u4e00\u7684ViT\u57fa\u5bc6\u96c6\u9884\u6d4b\u5668\uff0c\u901a\u8fc7CSE\u5d4c\u5165\u6ce8\u5165\u663e\u5f0f\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u9053\u91cd\u52a0\u6743\u6a21\u5757\u63d0\u9ad8\u51e0\u4f55\u7279\u5f81\u53ef\u9760\u6027\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9759\u6001\u9884\u8bad\u7ec3+\u52a8\u6001\u5e8f\u5217\u76d1\u7763\u3002", "result": "\u5728THuman2.1\u548cHi4D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u4e00\u81f4\u7684\u4eba\u4f53\u4e2d\u5fc3\u5bc6\u96c6\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u95ea\u70c1\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.01140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01140", "abs": "https://arxiv.org/abs/2602.01140", "authors": ["Haochen You", "Heng Zhang", "Hongyang He", "Yuqi Li", "Baojing Liu"], "title": "Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization", "comment": "This paper has been accepted as a conference paper at CPAL 2026", "summary": "Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.", "AI": {"tldr": "GRIT-VQ\u662f\u4e00\u79cd\u65b0\u578b\u5411\u91cf\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u5f84\u66f4\u65b0\u548c\u96c6\u6210\u53d8\u6362\u89e3\u51b3\u4f20\u7edfVQ\u7684\u68af\u5ea6\u4e0d\u7a33\u5b9a\u548c\u7801\u672c\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u524d\u5411\u786c\u5206\u914d\u7684\u540c\u65f6\u5b9e\u73b0\u5b8c\u5168\u53ef\u5fae\u3002", "motivation": "\u4f20\u7edf\u5411\u91cf\u91cf\u5316\u4f7f\u7528\u786c\u6700\u8fd1\u90bb\u5206\u914d\u548c\u76f4\u901a\u4f30\u8ba1\u5668\uff0c\u5bfc\u81f4\u68af\u5ea6\u4e0d\u7a33\u5b9a\u3001\u7801\u672c\u5229\u7528\u7387\u4f4e\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\u3002", "method": "\u63d0\u51faGRIT-VQ\u6846\u67b6\uff1a1\uff09\u7528\u57fa\u4e8e\u534a\u5f84\u7684\u66f4\u65b0\u66ff\u4ee3\u76f4\u901a\u4f30\u8ba1\u5668\uff0c\u6cbf\u91cf\u5316\u65b9\u5411\u4ee5\u53ef\u63a7\u7684\u51e0\u4f55\u611f\u77e5\u6b65\u957f\u79fb\u52a8\u6f5c\u5728\u8868\u793a\uff1b2\uff09\u5bf9\u7801\u672c\u5e94\u7528\u6570\u636e\u65e0\u5173\u7684\u96c6\u6210\u53d8\u6362\uff0c\u4f7f\u6240\u6709\u7801\u901a\u8fc7\u5171\u4eab\u53c2\u6570\u66f4\u65b0\u800c\u975e\u72ec\u7acb\u66f4\u65b0\u3002", "result": "\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u56fe\u50cf\u751f\u6210\u548c\u63a8\u8350\u7cfb\u7edf\u6807\u8bb0\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRIT-VQ\u6301\u7eed\u6539\u5584\u91cd\u5efa\u8bef\u5dee\u3001\u751f\u6210\u8d28\u91cf\u548c\u63a8\u8350\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u7801\u672c\u5229\u7528\u7387\u3002", "conclusion": "GRIT-VQ\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9610\u660e\u4e86\u5176\u4f18\u5316\u52a8\u6001\uff0c\u4e3a\u7a33\u5b9a\u68af\u5ea6\u6d41\u3001\u534f\u8c03\u7801\u672c\u6f14\u5316\u548c\u53ef\u9760\u907f\u514d\u5d29\u6e83\u63d0\u4f9b\u4e86\u6761\u4ef6\uff0c\u662f\u5411\u91cf\u91cf\u5316\u9886\u57df\u7684\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2602.01666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01666", "abs": "https://arxiv.org/abs/2602.01666", "authors": ["Yan Wang", "Partho Hassan", "Samiha Sadeka", "Nada Soliman", "M M Sayeef Abdullah", "Sabit Hassan"], "title": "Moonworks Lunara Aesthetic II: An Image Variation Dataset", "comment": null, "summary": "We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.", "AI": {"tldr": "Lunara Aesthetic II\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b2,854\u5bf9\u951a\u70b9\u94fe\u63a5\u7684\u53d8\u4f53\u56fe\u50cf\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u5b66\u4e60\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u4e2d\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7a33\u5b9a\u6027\u548c\u9ad8\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7cfb\u7edf\u9700\u8981\u8bc4\u4f30\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u6b64\u8bbe\u8ba1\u7684\u516c\u5f00\u6570\u636e\u96c6\u3002Lunara Aesthetic II\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u53ef\u63a7\u8bc4\u4f30\u63d0\u4f9b\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u6570\u636e\u96c6\u57fa\u4e8eMoonworks\u7684\u539f\u521b\u827a\u672f\u4f5c\u54c1\u548c\u7167\u7247\u6784\u5efa\uff0c\u6bcf\u5bf9\u56fe\u50cf\u5305\u542b\u951a\u70b9\u56fe\u50cf\u548c\u7ecf\u8fc7\u4e0a\u4e0b\u6587\u53d8\u6362\u7684\u53d8\u4f53\u56fe\u50cf\u3002\u53d8\u6362\u5305\u62ec\u5149\u7167\u3001\u5929\u6c14\u3001\u89c6\u89d2\u3001\u573a\u666f\u6784\u56fe\u3001\u8272\u8c03\u6216\u60c5\u7eea\u7b49\uff0c\u540c\u65f6\u4fdd\u6301\u5e95\u5c42\u8eab\u4efd\u7a33\u5b9a\u3002", "result": "\u6570\u636e\u96c6\u8868\u73b0\u51fa\u9ad8\u8eab\u4efd\u7a33\u5b9a\u6027\u3001\u5f3a\u76ee\u6807\u5c5e\u6027\u5b9e\u73b0\u80fd\u529b\uff0c\u4ee5\u53ca\u8d85\u8d8a\u5927\u89c4\u6a21\u7f51\u7edc\u6570\u636e\u96c6\u7684\u7a33\u5065\u7f8e\u5b66\u7279\u5f81\u3002\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u4e0a\u4e0b\u6587\u53d8\u6362\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Lunara Aesthetic II\u4f5c\u4e3aApache 2.0\u8bb8\u53ef\u4e0b\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u5fae\u8c03\u548c\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u4e0a\u4e0b\u6587\u6cdb\u5316\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u7f16\u8f91\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.01150", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01150", "abs": "https://arxiv.org/abs/2602.01150", "authors": ["Jialong Sun", "Zeming Wei", "Jiaxuan Zou", "Jiacheng Gong", "Guanheng Wang", "Chengyang Dong", "Jialong Li", "Bo Liu"], "title": "Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing", "comment": null, "summary": "Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSMIA\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u68c0\u9a8c\u76f4\u63a5\u6bd4\u8f83\u6210\u5458\u4e0e\u975e\u6210\u5458\u6570\u636e\u5206\u5e03\uff0c\u65e0\u9700\u8bad\u7ec3\u653b\u51fb\u6a21\u578b\uff0c\u63d0\u4f9b\u5e26\u7f6e\u4fe1\u533a\u95f4\u7684\u9057\u5fd8\u7387\u8bc4\u4f30\uff0c\u6bd4\u4f20\u7edfMIA\u65b9\u6cd5\u66f4\u53ef\u9760\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6210\u5458\u63a8\u65ad\u653b\u51fb(MIA)\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u5ba1\u8ba1\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1a\u6210\u5458\u63a8\u65ad\u5931\u8d25\u5e76\u4e0d\u7b49\u540c\u4e8e\u771f\u6b63\u9057\u5fd8\u3002MIA\u4f5c\u4e3a\u4e8c\u5206\u7c7b\u95ee\u9898\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u7edf\u8ba1\u8bef\u5dee\uff0c\u5bfc\u81f4\u5bf9\u9057\u5fd8\u6027\u80fd\u7684\u8bc4\u4f30\u8fc7\u4e8e\u4e50\u89c2\uff0c\u540c\u65f6\u9700\u8981\u8bad\u7ec3\u5f71\u5b50\u6a21\u578b\u5e26\u6765\u5de8\u5927\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u7edf\u8ba1\u6210\u5458\u63a8\u65ad\u653b\u51fb(SMIA)\u6846\u67b6\uff1a1) \u76f4\u63a5\u6bd4\u8f83\u6210\u5458\u6570\u636e\u4e0e\u975e\u6210\u5458\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4f7f\u7528\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\uff1b2) \u65e0\u9700\u8bad\u7ec3\u653b\u51fb\u6a21\u578b\uff0c\u907f\u514d\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u504f\u5dee\uff1b3) \u8f93\u51fa\u9057\u5fd8\u7387\u53ca\u5176\u7f6e\u4fe1\u533a\u95f4\uff0c\u91cf\u5316\u5ba1\u8ba1\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSMIA\u76f8\u6bd4\u4f20\u7edfMIA\u65b9\u6cd5\uff1a1) \u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9057\u5fd8\u5ba1\u8ba1\u7ed3\u679c\uff1b2) \u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b3) \u8f93\u51fa\u5e26\u7f6e\u4fe1\u533a\u95f4\u7684\u91cf\u5316\u8bc4\u4f30\uff1b4) \u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u6548\u679c\u8868\u660e\u5176\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u5ba1\u8ba1\u65b0\u8303\u5f0f\u3002", "conclusion": "SMIA\u901a\u8fc7\u7edf\u8ba1\u68c0\u9a8c\u76f4\u63a5\u6bd4\u8f83\u6570\u636e\u5206\u5e03\uff0c\u907f\u514d\u4e86\u4f20\u7edfMIA\u65b9\u6cd5\u7684\u7edf\u8ba1\u8bef\u5dee\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u5ba1\u8ba1\u6846\u67b6\uff0c\u6709\u671b\u6210\u4e3a\u8be5\u9886\u57df\u7684\u65b0\u6807\u51c6\u65b9\u6cd5\u3002"}}
{"id": "2602.01673", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01673", "abs": "https://arxiv.org/abs/2602.01673", "authors": ["Enguang Fan"], "title": "Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss", "comment": null, "summary": "Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86NetVLAD\u4f5c\u4e3a\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\u5728SLAM\u4e2d\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edfDBoW\u65b9\u6cd5\uff0cNetVLAD\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u8bcd\u888b\u65b9\u6cd5\uff08\u5982DBoW\uff09\u5728\u95ed\u73af\u68c0\u6d4b\u4e2d\u6548\u7387\u9ad8\u4f46\u6613\u53d7\u5916\u89c2\u53d8\u5316\u548c\u611f\u77e5\u6df7\u6dc6\u5f71\u54cd\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\uff08\u5982NetVLAD\uff09\u867d\u9c81\u68d2\u6027\u5f3a\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u5b9e\u65f6SLAM\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1NetVLAD\u80fd\u5426\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u63d0\u5347\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5bf9NetVLAD\u4f5c\u4e3a\u95ed\u73af\u68c0\u6d4b\u6a21\u5757\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u4e0eDBoW\u5bf9\u6bd4\u3002\u5f15\u5165\u7ec6\u7c92\u5ea6Top-K\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u66f2\u7ebf\u4ee5\u66f4\u597d\u53cd\u6620\u95ed\u73af\u68c0\u6d4b\u573a\u666f\uff08\u67e5\u8be2\u53ef\u80fd\u6709\u96f6\u4e2a\u6216\u591a\u4e2a\u6709\u6548\u5339\u914d\uff09\u3002\u4f7f\u7528Faiss\u52a0\u901f\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u5b9e\u73b0\u5b9e\u65f6\u67e5\u8be2\u901f\u5ea6\u3002", "result": "NetVLAD\u5728\u4fdd\u6301\u5b9e\u65f6\u67e5\u8be2\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4DBoW\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002Faiss\u52a0\u901f\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u4f7fNetVLAD\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u4f7f\u5176\u6210\u4e3aSLAM\u4e2d\u95ed\u73af\u68c0\u6d4b\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "NetVLAD\u7ed3\u5408Faiss\u52a0\u901f\u53ef\u4ee5\u4f5c\u4e3aSLAM\u4e2d\u95ed\u73af\u68c0\u6d4b\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4f20\u7edf\u8bcd\u888b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60VPR\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u969c\u788d\u3002"}}
{"id": "2602.01156", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01156", "abs": "https://arxiv.org/abs/2602.01156", "authors": ["Shunpeng Yang", "Ben Liu", "Hua Chen"], "title": "PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning", "comment": "Submitted to ICLR 2026", "summary": "Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.", "AI": {"tldr": "PolicyFlow\uff1a\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\u907f\u514d\u6602\u8d35\u7684\u4f3c\u7136\u8ba1\u7b97\uff0c\u7ed3\u5408\u5e03\u6717\u6b63\u5219\u5316\u5668\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edfPPO\u548c\u9ad8\u65af\u7b56\u7565\u3002", "motivation": "PPO\u7b97\u6cd5\u867d\u7136\u7b80\u5355\u7a33\u5b9a\uff0c\u4f46\u5176\u4f9d\u8d56\u91cd\u8981\u6027\u6bd4\u7387\u9700\u8981\u8bc4\u4f30\u7b56\u7565\u4f3c\u7136\uff0c\u5f53\u4f7f\u7528\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\uff08CNF\uff09\u4f5c\u4e3a\u7b56\u7565\u6a21\u578b\u65f6\uff0c\u6cbf\u6574\u4e2a\u6d41\u8f68\u8ff9\u7684\u4f3c\u7136\u8ba1\u7b97\u975e\u5e38\u6602\u8d35\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u9650\u5236\u4e86PPO\u4e0e\u9ad8\u5bb9\u91cf\u7b56\u7565\u6a21\u578b\u7684\u7ed3\u5408\u3002", "method": "\u63d0\u51faPolicyFlow\u7b97\u6cd5\uff1a1\uff09\u901a\u8fc7\u6cbf\u7b80\u5355\u63d2\u503c\u8def\u5f84\u7684\u6d41\u901f\u573a\u53d8\u5316\u6765\u8fd1\u4f3c\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u907f\u514d\u6cbf\u5b8c\u6574\u6d41\u8def\u5f84\u7684\u4f3c\u7136\u8ba1\u7b97\uff1b2\uff09\u5f15\u5165\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u5e03\u6717\u8fd0\u52a8\u542f\u53d1\u7684\u9690\u5f0f\u7b56\u7565\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u5e76\u9f13\u52b1\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728MultiGoal\u3001PointMaze\u3001IsaacLab\u548cMuJoCo Playground\u7b49\u591a\u79cd\u73af\u5883\u4efb\u52a1\u4e0a\uff0cPolicyFlow\u76f8\u6bd4\u4f7f\u7528\u9ad8\u65af\u7b56\u7565\u7684PPO\u4ee5\u53caFPO\u3001DPPO\u7b49\u6d41\u57fa\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\u5728MultiGoal\u4efb\u52a1\u4e2d\uff0cPolicyFlow\u80fd\u591f\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u3002", "conclusion": "PolicyFlow\u6210\u529f\u5730\u5c06\u8868\u8fbe\u80fd\u529b\u5f3a\u7684CNF\u7b56\u7565\u4e0ePPO\u5f0f\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u91cd\u8981\u6027\u6bd4\u7387\u8fd1\u4f3c\u548c\u5e03\u6717\u6b63\u5219\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u9ad8\u5bb9\u91cf\u7b56\u7565\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01674", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.01674", "abs": "https://arxiv.org/abs/2602.01674", "authors": ["Hail Song", "Boram Yoon", "Seokhwan Yang", "Seoyoung Kang", "Hyunjeong Kim", "Henning Metzmacher", "Woontack Woo"], "title": "VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR", "comment": "Accepted as an IEEE TVCG paper at IEEE VR 2026 (journal track)", "summary": "We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.", "AI": {"tldr": "VRGaussianAvatar\uff1a\u57fa\u4e8e\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u9ad8\u65af\u6cfc\u6e85\u5168\u8eab\u865a\u62df\u5316\u8eab\uff0c\u901a\u8fc7\u53cc\u76ee\u6279\u5904\u7406\u5b9e\u73b0\u5b9e\u65f6VR\u6e32\u67d3\u7684\u7cfb\u7edf", "motivation": "\u5f53\u524dVR\u4e2d\u7684\u865a\u62df\u5316\u8eab\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u591a\u89c6\u89d2\u91c7\u96c6\u6216\u8868\u73b0\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u5feb\u901f\u91cd\u5efa\u9ad8\u8d28\u91cf\u5168\u8eab\u5316\u8eab\u5e76\u80fd\u5728VR\u4e2d\u5b9e\u65f6\u6e32\u67d3\u7684\u65b9\u6cd5", "method": "\u91c7\u7528\u5e76\u884c\u6d41\u6c34\u7ebf\u67b6\u6784\uff1aVR\u524d\u7aef\u4f7f\u7528\u9006\u8fd0\u52a8\u5b66\u4eceHMD\u4fe1\u53f7\u4f30\u8ba1\u5168\u8eab\u59ff\u6001\uff0cGA\u540e\u7aef\u57fa\u4e8e\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u9ad8\u65af\u6cfc\u6e85\u5316\u8eab\uff0c\u5e76\u5f15\u5165\u53cc\u76ee\u6279\u5904\u7406\u6280\u672f\u8054\u5408\u5904\u7406\u5de6\u53f3\u773c\u89c6\u56fe\u4ee5\u63d0\u9ad8\u6e32\u67d3\u6548\u7387", "result": "\u7cfb\u7edf\u80fd\u7ef4\u6301\u4ea4\u4e92\u5f0fVR\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u4e8e\u56fe\u50cf\u548c\u7f51\u683c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u5916\u89c2\u76f8\u4f3c\u6027\u3001\u5177\u8eab\u611f\u548c\u5408\u7406\u6027\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u8bc4\u5206", "conclusion": "VRGaussianAvatar\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u9ad8\u65af\u6cfc\u6e85\u5316\u8eab\u5e76\u5728VR\u4e2d\u5b9e\u65f6\u6e32\u67d3\uff0c\u4e3aVR\u793e\u4ea4\u4e92\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5316\u8eab\u521b\u5efa\u65b9\u6848"}}
{"id": "2602.01157", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01157", "abs": "https://arxiv.org/abs/2602.01157", "authors": ["Mohammed Osman Gani", "Zhipeng He", "Chun Ouyang", "Sara Khalifa"], "title": "Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market", "comment": "63 Pages", "summary": "Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u65e5\u524d\u7535\u4ef7\u9884\u6d4b\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86SOTA\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5728\u6fb3\u5927\u5229\u4e9a\u7535\u529b\u5e02\u573a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6807\u51c6DL\u6a21\u578b\u5728\u591a\u6570\u5730\u533a\u8868\u73b0\u66f4\u4f18\uff0c\u800cSOTA\u6a21\u578b\u5bf9\u9884\u6d4b\u65f6\u57df\u6269\u5c55\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u7535\u4ef7\u9884\u6d4b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u591a\u65e5\u9884\u6d4b\u65f6\u57df\u7814\u7a76\u4e0d\u8db3\uff1b2) \u5bf9SOTA\u65f6\u95f4\u5e8f\u5217DL\u6a21\u578b\u63a2\u7d22\u4e0d\u591f\uff1b3) \u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u805a\u5408\uff0c\u63a9\u76d6\u4e86\u65e5\u5185\u9884\u6d4b\u5dee\u5f02\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u53e3\u4ee5\u63d0\u5347\u7535\u4ef7\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u7535\u4ef7\u9884\u6d4b\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u65f6\u57df\u6269\u5c55\u5230\u591a\u65e5\u524d\uff0c\u7cfb\u7edf\u6784\u5efa\u57fa\u4e8e\u57fa\u51c6SOTA\u65f6\u95f4\u5e8f\u5217DL\u6a21\u578b\u7684\u9884\u6d4b\u6a21\u578b\u3002\u5728\u6fb3\u5927\u5229\u4e9a\u56fd\u5bb6\u7535\u529b\u5e02\u573a\u4e94\u4e2a\u533a\u57df\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u91c7\u7528\u65e5\u5185\u95f4\u9694\u7ea7\u522b\u7684\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u533a\u57df\u3001\u6307\u6807\u548c\u65f6\u57df\u4e0a\u8868\u73b0\u4e00\u81f4\u6700\u4f18\u3002\u6807\u51c6DL\u6a21\u578b\u5728\u591a\u6570\u5730\u533a\u8868\u73b0\u66f4\u4f18\uff0c\u800cSOTA\u65f6\u95f4\u5e8f\u5217DL\u6a21\u578b\u5bf9\u9884\u6d4b\u65f6\u57df\u6269\u5c55\u66f4\u5177\u9c81\u68d2\u6027\u3002\u65e5\u5185\u8bc4\u4f30\u663e\u793a\u660e\u663e\u7684\u663c\u591c\u8bef\u5dee\u6a21\u5f0f\uff1a\u7edd\u5bf9\u8bef\u5dee\u5728\u665a\u95f4\u722c\u5761\u671f\u8fbe\u5230\u5cf0\u503c\uff0c\u76f8\u5bf9\u8bef\u5dee\u5728\u5348\u95f4\u8d1f\u4ef7\u65f6\u6bb5\u81a8\u80c0\uff0c\u65b9\u5411\u51c6\u786e\u6027\u5728\u8d8b\u52bf\u9891\u7e41\u53d8\u5316\u671f\u4e0b\u964d\u3002", "conclusion": "\u672a\u6765\u57fa\u4e8eDL\u7684\u7535\u4ef7\u9884\u6d4b\u7814\u7a76\u5e94\u5173\u6ce8\u589e\u5f3a\u7279\u5f81\u8868\u793a\u548c\u5efa\u6a21\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u957f\u671f\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65e5\u5185\u6ce2\u52a8\u548c\u7ed3\u6784\u6027\u4ef7\u683c\u52a8\u6001\u7684\u654f\u611f\u6027\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u63ed\u793a\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u9884\u6d4b\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2602.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01677", "abs": "https://arxiv.org/abs/2602.01677", "authors": ["Yinchao Ma", "Dengqing Yang", "Zhangyu He", "Wenfei Yang", "Tianzhu Zhang"], "title": "SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking", "comment": "This paper is accepted by IEEE TIP", "summary": "Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u89c6\u89c9\u8ddf\u8e2a\u65b9\u6cd5SMTrack\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u72b6\u6001\u611f\u77e5\u7a7a\u95f4\u6a21\u578b\u6355\u83b7\u591a\u6837\u5316\u65f6\u5e8f\u7ebf\u7d22\uff0c\u4ee5\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u5b9e\u73b0\u957f\u7a0b\u65f6\u5e8f\u4ea4\u4e92\uff0c\u5728\u8bad\u7ec3\u548c\u8ddf\u8e2a\u4e2d\u5747\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edfCNN\u548cTransformer\u67b6\u6784\u5728\u5efa\u6a21\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u5b9a\u5236\u6a21\u5757\u6216\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u6765\u6574\u5408\u65f6\u5e8f\u7ebf\u7d22\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u72b6\u6001\u611f\u77e5Mamba\u8ddf\u8e2a\u5668(SMTrack)\uff0c\u91c7\u7528\u9009\u62e9\u6027\u72b6\u6001\u611f\u77e5\u7a7a\u95f4\u6a21\u578b\uff0c\u5177\u6709\u72b6\u6001\u76f8\u5173\u53c2\u6570\u4ee5\u6355\u83b7\u591a\u6837\u5316\u65f6\u5e8f\u7ebf\u7d22\uff1b\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u4f20\u64ad\u548c\u66f4\u65b0\u5b9e\u73b0\u5e27\u95f4\u4ea4\u4e92\uff1b\u5728\u8bad\u7ec3\u65f6\u4ee5\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u4fc3\u8fdb\u957f\u7a0b\u65f6\u5e8f\u4ea4\u4e92\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMTrack\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u6709\u524d\u666f\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SMTrack\u4e3a\u89c6\u89c9\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u5e8f\u5efa\u6a21\u8303\u5f0f\uff0c\u65e0\u9700\u5b9a\u5236\u6a21\u5757\u6216\u5927\u91cf\u8ba1\u7b97\u6210\u672c\u5373\u53ef\u5efa\u7acb\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.01683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01683", "abs": "https://arxiv.org/abs/2602.01683", "authors": ["Kangcong Li", "Peng Ye", "Lin Zhang", "Chao Wang", "Huafeng Qin", "Tao Chen"], "title": "FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding", "comment": null, "summary": "Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical \"gist\"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.", "AI": {"tldr": "FreshMem\u662f\u4e00\u79cd\u7528\u4e8e\u5728\u7ebf\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u7684\u9891\u7387-\u7a7a\u95f4\u6df7\u5408\u8bb0\u5fc6\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u9891\u7387\u8bb0\u5fc6\u548c\u7a7a\u95f4\u7f29\u7565\u56fe\u8bb0\u5fc6\u6a21\u5757\uff0c\u5728\u77ed\u671f\u4fdd\u771f\u5ea6\u548c\u957f\u671f\u8fde\u8d2f\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9006\u7684\u7ec6\u8282\u4e22\u5931\u548c\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u3002\u9700\u8981\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u79bb\u7ebf\u7406\u89e3\u8fc7\u6e21\u5230\u5728\u7ebf\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\uff0c\u4ee5\u5b9e\u73b0\u8fde\u7eed\u611f\u77e5\u3002", "method": "\u63d0\u51faFreshMem\uff0c\u53d7\u5927\u8111\u5bf9\u6570\u611f\u77e5\u548c\u8bb0\u5fc6\u5de9\u56fa\u542f\u53d1\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u6a21\u5757\uff1a1) \u591a\u5c3a\u5ea6\u9891\u7387\u8bb0\u5fc6(MFM)\uff1a\u5c06\u6ea2\u51fa\u7684\u5e27\u6295\u5f71\u5230\u4ee3\u8868\u6027\u9891\u7387\u7cfb\u6570\uff0c\u8f85\u4ee5\u6b8b\u5dee\u7ec6\u8282\u91cd\u5efa\u5168\u5c40\u5386\u53f2\"\u8981\u70b9\"\uff1b2) \u7a7a\u95f4\u7f29\u7565\u56fe\u8bb0\u5fc6(STM)\uff1a\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u7b56\u7565\u5c06\u8fde\u7eed\u6d41\u79bb\u6563\u5316\u4e3a\u60c5\u8282\u805a\u7c7b\uff0c\u84b8\u998f\u4e3a\u9ad8\u5bc6\u5ea6\u7a7a\u95f4\u7f29\u7565\u56fe\u3002", "result": "\u5728StreamingBench\u3001OV-Bench\u548cOVO-Bench\u4e0a\u5206\u522b\u83b7\u5f975.20%\u30014.52%\u548c2.34%\u7684\u6027\u80fd\u63d0\u5347\u3002\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0cFreshMem\u4f18\u4e8e\u591a\u4e2a\u5b8c\u5168\u5fae\u8c03\u7684\u65b9\u6cd5\u3002", "conclusion": "FreshMem\u4e3a\u957f\u65f6\u57df\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u8303\u5f0f\uff0c\u901a\u8fc7\u9891\u7387-\u7a7a\u95f4\u6df7\u5408\u8bb0\u5fc6\u7f51\u7edc\u5b9e\u73b0\u4e86\u77ed\u671f\u4fdd\u771f\u5ea6\u548c\u957f\u671f\u8fde\u8d2f\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2602.01179", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01179", "abs": "https://arxiv.org/abs/2602.01179", "authors": ["Zhichao Chen", "Zhan Zhuang", "Yunfei Teng", "Hao Wang", "Fangyikang Wang", "Zhengnan Li", "Tianqiao Liu", "Haoxuan Li", "Zhouchen Lin"], "title": "Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective", "comment": null, "summary": "Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.", "AI": {"tldr": "\u63d0\u51faE-SUOT\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u534a\u5bf9\u5076\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u6784\u5efa\u4e2d\u95f4\u57df\uff0c\u907f\u514d\u57fa\u4e8e\u4f3c\u7136\u4f30\u8ba1\u7684\u6d41\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\uff0c\u63d0\u5347\u6e10\u8fdb\u57df\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u6e10\u8fdb\u57df\u9002\u5e94\u9700\u8981\u4e2d\u95f4\u57df\uff0c\u4f46\u771f\u5b9e\u4e2d\u95f4\u57df\u5e38\u4e0d\u53ef\u5f97\u6216\u65e0\u6548\u3002\u73b0\u6709\u6d41\u6a21\u578b\u901a\u8fc7\u63d2\u503c\u6784\u5efa\u4e2d\u95f4\u57df\uff0c\u4f46\u57fa\u4e8e\u6837\u672c\u7684\u4f3c\u7136\u4f30\u8ba1\u8bad\u7ec3\u4f1a\u4e22\u5f03\u6709\u7528\u4fe1\u606f\uff0c\u5f71\u54cdGDA\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u71b5\u6b63\u5219\u5316\u534a\u5bf9\u5076\u975e\u5e73\u8861\u6700\u4f18\u4f20\u8f93(E-SUOT)\u6846\u67b6\uff1a\u5c06\u6d41\u5f0fGDA\u91cd\u65b0\u8868\u8ff0\u4e3a\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u95ee\u9898\uff0c\u63a8\u5bfc\u7b49\u6548\u534a\u5bf9\u5076\u76ee\u6807\u4ee5\u89c4\u907f\u4f3c\u7136\u4f30\u8ba1\uff1b\u5f15\u5165\u71b5\u6b63\u5219\u5316\u5c06\u4e0d\u7a33\u5b9a\u7684min-max\u8bad\u7ec3\u8f6c\u6362\u4e3a\u66f4\u7a33\u5b9a\u7684\u4ea4\u66ff\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86E-SUOT\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "E-SUOT\u6846\u67b6\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u6784\u5efa\u4e2d\u95f4\u57df\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6d41\u6a21\u578b\u7684\u8bad\u7ec3\u9650\u5236\uff0c\u4e3a\u6e10\u8fdb\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01696", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01696", "abs": "https://arxiv.org/abs/2602.01696", "authors": ["Jiaming Cui", "Shuai Zhou", "Wenqiang Li", "Ruifeng Qin", "Feng Shen"], "title": "Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection", "comment": null, "summary": "Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.", "AI": {"tldr": "CMAFNet\uff1a\u4e00\u79cd\u7528\u4e8e\u8f93\u7535\u7ebf\u8def\u7f3a\u9677\u68c0\u6d4b\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\u7f51\u7edc\uff0c\u901a\u8fc7RGB\u5916\u89c2\u548c\u6df1\u5ea6\u51e0\u4f55\u4fe1\u606f\u7684\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u5c3a\u5ea6\u7f3a\u9677\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u8f93\u7535\u7ebf\u8def\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5c0f\u5c3a\u5ea6\u7f3a\u9677\u5360\u4e3b\u5bfc\uff0894.5%\u4e3a\u5c0f\u76ee\u6807\uff09\u3001\u590d\u6742\u80cc\u666f\u5e72\u6270\u3001\u5149\u7167\u53d8\u5316\u3002\u73b0\u6709RGB\u68c0\u6d4b\u5668\u5728\u51e0\u4f55\u7279\u5f81\u4e0d\u660e\u663e\u3001\u8272\u5f69\u5bf9\u6bd4\u5ea6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u533a\u5206\u7f3a\u9677\u4e0e\u80cc\u666f\u7ed3\u6784\u3002", "method": "\u63d0\u51faCMAFNet\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\u7f51\u7edc\uff0c\u91c7\u7528\"\u5148\u51c0\u5316\u540e\u878d\u5408\"\u8303\u5f0f\uff1a1\uff09\u8bed\u4e49\u91cd\u7ec4\u6a21\u5757\uff1a\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u7801\u672c\u8fdb\u884c\u57fa\u4e8e\u5b57\u5178\u7684\u7279\u5f81\u51c0\u5316\uff0c\u6291\u5236\u6a21\u6001\u7279\u5b9a\u566a\u58f0\uff1b2\uff09\u4e0a\u4e0b\u6587\u8bed\u4e49\u96c6\u6210\u6846\u67b6\uff1a\u4f7f\u7528\u90e8\u5206\u901a\u9053\u6ce8\u610f\u529b\u6355\u83b7\u5168\u5c40\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff1b3\uff09\u4f4d\u7f6e\u5f52\u4e00\u5316\uff1a\u5728\u51c0\u5316\u9636\u6bb5\u5f3a\u5236\u6267\u884c\u91cd\u5efa\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u786e\u4fdd\u5f02\u6784\u7279\u5f81\u5728\u878d\u5408\u524d\u7684\u7edf\u8ba1\u517c\u5bb9\u6027\u3002", "result": "\u5728TLRGBD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0894.5%\u4e3a\u5c0f\u76ee\u6807\uff09\uff0cCMAFNet\u8fbe\u523032.2% mAP@50\u548c12.5% APs\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u53479.8\u548c4.0\u4e2a\u767e\u5206\u70b9\u3002\u8f7b\u91cf\u7ea7\u53d8\u4f53\u57284.9M\u53c2\u6570\u4e0b\u8fbe\u523024.8% mAP50\u548c228 FPS\uff0c\u8d85\u8d8a\u6240\u6709YOLO\u68c0\u6d4b\u5668\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u3002", "conclusion": "CMAFNet\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u4e0e\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8f93\u7535\u7ebf\u8def\u5c0f\u5c3a\u5ea6\u7f3a\u9677\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u65e0\u4eba\u673a\u81ea\u52a8\u5316\u5de1\u68c0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01182", "abs": "https://arxiv.org/abs/2602.01182", "authors": ["Zhichao Chen", "Hao Wang", "Fangyikang Wang", "Licheng Pan", "Zhengnan Li", "Yunfei Teng", "Haoxuan Li", "Zhouchen Lin"], "title": "Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective", "comment": null, "summary": "Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.", "AI": {"tldr": "SPIRIT\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u534a\u8fd1\u7aef\u4f20\u8f93\u8ddd\u79bb\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d2\u8865\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u969c\u788d\uff1a1) \u975e\u5e73\u7a33\u65f6\u95f4\u52a8\u6001\u4f1a\u504f\u5dee\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u5bf9\u5f02\u5e38\u503c\u654f\u611f\u7684\u63d2\u8865\uff1b2) \u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u56e0\u4e3a\u63d2\u8865\u9700\u8981\u51c6\u786e\u7684\u70b9\u6062\u590d\uff0c\u800c\u6269\u6563\u6a21\u578b\u672c\u8d28\u4e0a\u662f\u8bad\u7ec3\u6765\u751f\u6210\u591a\u6837\u6837\u672c\u7684\u3002", "method": "\u4ece\u8fd1\u7aef\u7b97\u5b50\u89c6\u89d2\u5206\u6790\u6269\u6563\u6a21\u578b\u63d2\u8865\u8fc7\u7a0b\uff0c\u53d1\u73b0\u9690\u542b\u7684Wasserstein\u8ddd\u79bb\u6b63\u5219\u5316\u963b\u788d\u4e86\u6a21\u578b\u5bf9\u6297\u975e\u5e73\u7a33\u6027\u7684\u80fd\u529b\u3002\u63d0\u51faSPIRIT\u6846\u67b6\uff1a\u5f15\u5165\u71b5\u8bf1\u5bfc\u7684Bregman\u6563\u5ea6\u6765\u653e\u677eWasserstein\u8ddd\u79bb\u7684\u8d28\u91cf\u4fdd\u6301\u7ea6\u675f\uff0c\u6784\u5efa\u534a\u8fd1\u7aef\u4f20\u8f93\u8ddd\u79bb\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u5176\u5bf9\u975e\u5e73\u7a33\u6027\u7684\u9c81\u68d2\u6027\u3002\u79fb\u9664\u8017\u6563\u7ed3\u6784\uff0c\u4ee5SPT\u4f5c\u4e3a\u8fd1\u7aef\u7b97\u5b50\u6784\u5efa\u5b8c\u6574\u5de5\u4f5c\u6d41\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86SPIRIT\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u63d2\u8865\u8fc7\u7a0b\u4e2d\u7684\u5185\u5728\u9650\u5236\uff0c\u63d0\u51fa\u7684SPIRIT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u6027\u548c\u76ee\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01710", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01710", "abs": "https://arxiv.org/abs/2602.01710", "authors": ["Salma Zahran", "Zhou Ao", "Zhengyang Zhang", "Chen Chi", "Chenchen Yuan", "Yanming Wang"], "title": "Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis", "comment": null, "summary": "Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u663e\u5fae\u955c\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u573a\u6a21\u62df\u751f\u6210\u5fae\u7ed3\u6784\u5f62\u6001\uff0c\u7528CycleGAN\u5c06\u6a21\u62df\u6570\u636e\u8f6c\u6362\u4e3a\u903c\u771f\u7684SEM\u56fe\u50cf\uff0c\u8bad\u7ec3U-Net\u6a21\u578b\u5728\u5b9e\u9a8c\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4f18\u5f02\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6750\u6599\u8868\u5f81\u4e2d\u663e\u5fae\u955c\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u81ea\u52a8\u5316\u96be\u9898\uff1a\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u3001\u4e3b\u89c2\u6027\u5f3a\u4e14\u7a00\u7f3a\uff1b\u4f20\u7edf\u7269\u7406\u6a21\u62df\u6570\u636e\u4e0e\u5b9e\u9a8c\u6570\u636e\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u8ddd\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u5931\u8d25\u3002", "method": "1) \u5229\u7528\u76f8\u573a\u6a21\u62df\u751f\u6210\u5927\u91cf\u5fae\u7ed3\u6784\u5f62\u6001\u548c\u5b8c\u7f8e\u6807\u6ce8\u63a9\u7801\uff1b2) \u4f7f\u7528CycleGAN\u8fdb\u884c\u65e0\u914d\u5bf9\u56fe\u50cf\u7ffb\u8bd1\uff0c\u5c06\u5e72\u51c0\u6a21\u62df\u6570\u636e\u8f6c\u6362\u4e3a\u903c\u771f\u7684SEM\u56fe\u50cf\uff1b3) \u4ec5\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3U-Net\u5206\u5272\u6a21\u578b\u3002", "result": "\u5728\u672a\u89c1\u5b9e\u9a8c\u56fe\u50cf\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff1a\u5e73\u5747\u8fb9\u754cF1\u5206\u65700.90\uff0c\u4ea4\u5e76\u6bd40.88\u3002t-SNE\u7279\u5f81\u7a7a\u95f4\u6295\u5f71\u548c\u9999\u519c\u71b5\u5206\u6790\u8bc1\u5b9e\u5408\u6210\u56fe\u50cf\u5728\u7edf\u8ba1\u548c\u7279\u5f81\u4e0a\u4e0e\u771f\u5b9e\u6570\u636e\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "\u8be5\u751f\u6210\u6846\u67b6\u5b8c\u5168\u89e3\u8026\u6a21\u578b\u8bad\u7ec3\u4e0e\u4eba\u5de5\u6807\u6ce8\uff0c\u5c06\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u636e\u4e30\u5bcc\u95ee\u9898\uff0c\u4e3a\u52a0\u901f\u6750\u6599\u53d1\u73b0\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01186", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01186", "abs": "https://arxiv.org/abs/2602.01186", "authors": ["Fabio Turazza", "Marco Picone", "Marco Mamei"], "title": "The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics", "comment": "Accepted at the International Conference on Learning Representations (ICLR) 2026", "summary": "Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.", "AI": {"tldr": "\u63d0\u51faGH-OFL\u7cfb\u5217\u65b9\u6cd5\uff0c\u901a\u8fc7\u5047\u8bbe\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u7c7b\u6761\u4ef6\u9ad8\u65af\u6027\uff0c\u5728\u5355\u8f6e\u901a\u4fe1\u4e2d\u5b9e\u73b0\u8054\u90a6\u5b66\u4e60\uff0c\u4ec5\u4f20\u8f93\u7edf\u8ba1\u91cf\u800c\u975e\u6a21\u578b\uff0c\u65e0\u9700\u516c\u5171\u6570\u636e\u96c6\u4e14\u4fdd\u6301\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u591a\u8f6e\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u9690\u79c1\u98ce\u9669\u5927\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4e0d\u5b9e\u7528\uff0c\u8981\u4e48\u53d7\u5230\u9650\u5236\uff08\u5982\u9700\u8981\u516c\u5171\u6570\u636e\u96c6\u3001\u5047\u8bbe\u540c\u8d28\u6a21\u578b\u6216\u9700\u4e0a\u4f20\u989d\u5916\u6570\u636e\uff09\u3002", "method": "\u63d0\u51faGH-OFL\u65b9\u6cd5\u65cf\uff1a\u5047\u8bbe\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u7c7b\u6761\u4ef6\u9ad8\u65af\u6027\uff0c\u5ba2\u6237\u7aef\u4ec5\u4f20\u8f93\u7edf\u8ba1\u91cf\uff08\u6bcf\u7c7b\u8ba1\u6570\u548c\u4e00/\u4e8c\u9636\u77e9\uff09\uff0c\u670d\u52a1\u5668\u901a\u8fc7\u4e09\u79cd\u7ec4\u4ef6\u6784\u5efa\u5206\u7c7b\u5934\uff1a1) \u95ed\u5f0f\u9ad8\u65af\u5934\uff1b2) FisherMix\u7ebf\u6027\u5934\uff1b3) Proto-Hyper\u8f7b\u91cf\u6b8b\u5dee\u5934\u3002", "result": "GH-OFL\u65b9\u6cd5\u5728\u5f3a\u975eIID\u504f\u659c\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u6570\u636e\u65e0\u5173\u6027\u3002", "conclusion": "GH-OFL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u7684\u5b9e\u7528\u6027\u95ee\u9898\uff0c\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5728\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2602.01723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01723", "abs": "https://arxiv.org/abs/2602.01723", "authors": ["Yikun Ma", "Yiqing Li", "Jingwen Ye", "Zhongkai Wu", "Weidong Zhang", "Lin Gao", "Zhi Jin"], "title": "FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization", "comment": null, "summary": "Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.", "AI": {"tldr": "FastPhysGS\uff1a\u4e00\u4e2a\u5feb\u901f\u9c81\u68d2\u7684\u7269\u7406\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u611f\u77e5\u7c92\u5b50\u586b\u5145\u548c\u53cc\u5411\u56fe\u89e3\u8026\u4f18\u5316\uff0c\u57281\u5206\u949f\u5185\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\uff0c\u4ec5\u97007GB\u8fd0\u884c\u5185\u5b58\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6269\u5c55\u52304D\u7269\u7406\u6a21\u62df\u5b58\u5728\u6311\u6218\uff1a\u57fa\u4e8eMPM\u7684\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u53c2\u6570\u8c03\u6574\u6216\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u52a8\u6001\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u4f18\u5316\u6548\u7387\uff1b\u57fa\u4e8eLLMs/VLMs\u7684\u65b9\u6cd5\u5b58\u5728\u6587\u672c/\u56fe\u50cf\u52303D\u7684\u611f\u77e5\u5dee\u8ddd\uff0c\u5bfc\u81f4\u7269\u7406\u884c\u4e3a\u4e0d\u7a33\u5b9a\uff0c\u4e14\u5e38\u5ffd\u75653DGS\u7684\u8868\u9762\u7ed3\u6784\uff0c\u4ea7\u751f\u4e0d\u5408\u7406\u7684\u8fd0\u52a8\u3002", "method": "\u63d0\u51faFastPhysGS\u6846\u67b6\uff1a1\uff09\u5b9e\u4f8b\u611f\u77e5\u7c92\u5b50\u586b\u5145\uff08IPF\uff09\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u91cd\u8981\u6027\u91c7\u6837\uff08MCIS\uff09\uff0c\u9ad8\u6548\u586b\u5145\u5185\u90e8\u7c92\u5b50\u540c\u65f6\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\uff1b2\uff09\u53cc\u5411\u56fe\u89e3\u8026\u4f18\u5316\uff08BGDO\uff09\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5feb\u901f\u4f18\u5316\u4eceVLM\u9884\u6d4b\u7684\u6750\u6599\u53c2\u6570\u3002", "result": "FastPhysGS\u5728\u4ec51\u5206\u949f\u5185\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\uff0c\u4ec5\u97007GB\u8fd0\u884c\u5185\u5b58\uff0c\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "FastPhysGS\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u9c81\u68d2\u7684\u7269\u7406\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7c92\u5b50\u586b\u5145\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406\u6a21\u62df\u7684\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2602.01196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01196", "abs": "https://arxiv.org/abs/2602.01196", "authors": ["Jin Li", "Yue Wu", "Mengsha Huang", "Yuhao Sun", "Hao He", "Xianyuan Zhan"], "title": "Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies", "comment": null, "summary": "Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \\textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u5728\u8bad\u7ec3\u4e2d\u4f1a\u81ea\u53d1\u5f62\u6210\u7a33\u5b9a\u7684\u5faa\u73af\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u4e0e\u52a8\u529b\u7cfb\u7edf\u4e2d\u7684\u6781\u9650\u73af\u76f8\u4f3c\uff0c\u89e3\u91ca\u4e86\u5faa\u73af\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u63a7\u5236\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4f18\u8d8a\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7684\u5185\u5728\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5728\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u3001\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u4e2d\u5b66\u4e60\u7684\u5faa\u73af\u7b56\u7565\u7684\u9690\u85cf\u72b6\u6001\u57df\uff0c\u8bc6\u522b\u51fa\u4e0e\u73af\u5883\u4ea4\u4e92\u65f6\u51fa\u73b0\u7684\u7a33\u5b9a\u5faa\u73af\u7ed3\u6784\u3002", "result": "\u53d1\u73b0\u5faa\u73af\u7b56\u7565\u4f1a\u5f62\u6210\u4e0e\u52a8\u529b\u7cfb\u7edf\u6781\u9650\u73af\u76f8\u4f3c\u7684\u7ed3\u6784\uff0c\u8fd9\u4e9b\u6781\u9650\u73af\u7684\u51e0\u4f55\u5f62\u72b6\u4e0e\u7b56\u7565\u884c\u4e3a\u5b58\u5728\u7ed3\u6784\u5316\u5bf9\u5e94\u5173\u7cfb\uff0c\u7a33\u5b9a\u4e86\u5185\u90e8\u8bb0\u5fc6\u548c\u4efb\u52a1\u76f8\u5173\u73af\u5883\u72b6\u6001\u3002", "conclusion": "\u6781\u9650\u73af\u7684\u51fa\u73b0\u89e3\u91ca\u4e86\u5faa\u73af\u7b56\u7565\u7684\u4f18\u826f\u7279\u6027\uff1a\u7a33\u5b9a\u8bb0\u5fc6\u548c\u73af\u5883\u72b6\u6001\uff0c\u6291\u5236\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5e26\u6765\u7684\u5e72\u6270\uff0c\u5176\u51e0\u4f55\u7ed3\u6784\u7f16\u7801\u884c\u4e3a\u5173\u7cfb\uff0c\u4fc3\u8fdb\u6280\u80fd\u9002\u5e94\u975e\u5e73\u7a33\u73af\u5883\u3002"}}
{"id": "2602.01724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01724", "abs": "https://arxiv.org/abs/2602.01724", "authors": ["Tushar Anand", "Maheswar Bora", "Antitza Dantcheva", "Abhijit Das"], "title": "DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation", "comment": "IEEE International Conference on Robotics and Automation 2026", "summary": "In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.", "AI": {"tldr": "\u63d0\u51faDenVisCoM Mamba\u5757\u548c\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u8054\u5408\u4f30\u8ba1\u5149\u6d41\u548c\u89c6\u5dee", "motivation": "\u591a\u89c6\u56fe\u51e0\u4f55\u548c\u8fd0\u52a8\u4efb\u52a1\u672c\u8d28\u76f8\u5173\uff0c\u9700\u8981\u7edf\u4e00\u67b6\u6784\u540c\u65f6\u5904\u7406\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u3001\u5185\u5b58\u5360\u7528\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861", "method": "\u63d0\u51fa\u57fa\u4e8eDenVisCoM Mamba\u5757\u548cTransformer\u6ce8\u610f\u529b\u5757\u7684\u6df7\u5408\u67b6\u6784\uff0c\u4e13\u95e8\u4e3a\u5149\u6d41\u548c\u89c6\u5dee\u8054\u5408\u4f30\u8ba1\u8bbe\u8ba1\uff0c\u517c\u987e\u5b9e\u65f6\u63a8\u7406\u3001\u5185\u5b58\u5360\u7528\u548c\u51c6\u786e\u6027", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u80fd\u51c6\u786e\u5b9e\u65f6\u4f30\u8ba1\u5149\u6d41\u548c\u89c6\u5dee\uff0c\u5728\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u5904\u7406\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861", "conclusion": "\u63d0\u51fa\u7684DenVisCoM\u6df7\u5408\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3\u8fd0\u52a8\u4f30\u8ba1\u548c3D\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u7684\u8054\u5408\u5b9e\u65f6\u4f30\u8ba1\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2602.01212", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01212", "abs": "https://arxiv.org/abs/2602.01212", "authors": ["Marco Chen", "Xianbiao Qi", "Yelin He", "Jiaquan Ye", "Rong Xiao"], "title": "SimpleGPT: Improving GPT via A Simple Normalization Strategy", "comment": "We propose SimpleGPT, a simple yet effective GPT model, and provide theoretical insights into its mathematical foundations. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B", "summary": "In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\\times$-10$\\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSimpleNorm\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u7a33\u5b9a\u4e2d\u95f4\u6fc0\u6d3b\u5c3a\u5ea6\u6765\u964d\u4f4eHessian\u77e9\u9635\u7684\u8c31\u8303\u6570\uff0c\u4ece\u800c\u5141\u8bb8\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\uff0c\u5728GPT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4ece\u4e8c\u9636\u51e0\u4f55\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6Transformer\u4f18\u5316\uff0c\u63a2\u7d22\u67b6\u6784\u8bbe\u8ba1\u3001\u6fc0\u6d3b\u5c3a\u5ea6\u3001Hessian\u77e9\u9635\u4e0e\u6700\u5927\u53ef\u5bb9\u5fcd\u5b66\u4e60\u7387\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u89e3\u51b3\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u4f18\u5316\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faSimpleNorm\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6784\u9020\u7a33\u5b9a\u4e2d\u95f4\u6fc0\u6d3b\u5c3a\u5ea6\uff1b\u7406\u8bba\u5206\u6790\u635f\u5931\u51fd\u6570\u5bf9\u7f51\u7edc\u6fc0\u6d3b\u7684Hessian\u77e9\u9635\uff0c\u8bc1\u660eSimpleNorm\u80fd\u663e\u8457\u964d\u4f4eHessian\u8c31\u8303\u6570\u3002", "result": "SimpleGPT\uff08\u57fa\u4e8eSimpleNorm\u7684\u7f51\u7edc\uff09\u80fd\u5bb9\u5fcd\u6bd4\u6807\u51c6\u65b9\u6cd5\u59273-10\u500d\u7684\u5b66\u4e60\u7387\uff0c\u4f18\u5316\u7a33\u5b9a\u6027\u5f3a\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002\u57287B\u6a21\u578b60K\u6b65\u8bad\u7ec3\u4e2d\uff0c\u8bad\u7ec3\u635f\u5931\u4ece2.290\u964d\u81f32.208\u3002", "conclusion": "SimpleNorm\u901a\u8fc7\u7a33\u5b9a\u6fc0\u6d3b\u5c3a\u5ea6\u548c\u964d\u4f4eHessian\u8c31\u8303\u6570\uff0c\u6709\u6548\u63d0\u5347Transformer\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u5141\u8bb8\u4f7f\u7528\u66f4\u5927\u5b66\u4e60\u7387\uff0c\u5728\u5927\u89c4\u6a21GPT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2602.01738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01738", "abs": "https://arxiv.org/abs/2602.01738", "authors": ["Yue Zhou", "Xinan He", "Kaiqing Lin", "Bing Fan", "Feng Ding", "Bin Li"], "title": "Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models", "comment": null, "summary": "While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.", "AI": {"tldr": "\u57fa\u4e8e\u73b0\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u51bb\u7ed3\u7279\u5f81\u7684\u7b80\u5355\u7ebf\u6027\u5206\u7c7b\u5668\u5728AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u4e2d\u8d85\u8d8a\u590d\u6742\u4e13\u7528\u68c0\u6d4b\u5668\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u63d0\u5347\u51c6\u786e\u7387\u8d8530%\uff0c\u4f46\u5b58\u5728\u91cd\u6355\u83b7\u3001\u4f20\u8f93\u7b49\u5c40\u9650\u6027\u3002", "motivation": "\u4e13\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7b80\u5355\u65b9\u6cd5\u662f\u5426\u80fd\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8d85\u8d8a\u590d\u6742\u67b6\u6784\u3002", "method": "\u4f7f\u7528\u73b0\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5305\u62ecPerception Encoder\u3001MetaCLIP 2\u3001DINOv3\uff09\u7684\u51bb\u7ed3\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684\u7ebf\u6027\u5206\u7c7b\u5668\u8fdb\u884cAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4e13\u7528\u68c0\u6d4b\u5668\u76f8\u5f53\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u751f\u6210\u5668\u548c\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e2d\u663e\u8457\u8d85\u8d8a\u4e13\u7528\u68c0\u6d4b\u5668\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc730%\u3002", "conclusion": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u83b7\u5f97\u4e86\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u91cd\u6355\u83b7\u3001\u4f20\u8f93\u7b49\u5c40\u9650\u6027\u3002\u5efa\u8baeAI\u53d6\u8bc1\u9886\u57df\u5e94\u4ece\u8fc7\u62df\u5408\u9759\u6001\u57fa\u51c6\u8f6c\u5411\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e0d\u65ad\u6f14\u8fdb\u7684\u4e16\u754c\u77e5\u8bc6\u3002"}}
{"id": "2602.01217", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.01217", "abs": "https://arxiv.org/abs/2602.01217", "authors": ["Lucas Lange", "Adrian B\u00f6ttinger", "Victor Christen", "Anushka Vidanage", "Peter Christen", "Erhard Rahm"], "title": "Learning from Anonymized and Incomplete Tabular Data", "comment": null, "summary": "User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9488\u5bf9\u7528\u6237\u9a71\u52a8\u9690\u79c1\u4fdd\u62a4\u4e0b\u6df7\u5408\u539f\u59cb\u3001\u6cdb\u5316\u548c\u7f3a\u5931\u503c\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u8f6c\u6362\u7b56\u7565\u6062\u590d\u673a\u5668\u5b66\u4e60\u6548\u7528", "motivation": "\u7528\u6237\u9a71\u52a8\u9690\u79c1\u4fdd\u62a4\u5bfc\u81f4\u6570\u636e\u96c6\u4e2d\u6df7\u5408\u539f\u59cb\u503c\u3001\u6cdb\u5316\u503c\u548c\u7f3a\u5931\u503c\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c06\u975e\u539f\u59cb\u503c\u89c6\u4e3a\u65b0\u7c7b\u522b\u6216\u7f3a\u5931\u503c\uff0c\u4e22\u5f03\u4e86\u6cdb\u5316\u8bed\u4e49\uff0c\u9700\u8981\u65b0\u7684\u5904\u7406\u65b9\u6cd5", "method": "\u63d0\u51fa\u8003\u8651\u5f02\u6784\u533f\u540d\u5316\u7684\u65b0\u578b\u6570\u636e\u8f6c\u6362\u7b56\u7565\uff0c\u4e0e\u6807\u51c6\u63d2\u8865\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\uff0c\u4f7f\u7528\u591a\u4e2a\u6570\u636e\u96c6\u3001\u9690\u79c1\u914d\u7f6e\u548c\u90e8\u7f72\u573a\u666f", "result": "\u65b9\u6cd5\u80fd\u53ef\u9760\u6062\u590d\u6548\u7528\uff0c\u6cdb\u5316\u503c\u4f18\u4e8e\u7eaf\u6291\u5236\uff0c\u6700\u4f73\u6570\u636e\u51c6\u5907\u7b56\u7565\u53d6\u51b3\u4e8e\u5177\u4f53\u573a\u666f\uff0c\u4e00\u81f4\u7684\u6570\u636e\u8868\u793a\u5bf9\u4fdd\u6301\u4e0b\u6e38\u6548\u7528\u81f3\u5173\u91cd\u8981", "conclusion": "\u6709\u6548\u5b66\u4e60\u4e0e\u9002\u5f53\u5904\u7406\u533f\u540d\u5316\u503c\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8981\u6839\u636e\u9690\u79c1\u914d\u7f6e\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5"}}
{"id": "2602.01741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01741", "abs": "https://arxiv.org/abs/2602.01741", "authors": ["Sicheng Pan", "Chen Tang", "Shuzhao Xie", "Ke Yang", "Weixiang Zhang", "Jiawei Li", "Bin Chen", "Shu-Tao Xia", "Zhi Wang"], "title": "Tail-Aware Post-Training Quantization for 3D Geometry Models", "comment": null, "summary": "The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.", "AI": {"tldr": "TAPTQ\uff1a\u9488\u5bf93D\u51e0\u4f55\u5b66\u4e60\u7684\u5c3e\u90e8\u611f\u77e5\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u6821\u51c6\u6784\u9020\u3001\u4e09\u5143\u641c\u7d22\u4f18\u5316\u548cTRE\u5f15\u5bfc\u7684\u6a21\u5757\u8865\u507f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6821\u51c6\u65f6\u95f4\u3002", "motivation": "3D\u51e0\u4f55\u6a21\u578b\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u5bf9\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u90e8\u7f72\u6784\u6210\u6311\u6218\u3002\u4f20\u7edf\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf92D\u89c6\u89c9Transformer\u4f18\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u8fc1\u79fb\u52303D\u6a21\u578b\uff0c\u56e0\u4e3a3D\u6a21\u578b\u5177\u6709\u590d\u6742\u7684\u7279\u5f81\u5206\u5e03\u548c\u8fc7\u9ad8\u7684\u6821\u51c6\u5f00\u9500\u3002", "method": "\u63d0\u51faTAPTQ\uff08\u5c3e\u90e8\u611f\u77e5\u540e\u8bad\u7ec3\u91cf\u5316\uff09\u7ba1\u9053\uff1a1\uff09\u6e10\u8fdb\u7c97\u5230\u7ec6\u6821\u51c6\u6784\u9020\u7b56\u7565\uff0c\u6784\u5efa\u7d27\u51d1\u5b50\u96c6\u5b9e\u73b0\u7edf\u8ba1\u7eaf\u5ea6\u548c\u51e0\u4f55\u4ee3\u8868\u6027\uff1b2\uff09\u5c06\u91cf\u5316\u533a\u95f4\u641c\u7d22\u91cd\u65b0\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165\u4e09\u5143\u641c\u7d22\u6c42\u89e3\u5668\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(N)\u964d\u4f4e\u5230O(log N)\uff1b3\uff09TRE\u5f15\u5bfc\u7684\u6a21\u5757\u8865\u507f\uff0c\u4f7f\u7528\u5c3e\u90e8\u76f8\u5bf9\u8bef\u5dee\u6307\u6807\u81ea\u9002\u5e94\u8bc6\u522b\u548c\u4fee\u6b63\u5bf9\u957f\u5c3e\u6fc0\u6d3b\u5f02\u5e38\u503c\u654f\u611f\u7684\u6a21\u5757\u4e2d\u7684\u5931\u771f\u3002", "result": "\u5728VGGT\u548cPi3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTAPTQ\u5728\u7cbe\u5ea6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6821\u51c6\u65f6\u95f4\u3002", "conclusion": "TAPTQ\u4e3a3D\u51e0\u4f55\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u57283D\u6a21\u578b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.01219", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01219", "abs": "https://arxiv.org/abs/2602.01219", "authors": ["Qishuai Wen", "Zhiyuan Huang", "Xianghan Meng", "Wei He", "Chun-Guang Li"], "title": "MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations", "comment": null, "summary": "The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.", "AI": {"tldr": "\u63d0\u51faMiTA\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u8def\u7531\u7b56\u7565\u5c06\u4f20\u7edf\u6ce8\u610f\u529b\u4e2d\u7684N\u5bbd\u5ea6MLP\u538b\u7f29\u4e3a\u66f4\u7a84\u7684MLP\uff0c\u4f7f\u7528\u5730\u6807\u67e5\u8be2\u548ctop-k\u6fc0\u6d3b\u952e\u503c\u5bf9\u6784\u5efa\u53ef\u53d8\u5f62\u4e13\u5bb6\u3002", "motivation": "\u4f20\u7edfTransformer\u6ce8\u610f\u529b\u53ef\u89c6\u4e3a\u4e24\u5c42\u5feb\u901f\u6743\u91cdMLP\uff0c\u5176\u5bbd\u5ea6\u7b49\u4e8e\u5e8f\u5217\u957f\u5ea6N\u3002\u968f\u7740\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u8fd9\u79cdN\u5bbd\u5ea6MLP\u7684\u8868\u8fbe\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u5feb\u901f\u6743\u91cd\u7684\u6269\u5c55\u5bf9\u8d85\u957f\u5e8f\u5217\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u53c8\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u63d0\u51fa\u538b\u7f29-\u8def\u7531\u7b56\u7565\uff1a1) \u4f7f\u7528\u5c11\u91cf\u5730\u6807\u67e5\u8be2\u5c06N\u5bbd\u5ea6MLP\u538b\u7f29\u4e3a\u66f4\u7a84\u7684MLP\uff1b2) \u4e3a\u6bcf\u4e2a\u5730\u6807\u67e5\u8be2\u6536\u96c6top-k\u6fc0\u6d3b\u7684\u952e\u503c\u5bf9\u6784\u5efa\u53ef\u53d8\u5f62\u4e13\u5bb6\uff1b3) \u5c06\u8fd9\u79cd\u7b56\u7565\u79f0\u4e3aTop-k\u6fc0\u6d3b\u6df7\u5408(MiTA)\uff0c\u76f8\u5e94\u7684\u6ce8\u610f\u529b\u673a\u5236\u79f0\u4e3aMiTA\u6ce8\u610f\u529b\u3002", "result": "\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660eMiTA\u6ce8\u610f\u529b\u5177\u6709\u6f5c\u529b\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u4f18\u5316\u548c\u5728\u66f4\u5177\u6311\u6218\u6027\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "conclusion": "\u5c06\u9ad8\u6548\u6ce8\u610f\u529b\u65b9\u6cd5\u7edf\u4e00\u89e3\u91ca\u4e3a\u901a\u8fc7\u8def\u7531\u548c/\u6216\u538b\u7f29\u6765\u6269\u5c55\u5feb\u901f\u6743\u91cd\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u7684MiTA\u6ce8\u610f\u529b\u901a\u8fc7\u538b\u7f29-\u8def\u7531\u7b56\u7565\u5728\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01322", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01322", "abs": "https://arxiv.org/abs/2602.01322", "authors": ["Panagiotis Koromilas", "Andreas D. Demou", "James Oldfield", "Yannis Panagakis", "Mihalis Nicolaou"], "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.", "AI": {"tldr": "PolySAE\u901a\u8fc7\u5f15\u5165\u9ad8\u9636\u591a\u9879\u5f0f\u9879\u6269\u5c55\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u6355\u6349\u7279\u5f81\u95f4\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSAE\u53ea\u80fd\u7ebf\u6027\u7ec4\u5408\u7279\u5f81\u3001\u65e0\u6cd5\u533a\u5206\u7ec4\u5408\u4e0e\u5171\u73b0\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5047\u8bbe\u7279\u5f81\u901a\u8fc7\u7ebf\u6027\u91cd\u6784\u76f8\u52a0\uff0c\u65e0\u6cd5\u6355\u6349\u7ec4\u5408\u7ed3\u6784\u3002\u7ebf\u6027\u6a21\u578b\u65e0\u6cd5\u533a\u5206\"Starbucks\"\u662f\u6765\u81ea\"star\"\u548c\"coffee\"\u7279\u5f81\u7684\u7ec4\u5408\u8fd8\u662f\u4ec5\u4ec5\u5171\u73b0\uff0c\u5bfc\u81f4SAE\u4e3a\u590d\u5408\u6982\u5ff5\u5206\u914d\u6574\u4f53\u7279\u5f81\u800c\u975e\u53ef\u89e3\u91ca\u7684\u7ec4\u6210\u90e8\u5206\u3002", "method": "PolySAE\u6269\u5c55SAE\u89e3\u7801\u5668\uff0c\u5f15\u5165\u9ad8\u9636\u9879\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u7559\u7ebf\u6027\u7f16\u7801\u5668\u4ee5\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u5b50\u7a7a\u95f4\u4e0a\u7684\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\uff0c\u6355\u6349\u7279\u5f81\u5bf9\u548c\u4e09\u5143\u7ec4\u4ea4\u4e92\uff0c\u53c2\u6570\u5f00\u9500\u5c0f(GPT2\u4e0a\u4ec53%)\u3002", "result": "\u5728\u56db\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e09\u79cdSAE\u53d8\u4f53\u4e0a\uff0cPolySAE\u5e73\u5747\u63d0\u5347\u7ea68%\u7684\u63a2\u6d4bF1\u5206\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u91cd\u6784\u8bef\u5dee\uff0c\u5e76\u4ea7\u751f2-10\u500d\u5927\u7684\u7c7b\u522b\u6761\u4ef6\u7279\u5f81\u5206\u5e03Wasserstein\u8ddd\u79bb\u3002\u5b66\u4e60\u5230\u7684\u4ea4\u4e92\u6743\u91cd\u4e0e\u5171\u73b0\u9891\u7387\u76f8\u5173\u6027\u6781\u4f4e(r=0.06 vs SAE\u7279\u5f81\u534f\u65b9\u5dee\u7684r=0.82)\u3002", "conclusion": "\u591a\u9879\u5f0f\u9879\u6355\u6349\u4e86\u7ec4\u5408\u7ed3\u6784(\u5982\u5f62\u6001\u7ed1\u5b9a\u548c\u77ed\u8bed\u7ec4\u5408)\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u72ec\u7acb\u4e8e\u8868\u9762\u7edf\u8ba1\u7279\u5f81\uff0c\u8868\u660ePolySAE\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u8bed\u8a00\u4e2d\u7684\u7ec4\u5408\u6027\u3002"}}
{"id": "2602.01753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01753", "abs": "https://arxiv.org/abs/2602.01753", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ObjEmbed: Towards Universal Multimodal Object Embeddings", "comment": null, "summary": "Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.", "AI": {"tldr": "ObjEmbed\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u533a\u57df\u5d4c\u5165\uff08\u6bcf\u4e2a\u5bf9\u5e94\u4e00\u4e2a\u5bf9\u8c61\uff09\u548c\u5168\u5c40\u5d4c\u5165\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u652f\u6301\u89c6\u89c9\u5b9a\u4f4d\u3001\u5c40\u90e8\u56fe\u50cf\u68c0\u7d22\u548c\u5168\u5c40\u56fe\u50cf\u68c0\u7d22\u7b49\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u867d\u7136\u5728\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u56fe\u50cf\u533a\u57df\u4e0e\u7279\u5b9a\u77ed\u8bed\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u5bf9\u8c61\u7ea7\u8bed\u4e49\u5339\u914d\u548c\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6a21\u578b\u3002", "method": "ObjEmbed\u5c06\u8f93\u5165\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u533a\u57df\u5d4c\u5165\uff08\u6bcf\u4e2a\u5bf9\u5e94\u4e00\u4e2a\u5bf9\u8c61\uff09\u548c\u5168\u5c40\u5d4c\u5165\u3002\u4e3a\u6bcf\u4e2a\u533a\u57df\u751f\u6210\u4e24\u79cd\u4e92\u8865\u5d4c\u5165\uff1a\u7528\u4e8e\u8bed\u4e49\u5339\u914d\u7684\u5bf9\u8c61\u5d4c\u5165\u548c\u9884\u6d4b\u5b9a\u4f4d\u8d28\u91cf\u7684IoU\u5d4c\u5165\u3002\u6700\u7ec8\u7684\u5bf9\u8c61\u5339\u914d\u5206\u6570\u7ed3\u5408\u4e86\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u9884\u6d4b\u7684IoU\u3002", "result": "\u572818\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8bed\u4e49\u5224\u522b\u80fd\u529b\u3002\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5730\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u7f16\u7801\u56fe\u50cf\u4e2d\u7684\u6240\u6709\u5bf9\u8c61\u548c\u5b8c\u6574\u56fe\u50cf\u3002", "conclusion": "ObjEmbed\u901a\u8fc7\u5bf9\u8c61\u5bfc\u5411\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u5177\u6709\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u7f16\u7801\u7684\u7279\u70b9\uff0c\u4e3a\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01233", "abs": "https://arxiv.org/abs/2602.01233", "authors": ["Tianhao Miao", "Zhongyuan Bao", "Lejun Zhang"], "title": "Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching", "comment": null, "summary": "Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.", "AI": {"tldr": "Lotus\u662f\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6295\u5f71\u8fc7\u7a0b\u89e3\u51b3\u5185\u5b58\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u76f8\u6bd4GaLore\u51cf\u5c1130%\u8bad\u7ec3\u65f6\u95f4\u548c40%\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u5b58\u5728\u5185\u5b58\u6d88\u8017\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002GaLore\u867d\u7136\u80fd\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u5bf9\u68af\u5ea6\u8fdb\u884cSVD\u5206\u89e3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u6210\u672c\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u63d0\u51faLotus\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u6295\u5f71\u8fc7\u7a0b\uff0c\u5f15\u5165\u91cf\u5316\u5355\u4f4d\u68af\u5ea6\u4f4d\u79fb\u7684\u51c6\u5219\uff0c\u5b9e\u73b0\u4f4e\u79e9\u68af\u5ea6\u5b50\u7a7a\u95f4\u4e4b\u95f4\u7684\u9ad8\u6548\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLotus\u662f\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1130%\u8bad\u7ec3\u65f6\u95f4\u548c40%\u68af\u5ea6\u53ca\u4f18\u5316\u5668\u72b6\u6001\u7684\u5185\u5b58\u6d88\u8017\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Lotus\u901a\u8fc7\u7b80\u5355\u7684\u6295\u5f71\u8fc7\u7a0b\u4fee\u6539\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u5185\u5b58\u3001\u65f6\u95f4\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002"}}
{"id": "2602.01365", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01365", "abs": "https://arxiv.org/abs/2602.01365", "authors": ["Wang Yang", "Shouren Wang", "Chaoda Song", "Chuang Ma", "Xinpeng Li", "Nengbo Wang", "Kaixiong Zhou", "Vipin Chaudhary", "Xiaotian Han"], "title": "When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\\rightarrow$science achieves 83\\% / 41\\% accuracy on math / science, while reversing the order to science$\\rightarrow$math degrades performance to 77\\% / 25\\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\\% to 56\\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.", "AI": {"tldr": "GRPO\u5728\u591a\u9886\u57df\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\u6027\u3001\u987a\u5e8f\u654f\u611f\u6027\u548c\u7b56\u7565\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u9886\u57df\u611f\u77e5\u548c\u987a\u5e8f\u611f\u77e5\u7684\u8bad\u7ec3\u8bbe\u8ba1\u3002", "motivation": "\u5c3d\u7ba1GRPO\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u9886\u57df\u6392\u5e8f\u7b56\u7565\u4e0b\u7684\u884c\u4e3a\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u7279\u522b\u662f\u987a\u5e8f\u8bad\u7ec3\u4e0e\u6df7\u5408\u9886\u57df\u8bad\u7ec3\u7684\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5bf9\u6570\u5b66\u3001\u79d1\u5b66\u3001\u903b\u8f91\u548c\u8c1c\u9898\u63a8\u7406\u4efb\u52a1\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u6bd4\u8f83\u987a\u5e8f\u8bad\u7ec3\uff08\u4e00\u6b21\u4e00\u4e2a\u9886\u57df\uff09\u4e0e\u6df7\u5408\u9886\u57df\u8bad\u7ec3\uff08\u591a\u4e2a\u9886\u57df\u540c\u65f6\uff09\u7684\u6548\u679c\uff0c\u7814\u7a76\u8bad\u7ec3\u987a\u5e8f\u5bf9\u8de8\u9886\u57df\u4ea4\u4e92\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\uff1a(1) \u5355\u9886\u57df\u6cdb\u5316\u9ad8\u5ea6\u4e0d\u5bf9\u79f0\uff1a\u5176\u4ed6\u9886\u57df\u8bad\u7ec3\u4f7f\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u7ea625%\uff0c\u4f46\u5bf9\u903b\u8f91\u548c\u8c1c\u9898\u51e0\u4e4e\u65e0\u8f6c\u79fb\uff1b(2) \u8de8\u9886\u57df\u4ea4\u4e92\u9ad8\u5ea6\u4f9d\u8d56\u987a\u5e8f\uff1a\u6570\u5b66\u2192\u79d1\u5b66\u987a\u5e8f\u83b7\u5f9783%/41%\u51c6\u786e\u7387\uff0c\u800c\u79d1\u5b66\u2192\u6570\u5b66\u987a\u5e8f\u964d\u81f377%/25%\uff1b(3) \u591a\u9886\u57df\u8bad\u7ec3\u65e0\u5355\u4e00\u6700\u4f18\u7b56\u7565\uff1a\u987a\u5e8f\u8bad\u7ec3\u5229\u4e8e\u6570\u5b66\uff08\u8fbe84%\uff09\uff0c\u6df7\u5408\u8bad\u7ec3\u5229\u4e8e\u79d1\u5b66\u548c\u903b\u8f91\uff0c\u4e0d\u826f\u6392\u5e8f\u53ef\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff08\u4ece70%\u523056%\uff09\u3002", "conclusion": "GRPO\u5728\u591a\u9886\u57df\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u5bf9\u79f0\u6027\u3001\u987a\u5e8f\u654f\u611f\u6027\u548c\u7b56\u7565\u4f9d\u8d56\u6027\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u611f\u77e5\u548c\u987a\u5e8f\u611f\u77e5\u8bad\u7ec3\u8bbe\u8ba1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2602.01754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01754", "abs": "https://arxiv.org/abs/2602.01754", "authors": ["Gustavo P. C. P. da Luz", "Alvaro M. Aspilcueta Narvaez", "Tiago Godoi Bannwart", "Gabriel Massuyoshi Sato", "Luis Fernando Gomez Gonzalez", "Juliana Freitag Borin"], "title": "Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration", "comment": "Submitted to Journal of Internet Services and Applications, 27 pages, 20 figures, 3 tables", "summary": "Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u611f\u77e5\u5339\u914d\u548c\u81ea\u9002\u5e94\u8fb9\u754c\u6846\u5206\u533a\u7684\u667a\u80fd\u505c\u8f66\u4f4d\u7ea7\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b098.80%\u7684\u51c6\u786e\u7387\u548c8\u79d2\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u5f15\u5165\u6570\u5b57\u5b6a\u751f\u57fa\u5ea7\u548c\u57fa\u4e8e\u7535\u89c6\u76d2\u7684\u5e94\u7528\u670d\u52a1\u5668\u3002", "motivation": "\u73b0\u6709\u505c\u8f66\u76d1\u63a7\u7cfb\u7edf\u53ea\u80fd\u4f30\u8ba1\u533a\u57df\u5185\u7684\u7a7a\u95f2\u8f66\u4f4d\u6570\u91cf\uff0c\u65e0\u6cd5\u63d0\u4f9b\u8f66\u4f4d\u7ea7\u522b\u7684\u8be6\u7ec6\u4fe1\u606f\u548c\u652f\u6491\u66f4\u9ad8\u7ea7\u7684\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u8f66\u4f4d\u7ea7\u76d1\u6d4b\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "method": "1. \u91c7\u7528\u57fa\u4e8e\u7a7a\u95f4\u5bb9\u5dee\u7684\u8ddd\u79bb\u611f\u77e5\u5339\u914d\u65b9\u6cd5\u8fdb\u884c\u8f66\u4f4d\u7ea7\u76d1\u63a7\uff1b2. \u5f15\u5165\u81ea\u9002\u5e94\u8fb9\u754c\u6846\u5206\u533a\u65b9\u6cd5\u5904\u7406\u590d\u6742\u505c\u8f66\u7a7a\u95f4\uff1b3. \u4f7f\u7528YOLOv11m\u6a21\u578b\uff0840.5MB\u5927\u5c0f\uff09\uff1b4. \u5f00\u53d1\u6570\u5b57\u5b6a\u751f\u57fa\u5ea7\u548c\u57fa\u4e8e\u7535\u89c6\u76d2\u7684\u5e94\u7528\u670d\u52a1\u5668\u3002", "result": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e8698.80%\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c8\u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86YOLOv11m\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u652f\u6301\u4e86\u8f66\u4f4d\u7ea7\u76d1\u63a7\u548c\u66f4\u9ad8\u7ea7\u7684\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f66\u4f4d\u7ea7\u76d1\u63a7\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u57fa\u5ea7\u548c\u786c\u4ef6\u91cd\u7528\u65b9\u6848\u63a8\u52a8\u4e86\u667a\u80fd\u505c\u8f66\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u548c\u529f\u80fd\u6269\u5c55\u3002"}}
{"id": "2602.01247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01247", "abs": "https://arxiv.org/abs/2602.01247", "authors": ["Maryam Maghsoudi", "Ayushi Mishra"], "title": "Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes", "comment": null, "summary": "Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63a2\u7a76\u8111\u5230\u8bed\u97f3\u89e3\u7801\u6a21\u578b\u4e2d\u4e0d\u540c\u8bed\u97f3\u6a21\u6001\uff08\u53d1\u58f0\u3001\u9ed8\u8bfb\u3001\u60f3\u8c61\uff09\u7684\u5185\u90e8\u8868\u5f81\u673a\u5236\uff0c\u53d1\u73b0\u8bed\u97f3\u6a21\u6001\u4f4d\u4e8e\u5171\u4eab\u7684\u8fde\u7eed\u56e0\u679c\u6d41\u5f62\u4e0a\uff0c\u8de8\u6a21\u6001\u8f6c\u6362\u7531\u7d27\u51d1\u7684\u5c42\u7279\u5b9a\u5b50\u7a7a\u95f4\u4ecb\u5bfc\u3002", "motivation": "\u867d\u7136\u8111\u5230\u8bed\u97f3\u89e3\u7801\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u97f3\u6a21\u6001\uff08\u53d1\u58f0\u3001\u9ed8\u8bfb\u3001\u60f3\u8c61\uff09\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u6355\u6349\u548c\u4f20\u9012\u4e0d\u540c\u8bed\u97f3\u6a21\u6001\u4fe1\u606f\u7684\u6839\u672c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4ece\u56e0\u679c\u89d2\u5ea6\u63a2\u7a76\u795e\u7ecf\u8bed\u97f3\u89e3\u7801\u5668\u7684\u5185\u90e8\u8868\u5f81\u673a\u5236\u3002", "method": "\u91c7\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a1\uff09\u8de8\u6a21\u6001\u6fc0\u6d3b\u4fee\u8865\uff0c\u5728\u4e0d\u540c\u8bed\u97f3\u6a21\u6001\u95f4\u4fee\u8865\u5185\u90e8\u6fc0\u6d3b\uff1b2\uff09\u4e09\u6a21\u6001\u63d2\u503c\uff0c\u68c0\u9a8c\u8bed\u97f3\u8868\u5f81\u662f\u79bb\u6563\u8fd8\u662f\u8fde\u7eed\u53d8\u5316\uff1b3\uff09\u7c97\u5230\u7ec6\u56e0\u679c\u8ffd\u8e2a\u548c\u56e0\u679c\u64e6\u6d17\uff0c\u5b9a\u4f4d\u56e0\u679c\u7ed3\u6784\uff1b4\uff09\u795e\u7ecf\u5143\u7ea7\u6fc0\u6d3b\u4fee\u8865\uff0c\u63a2\u7a76\u6548\u5e94\u5728\u5c42\u5185\u7684\u5206\u5e03\u7cbe\u7ec6\u7a0b\u5ea6\u3002", "result": "\u53d1\u73b0\uff1a1\uff09\u8bed\u97f3\u6a21\u6001\u4f4d\u4e8e\u5171\u4eab\u7684\u8fde\u7eed\u56e0\u679c\u6d41\u5f62\u4e0a\uff1b2\uff09\u8de8\u6a21\u6001\u8f6c\u6362\u7531\u7d27\u51d1\u7684\u5c42\u7279\u5b9a\u5b50\u7a7a\u95f4\u4ecb\u5bfc\uff0c\u800c\u975e\u6269\u6563\u6d3b\u52a8\uff1b3\uff09\u5f71\u54cd\u8de8\u6a21\u6001\u8f6c\u6362\u7684\u662f\u5c0f\u800c\u975e\u5206\u5e03\u7684\u5b50\u96c6\u795e\u7ecf\u5143\uff0c\u800c\u975e\u5b64\u7acb\u5355\u5143\uff1b4\uff09\u63ed\u793a\u4e86\u8de8\u8bed\u97f3\u6a21\u6001\u7684\u5c42\u6b21\u6027\u548c\u65b9\u5411\u4f9d\u8d56\u6027\u8868\u5f81\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8111\u5230\u8bed\u97f3\u89e3\u7801\u6a21\u578b\u4e2d\u8bed\u97f3\u6a21\u6001\u4fe1\u606f\u7684\u7ec4\u7ec7\u548c\u4f7f\u7528\u63d0\u4f9b\u4e86\u56e0\u679c\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u8de8\u8bed\u97f3\u6a21\u6001\u7684\u5c42\u6b21\u6027\u548c\u65b9\u5411\u4f9d\u8d56\u6027\u8868\u5f81\u7ed3\u6784\uff0c\u8868\u660e\u8de8\u6a21\u6001\u8f6c\u6362\u7531\u7d27\u51d1\u7684\u5c42\u7279\u5b9a\u5b50\u7a7a\u95f4\u4ecb\u5bfc\uff0c\u8bed\u97f3\u6a21\u6001\u4f4d\u4e8e\u5171\u4eab\u7684\u8fde\u7eed\u56e0\u679c\u6d41\u5f62\u4e0a\u3002"}}
{"id": "2602.01442", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01442", "abs": "https://arxiv.org/abs/2602.01442", "authors": ["Donald Ye"], "title": "The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks", "comment": "8 pages, 4 figures. Submitted to the ICLR 2026 Workshop on Latent & Implicit Thinking (LIT). Code:https://anonymous.4open.science/r/ICLR_2026_LIT-workshop_CG-D42B", "summary": "Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \\textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($\u03c1=0.73$ for reversal), this relationship collapses as task complexity increases ($\u03c1=0.32$ for sorting), sometimes becoming inverted ($\u03c1=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \\textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u5927\u5c0f\u4e0e\u56e0\u679c\u91cd\u8981\u6027\u4e4b\u95f4\u5b58\u5728\"\u68af\u5ea6-\u56e0\u679c\u9e3f\u6c9f\"\uff1a\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u68af\u5ea6\u5927\u7684\u7ec4\u4ef6\u53ef\u80fd\u4e0d\u91cd\u8981\uff0c\u800c\u68af\u5ea6\u5c0f\u7684\u7ec4\u4ef6\u53cd\u800c\u5173\u952e\uff0c\u5bfc\u81f4\u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\u4e0d\u53ef\u9760\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u5927\u5c0f\u7684\u526a\u679d\u65b9\u6cd5\u5047\u8bbe\u9ad8\u68af\u5ea6\u7ec4\u4ef6\u91cd\u8981\uff0c\u4f4e\u68af\u5ea6\u7ec4\u4ef6\u4e0d\u91cd\u8981\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e00\u5047\u8bbe\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u53ef\u80fd\u5931\u6548\uff0c\u9700\u8981\u63a2\u7a76\u68af\u5ea6\u5927\u5c0f\u4e0e\u56e0\u679c\u91cd\u8981\u6027\u4e4b\u95f4\u7684\u771f\u5b9e\u5173\u7cfb\u3002", "method": "\u5728Transformer\u6a21\u578b\u4e0a\u8bad\u7ec3\u7b97\u6cd5\u4efb\u52a1\uff08\u5982\u53cd\u8f6c\u3001\u6392\u5e8f\uff09\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\"\u68af\u5ea6-\u56e0\u679c\u9e3f\u6c9f\"\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u5206\u6790\uff08\u03c1\uff09\u91cf\u5316\u68af\u5ea6\u5927\u5c0f\u4e0e\u56e0\u679c\u91cd\u8981\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u8fdb\u884c\u526a\u679d\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7b80\u5355\u4efb\u52a1\u4e2d\u68af\u5ea6\u4e0e\u56e0\u679c\u91cd\u8981\u6027\u6b63\u76f8\u5173\uff08\u03c1=0.73\uff09\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u76f8\u5173\u6027\u4e0b\u964d\uff08\u03c1=0.32\uff09\u751a\u81f3\u53cd\u8f6c\uff08\u03c1=-0.11\uff09\u3002\u526a\u679d\u5b9e\u9a8c\u663e\u793a\uff1a\u79fb\u9664\u4f4e\u68af\u5ea6\"\u9690\u85cf\u82f1\u96c4\"\u4f1a\u4e25\u91cd\u635f\u5bb3OOD\u51c6\u786e\u7387\uff08-32%\uff09\uff0c\u79fb\u9664\u9ad8\u68af\u5ea6\"\u68af\u5ea6\u81a8\u80c0\"\u5219\u7ed3\u679c\u4e0d\u53ef\u9884\u6d4b\u3002", "conclusion": "\u68af\u5ea6\u5927\u5c0f\u4e0d\u80fd\u53ef\u9760\u5730\u6307\u793a\u7ec4\u4ef6\u91cd\u8981\u6027\uff0c\u57fa\u4e8e\u68af\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u65e0\u6cd5\u7a33\u5b9a\u5730\u4fdd\u7559\u6a21\u578b\u80fd\u529b\uff0c\u56e0\u4e3a\u68af\u5ea6\u4e0e\u56e0\u679c\u91cd\u8981\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u53d8\u5f97\u4e0d\u53ef\u9884\u6d4b\u3002"}}
{"id": "2602.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01756", "abs": "https://arxiv.org/abs/2602.01756", "authors": ["Jun He", "Junyan Ye", "Zilong Huang", "Dongzhi Jiang", "Chenjue Zhang", "Leqi Zhu", "Renrui Zhang", "Xiang Zhang", "Weijia Li"], "title": "Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation", "comment": "36 pages, 24 figures", "summary": "While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", "AI": {"tldr": "Mind-Brush\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u667a\u80fd\u6846\u67b6\uff0c\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u52a8\u6001\u7684\u77e5\u8bc6\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\"\u601d\u8003-\u7814\u7a76-\u521b\u9020\"\u8303\u5f0f\u4e3b\u52a8\u68c0\u7d22\u591a\u6a21\u6001\u8bc1\u636e\u5e76\u5229\u7528\u63a8\u7406\u5de5\u5177\u89e3\u51b3\u9690\u5f0f\u89c6\u89c9\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u4f5c\u4e3a\u9759\u6001\u7684\u6587\u672c\u5230\u50cf\u7d20\u89e3\u7801\u5668\uff0c\u96be\u4ee5\u7406\u89e3\u7528\u6237\u9690\u542b\u610f\u56fe\u3002\u867d\u7136\u65b0\u5174\u7684\u7edf\u4e00\u7406\u89e3-\u751f\u6210\u6a21\u578b\u6539\u5584\u4e86\u610f\u56fe\u7406\u89e3\uff0c\u4f46\u4ecd\u65e0\u6cd5\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5b8c\u6210\u590d\u6742\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\uff0c\u4e14\u53d7\u9650\u4e8e\u9759\u6001\u5185\u90e8\u5148\u9a8c\uff0c\u65e0\u6cd5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51faMind-Brush\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\"\u601d\u8003-\u7814\u7a76-\u521b\u9020\"\u8303\u5f0f\uff1a1\uff09\u4e3b\u52a8\u68c0\u7d22\u591a\u6a21\u6001\u8bc1\u636e\u4ee5\u652f\u6491\u5206\u5e03\u5916\u6982\u5ff5\uff1b2\uff09\u4f7f\u7528\u63a8\u7406\u5de5\u5177\u89e3\u51b3\u9690\u5f0f\u89c6\u89c9\u7ea6\u675f\uff1b3\uff09\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u52a8\u6001\u7684\u77e5\u8bc6\u9a71\u52a8\u5de5\u4f5c\u6d41\u3002\u540c\u65f6\u63d0\u51faMind-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u5b9e\u65f6\u65b0\u95fb\u3001\u65b0\u5174\u6982\u5ff5\u3001\u6570\u5b66\u548c\u5730\u7406\u63a8\u7406\u7b49\u9886\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMind-Brush\u663e\u8457\u589e\u5f3a\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5728Mind-Bench\u57fa\u51c6\u4e0a\u4e3aQwen-Image\u57fa\u7ebf\u5b9e\u73b0\u4e86\u4ece\u96f6\u5230\u4e00\u7684\u80fd\u529b\u98de\u8dc3\uff0c\u540c\u65f6\u5728WISE\u548cRISE\u7b49\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "Mind-Brush\u901a\u8fc7\u5c06\u751f\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u52a8\u6001\u7684\u77e5\u8bc6\u9a71\u52a8\u5de5\u4f5c\u6d41\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u9690\u542b\u610f\u56fe\u3001\u590d\u6742\u77e5\u8bc6\u63a8\u7406\u548c\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u53d8\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u9886\u57df\u5e26\u6765\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2602.01260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01260", "abs": "https://arxiv.org/abs/2602.01260", "authors": ["Soumyadeep Roy", "Shashwat Kushwaha", "Ambedkar Dukkipati"], "title": "Sample Efficient Active Algorithms for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $\u03b5$-optimal policy can be learned with ${\\mathcal{O}}(1/\u03b5^2)$ active transitions, improving upon the $\u03a9(1/\u03b5^2(1-\u03b3)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ActiveRL\u7b97\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u5728\u7ebf\u4ea4\u4e92\u9009\u62e9\u6027\u4f18\u5316\u4ef7\u503c\u51fd\u6570\u7684\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u6bd4\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u66f4\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9700\u8981\u5728\u7ebf\u4ea4\u4e92\u6765\u9009\u62e9\u6027\u4f18\u5316\u4ef7\u503c\u51fd\u6570\u7684\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790", "method": "\u63d0\u51faActiveRL\u7b97\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u901a\u8fc7GP\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u4fe1\u606f\u589e\u76ca\u754c\u9650\u8fdb\u884c\u7406\u8bba\u5206\u6790", "result": "\u8bc1\u660e\u4e86\u03b5-\u6700\u4f18\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7O(1/\u03b5\u00b2)\u6b21\u4e3b\u52a8\u8f6c\u79fb\u5b66\u4e60\uff0c\u4f18\u4e8e\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u7684\u03a9(1/\u03b5\u00b2(1-\u03b3)\u2074)\u901f\u7387\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u4fe1\u606f\u6548\u7387", "conclusion": "ActiveRL\u901a\u8fc7\u5f15\u5bfc\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u5b9e\u73b0\u4e86\u52a0\u901f\u4ef7\u503c\u51fd\u6570\u6536\u655b\uff0c\u4ee5\u6700\u5c11\u7684\u5728\u7ebf\u6570\u636e\u8fbe\u5230\u66f4\u597d\u7684\u6027\u80fd\uff0c\u8fde\u63a5\u4e86\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u56de\u5f52\u548c\u5f3a\u5316\u5b66\u4e60\u7406\u8bba"}}
{"id": "2602.01523", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01523", "abs": "https://arxiv.org/abs/2602.01523", "authors": ["Akifumi Wachi", "Hirota Kinoshita", "Shokichi Takakura", "Rei Higuchi", "Taiji Suzuki"], "title": "A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning", "comment": "28 pages", "summary": "Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \\emph{relative-budget} theory explaining this variation through a single quantity called relative budget $\u03be:= H/\\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $\u03be$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \\emph{deficient} regime ($\u03be\\to 0$), informative trajectories are rare and the sample complexity explodes; in the \\emph{balanced} regime ($\u03be=\u0398(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \\emph{ample} regime ($\u03be\\to \\infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $\u03be\\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u76f8\u5bf9\u9884\u7b97\u7406\u8bba\uff0c\u7528\u5355\u4e00\u91cf\u03be=H/E[T]\u89e3\u91caRL\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6548\u679c\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u4e09\u4e2a\u5b66\u4e60\u673a\u5236\uff1a\u4e0d\u8db3\u3001\u5e73\u8861\u548c\u5145\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u6548\u679c\u4e0d\u4e00\uff0c\u53d7\u4efb\u52a1\u548c\u8ba1\u7b97\u9884\u7b97\u5f71\u54cd\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u91ca\u8fd9\u79cd\u5dee\u5f02\uff0c\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3RL\u5728\u4e0d\u540c\u9884\u7b97\u6761\u4ef6\u4e0b\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u76f8\u5bf9\u9884\u7b97\u7406\u8bba\uff0c\u5b9a\u4e49\u03be=H/E[T]\uff08\u751f\u6210\u65f6\u57df/\u9996\u6b21\u6b63\u786e\u89e3\u7684\u5e73\u5747token\u6570\uff09\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e09\u4e2a\u5b66\u4e60\u673a\u5236\uff0c\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u5e76\u5728\u7406\u60f3\u5206\u5e03\u5047\u8bbe\u4e0b\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u6700\u540e\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u03be\u51b3\u5b9a\u6837\u672c\u6548\u7387\uff0c\u63a7\u5236\u5956\u52b1\u65b9\u5dee\u548c\u4fe1\u606f\u8f68\u8ff9\u6982\u7387\u3002\u5b9e\u8bc1\u53d1\u73b0\u03be\u2208[1.5,2.0]\u65f6\u5b66\u4e60\u6548\u7387\u6700\u9ad8\uff0c\u4e0e\u63a8\u7406\u6027\u80fd\u5cf0\u503c\u4e00\u81f4\u3002\u76f8\u5bf9\u9884\u7b97\u5728\u8fed\u4ee3\u4e2d\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u76f8\u5bf9\u9884\u7b97\u03be\u662f\u7406\u89e3RL\u5b66\u4e60\u6548\u7387\u7684\u5173\u952e\u91cf\uff0c\u80fd\u89e3\u91ca\u4e0d\u540c\u4efb\u52a1\u548c\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002\u5e73\u8861\u673a\u5236\uff08\u03be=\u0398(1)\uff09\u4e0bRL\u6837\u672c\u6548\u7387\u6700\u9ad8\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9884\u7b97\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.01760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01760", "abs": "https://arxiv.org/abs/2602.01760", "authors": ["Hao Zhang", "Yanping Zha", "Zizhuo Li", "Meiqi Gong", "Jiayi Ma"], "title": "MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement", "comment": null, "summary": "This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.", "AI": {"tldr": "MagicFuse\uff1a\u4e00\u79cd\u5355\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5355\u5f20\u4f4e\u8d28\u91cf\u53ef\u89c1\u5149\u56fe\u50cf\u751f\u6210\u8de8\u5149\u8c31\u573a\u666f\u8868\u793a\uff0c\u6027\u80fd\u5ab2\u7f8e\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u53ea\u6709\u53ef\u89c1\u5149\u6210\u50cf\u4f20\u611f\u5668\u53ef\u7528\u65f6\uff0c\u5982\u4f55\u7ee7\u7eed\u53d7\u76ca\u4e8e\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4f18\u52bf\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u5355\u56fe\u50cf\u878d\u5408\u6982\u5ff5\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u4e09\u4e2a\u5206\u652f\uff1a1) \u5149\u8c31\u5185\u77e5\u8bc6\u589e\u5f3a\u5206\u652f\u6316\u6398\u53ef\u89c1\u5149\u8c31\u4e2d\u88ab\u906e\u6321\u7684\u573a\u666f\u4fe1\u606f\uff1b2) \u8de8\u5149\u8c31\u77e5\u8bc6\u751f\u6210\u5206\u652f\u5b66\u4e60\u7ea2\u5916\u5149\u8c31\u7684\u70ed\u8f90\u5c04\u5206\u5e03\u6a21\u5f0f\uff1b3) \u591a\u57df\u77e5\u8bc6\u878d\u5408\u5206\u652f\u6574\u5408\u4e24\u4e2a\u5206\u652f\u7684\u6982\u7387\u566a\u58f0\uff0c\u901a\u8fc7\u8fde\u7eed\u91c7\u6837\u83b7\u5f97\u8de8\u5149\u8c31\u573a\u666f\u8868\u793a", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMagicFuse\u4ec5\u4f9d\u8d56\u5355\u5f20\u9000\u5316\u53ef\u89c1\u5149\u56fe\u50cf\uff0c\u5c31\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u6700\u5148\u8fdb\u591a\u6a21\u6001\u8f93\u5165\u878d\u5408\u65b9\u6cd5\u7684\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u793a\u6027\u80fd", "conclusion": "\u6210\u529f\u5c06\u4f20\u7edf\u6570\u636e\u7ea7\u878d\u5408\u6269\u5c55\u5230\u77e5\u8bc6\u7ea7\uff0c\u5b9e\u73b0\u4e86\u5728\u4ec5\u6709\u53ef\u89c1\u5149\u4f20\u611f\u5668\u6761\u4ef6\u4e0b\u7684\u9ad8\u8d28\u91cf\u8de8\u5149\u8c31\u573a\u666f\u8868\u793a"}}
{"id": "2602.01265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01265", "abs": "https://arxiv.org/abs/2602.01265", "authors": ["Jiangnan Zhu", "Yukai Xu", "Li Xiong", "Yixuan Liu", "Junxu Liu", "Hong kyu Lee", "Yujie Gu"], "title": "BicKD: Bilateral Contrastive Knowledge Distillation", "comment": null, "summary": "Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u8fb9\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f(BicKD)\uff0c\u901a\u8fc7\u53cc\u8fb9\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u4e0d\u540c\u7c7b\u522b\u95f4\u7684\u6b63\u4ea4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u540c\u7c7b\u4e00\u81f4\u6027\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfKD\u4ec5\u8fdb\u884c\u6837\u672c\u7ea7\u6982\u7387\u5bf9\u9f50\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f(KD)\u4ec5\u8fdb\u884c\u6837\u672c\u7ea7\u6982\u7387\u5bf9\u9f50\uff0c\u7f3a\u4e4f\u7c7b\u522b\u7ea7\u6bd4\u8f83\u673a\u5236\uff0c\u4e14\u5bf9\u6982\u7387\u7a7a\u95f4\u6ca1\u6709\u7ed3\u6784\u7ea6\u675f\u3002\u8fd9\u9650\u5236\u4e86\u77e5\u8bc6\u4f20\u9012\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u53cc\u8fb9\u5bf9\u6bd4\u77e5\u8bc6\u84b8\u998f(BicKD)\uff0c\u5f15\u5165\u53cc\u8fb9\u5bf9\u6bd4\u635f\u5931\uff0c\u589e\u5f3a\u4e0d\u540c\u7c7b\u522b\u6cdb\u5316\u7a7a\u95f4\u7684\u6b63\u4ea4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u540c\u7c7b\u4e00\u81f4\u6027\u3002\u53cc\u8fb9\u516c\u5f0f\u652f\u6301\u6559\u5e08\u548c\u5b66\u751f\u4e4b\u95f4\u6837\u672c\u7ea7\u548c\u7c7b\u522b\u7ea7\u9884\u6d4b\u6a21\u5f0f\u7684\u663e\u5f0f\u6bd4\u8f83\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cBicKD\u65b9\u6cd5\u589e\u5f3a\u4e86\u77e5\u8bc6\u4f20\u9012\uff0c\u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "conclusion": "BicKD\u901a\u8fc7\u5f3a\u8c03\u6982\u7387\u6b63\u4ea4\u6027\u6765\u6b63\u5219\u5316\u9884\u6d4b\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u4f20\u9012\u6548\u679c\u3002"}}
{"id": "2602.01764", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01764", "abs": "https://arxiv.org/abs/2602.01764", "authors": ["Dennis Basile", "Dennis Sprute", "Helene D\u00f6rksen", "Holger Flatt"], "title": "GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data", "comment": "Accepted at 19th CIRP Conference on Intelligent Computation in Manufacturing Engineering", "summary": "The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMEMS-LiDAR\u7684\u9690\u79c1\u5408\u89c4\u4eba\u5458\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u65b9\u6cd5\u5728\u5de5\u4e1a\u5ba4\u5185\u73af\u5883\u4e2d\u5b58\u5728\u9690\u79c1\u8fdd\u89c4\u3001\u5bf9\u5149\u7167\u6761\u4ef6\u654f\u611f\u3001\u6570\u636e\u6807\u6ce8\u8017\u65f6\u4e14\u6613\u51fa\u9519\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u9690\u79c1\u5408\u89c4\u7684\u53ef\u9760\u68c0\u6d4b\u65b9\u6848", "method": "\u4f7f\u7528MEMS-LiDAR\u91c7\u96c6\u533f\u540d\u53163D\u70b9\u4e91\u6570\u636e\uff0c\u7ed3\u5408CARLA\u4eff\u771f\u6846\u67b6\u751f\u6210\u5408\u6210\u573a\u666f\u6570\u636e\u589e\u5f3a\u771f\u5b9e\u6570\u636e\uff0c\u6784\u5efa\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b", "result": "\u6df7\u5408\u6570\u636e\u65b9\u6cd5\u76f8\u6bd4\u7eaf\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u5c06\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad844\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9690\u79c1\u5408\u89c4\u4eba\u5458\u68c0\u6d4b\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5408\u6210LiDAR\u6570\u636e\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7ed3\u5408\u9ad8\u6027\u80fd\u68c0\u6d4b\u4e0eGDPR\u5408\u89c4\u6027\u7684\u6f5c\u529b"}}
{"id": "2602.01267", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01267", "abs": "https://arxiv.org/abs/2602.01267", "authors": ["Jiayu Bai", "Danchen Yu", "Zhenyu Liao", "TianQi Hou", "Feng Zhou", "Robert C. Qiu", "Zenan Ling"], "title": "Diving into Kronecker Adapters: Component Design Matters", "comment": null, "summary": "Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.", "AI": {"tldr": "CDKA\u901a\u8fc7\u7cbe\u7ec6\u5206\u6790Kronecker\u9002\u914d\u5668\u7684\u7ec4\u4ef6\u7ed3\u6784\uff08\u7ef4\u5ea6\u4e0e\u6570\u91cf\uff09\uff0c\u63d0\u51fa\u53c2\u6570\u9884\u7b97\u611f\u77e5\u7684\u914d\u7f6e\u6307\u5bfc\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u7b56\u7565\uff0c\u63d0\u5347\u9002\u914d\u5668\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Kronecker\u9002\u914d\u5668\u7814\u7a76\u5927\u591a\u5c06\u7ec4\u4ef6\u7ed3\u6784\u89c6\u4e3a\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u8bbe\u8ba1\u9009\u62e9\uff0c\u5bf9\u7ec4\u4ef6\u7ef4\u5ea6\u4e0e\u6570\u91cf\u7684\u5f71\u54cd\u7f3a\u4e4f\u6df1\u5165\u63a2\u7d22\uff0c\u800c\u7ec4\u4ef6\u7ed3\u6784\u662f\u51b3\u5b9a\u9002\u914d\u5668\u5bb9\u91cf\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51faComponent Designed Kronecker Adapters (CDKA)\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5206\u6790Kronecker\u7ec4\u4ef6\u7ef4\u5ea6\u4e0e\u6570\u91cf\uff0c\u63d0\u4f9b\u53c2\u6570\u9884\u7b97\u611f\u77e5\u7684\u914d\u7f6e\u6307\u5bfc\uff0c\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u8bad\u7ec3\u7a33\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CDKA\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u7ec4\u4ef6\u7ed3\u6784\u662fKronecker\u9002\u914d\u5668\u5bb9\u91cf\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\uff0cCDKA\u901a\u8fc7\u53c2\u6570\u9884\u7b97\u611f\u77e5\u7684\u914d\u7f6e\u6307\u5bfc\u548c\u8bad\u7ec3\u7a33\u5b9a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01601", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01601", "abs": "https://arxiv.org/abs/2602.01601", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Wenao Ma", "Yuzhi Zhao", "Ruifeng She", "Viet Anh Nguyen"], "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards", "comment": "Accepted at ICLR 2026", "summary": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.", "AI": {"tldr": "\u63d0\u51faVIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u9884\u6d4b\u6027\u5206\u914d\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u91c7\u6837\u6548\u7387\uff0c\u6839\u636e\u63d0\u793a\u7684\u68af\u5ea6\u65b9\u5dee\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u9884\u7b97\uff0c\u76f8\u6bd4\u5747\u5300\u5206\u914d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5bf9\u6240\u6709\u8bad\u7ec3\u63d0\u793a\u91c7\u7528\u56fa\u5b9a\u6570\u91cf\u7684rollout\u5206\u914d\uff0c\u8fd9\u79cd\u5747\u5300\u5206\u914d\u9690\u542b\u5730\u5047\u8bbe\u6240\u6709\u63d0\u793a\u5177\u6709\u540c\u7b49\u4fe1\u606f\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u9884\u7b97\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u5e76\u963b\u788d\u8bad\u7ec3\u8fdb\u5c55\u3002", "method": "\u63d0\u51faVIP\uff08Variance-Informed Predictive allocation\uff09\u7b56\u7565\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u57fa\u4e8e\u6700\u8fd1rollout\u9884\u6d4b\u6bcf\u4e2a\u63d0\u793a\u7684\u6210\u529f\u6982\u7387\uff0c\u5c06\u8fd9\u4e9b\u6982\u7387\u9884\u6d4b\u8f6c\u6362\u4e3a\u65b9\u5dee\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u51f8\u4f18\u5316\u95ee\u9898\u5728\u786c\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\u4e0b\u786e\u5b9a\u6700\u4f18\u7684rollout\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVIP\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u91c7\u6837\u6548\u7387\uff0c\u76f8\u6bd4\u5747\u5300\u5206\u914d\u6216\u542f\u53d1\u5f0f\u5206\u914d\u7b56\u7565\u83b7\u5f97\u66f4\u9ad8\u6027\u80fd\u3002", "conclusion": "VIP\u65b9\u6cd5\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u9884\u6d4b\u6027\u5206\u914d\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u667a\u80fd\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002"}}
{"id": "2602.01780", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01780", "abs": "https://arxiv.org/abs/2602.01780", "authors": ["Shicheng Yin", "Kaixuan Yin", "Weixing Chen", "Yang Liu", "Guanbin Li", "Liang Lin"], "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models", "comment": "Codes will be available at https://github.com/HCPLabSYSU/DDP-WM", "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.", "AI": {"tldr": "DDP-WM\u662f\u4e00\u79cd\u65b0\u578b\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u52a8\u6001\u9884\u6d4b\u539f\u7406\uff0c\u5c06\u6f5c\u5728\u72b6\u6001\u6f14\u5316\u5206\u89e3\u4e3a\u7a00\u758f\u7684\u4e3b\u8981\u52a8\u6001\u548c\u6b21\u8981\u80cc\u666f\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bc6\u96c6Transformer\u7684\u4e16\u754c\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e25\u91cd\u963b\u788d\u5b9e\u65f6\u90e8\u7f72\uff0c\u9700\u8981\u89e3\u51b3\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faDDP-WM\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u52a8\u6001\u9884\u6d4b\u539f\u7406\uff0c\u5c06\u573a\u666f\u6f14\u5316\u5206\u89e3\u4e3a\u7269\u7406\u4ea4\u4e92\u9a71\u52a8\u7684\u7a00\u758f\u4e3b\u8981\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u80cc\u666f\u66f4\u65b0\u3002\u901a\u8fc7\u9ad8\u6548\u5386\u53f2\u5904\u7406\u548c\u52a8\u6001\u5b9a\u4f4d\u67b6\u6784\u5206\u79bb\u4e3b\u8981\u52a8\u6001\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u80cc\u666f\u66f4\u65b0\u3002", "result": "\u5728\u5bfc\u822a\u3001\u684c\u9762\u7cbe\u786e\u64cd\u4f5c\u3001\u590d\u6742\u53ef\u53d8\u5f62\u6216\u591a\u4f53\u4ea4\u4e92\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u3002\u5728Push-T\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7ea69\u500d\u63a8\u7406\u52a0\u901f\uff0cMPC\u6210\u529f\u7387\u4ece90%\u63d0\u5347\u81f398%\u3002", "conclusion": "DDP-WM\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bc6\u96c6\u6a21\u578b\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.01270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01270", "abs": "https://arxiv.org/abs/2602.01270", "authors": ["Boxuan Zhang", "Weipu Zhang", "Zhaohan Feng", "Wei Xiao", "Jian Sun", "Jie Chen", "Gang Wang"], "title": "Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics", "comment": null, "summary": "A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.", "AI": {"tldr": "MoW\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e16\u754c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u89c6\u89c9\u7f16\u7801\u3001\u6df7\u5408Transformer\u52a8\u6001\u6a21\u578b\u548c\u4efb\u52a1\u805a\u7c7b\u7b56\u7565\uff0c\u5728\u591a\u4efb\u52a1\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u548c\u6837\u672c\u9ad8\u6548\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u9886\u57df\u4e2d\u9762\u4e34\u6837\u672c\u6548\u7387\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u4efb\u52a1\u5728\u89c2\u6d4b\u548c\u52a8\u6001\u65b9\u9762\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\u65f6\u3002\u4f20\u7edf\u7684\u5355\u4f53\u4e16\u754c\u6a21\u578b\u67b6\u6784\u96be\u4ee5\u6355\u6349\u591a\u6837\u5316\u7684\u4efb\u52a1\u52a8\u6001\uff0c\u5bfc\u81f4\u91cd\u5efa\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Mixture-of-World Models (MoW)\u67b6\u6784\uff1a1) \u6a21\u5757\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89c9\u538b\u7f29\uff1b2) \u6df7\u5408Transformer\u52a8\u6001\u6a21\u578b\uff0c\u5305\u542b\u4efb\u52a1\u6761\u4ef6\u4e13\u5bb6\u548c\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\uff1b3) \u57fa\u4e8e\u68af\u5ea6\u7684\u4efb\u52a1\u805a\u7c7b\u7b56\u7565\u7528\u4e8e\u9ad8\u6548\u53c2\u6570\u5206\u914d\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2aMoW\u4ee3\u7406\u572826\u4e2aAtari\u6e38\u620f\u4e0a\u8fbe\u5230110.4%\u7684\u5e73\u5747\u4eba\u7c7b\u6807\u51c6\u5316\u5206\u6570\uff0c\u4e0e26\u4e2a\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u96c6\u6210(STORM)\u7684114.2%\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u51cf\u5c1150%\u3002\u5728Meta-World\u4e0a\uff0cMoW\u572830\u4e07\u73af\u5883\u6b65\u5185\u8fbe\u523074.5%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "MoW\u4e3a\u901a\u7528\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53c2\u6570\u9ad8\u6548\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5728\u591a\u4efb\u52a1\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.01703", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01703", "abs": "https://arxiv.org/abs/2602.01703", "authors": ["Pengyu Li", "Lingling Zhang", "Zhitao Gao", "Yanrui Wu", "Yuxuan Dong", "Huan Liu", "Bifan Wei", "Jun Liu"], "title": "$\\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAGT^AO\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u4ea4\u6027\u548c\u5bf9\u6297\u6027\u95e8\u63a7\u8bad\u7ec3\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u654f\u611f\u6570\u636e\u65f6\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u5b9e\u73b0\u7a33\u5065\u64e6\u9664\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f1a\u65e0\u610f\u4e2d\u8bb0\u5fc6\u654f\u611f\u6570\u636e\uff0c\u5e26\u6765\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u6839\u672c\u6027\u56f0\u5883\uff1a\u6fc0\u8fdb\u7684\u9057\u5fd8\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u964d\u4f4e\u6a21\u578b\u6548\u7528\uff1b\u800c\u4fdd\u5b88\u7684\u7b56\u7565\u5219\u53ef\u80fd\u5bfc\u81f4\u8868\u9762\u9057\u5fd8\uff0c\u4f7f\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6062\u590d\u653b\u51fb\u3002", "method": "\u63d0\u51faAGT^AO\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u81ea\u9002\u5e94\u6b63\u4ea4\u6027(AO)\uff1a\u52a8\u6001\u7f13\u89e3\u9057\u5fd8\u548c\u4fdd\u7559\u76ee\u6807\u4e4b\u95f4\u7684\u51e0\u4f55\u68af\u5ea6\u51b2\u7a81\uff0c\u51cf\u5c11\u610f\u5916\u77e5\u8bc6\u9000\u5316\uff1b2) \u5bf9\u6297\u6027\u95e8\u63a7\u8bad\u7ec3(AGT)\uff1a\u5c06\u9057\u5fd8\u5efa\u6a21\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7684\u6700\u5c0f-\u6700\u5927\u535a\u5f08\uff0c\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u95e8\u63a7\u673a\u5236\u6765\u6a21\u62df\u548c\u5bf9\u6297\u5185\u90e8\u6062\u590d\u5c1d\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAGT^AO\u5728\u9057\u5fd8\u6548\u679c(KUR \u2248 0.01)\u548c\u6a21\u578b\u6548\u7528(MMLU 58.30)\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6743\u8861\u5e73\u8861\u3002", "conclusion": "AGT^AO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u654f\u611f\u6570\u636e\u65f6\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2602.01783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01783", "abs": "https://arxiv.org/abs/2602.01783", "authors": ["Dibyayan Patra", "Pasindu Ranasinghe", "Bikram Banerjee", "Simit Raval"], "title": "Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation", "comment": null, "summary": "Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95\u00b0 and 2.20\u00b0 in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3\u00b0.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u8868\u5f81\u5ca9\u4f53\u4e0d\u8fde\u7eed\u9762\u7ec4\u7684\u65b0\u65b9\u6cd5\uff0c\u91c7\u7528\u5355\u6b21\u6ee4\u6ce2\u3001\u5faa\u73af\u65b9\u4f4d\u53d8\u6362\u548c\u5c42\u6b21\u805a\u7c7b\u6280\u672f\uff0c\u5728\u771f\u5b9e\u77ff\u5c71\u91c7\u573a\u6570\u636e\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e0d\u8fde\u7eed\u9762\u7ec4\u8bc6\u522b\u3002", "motivation": "\u5730\u4e0b\u77ff\u5c71\u5ca9\u4f53\u4e0d\u8fde\u7eed\u9762\u7ec4\u7684\u51c6\u786e\u8868\u5f81\u5bf9\u5ca9\u4f53\u7a33\u5b9a\u6027\u8bc4\u4f30\u3001\u5f00\u6316\u5b89\u5168\u548c\u4f5c\u4e1a\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u6fc0\u5149\u626b\u63cf\u6280\u672f\u80fd\u9ad8\u6548\u91c7\u96c6\u70b9\u4e91\u6570\u636e\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982\u5168\u5c01\u95ed\u5ca9\u4f53\u8868\u9762\uff09\u4e2d\u5f00\u53d1\u9c81\u68d2\u9ad8\u6548\u7684\u81ea\u52a8\u8868\u5f81\u65b9\u6cd5\u4ecd\u662f\u672a\u89e3\u51b3\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5355\u6b21\u6ee4\u6ce2\u6b65\u9aa4\u4f7f\u7528\u4fe1\u53f7\u5904\u7406\u6280\u672f\u4e00\u6b21\u6027\u9694\u79bb\u5e73\u9762\u533a\u57df\u5e76\u6291\u5236\u566a\u58f0\u548c\u9ad8\u66f2\u7387\u4f2a\u5f71\uff1b2) \u521b\u65b0\u7684\u5faa\u73af\u65b9\u4f4d\u53d8\u6362\u65b9\u6848\uff0c\u5c06\u503e\u89d2\u548c\u503e\u5411\u7684\u6781\u5750\u6807\u6570\u636e\u51c6\u786e\u6620\u5c04\u5230\u7b1b\u5361\u5c14\u7a7a\u95f4\uff1b3) \u4f7f\u7528\u5c42\u6b21\u805a\u7c7b\u6280\u672f\u5bf9\u53d8\u6362\u540e\u7684\u65b9\u4f4d\u8fdb\u884c\u5206\u7ec4\uff0c\u65e0\u9700\u7528\u6237\u9884\u8bbe\u7ec4\u6570\u5373\u53ef\u5904\u7406\u4e0d\u540c\u5bc6\u5ea6\u5206\u5e03\u5e76\u8bc6\u522b\u805a\u7c7b\u3002", "result": "\u5728\u771f\u5b9e\u77ff\u5c71\u91c7\u573a\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u4e0e\u624b\u52a8\u9009\u53d6\u7684\u4e0d\u8fde\u7eed\u9762\uff08\u4f7f\u7528Virtual Compass\u5de5\u5177\uff09\u548c\u5e7f\u6cdb\u4f7f\u7528\u7684\u81ea\u52a8\u7ed3\u6784\u6620\u5c04\u6280\u672f\u5bf9\u6bd4\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u6700\u4f18\u3002\u5728\u503e\u89d2\u548c\u503e\u5411\u4f30\u8ba1\u4e2d\u5206\u522b\u5b9e\u73b01.95\u00b0\u548c2.20\u00b0\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u79bb\u6563\u8bef\u5dee\u4f4e\u4e8e3\u00b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5730\u4e0b\u77ff\u5c71\u5ca9\u4f53\u4e0d\u8fde\u7eed\u9762\u7ec4\u7684\u81ea\u52a8\u8868\u5f81\u63d0\u4f9b\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5ca9\u4f53\u7a33\u5b9a\u6027\u8bc4\u4f30\u548c\u77ff\u5c71\u5b89\u5168\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.01271", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01271", "abs": "https://arxiv.org/abs/2602.01271", "authors": ["Burak Demirel", "Pablo Soldati", "Yu Wang"], "title": "From Intents to Actions: Agentic AI in Autonomous Networks", "comment": null, "summary": "Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u4e2a\u667a\u80fd\u4f53\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u6cbb\u7f51\u7edc\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u610f\u56fe\u3001\u4f18\u5316\u5668\u5206\u6790\u6743\u8861\u3001\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u6267\u884c\uff0c\u5b9e\u73b0\u7f51\u7edc\u81ea\u4e3b\u9002\u5e94\u591a\u6837\u5316\u610f\u56fe\u548c\u6761\u4ef6\u3002", "motivation": "\u7535\u4fe1\u7f51\u7edc\u9700\u8981\u81ea\u4e3b\u8fd0\u884c\u5e76\u652f\u6301\u5177\u6709\u591a\u6837\u5316\u4e14\u7ecf\u5e38\u51b2\u7a81\u610f\u56fe\u7684\u5f02\u6784\u670d\u52a1\uff0c\u4f46\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u5c06\u9ad8\u5c42\u610f\u56fe\uff08\u5982\u8d85\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u3001\u80fd\u6548\uff09\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u63a7\u5236\u52a8\u4f5c\u3002", "method": "\u6784\u5efa\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a1) \u76d1\u7763\u89e3\u91ca\u5668\u667a\u80fd\u4f53\uff08\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5c06\u610f\u56fe\u89e3\u6790\u4e3a\u53ef\u6267\u884c\u4f18\u5316\u6a21\u677f\u5e76\u8fdb\u884c\u8ba4\u77e5\u7ec6\u5316\uff1b2) \u4f18\u5316\u5668\u667a\u80fd\u4f53\uff0c\u5c06\u6a21\u677f\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u4f18\u5316\u95ee\u9898\u5e76\u5206\u6790\u6743\u8861\uff1b3) \u504f\u597d\u9a71\u52a8\u63a7\u5236\u5668\u667a\u80fd\u4f53\uff08\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5229\u7528\u504f\u597d\u64cd\u4f5c\u63a5\u8fd1\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u8fd9\u4e9b\u667a\u80fd\u4f53\u5171\u540c\u4f7f\u7f51\u7edc\u80fd\u591f\u4ee5\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u81ea\u4e3b\u89e3\u91ca\u3001\u63a8\u7406\u3001\u9002\u5e94\u548c\u54cd\u5e94\u591a\u6837\u5316\u610f\u56fe\u548c\u7f51\u7edc\u6761\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e3a\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u6cbb\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u667a\u80fd\u4f53\u67b6\u6784\u5b9e\u73b0\u4e86\u4ece\u9ad8\u5c42\u610f\u56fe\u5230\u5177\u4f53\u63a7\u5236\u52a8\u4f5c\u7684\u81ea\u4e3b\u8f6c\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01799", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01799", "abs": "https://arxiv.org/abs/2602.01799", "authors": ["Ido Faran", "Nathan S. Netanyahu", "Maxim Shoshany"], "title": "Spatio-Temporal Transformers for Long-Term NDVI Forecasting", "comment": null, "summary": "Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.", "AI": {"tldr": "STT-LTF\u662f\u4e00\u4e2a\u65f6\u7a7aTransformer\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u957f\u671f\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e0e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5728\u5f02\u8d28\u6027\u5730\u4e2d\u6d77\u666f\u89c2\u4e2d\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5206\u6790\u548c\u957f\u671f\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5f02\u8d28\u6027\u5730\u4e2d\u6d77\u666f\u89c2\u4e2d\u957f\u671f\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7a7a\u95f4\u6a21\u5f0f\u3001\u5b63\u8282\u53d8\u5316\u548c\u591a\u5e74\u4ee3\u73af\u5883\u53d8\u5316\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51faSTT-LTF\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684Transformer\u67b6\u6784\u5904\u7406\u591a\u5c3a\u5ea6\u7a7a\u95f4\u5757\u548c\u65f6\u95f4\u5e8f\u5217\uff08\u957f\u8fbe20\u5e74\uff09\uff0c\u91c7\u7528\u7a7a\u95f4\u63a9\u7801\u3001\u65f6\u95f4\u63a9\u7801\u548c\u6c34\u5e73\u91c7\u6837\u7b56\u7565\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u76f4\u63a5\u9884\u6d4b\u4efb\u610f\u672a\u6765\u65f6\u95f4\u70b9\u3002", "result": "\u5728Landsat\u6570\u636e\uff081984-2024\uff09\u4e0a\u8bc4\u4f30\uff0cSTT-LTF\u5b9e\u73b0\u4e86MAE\u4e3a0.0328\u548cR^2\u4e3a0.8412\u7684\u6b21\u5e74\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u3001CNN\u3001LSTM\u548c\u6807\u51c6Transformer\u3002", "conclusion": "STT-LTF\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u91c7\u6837\u548c\u53ef\u53d8\u9884\u6d4b\u6c34\u5e73\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ecf\u5386\u5feb\u901f\u751f\u6001\u8f6c\u53d8\u7684\u5f02\u8d28\u6027\u5730\u4e2d\u6d77\u666f\u89c2\u5206\u6790\uff0c\u4e3a\u957f\u671f\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2602.01279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01279", "abs": "https://arxiv.org/abs/2602.01279", "authors": ["Sergio Calvo-Ordo\u00f1ez", "Jonathan Plenk", "Richard Bergna", "\u00c1lvaro Cartea", "Yarin Gal", "Jose Miguel Hern\u00e1ndez-Lobato", "Kamil Ciosek"], "title": "Richer Bayesian Last Layers with Subsampled NTK Features", "comment": "Preprint, work in progress", "summary": "Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u795e\u7ecf\u6b63\u5207\u6838\u7279\u5f81\u6295\u5f71\u5230\u6700\u540e\u4e00\u5c42\u7279\u5f81\u7a7a\u95f4\uff0c\u63d0\u9ad8\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u4f4e\u4f30\u4e86\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u4e3a\u5b83\u53ea\u5bf9\u6700\u540e\u4e00\u5c42\u8fdb\u884c\u8d1d\u53f6\u65af\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u524d\u9762\u5c42\u5f15\u5165\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5229\u7528\u795e\u7ecf\u6b63\u5207\u6838\u7279\u5f81\u6295\u5f71\u5230\u6700\u540e\u4e00\u5c42\u7279\u5f81\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4f7f\u540e\u9a8c\u63a8\u65ad\u80fd\u591f\u8003\u8651\u6574\u4e2a\u7f51\u7edc\u7684\u53d8\u5f02\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6BLL\u7684\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u8fd8\u5f15\u5165\u4e86\u5747\u5300\u5b50\u91c7\u6837\u65b9\u6848\u6765\u4f30\u8ba1\u6295\u5f71\u77e9\u9635\u548c\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u540e\u9a8c\u65b9\u5dee\u7406\u8bba\u4e0a\u5927\u4e8e\u6216\u7b49\u4e8e\u6807\u51c6BLL\uff0c\u7ea0\u6b63\u4e86\u5176\u4f4e\u4f30\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u8d8b\u52bf\u3002\u5728UCI\u56de\u5f52\u3001\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u3001\u56fe\u50cf\u5206\u7c7b\u4ee5\u53ca\u56fe\u50cf\u548c\u8868\u683c\u6570\u636e\u96c6\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6BLL\u548c\u7ade\u4e89\u57fa\u7ebf\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6539\u8fdb\u4e86\u8d1d\u53f6\u65af\u6700\u540e\u4e00\u5c42\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8003\u8651\u5168\u7f51\u7edc\u53d8\u5f02\u6027\u6765\u66f4\u597d\u5730\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2602.02014", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02014", "abs": "https://arxiv.org/abs/2602.02014", "authors": ["Hongxin Xiang", "Pengsen Ma", "Yunkang Cao", "Di Yu", "Haowen Chen", "Xinyu Yang", "Xiangxiang Zeng"], "title": "Rethinking Genomic Modeling Through Optical Character Recognition", "comment": null, "summary": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.", "AI": {"tldr": "OpticalDNA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u56e0\u7ec4\u5efa\u6a21\u6846\u67b6\uff0c\u5c06DNA\u5904\u7406\u4e3aOCR\u98ce\u683c\u7684\u6587\u6863\u7406\u89e3\uff0c\u901a\u8fc7\u89c6\u89c9DNA\u7f16\u7801\u5668\u548c\u6587\u6863\u89e3\u7801\u5668\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u5728\u591a\u4e2a\u57fa\u56e0\u7ec4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f7f\u7528\u66f4\u5c11\u7684token\u548c\u53c2\u6570\u8fbe\u5230\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\u5927\u591a\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u5c06DNA\u89c6\u4e3a\u4e00\u7ef4token\u5e8f\u5217\u3002\u8fd9\u79cd\u987a\u5e8f\u8bfb\u53d6\u65b9\u5f0f\u4e0e\u7a00\u758f\u3001\u4e0d\u8fde\u7eed\u7684\u57fa\u56e0\u7ec4\u8bed\u4e49\u7ed3\u6784\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u5728\u4f4e\u4fe1\u606f\u80cc\u666f\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u65e0\u6cd5\u5b9e\u73b0\u7406\u89e3\u9a71\u52a8\u7684\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u3002", "method": "OpticalDNA\u5c06DNA\u6e32\u67d3\u4e3a\u7ed3\u6784\u5316\u89c6\u89c9\u5e03\u5c40\uff0c\u8bad\u7ec3OCR\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b\u89c6\u89c9DNA\u7f16\u7801\u5668\u548c\u6587\u6863\u89e3\u7801\u5668\u3002\u7f16\u7801\u5668\u751f\u6210\u7d27\u51d1\u3001\u53ef\u91cd\u6784\u7684\u89c6\u89c9token\u5b9e\u73b0\u9ad8\u4fdd\u771f\u538b\u7f29\u3002\u57fa\u4e8e\u6b64\u8868\u793a\uff0c\u5b9a\u4e49\u4e86\u9488\u5bf9\u6838\u5fc3\u57fa\u56e0\u7ec4\u539f\u8bed\uff08\u8bfb\u53d6\u3001\u533a\u57df\u5b9a\u4f4d\u3001\u5b50\u5e8f\u5217\u68c0\u7d22\u3001\u63a9\u7801\u8de8\u5ea6\u8865\u5168\uff09\u7684\u63d0\u793a\u6761\u4ef6\u76ee\u6807\uff0c\u5b66\u4e60\u5e03\u5c40\u611f\u77e5\u7684DNA\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u56e0\u7ec4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpticalDNA\u59cb\u7ec8\u4f18\u4e8e\u6700\u8fd1\u7684\u57fa\u7ebf\u6a21\u578b\uff1b\u5728\u957f\u8fbe450k\u78b1\u57fa\u7684\u5e8f\u5217\u4e0a\uff0c\u5b83\u4ee5\u8fd120\u500d\u66f4\u5c11\u7684\u6709\u6548token\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u5e76\u4e14\u8d85\u8d8a\u4e86\u6fc0\u6d3b\u53c2\u6570\u591a\u8fbe985\u500d\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4ec5\u8c03\u6574256k\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "OpticalDNA\u901a\u8fc7\u5c06\u57fa\u56e0\u7ec4\u5efa\u6a21\u91cd\u65b0\u6784\u5efa\u4e3aOCR\u98ce\u683c\u7684\u6587\u6863\u7406\u89e3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5bf9\u9f50\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u538b\u7f29\u957f\u4e0a\u4e0b\u6587DNA\u5e8f\u5217\uff0c\u540c\u65f6\u4fdd\u7559\u7ec6\u7c92\u5ea6\u57fa\u56e0\u7ec4\u4fe1\u606f\uff0c\u4e3a\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.01801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01801", "abs": "https://arxiv.org/abs/2602.01801", "authors": ["Dvir Samuel", "Issar Tzachor", "Matan Levy", "Micahel Green", "Gal Chechik", "Rami Ben-Ari"], "title": "Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention", "comment": "Project Page: https://dvirsamuel.github.io/fast-auto-regressive-video/", "summary": "Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.", "AI": {"tldr": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u7edf\u4e00\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u7f13\u5b58\u538b\u7f29\u3001\u8fd1\u4f3c\u6700\u8fd1\u90bb\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u81ea\u6ce8\u610f\u529b\u7a00\u758f\u5316\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u63a8\u7406\u65f6\u7684KV\u7f13\u5b58\u589e\u957f\u95ee\u9898\uff0c\u5b9e\u73b05-10\u500d\u52a0\u901f\u5e76\u4fdd\u6301\u7a33\u5b9a\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34KV\u7f13\u5b58\u589e\u957f\u95ee\u9898\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u3001GPU\u5185\u5b58\u5360\u7528\u4e0a\u5347\uff0c\u9650\u5236\u4e86\u65f6\u95f4\u4e0a\u4e0b\u6587\u4f7f\u7528\u5e76\u635f\u5bb3\u957f\u7a0b\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u8bad\u7ec3\u514d\u8d39\u6a21\u5757\uff1a1) TempCache\u901a\u8fc7\u65f6\u95f4\u5bf9\u5e94\u538b\u7f29KV\u7f13\u5b58\uff1b2) AnnCA\u4f7f\u7528\u8fd1\u4f3c\u6700\u8fd1\u90bb\u5339\u914d\u9009\u62e9\u5e27\u76f8\u5173\u63d0\u793a\u8bcd\u52a0\u901f\u4ea4\u53c9\u6ce8\u610f\u529b\uff1b3) AnnSA\u901a\u8fc7\u9650\u5236\u67e5\u8be2\u5230\u8bed\u4e49\u5339\u914d\u7684\u952e\u6765\u7a00\u758f\u5316\u81ea\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7aef\u5230\u7aef\u901f\u5ea6\u63d0\u53475-10\u500d\uff0c\u4fdd\u6301\u8fd1\u4e4e\u76f8\u540c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7ef4\u6301\u7a33\u5b9a\u541e\u5410\u91cf\u548c\u8fd1\u4e4e\u6052\u5b9a\u7684\u5cf0\u503cGPU\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6ce8\u610f\u529b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u751f\u6210\uff0c\u4e0e\u73b0\u6709\u81ea\u56de\u5f52\u6269\u6563\u9aa8\u5e72\u548c\u4e16\u754c\u6a21\u578b\u517c\u5bb9\u3002"}}
{"id": "2602.01285", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01285", "abs": "https://arxiv.org/abs/2602.01285", "authors": ["Kangjun Noh", "Seongchan Lee", "Ilmun Kim", "Kyungwoo Song"], "title": "Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses", "comment": "Accepted to ICLR 2026", "summary": "Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI", "AI": {"tldr": "\u63d0\u51faMACI\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e58\u6cd5\u8fc7\u6ee4\u6846\u67b6\u548c\u96c6\u6210\u5b66\u4e60\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u9a8c\u8bc1\uff0c\u5728\u4fdd\u8bc1\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4fdd\u7559\u7387\u548c\u964d\u4f4e\u65f6\u95f4\u6210\u672c", "motivation": "\u5728\u533b\u7597\u548c\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u786e\u4fdd\u4e8b\u5b9e\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u4fdd\u5f62\u63a8\u7406\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\uff08\u4e22\u5f03\u592a\u591a\u771f\u5b9e\u58f0\u660e\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u81ea\u9002\u5e94\u9519\u8bef\u7387\u548c\u7b80\u5355\u7ebf\u6027\u6a21\u578b\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u7fa4\u4f53\u7ed3\u6784", "method": "\u5c06\u4fdd\u5f62\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e58\u6cd5\u8fc7\u6ee4\u6846\u67b6\uff0c\u5c06\u4e8b\u5b9e\u6027\u5efa\u6a21\u4e3a\u58f0\u660e\u7ea7\u5206\u6570\u7684\u4e58\u79ef\u3002\u63d0\u51faMACI\u65b9\u6cd5\uff0c\u5229\u7528\u96c6\u6210\u5b66\u4e60\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u4e8b\u5b9e\u6027\u5206\u6570\uff0c\u901a\u8fc7\u7fa4\u4f53\u6761\u4ef6\u6821\u51c6\u4fdd\u6301\u6709\u6548\u6027", "result": "\u5b9e\u9a8c\u8868\u660eMACI\u59cb\u7ec8\u8fbe\u5230\u7528\u6237\u6307\u5b9a\u7684\u8986\u76d6\u7387\uff0c\u540c\u65f6\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4fdd\u7559\u7387\u5e76\u964d\u4f4e\u65f6\u95f4\u6210\u672c", "conclusion": "MACI\u65b9\u6cd5\u901a\u8fc7\u4e58\u6cd5\u8fc7\u6ee4\u6846\u67b6\u548c\u96c6\u6210\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u4e2d\u7684\u4fdd\u5b88\u6027\u548c\u590d\u6742\u7fa4\u4f53\u7ed3\u6784\u95ee\u9898\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01805", "abs": "https://arxiv.org/abs/2602.01805", "authors": ["Menglin Han", "Zhangkai Ni"], "title": "FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing", "comment": null, "summary": "Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.", "AI": {"tldr": "FlowBypass\uff1a\u57fa\u4e8eRectified Flow\u7684\u514d\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8fde\u63a5\u53cd\u8f6c\u548c\u91cd\u5efa\u8f68\u8ff9\u7684\u65c1\u8def\uff0c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff0c\u63d0\u5347\u63d0\u793a\u5bf9\u9f50\u548c\u4fdd\u771f\u5ea6", "motivation": "\u73b0\u6709\u514d\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4f9d\u8d56\u53cd\u8f6c-\u91cd\u5efa\u8f68\u8ff9\uff0c\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1a\u957f\u8f68\u8ff9\u7d2f\u79ef\u8bef\u5dee\u635f\u5bb3\u4fdd\u771f\u5ea6\uff0c\u77ed\u8f68\u8ff9\u65e0\u6cd5\u786e\u4fdd\u4e0e\u7f16\u8f91\u63d0\u793a\u5145\u5206\u5bf9\u9f50\u3002\u5148\u524d\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f7f\u7528\u7279\u5b9a\u4e8e\u9aa8\u5e72\u7f51\u7edc\u7684\u7279\u5f81\u64cd\u4f5c\uff0c\u901a\u7528\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faFlowBypass\u6846\u67b6\uff0c\u57fa\u4e8eRectified Flow\u6784\u5efa\u8fde\u63a5\u53cd\u8f6c\u548c\u91cd\u5efa\u8f68\u8ff9\u7684\u65c1\u8def\u3002\u901a\u8fc7\u5f62\u5f0f\u5316\u63a8\u5bfc\u4e24\u4e2a\u8f68\u8ff9\uff0c\u83b7\u5f97\u8fd1\u4f3c\u65c1\u8def\u516c\u5f0f\u53ca\u5176\u6570\u503c\u89e3\uff0c\u5b9e\u73b0\u65e0\u7f1d\u8f68\u8ff9\u8f6c\u6362\uff0c\u907f\u514d\u4f9d\u8d56\u7279\u5f81\u64cd\u4f5c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlowBypass\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u65e0\u5173\u533a\u57df\u9ad8\u4fdd\u771f\u7ec6\u8282\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u63d0\u793a\u5bf9\u9f50\u3002", "conclusion": "FlowBypass\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u6790\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u514d\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u4e2d\u53cd\u8f6c-\u91cd\u5efa\u8f68\u8ff9\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u8f68\u8ff9\u65c1\u8def\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff0c\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.01288", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01288", "abs": "https://arxiv.org/abs/2602.01288", "authors": ["Chenghua Zhu", "Siyan Wu", "Xiangkang Zeng", "Zishan Xu", "Zhaolu Kang", "Yifu Guo", "Yuquan Lu", "Junduan Huang", "Guojing Zhou"], "title": "EDIS: Diagnosing LLM Reasoning via Entropy Dynamics", "comment": "Under review at ICML 2026", "summary": "Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \\emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u71b5\u7684\u52a8\u6001\u6f14\u5316\u800c\u975e\u9759\u6001\u805a\u5408\u6765\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u901a\u8fc7\u5206\u6790\u71b5\u8f68\u8ff9\u6a21\u5f0f\u8bc6\u522b\u9519\u8bef\u63a8\u7406\uff0c\u5e76\u5f15\u5165EDIS\u6307\u6807\u91cf\u5316\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u9759\u6001\u91cf\uff08\u901a\u5e38\u805a\u5408\u5728token\u4e0a\uff09\uff0c\u4f46\u7f6e\u4fe1\u5ea6\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u6f14\u5316\u53ef\u80fd\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u71b5\u52a8\u6001\u6f14\u5316\u5bf9\u7406\u89e3LLM\u63a8\u7406\u5931\u8d25\u7684\u4ef7\u503c\u3002", "method": "\u5206\u6790token\u7ea7\u71b5\u8f68\u8ff9\uff0c\u8bc6\u522b\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u7684\u7279\u5f81\u6a21\u5f0f\uff08\u5982\u4e0d\u7a33\u5b9a\u52a8\u6001\u3001\u7206\u53d1\u6027\u5c16\u5cf0\u3001\u5cf0\u8c37\u5c16\u5cf0\uff09\u3002\u5f15\u5165\u71b5\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\u8bc4\u5206\uff08EDIS\uff09\u6765\u91cf\u5316\u71b5\u6f14\u5316\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u52a8\u6001\uff0c\u5305\u62ec\u7206\u53d1\u6027\u5c16\u5cf0\uff08\u6301\u7eed\u4e0d\u786e\u5b9a\u6027\u589e\u957f\uff09\u548c\u5cf0\u8c37\u5c16\u5cf0\uff08\u77ed\u6682\u7f6e\u4fe1\u540e\u6025\u5267\u53cd\u5f39\uff09\u3002\u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u8bad\u7ec3\u9636\u6bb5\u6301\u7eed\u5b58\u5728\u3002EDIS\u4f5c\u4e3a\u63a8\u7406\u65f6\u9009\u62e9\u7684\u6709\u6548\u8bca\u65ad\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u71b5\u52a8\u6001\u662f\u7406\u89e3\u548c\u6539\u8fdbLLM\u63a8\u7406\u7684\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89d2\u3002EDIS\u4e0d\u4ec5\u53ef\u7528\u4e8e\u63a8\u7406\u65f6\u9009\u62e9\uff0c\u8fd8\u4e3a\u8bad\u7ec3\u65f6\u6837\u672c\u7b5b\u9009\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.02047", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02047", "abs": "https://arxiv.org/abs/2602.02047", "authors": ["Peijie Dong", "Ruibo Fan", "Yuechen Tao", "Di Mou", "Wenhu Hu", "Zhenheng Tang", "Yinghao Yu", "Jiamang Wang", "Wenbo Su", "Guodong Yang", "Liping Zhang", "Xiaowen Chu", "Baochun Li", "Bo Li"], "title": "Dissecting Outlier Dynamics in LLM NVFP4 Pretraining", "comment": "39 pages, 32 figures", "summary": "Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with \"post-QK\" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86NVFP4\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u53d1\u73b0\u5f02\u5e38\u503c\u4e3b\u8981\u6765\u81eaSoftmax\u6ce8\u610f\u529b\u3001\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u548cSwiGLU\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u63d0\u51faHot-Channel Patch\u8865\u507f\u673a\u5236\u548cCHON\u8bad\u7ec3\u65b9\u6848\uff0c\u5c06\u635f\u5931\u5dee\u8ddd\u4ece0.94%\u964d\u4f4e\u52300.58%\u3002", "motivation": "\u4f7f\u75284\u4f4d\u7b97\u672f\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u5185\u5b58\u6548\u7387\uff0c\u4f46FP4\u7684\u52a8\u6001\u8303\u56f4\u6709\u9650\u5bfc\u81f4\u5bf9\u5f02\u5e38\u503c\u654f\u611f\u3002\u867d\u7136NVFP4\u901a\u8fc7\u5206\u5c42\u5fae\u7f29\u653e\u7f13\u89e3\u91cf\u5316\u8bef\u5dee\uff0c\u4f46\u4e0eBF16\u76f8\u6bd4\u4ecd\u5b58\u5728\u635f\u5931\u5dee\u8ddd\u3002\u9700\u8981\u6df1\u5165\u5206\u6790\u5f02\u5e38\u503c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "1. \u7eb5\u5411\u5206\u6790NVFP4\u9884\u8bad\u7ec3\u4e2d\u5f02\u5e38\u503c\u7684\u52a8\u6001\u7279\u6027\uff1a\u5b9a\u4f4d\u5f02\u5e38\u503c\u4f4d\u7f6e\u3001\u5206\u6790\u6210\u56e0\u3001\u89c2\u5bdf\u6f14\u5316\u8fc7\u7a0b\uff1b2. \u63d0\u51faHot-Channel Patch\u5728\u7ebf\u8865\u507f\u673a\u5236\uff0c\u8bc6\u522b\u70ed\u901a\u9053\u5e76\u91cd\u65b0\u6ce8\u5165\u6b8b\u5dee\uff1b3. \u5f00\u53d1CHON\u8bad\u7ec3\u65b9\u6848\uff0c\u6574\u5408HCP\u5e76\u4fdd\u62a4post-QK\u64cd\u4f5c\u3002", "result": "1. \u53d1\u73b0\u5f02\u5e38\u503c\u4e3b\u8981\u6765\u81eaSoftmax\u6ce8\u610f\u529b\u3001\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u548cSwiGLU\u6fc0\u6d3b\u51fd\u6570\uff1b2. \u5f02\u5e38\u503c\u4ece\u8bad\u7ec3\u65e9\u671f\u7684\u77ac\u65f6\u5c16\u5cf0\u6f14\u53d8\u4e3a\u540e\u671f\u7684\u6301\u4e45\u70ed\u901a\u9053\uff1b3. \u5728GLA-1.3B\u6a21\u578b\u4e0a\uff0cCHON\u5c06NVFP4\u4e0eBF16\u7684\u635f\u5931\u5dee\u8ddd\u4ece0.94%\u964d\u4f4e\u52300.58%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u7684\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5f02\u5e38\u503c\u7684\u6765\u6e90\u548c\u6f14\u5316\u89c4\u5f8b\uff0c\u63d0\u51fa\u7684Hot-Channel Patch\u8865\u507f\u673a\u5236\u548cCHON\u8bad\u7ec3\u65b9\u6848\u6709\u6548\u51cf\u5c11\u4e86\u91cf\u5316\u635f\u5931\uff0c\u4e3a\u9ad8\u6548\u7684\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01812", "abs": "https://arxiv.org/abs/2602.01812", "authors": ["Cheng Wang", "Qiyu Gao", "Fandong Zhang", "Shu Zhang", "Yizhou Yu"], "title": "LDRNet: Large Deformation Registration Model for Chest CT Registration", "comment": null, "summary": "Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.", "AI": {"tldr": "LDRNet\uff1a\u4e00\u79cd\u7528\u4e8e\u80f8\u90e8CT\u56fe\u50cf\u5927\u53d8\u5f62\u914d\u51c6\u7684\u5feb\u901f\u65e0\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u914d\u51c6\u573a\u4f18\u5316\u548c\u521a\u6027\u53d8\u6362\u5b66\u4e60\uff0c\u5728\u5927\u53d8\u5f62\u914d\u51c6\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u4e14\u901f\u5ea6\u66f4\u5feb", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u914d\u51c6\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8111\u90e8\u56fe\u50cf\uff0c\u800c\u80f8\u90e8CT\u914d\u51c6\u9762\u4e34\u66f4\u5927\u53d8\u5f62\u3001\u66f4\u590d\u6742\u80cc\u666f\u548c\u533a\u57df\u91cd\u53e0\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5927\u53d8\u5f62\u914d\u51c6\u65b9\u6cd5", "method": "\u63d0\u51faLDRNet\u65b9\u6cd5\uff1a1\uff09\u5148\u9884\u6d4b\u7c97\u5206\u8fa8\u7387\u914d\u51c6\u573a\uff0c\u7136\u540e\u4ece\u7c97\u5230\u7ec6\u8fdb\u884c\u7ec6\u5316\uff1b2\uff09\u5f15\u5165\u7ec6\u5316\u5757\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u4f18\u5316\u914d\u51c6\u573a\uff1b3\uff09\u4f7f\u7528\u521a\u6027\u5757\u4ece\u9ad8\u5c42\u7279\u5f81\u5b66\u4e60\u53d8\u6362\u77e9\u9635", "result": "\u5728\u79c1\u6709\u6570\u636e\u96c6\u548c\u516c\u5171\u6570\u636e\u96c6SegTHOR\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08VoxelMorph\u3001RCN\u3001LapIRN\uff09\uff0c\u5728\u5927\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u4e14\u901f\u5ea6\u66f4\u5feb", "conclusion": "LDRNet\u80fd\u591f\u6709\u6548\u5904\u7406\u80f8\u90e8CT\u56fe\u50cf\u7684\u5927\u53d8\u5f62\u914d\u51c6\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01289", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01289", "abs": "https://arxiv.org/abs/2602.01289", "authors": ["Dung Anh Hoang", "Cuong Pham anh Trung Le", "Jianfei Cai", "Toan Do"], "title": "Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models", "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e3a\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u6821\u51c6\u6837\u672c\u5206\u914d\u6700\u4f18\u6743\u91cd\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u5747\u5300\u6743\u91cd\u5206\u914d\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u5185\u5b58\u5360\u7528\u9ad8\u3001\u566a\u58f0\u4f30\u8ba1\u8ba1\u7b97\u9700\u6c42\u5927\u7684\u95ee\u9898\u3002\u540e\u8bad\u7ec3\u91cf\u5316\u662f\u52a0\u901f\u91c7\u6837\u548c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u4f7f\u7528\u5747\u5300\u6743\u91cd\u5206\u914d\u6821\u51c6\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u65f6\u95f4\u6b65\u6570\u636e\u5bf9\u6269\u6563\u8fc7\u7a0b\u7684\u8d21\u732e\u5dee\u5f02\uff0c\u4ee5\u53ca\u6fc0\u6d3b\u5206\u5e03\u548c\u68af\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u91cf\u5316\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e3a\u6821\u51c6\u6837\u672c\u5206\u914d\u6700\u4f18\u6743\u91cd\uff0c\u4f7f\u91cf\u5316\u6a21\u578b\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u68af\u5ea6\u5bf9\u9f50\uff0c\u4ece\u800c\u4f18\u5316\u91cf\u5316\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u4e0d\u540c\u65f6\u95f4\u6b65\u9700\u8981\u4e0d\u540c\u68af\u5ea6\u65b9\u5411\u8fdb\u884c\u6700\u4f18\u91cf\u5316\u7684\u95ee\u9898\u3002", "result": "\u5728CIFAR-10\u3001LSUN-Bedrooms\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5176\u4ed6\u6269\u6563\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u4e3a\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u6821\u51c6\u6837\u672c\u5b66\u4e60\u6700\u4f18\u6743\u91cd\u5206\u914d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91cf\u5316\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2602.02103", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02103", "abs": "https://arxiv.org/abs/2602.02103", "authors": ["Liyan Xu", "Mo Yu", "Fandong Meng", "Jie Zhou"], "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "comment": null, "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Tele-Lens\u63a2\u6d4b\u65b9\u6cd5\u7814\u7a76LLM\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5177\u6709\u77ed\u89c6\u89c6\u91ce\uff0c\u4e3b\u8981\u8fdb\u884c\u589e\u91cf\u5f0f\u63a8\u7406\u800c\u975e\u5168\u5c40\u89c4\u5212\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u6539\u8fdbCoT\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u89c2\u5bdf\u5230CoT\u52a8\u6001\u7684\u4e92\u8865\u73b0\u8c61\uff1aLLM\u5728CoT\u51fa\u73b0\u524d\u5df2\u8fdb\u884c\u6f5c\u5728\u89c4\u5212\uff0c\u964d\u4f4e\u4e86\u663e\u5f0fCoT\u7684\u91cd\u8981\u6027\uff1b\u4f46CoT\u5bf9\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4ecd\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u6df1\u5165\u7406\u89e3LLM\u5185\u90e8\u72b6\u6001\u4e0e\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u7684\u6f5c\u5728\u89c4\u5212\u5f3a\u5ea6\u3002", "method": "\u63d0\u51faTele-Lens\u63a2\u6d4b\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u9886\u57df\u7684\u9690\u85cf\u72b6\u6001\uff0c\u5206\u6790LLM\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\u3002\u57fa\u4e8e\u53d1\u73b0\u7684\"\u77ed\u89c6\u89c6\u91ce\"\u7279\u6027\uff0c\u63d0\u51fa\u6539\u8fdbCoT\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5047\u8bbe\uff0c\u5e76\u9a8c\u8bc1\u5c11\u91cfCoT\u4f4d\u7f6e\u5373\u53ef\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eLLM\u8868\u73b0\u51fa\u77ed\u89c6\u89c6\u91ce\uff0c\u4e3b\u8981\u8fdb\u884c\u589e\u91cf\u5f0f\u8f6c\u6362\u800c\u975e\u7cbe\u786e\u7684\u5168\u5c40\u89c4\u5212\u3002\u9a8c\u8bc1\u4e86\u5c11\u91cfCoT\u4f4d\u7f6e\u80fd\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u65e0\u9700\u6027\u80fd\u4e0b\u964d\u7684CoT\u7ed5\u8fc7\u81ea\u52a8\u8bc6\u522b\u3002", "conclusion": "LLM\u7684\u63a8\u7406\u8fc7\u7a0b\u5177\u6709\u77ed\u89c6\u7279\u6027\uff0c\u8fd9\u4e3a\u6539\u8fdbCoT\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u5229\u7528CoT\u52a8\u6001\u7279\u6027\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u8def\u5f84\u7ba1\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01814", "abs": "https://arxiv.org/abs/2602.01814", "authors": ["Xiao Liang", "Yunzhu Zhang", "Linchao Zhu"], "title": "GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation", "comment": null, "summary": "Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.", "AI": {"tldr": "\u63d0\u51faGPD\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u84b8\u998f\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u6b65\u6570\u4ece48\u6b65\u51cf\u5c11\u52306\u6b65\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u53bb\u566a\u8fc7\u7a0b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u6269\u6563\u6b65\u6570\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u89c6\u9891\u751f\u6210\u65f6\u5f80\u5f80\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u5f15\u5bfc\u6e10\u8fdb\u5f0f\u84b8\u998f\uff08GPD\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u5728\u7ebf\u751f\u6210\u8bad\u7ec3\u76ee\u6807\uff0c\u964d\u4f4e\u4f18\u5316\u96be\u5ea6\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff1b2\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9891\u57df\u7ea6\u675f\uff0c\u4fc3\u8fdb\u7ec6\u7c92\u5ea6\u7ec6\u8282\u548c\u65f6\u95f4\u52a8\u6001\u7684\u4fdd\u7559\u3002\u6559\u5e08\u6a21\u578b\u9010\u6b65\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u4ee5\u66f4\u5927\u7684\u6b65\u957f\u64cd\u4f5c\u3002", "result": "\u5e94\u7528\u4e8eWan2.1\u6a21\u578b\u65f6\uff0cGPD\u5c06\u91c7\u6837\u6b65\u6570\u4ece48\u6b65\u51cf\u5c11\u52306\u6b65\uff0c\u540c\u65f6\u5728VBench\u4e0a\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u89c6\u89c9\u8d28\u91cf\u3002\u4e0e\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0cGPD\u5728\u6d41\u7a0b\u7b80\u5355\u6027\u548c\u8d28\u91cf\u4fdd\u6301\u65b9\u9762\u90fd\u663e\u793a\u51fa\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "GPD\u6846\u67b6\u80fd\u591f\u663e\u8457\u52a0\u901f\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5feb\u901f\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01295", "abs": "https://arxiv.org/abs/2602.01295", "authors": ["Yu Chen", "Yuhao Liu", "Jiatai Huang", "Yihan Du", "Longbo Huang"], "title": "The BoBW Algorithms for Heavy-Tailed MDPs", "comment": null, "summary": "We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\\widetilde{\\mathcal{O}}(T^{1/\u03b1})$ regret bound in adversarial regimes and a $\\mathcal{O}(\\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\\widetilde{\\mathcal{O}}(T^{1/\u03b1} + \\sqrt{T})$ regret in adversarial regimes and a $\\mathcal{O}(\\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5HT-FTRL-OM\u548cHT-FTRL-UOB\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u91cd\u5c3e\u53cd\u9988\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0\u5bf9\u6570\u9057\u61be\u3002", "motivation": "\u73b0\u6709\u5904\u7406\u91cd\u5c3e\u53cd\u9988MDP\u7684\u65b9\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u9002\u5e94\u4e24\u79cd\u73af\u5883\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\"\u4e24\u5168\u5176\u7f8e\"\u7684\u4fdd\u8bc1\u3002", "method": "HT-FTRL-OM\u4f7f\u7528FTRL\u6846\u67b6\u7ed3\u5408\u65b0\u9896\u7684\u8df3\u8fc7\u635f\u5931\u4f30\u8ba1\u5668\u5904\u7406\u5df2\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\uff1bHT-FTRL-UOB\u4f7f\u7528\u60b2\u89c2\u8df3\u8fc7\u635f\u5931\u4f30\u8ba1\u5668\u5904\u7406\u672a\u77e5\u8f6c\u79fb\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u5c40\u90e8\u63a7\u5236\u673a\u5236\u3001\u6b21\u4f18\u8d28\u91cf\u4f20\u64ad\u539f\u7406\u548c\u65b0\u7684\u9057\u61be\u5206\u89e3\u7b49\u6280\u672f\u514b\u670d\u5173\u952e\u969c\u788d\u3002", "result": "HT-FTRL-OM\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0$\\widetilde{\\mathcal{O}}(T^{1/\u03b1})$\u9057\u61be\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0$\\mathcal{O}(\\log T)$\u9057\u61be\uff1bHT-FTRL-UOB\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0$\\widetilde{\\mathcal{O}}(T^{1/\u03b1} + \\sqrt{T})$\u9057\u61be\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0$\\mathcal{O}(\\log^2(T))$\u9057\u61be\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u91cd\u5c3e\u53cd\u9988MDP\u4e2d\u5b9e\u73b0\u4e86\"\u4e24\u5168\u5176\u7f8e\"\u7684\u4fdd\u8bc1\uff0c\u901a\u8fc7\u6280\u672f\u521b\u65b0\u514b\u670d\u4e86\u91cd\u5c3e\u4f30\u8ba1\u8bef\u5dee\u548c\u8f6c\u79fb\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\uff0c\u4e3a\u5df2\u77e5\u548c\u672a\u77e5\u8f6c\u79fb\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02112", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02112", "abs": "https://arxiv.org/abs/2602.02112", "authors": ["Chunsan Hong", "Sanghyun Lee", "Jong Chul Ye"], "title": "Unifying Masked Diffusion Models with Various Generation Orders and Beyond", "comment": "Preprint", "summary": "Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u63a9\u7801\u6269\u6563\u6a21\u578b\uff1aOeMDM\u7edf\u4e00\u591a\u79cd\u751f\u6210\u987a\u5e8f\u6846\u67b6\uff0cLoMDM\u8054\u5408\u5b66\u4e60\u751f\u6210\u987a\u5e8f\u548c\u6269\u6563\u6a21\u578b\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u79bb\u6563\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u7684\u751f\u6210\u8d28\u91cf\u4e25\u91cd\u4f9d\u8d56\u751f\u6210\u987a\u5e8f\uff0c\u5148\u524d\u5de5\u4f5c\u8981\u4e48\u786c\u7f16\u7801\u987a\u5e8f\uff0c\u8981\u4e48\u9700\u8981\u4e24\u9636\u6bb5\u4f18\u5316\u5b66\u4e60\u987a\u5e8f\u7b56\u7565\uff0c\u5bfc\u81f4\u989d\u5916\u6210\u672c\u4e14\u53ef\u80fd\u5f97\u5230\u6b21\u4f18\u89e3\u3002", "method": "\u63d0\u51faOeMDM\u6846\u67b6\u7edf\u4e00\u591a\u79cd\u751f\u6210\u987a\u5e8f\u7684\u6269\u6563\u751f\u6210\u8fc7\u7a0b\uff0c\u5305\u62ecMDM\u3001ARM\u548c\u5757\u6269\u6563\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faLoMDM\uff0c\u901a\u8fc7\u5355\u4e00\u76ee\u6807\u4ece\u5934\u8054\u5408\u5b66\u4e60\u751f\u6210\u987a\u5e8f\u548c\u6269\u6563\u6a21\u578b\u3002", "result": "LoMDM\u5728\u591a\u4e2a\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5404\u79cd\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f9d\u8d56\u7684\u751f\u6210\u987a\u5e8f\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u987a\u5e8f\u4f9d\u8d56\u95ee\u9898\u3002"}}
{"id": "2602.01816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01816", "abs": "https://arxiv.org/abs/2602.01816", "authors": ["Wenjin Hou", "Wei Liu", "Han Hu", "Xiaoxiao Sun", "Serena Yeung-Levy", "Hehe Fan"], "title": "Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.", "AI": {"tldr": "VIA-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u5f02\u5e38\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6\u7c7b\u89c6\u89c9\u5e7b\u89c9\uff0c\u8d85\u8fc71000\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u3002\u6d4b\u8bd5\u53d1\u73b0\u5f53\u524dMLLMs\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0cCoT\u63a8\u7406\u4e5f\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u6807\u51c6\u5206\u5e03\u5185\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5728\u9762\u5bf9\u8fdd\u53cd\u5e38\u8bc6\u5148\u9a8c\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u8003\u5bdf\u3002\u9700\u8981\u6784\u5efa\u4e13\u95e8\u57fa\u51c6\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u5f02\u5e38\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efaVIA-Bench\u57fa\u51c6\uff0c\u5305\u542b\u516d\u7c7b\u6838\u5fc3\u89c6\u89c9\u5e7b\u89c9\uff1a\u989c\u8272\u5e7b\u89c9\u3001\u8fd0\u52a8\u5e7b\u89c9\u3001\u683c\u5f0f\u5854\u5e7b\u89c9\u3001\u51e0\u4f55\u4e0e\u7a7a\u95f4\u5e7b\u89c9\u3001\u4e00\u822c\u89c6\u89c9\u5e7b\u89c9\u548c\u89c6\u89c9\u5f02\u5e38\u3002\u901a\u8fc7\u4eba\u5de5\u53c2\u4e0e\u5faa\u73af\u5ba1\u67e5\uff0c\u6784\u5efa\u8d85\u8fc71000\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\uff0c\u9700\u8981\u7ec6\u81f4\u7684\u89c6\u89c9\u63a8\u7406\u3002\u8bc4\u4f30\u4e86\u8d85\u8fc720\u4e2a\u6700\u5148\u8fdb\u7684MLLMs\uff0c\u5305\u62ec\u4e13\u6709\u6a21\u578b\u3001\u5f00\u6e90\u6a21\u578b\u548c\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86MLLMs\u5728\u89c6\u89c9\u5e7b\u89c9\u548c\u5f02\u5e38\u573a\u666f\u4e0b\u7684\u663e\u8457\u8106\u5f31\u6027\u3002\u7279\u522b\u53d1\u73b0\u601d\u7ef4\u94fe\u63a8\u7406\u63d0\u4f9b\u7684\u9c81\u68d2\u6027\u5fae\u4e4e\u5176\u5fae\uff0c\u7ecf\u5e38\u4ea7\u751f\"\u8106\u5f31\u5e7b\u8c61\"\uff0c\u5373\u6a21\u578b\u7684\u903b\u8f91\u5728\u5e7b\u89c9\u523a\u6fc0\u4e0b\u5d29\u6e83\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u673a\u5668\u611f\u77e5\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002", "conclusion": "\u89e3\u51b3\u8fd9\u79cd\u611f\u77e5\u74f6\u9888\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002VIA-Bench\u57fa\u51c6\u7684\u53d1\u5e03\u5c06\u6709\u52a9\u4e8e\u63a8\u52a8MLLMs\u5728\u6311\u6218\u6027\u89c6\u89c9\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u586b\u8865\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.01308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01308", "abs": "https://arxiv.org/abs/2602.01308", "authors": ["Hengjie Cao", "Mengyi Chen", "Yifeng Yang", "Fang Dong", "Ruijun Huang", "Anrui Chen", "Jixian Zhou", "Mingzhi Dong", "Yujiang Wang", "Dongsheng Li", "Wenyi Fang", "Yuanyi Lin", "Fan Wu", "Li Shang"], "title": "Dispelling the Curse of Singularities in Neural Network Optimizations", "comment": null, "summary": "This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.", "AI": {"tldr": "\u8bba\u6587\u4ece\u53c2\u6570\u7a7a\u95f4\u5947\u5f02\u6027\u7684\u89d2\u5ea6\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u53c2\u6570\u5947\u5f02\u6027\u968f\u68af\u5ea6\u66f4\u65b0\u800c\u589e\u957f\u5e76\u4e0e\u8868\u793a\u5bf9\u9f50\uff0c\u5bfc\u81f4\u8868\u793a\u7a7a\u95f4\u5947\u5f02\u6027\u589e\u52a0\uff0c\u63d0\u51fa\u53c2\u6570\u5947\u5f02\u6027\u5e73\u6ed1\u65b9\u6cd5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u7684\u6df1\u5c42\u539f\u56e0\uff0c\u4ece\u53c2\u6570\u7a7a\u95f4\u5947\u5f02\u6027\u8fd9\u4e00\u8f83\u5c11\u63a2\u7d22\u4f46\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u63ed\u793a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5947\u5f02\u6027\u589e\u957f\u5982\u4f55\u5bfc\u81f4\u635f\u5931\u7206\u70b8\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u5947\u5f02\u6027\u5e73\u6ed1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u6ed1\u6743\u91cd\u77e9\u9635\u7684\u5947\u5f02\u8c31\u6765\u7f13\u89e3\u5947\u5f02\u6027\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u8f7b\u91cf\u3001\u7075\u6d3b\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6570\u636e\u96c6\u3001\u67b6\u6784\u548c\u4f18\u5316\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePSS\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u5931\u8d25\u540e\u4e5f\u80fd\u6062\u590d\u53ef\u8bad\u7ec3\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u53c2\u6570\u7a7a\u95f4\u5947\u5f02\u6027\u7684\u589e\u957f\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u91cd\u8981\u539f\u56e0\uff0c\u63d0\u51fa\u7684PSS\u65b9\u6cd5\u901a\u8fc7\u5e73\u6ed1\u5947\u5f02\u8c31\u6709\u6548\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.01836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01836", "abs": "https://arxiv.org/abs/2602.01836", "authors": ["Yin Wu", "Daniel Slieter", "Carl Esselborn", "Ahmed Abouelazm", "Tsung Yuan Tseng", "J. Marius Z\u00f6llner"], "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery", "comment": null, "summary": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u5f15\u5bfc\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\uff0c\u5229\u7528\u516c\u5f00\u8857\u666f\u56fe\u50cf\u8bc6\u522b\u5174\u8da3\u70b9\uff0c\u7528\u4e8e\u8de8\u56fd\u5bb6ADAS/ADS\u611f\u77e5\u6a21\u578b\u9002\u5e94\uff0c\u4ec5\u9700\u4e00\u534a\u76ee\u6807\u57df\u6570\u636e\u5373\u53ef\u8fbe\u5230\u968f\u673a\u91c7\u6837\u76f8\u5f53\u6027\u80fd", "motivation": "ADAS\u548cADS\u5728\u4e0d\u540c\u56fd\u5bb6\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u7531\u4e8e\u6cd5\u89c4\u3001\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u548c\u89c6\u89c9\u60ef\u4f8b\u5dee\u5f02\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u4f20\u7edf\u8de8\u56fd\u5bb6\u6570\u636e\u91c7\u96c6\u4f9d\u8d56\u5927\u91cf\u9053\u8def\u9a7e\u9a76\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e", "method": "\u63d0\u51fa\u8857\u666f\u5f15\u5bfc\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\uff0c\u5229\u7528\u516c\u5f00\u8857\u666f\u56fe\u50cf\u8bc6\u522b\u5174\u8da3\u70b9\uff0c\u5f15\u5165\u4e24\u79cdPOI\u8bc4\u5206\u65b9\u6cd5\uff1a\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684KNN\u7279\u5f81\u8ddd\u79bb\u65b9\u6cd5\u548c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5f52\u56e0\u65b9\u6cd5\uff0c\u91c7\u7528\u6536\u96c6-\u68c0\u6d4b\u534f\u8bae\u6784\u5efa\u5171\u5b9a\u4f4d\u6570\u636e\u96c6", "result": "\u5728\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u4e00\u534a\u76ee\u6807\u57df\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u968f\u673a\u91c7\u6837\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5168\u56fd\u5bb6\u5206\u6790\u7684\u6210\u672c\u4f30\u7b97\u663e\u793a\u5927\u89c4\u6a21\u8857\u666f\u5904\u7406\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c", "conclusion": "\u8857\u666f\u5f15\u5bfc\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\u4e3a\u8de8\u56fd\u5bb6\u6a21\u578b\u9002\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2602.01312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01312", "abs": "https://arxiv.org/abs/2602.01312", "authors": ["Han Tong", "Shubhangi Ghosh", "Haolin Zou", "Arian Maleki"], "title": "Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution", "comment": null, "summary": "Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.", "AI": {"tldr": "TRAK\u7b97\u6cd5\u7528\u4e8e\u6570\u636e\u5f52\u56e0\uff0c\u4f46\u7406\u8bba\u6761\u4ef6\u672a\u660e\u3002\u672c\u6587\u5206\u6790\u5176\u8fd1\u4f3c\u8bef\u5dee\uff0c\u53d1\u73b0\u867d\u8bef\u5dee\u663e\u8457\u4f46\u76f8\u5173\u6027\u9ad8\uff0c\u80fd\u4fdd\u6301\u6570\u636e\u70b9\u76f8\u5bf9\u6392\u5e8f\u3002", "motivation": "\u6570\u636e\u5f52\u56e0\u5bf9\u4e8e\u89e3\u91ca\u590d\u6742AI\u6a21\u578b\u5f88\u91cd\u8981\u3002TRAK\u7b97\u6cd5\u867d\u7136\u7ecf\u9a8c\u8868\u73b0\u597d\uff0c\u4f46\u5176\u7406\u8bba\u6761\u4ef6\u548c\u5931\u6548\u673a\u5236\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5bf9TRAK\u7b97\u6cd5\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u91cf\u5316\u5176\u8fd1\u4f3c\u5f15\u5165\u7684\u8bef\u5dee\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "TRAK\u7684\u8fd1\u4f3c\u786e\u5b9e\u5f15\u5165\u663e\u8457\u8bef\u5dee\uff0c\u4f46\u5176\u4f30\u8ba1\u7684\u5f71\u54cd\u503c\u4e0e\u539f\u59cb\u5f71\u54cd\u503c\u9ad8\u5ea6\u76f8\u5173\uff0c\u80fd\u6709\u6548\u4fdd\u6301\u6570\u636e\u70b9\u7684\u76f8\u5bf9\u6392\u5e8f\u3002", "conclusion": "TRAK\u7b97\u6cd5\u867d\u7136\u5b58\u5728\u8fd1\u4f3c\u8bef\u5dee\uff0c\u4f46\u5728\u4fdd\u6301\u6570\u636e\u70b9\u76f8\u5bf9\u6392\u5e8f\u65b9\u9762\u6709\u6548\uff0c\u4e3a\u6570\u636e\u5f52\u56e0\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.02139", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02139", "abs": "https://arxiv.org/abs/2602.02139", "authors": ["Pawel Batorski", "Paul Swoboda"], "title": "EvoMU: Evolutionary Machine Unlearning", "comment": null, "summary": "Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.", "AI": {"tldr": "EvoMU\u4f7f\u7528\u8fdb\u5316\u641c\u7d22\u81ea\u52a8\u53d1\u73b0\u7279\u5b9a\u4efb\u52a1\u7684\u65e0\u5b66\u4e60\u635f\u5931\u51fd\u6570\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u673a\u5668\u9057\u5fd8\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u9057\u5fd8\u548c\u4fdd\u7559\u6570\u636e\u7684\u7ed3\u6784\u5dee\u5f02\u627e\u5230\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u5316\u7684\u635f\u5931\u51fd\u6570\u641c\u7d22\u673a\u5236", "method": "\u91c7\u7528\u8fdb\u5316\u641c\u7d22\u7a0b\u5e8f\u5728\u5927\u91cf\u53ef\u80fd\u7684\u65e0\u5b66\u4e60\u635f\u5931\u51fd\u6570\u7a7a\u95f4\u4e2d\u81ea\u52a8\u53d1\u73b0\u4efb\u52a1\u7279\u5b9a\u7684\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u5c0f\u578b4B\u53c2\u6570\u6a21\u578b\u5b9e\u73b0", "result": "\u5728TOFU-5%\u3001TOFU-10%\u3001MUSE\u548cWMDP\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u57fa\u4e8e\u635f\u5931\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u5408\u6210\u4e86\u65b0\u9896\u7684\u65e0\u5b66\u4e60\u635f\u5931\u51fd\u6570", "conclusion": "EvoMU\u5c55\u793a\u4e86\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0bAI\u534f\u540c\u79d1\u5b66\u5bb6\u5728\u81ea\u52a8\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570"}}
{"id": "2602.01843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01843", "abs": "https://arxiv.org/abs/2602.01843", "authors": ["Qian Xu", "Xi Li", "Fei Gao", "Jie Guo", "Haojuan Yuan", "Shuaipeng Fan", "Mingjin Zhang"], "title": "SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.", "AI": {"tldr": "SPIRIT\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7269\u7406\u4fe1\u606f\u63d2\u4ef6\u9002\u914d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5230\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u89e3\u51b3\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u5e27\u548c\u591a\u5e27\u7edf\u4e00\u63a8\u7406\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9700\u8981\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u7ea2\u5916\u76ee\u6807\u4fe1\u53f7\u5f31\u3001\u8bed\u4e49\u7ebf\u7d22\u6709\u9650\uff0c\u4e0e\u53ef\u89c1\u5149\u8c31\u56fe\u50cf\u5b58\u5728\u6a21\u6001\u5dee\u5f02\uff0c\u76f4\u63a5\u4f7f\u7528\u8bed\u4e49\u5bfc\u5411\u7684VFM\u548c\u5916\u89c2\u9a71\u52a8\u7684\u8de8\u5e27\u5173\u8054\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faSPIRIT\u6846\u67b6\uff1a\u7a7a\u95f4\u4e0a\u4f7f\u7528PIFR\u901a\u8fc7\u8fd1\u4f3c\u79e9\u7a00\u758f\u5206\u89e3\u6291\u5236\u7ed3\u6784\u5316\u80cc\u666f\u3001\u589e\u5f3a\u7a00\u758f\u76ee\u6807\u4fe1\u53f7\uff1b\u65f6\u95f4\u4e0a\u4f7f\u7528PGMA\u5c06\u5386\u53f2\u8f6f\u7a7a\u95f4\u5148\u9a8c\u6ce8\u5165\u8bb0\u5fc6\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u7ea6\u675f\u8de8\u5e27\u5173\u8054\u3002", "result": "\u5728\u591a\u4e2a\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u4e8eVFM\u7684\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "SPIRIT\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u63d2\u4ef6\u6709\u6548\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\uff0c\u5b9e\u73b0\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u9002\u914d\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u5355\u5e27\u548c\u591a\u5e27\u63a8\u7406\u6846\u67b6\u3002"}}
{"id": "2602.02143", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02143", "abs": "https://arxiv.org/abs/2602.02143", "authors": ["Shubham Toshniwal", "Aleksander Ficek", "Siddhartha Jain", "Wei Du", "Vahid Noroozi", "Sadegh Mahdavi", "Somshubra Majumdar", "Igor Gitman"], "title": "Learning Generative Selection for Best-of-N", "comment": null, "summary": "Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.", "AI": {"tldr": "\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u5f3a\u5927\u7684\u751f\u6210\u9009\u62e9\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8a\u63d0\u793a\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\uff0c\u63a5\u8fd1\u6216\u8d85\u8fc7\u66f4\u5927\u6a21\u578b\u6027\u80fd", "motivation": "\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u663e\u8457\u6539\u5584LLM\u63a8\u7406\uff0c\u4f46\u53d7\u9650\u4e8eBest-of-N\u9009\u62e9\u8d28\u91cf\u3002\u73b0\u6709\u751f\u6210\u9009\u62e9\u65b9\u6cd5\uff08\u5982GenSelect\uff09\u5728\u5927\u578b\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u7684\u9009\u62e9\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u8ba9\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u5f3a\u5927\u7684\u751f\u6210\u9009\u62e9\u80fd\u529b\u3002", "method": "\u4ece\u5927\u89c4\u6a21\u6570\u5b66\u548c\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\u4e2d\u5408\u6210\u9009\u62e9\u4efb\u52a1\uff0c\u7b5b\u9009\u51fa\u540c\u65f6\u5305\u542b\u6b63\u786e\u548c\u9519\u8bef\u5019\u9009\u89e3\u51b3\u65b9\u6848\u7684\u5b9e\u4f8b\u3002\u4f7f\u7528DAPO\uff08\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff09\u8bad\u7ec31.7B\u53c2\u6570\u6a21\u578b\uff0c\u5956\u52b1\u6b63\u786e\u7684\u9009\u62e9\u51b3\u7b56\u3002\u8bad\u7ec3\u4ec5\u5728\u8f83\u5f31\u6a21\u578b\u7684\u8f93\u51fa\u4e0a\u8fdb\u884c\uff0c\u4f46\u8bc4\u4f30\u65f6\u6d4b\u8bd5\u5bf9\u66f4\u5f3a\u6a21\u578b\u8f93\u51fa\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff08AIME24\u3001AIME25\u3001HMMT25\uff09\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\uff08LiveCodeBench\uff09\u4e0a\uff0c\u8bad\u7ec3\u7684\u5c0f\u578b\u6a21\u578b\u4e00\u81f4\u4f18\u4e8e\u63d0\u793a\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\uff0c\u901a\u5e38\u63a5\u8fd1\u6216\u8d85\u8fc7\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002\u6a21\u578b\u7684\u9009\u62e9\u80fd\u529b\u80fd\u591f\u6cdb\u5316\u5230\u66f4\u5f3a\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u4ec5\u4f7f\u7528\u8f83\u5f31\u6a21\u578b\u7684\u8f93\u51fa\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u9501\u5c0f\u578b\u6a21\u578b\u5f3a\u5927\u751f\u6210\u9009\u62e9\u80fd\u529b\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u751f\u6210\u9009\u62e9\u80fd\u529b\uff0c\u4ece\u800c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.01844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01844", "abs": "https://arxiv.org/abs/2602.01844", "authors": ["Yuliang Zhan", "Jian Li", "Wenbing Huang", "Wenbing Huang", "Yang Liu", "Hao Sun"], "title": "CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions", "comment": "ICLR 2026", "summary": "Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\\footnote{As in this example.", "AI": {"tldr": "\u63d0\u51faCloth Dynamics Splatting (CloDS)\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u89c6\u89d2\u89c6\u89c9\u89c2\u6d4b\u4e2d\u65e0\u76d1\u7763\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u89c6\u9891\u5230\u51e0\u4f55\u7684\u6620\u5c04\u548c\u52a8\u529b\u5b66\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u7269\u7406\u5c5e\u6027\u4f5c\u4e3a\u76d1\u7763\u6216\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u7684\u5e94\u7528\u3002\u9700\u8981\u63a2\u7d22\u4ece\u89c6\u89c9\u89c2\u6d4b\u4e2d\u65e0\u76d1\u7763\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCloDS\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u89c6\u9891\u5230\u51e0\u4f55\u6620\u5c04\uff1b2) \u5728\u6620\u5c04\u7684\u7f51\u683c\u4e0a\u8bad\u7ec3\u52a8\u529b\u5b66\u6a21\u578b\u3002\u4e3a\u5904\u7406\u5927\u53d8\u5f62\u548c\u81ea\u906e\u6321\uff0c\u5f15\u5165\u57fa\u4e8e\u7f51\u683c\u9ad8\u65af\u6e85\u5c04\u7684\u53cc\u4f4d\u7f6e\u4e0d\u900f\u660e\u5ea6\u8c03\u5236\uff0c\u652f\u63012D\u89c2\u6d4b\u52303D\u51e0\u4f55\u7684\u53cc\u5411\u6620\u5c04\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cCloDS\u80fd\u6709\u6548\u4ece\u89c6\u89c9\u6570\u636e\u4e2d\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\uff0c\u540c\u65f6\u5bf9\u672a\u89c1\u914d\u7f6e\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CloDS\u4e3a\u65e0\u76d1\u7763\u5b66\u4e60\u5e03\u6599\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u7269\u7406\u5c5e\u6027\u7684\u9650\u5236\uff0c\u5728\u672a\u77e5\u6761\u4ef6\u4e0b\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.01338", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01338", "abs": "https://arxiv.org/abs/2602.01338", "authors": ["Fan Chen", "Sinho Chewi", "Constantinos Daskalakis", "Alexander Rakhlin"], "title": "High-accuracy sampling for diffusion models and log-concave distributions", "comment": null, "summary": "We present algorithms for diffusion model sampling which obtain $\u03b4$-error in $\\mathrm{polylog}(1/\u03b4)$ steps, given access to $\\widetilde O(\u03b4)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\\widetilde O(d\\,\\mathrm{polylog}(1/\u03b4))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\\widetilde O(\\sqrt{dL}\\,\\mathrm{polylog}(1/\u03b4))$; and if the data distribution has intrinsic dimension $d_\\star$, then the complexity reduces to $\\widetilde O(d_\\star\\,\\mathrm{polylog}(1/\u03b4))$. Our approach also yields the first $\\mathrm{polylog}(1/\u03b4)$ complexity sampler for general log-concave distributions using only gradient evaluations.", "AI": {"tldr": "\u63d0\u51fa\u6269\u6563\u6a21\u578b\u91c7\u6837\u7b97\u6cd5\uff0c\u5728\u4ec5\u9700\u591a\u9879\u5f0f\u5bf9\u6570\u6b65\u6570\u5185\u8fbe\u5230\u03b4\u8bef\u5dee\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u6709\u6307\u6570\u7ea7\u6539\u8fdb", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u91c7\u6837\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u6b65\u9aa4\u624d\u80fd\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u7b97\u6cd5", "method": "\u57fa\u4e8eL\u00b2\u8303\u6570\u4e0bO(\u03b4)\u7cbe\u5ea6\u5f97\u5206\u4f30\u8ba1\u7684\u7b97\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e0d\u540c\u590d\u6742\u5ea6\u4fdd\u8bc1", "result": "\u5728\u6700\u5c0f\u6570\u636e\u5047\u8bbe\u4e0b\u590d\u6742\u5ea6\u4e3aO\u0303(d\u00b7polylog(1/\u03b4))\uff0c\u975e\u5747\u5300L-Lipschitz\u6761\u4ef6\u4e0b\u4e3aO\u0303(\u221a(dL)\u00b7polylog(1/\u03b4))\uff0c\u5185\u5728\u7ef4\u5ea6d\u22c6\u65f6\u964d\u4e3aO\u0303(d\u22c6\u00b7polylog(1/\u03b4))", "conclusion": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6269\u6563\u6a21\u578b\u91c7\u6837\u7684\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u5e76\u4e3a\u4e00\u822c\u5bf9\u6570\u51f9\u5206\u5e03\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u9879\u5f0f\u5bf9\u6570\u590d\u6742\u5ea6\u91c7\u6837\u5668"}}
{"id": "2602.02151", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02151", "abs": "https://arxiv.org/abs/2602.02151", "authors": ["Yuli Zhou", "Qingxuan Chen", "Luca Benini", "Guolei Sun", "Yawei Li"], "title": "Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization", "comment": "17 pages, 6 figures, 14 tables", "summary": "Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.", "AI": {"tldr": "VQRound\uff1a\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u820d\u5165\u77e9\u9635\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u7d27\u51d1\u7801\u672c\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u820d\u5165\u91cf\u5316\uff0c\u4ec5\u97000.2%\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u81ea\u9002\u5e94\u820d\u5165\u4f5c\u4e3a\u820d\u5165\u5230\u6700\u8fd1\uff08RTN\uff09\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u8de8\u5143\u7d20\u8bef\u5dee\u62b5\u6d88\u5b9e\u73b0\u66f4\u597d\u7684\u91cf\u5316\u6548\u679c\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u5341\u4ebf\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5bc6\u96c6\u7684\u9010\u5143\u7d20\u820d\u5165\u77e9\u9635\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4ece\u6548\u7387\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u81ea\u9002\u5e94\u820d\u5165\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVQRound\u6846\u67b6\uff1a1\uff09\u5c06\u820d\u5165\u77e9\u9635\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u7d27\u51d1\u7801\u672c\uff1b2\uff09\u5728L\u221e\u8303\u6570\u4e0b\u6700\u5c0f\u5316\u9010\u5143\u7d20\u6700\u574f\u60c5\u51b5\u8bef\u5dee\uff0c\u7279\u522b\u9002\u5408\u5904\u7406LLM\u4e2d\u7684\u91cd\u5c3e\u6743\u91cd\u5206\u5e03\uff1b3\uff09\u8bc6\u522b\u820d\u5165\u521d\u59cb\u5316\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\uff1b4\uff09\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u5fae\u8c03\u6d41\u7a0b\uff0c\u4ec5\u9700128\u4e2a\u6837\u672c\u5373\u53ef\u4f18\u5316\u6240\u6709\u5c42\u7684\u7801\u672c\u3002", "result": "\u5728OPT\u3001LLaMA\u3001LLaMA2\u548cQwen3\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVQRound\u5728\u76f8\u540c\u6b65\u6570\u4e0b\u6bd4\u4f20\u7edf\u81ea\u9002\u5e94\u820d\u5165\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u6027\uff0c\u540c\u65f6\u4ec5\u4f7f\u75280.2%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u8bc1\u660e\u81ea\u9002\u5e94\u820d\u5165\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u5feb\u901f\u62df\u5408\u3002", "conclusion": "VQRound\u901a\u8fc7\u7801\u672c\u91cd\u65b0\u53c2\u6570\u5316\u548c\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff0c\u4f7f\u81ea\u9002\u5e94\u820d\u5165\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e0a\u53d8\u5f97\u9ad8\u6548\u53ef\u884c\uff0c\u4e3a\u540e\u8bad\u7ec3\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01850", "abs": "https://arxiv.org/abs/2602.01850", "authors": ["Pei Li", "Jiaxi Yin", "Lei Ouyang", "Shihan Pan", "Ge Wang", "Han Ding", "Fei Wang"], "title": "WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?", "comment": "Under Review. 28 pages, 9 figures, 6 tables", "summary": "IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.", "AI": {"tldr": "WS-IMUBench\uff1a\u9996\u4e2a\u7cfb\u7edf\u6027\u5f31\u76d1\u7763IMU\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u57fa\u51c6\uff0c\u8bc4\u4f307\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\u57287\u4e2a\u516c\u5f00IMU\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65f6\u5e8f\u65b9\u6cd5\u66f4\u7a33\u5b9a\uff0c\u5f31\u76d1\u7763\u5728\u6709\u5229\u6570\u636e\u96c6\u4e0a\u53ef\u5ab2\u7f8e\u5168\u76d1\u7763\uff0c\u5e76\u6307\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4f20\u7edfIMU\u52a8\u4f5c\u8bc6\u522b\u91c7\u7528\u7247\u6bb5\u5206\u7c7b\u8303\u5f0f\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u884c\u4e3a\u7684\u4e30\u5bcc\u65f6\u5e8f\u7ed3\u6784\u3002\u867d\u7136IMU\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d(IMU-TAL)\u80fd\u9884\u6d4b\u8fde\u7eed\u6d41\u4e2d\u7684\u52a8\u4f5c\u7c7b\u522b\u548c\u8d77\u6b62\u65f6\u95f4\uff0c\u4f46\u5f53\u524d\u8fdb\u5c55\u53d7\u9650\u4e8e\u9700\u8981\u5bc6\u96c6\u7684\u5e27\u7ea7\u8fb9\u754c\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5f15\u5165WS-IMUBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5f31\u76d1\u7763IMU-TAL\u65b9\u6cd5\u3002\u4e0d\u63d0\u51fa\u65b0\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u800c\u662f\u8bc4\u4f30\u97f3\u9891\u3001\u56fe\u50cf\u548c\u89c6\u9891\u9886\u57df\u7684\u5f31\u76d1\u7763\u5b9a\u4f4d\u8303\u5f0f\u5728\u4ec5\u6709\u5e8f\u5217\u7ea7\u6807\u7b7e\u4e0b\u5411IMU-TAL\u7684\u8fc1\u79fb\u6548\u679c\u3002\u57287\u4e2a\u516c\u5f00IMU\u6570\u636e\u96c6\u4e0a\u57fa\u51c6\u6d4b\u8bd57\u79cd\u4ee3\u8868\u6027\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u8fdb\u884c\u4e863540\u6b21\u6a21\u578b\u8bad\u7ec3\u548c7080\u6b21\u63a8\u7406\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u8fc1\u79fb\u6548\u679c\u4f9d\u8d56\u4e8e\u6a21\u6001\uff0c\u65f6\u5e8f\u65b9\u6cd5\u901a\u5e38\u6bd4\u57fa\u4e8e\u56fe\u50cf\u63d0\u6848\u7684\u65b9\u6cd5\u66f4\u7a33\u5b9a\uff1b(2)\u5728\u6709\u5229\u6570\u636e\u96c6\u4e0a\uff08\u5982\u52a8\u4f5c\u8f83\u957f\u3001\u4f20\u611f\u7ef4\u5ea6\u8f83\u9ad8\uff09\uff0c\u5f31\u76d1\u7763\u53ef\u5ab2\u7f8e\u5168\u76d1\u7763\uff1b(3)\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\u6e90\u4e8e\u77ed\u52a8\u4f5c\u3001\u65f6\u5e8f\u6a21\u7cca\u6027\u548c\u63d0\u6848\u8d28\u91cf\u5dee\u3002", "conclusion": "WS-IMUBench\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u677f\u3001\u6570\u636e\u96c6\u3001\u534f\u8bae\u548c\u5206\u6790\uff0c\u52a0\u901f\u793e\u533a\u5411\u53ef\u6269\u5c55\u7684\u5f31\u76d1\u7763IMU-TAL\u53d1\u5c55\u3002\u672a\u6765\u65b9\u5411\u5305\u62ecIMU\u7279\u5b9a\u7684\u63d0\u6848\u751f\u6210\u3001\u8fb9\u754c\u611f\u77e5\u76ee\u6807\u548c\u66f4\u5f3a\u7684\u65f6\u5e8f\u63a8\u7406\u3002"}}
{"id": "2602.01339", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01339", "abs": "https://arxiv.org/abs/2602.01339", "authors": ["Difei Xu", "Youming Tao", "Meng Ding", "Chenglin Fan", "Di Wang"], "title": "Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization", "comment": null, "summary": "We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(\u03b1,\\sqrt{\u03c1_\u03a6\u03b1})$-approximate second-order stationary point with $\u03b1= \\mathcal{O}( (\\frac{\\sqrt{d}}{n\\varepsilon})^{2/3})$ for empirical risk objectives and $\\mathcal{O}(\\frac{1}{n^{1/3}} + (\\frac{\\sqrt{d}}{n\\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.", "AI": {"tldr": "\u9996\u6b21\u7814\u7a76\u968f\u673a\u975e\u51f8\u6781\u5c0f\u6781\u5927\u4f18\u5316\u4e2d\u5bfb\u627e\u5dee\u5206\u9690\u79c1\u4e8c\u9636\u5e73\u7a33\u70b9\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5d4c\u5957\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u3001\u65b9\u5dee\u7f29\u51cf\u548c\u9ad8\u65af\u6270\u52a8\u7684\u7eaf\u4e00\u9636\u65b9\u6cd5\uff0c\u5728\u7ecf\u9a8c\u98ce\u9669\u548c\u603b\u4f53\u98ce\u9669\u4e0b\u5747\u83b7\u5f97\u6700\u4f18\u6536\u655b\u7387\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u8981\u4e48\u53ea\u5173\u6ce8\u6781\u5c0f\u5316\u95ee\u9898\u7684\u4e8c\u9636\u5e73\u7a33\u70b9\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u6781\u5c0f\u6781\u5927\u95ee\u9898\u7684\u4e00\u9636\u5e73\u7a33\u70b9\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u968f\u673a\u975e\u51f8\u6781\u5c0f\u6781\u5927\u4f18\u5316\u4e2d\u7684\u5dee\u5206\u9690\u79c1\u4e8c\u9636\u5e73\u7a33\u70b9\u95ee\u9898\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u7eaf\u4e00\u9636\u65b9\u6cd5\uff1a\u7ed3\u5408\u5d4c\u5957\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u65b9\u6848\u3001SPIDER\u98ce\u683c\u65b9\u5dee\u7f29\u51cf\u548c\u9ad8\u65af\u6270\u52a8\u6765\u786e\u4fdd\u9690\u79c1\u3002\u5173\u952e\u6280\u672f\u662f\u5757\u72b6\uff08q\u5468\u671f\uff09\u5206\u6790\uff0c\u63a7\u5236\u968f\u673a\u65b9\u5dee\u548c\u9690\u79c1\u566a\u58f0\u7684\u7d2f\u79ef\uff0c\u65e0\u9700\u5728\u6574\u4e2a\u8fed\u4ee3\u8303\u56f4\u5185\u6c42\u548c\u3002", "result": "\u5728\u6807\u51c6\u5149\u6ed1\u6027\u3001Hessian-Lipschitz\u6027\u548c\u5f3a\u51f9\u6027\u5047\u8bbe\u4e0b\uff0c\u5efa\u7acb\u4e86\u9ad8\u6982\u7387\u4fdd\u8bc1\uff1a\u5bf9\u4e8e\u7ecf\u9a8c\u98ce\u9669\u76ee\u6807\u8fbe\u5230(\u03b1,\u221a(\u03c1_\u03a6\u03b1))-\u8fd1\u4f3c\u4e8c\u9636\u5e73\u7a33\u70b9\uff0c\u03b1=O((\u221ad/n\u03b5)^{2/3})\uff1b\u5bf9\u4e8e\u603b\u4f53\u76ee\u6807\u8fbe\u5230O(1/n^{1/3}+(\u221ad/n\u03b5)^{1/2})\uff0c\u5339\u914d\u79c1\u6709\u4e00\u9636\u5e73\u7a33\u6027\u7684\u6700\u4f73\u5df2\u77e5\u901f\u7387\u3002", "conclusion": "\u9996\u6b21\u4e3a\u968f\u673a\u975e\u51f8\u6781\u5c0f\u6781\u5927\u4f18\u5316\u4e2d\u7684\u5dee\u5206\u9690\u79c1\u4e8c\u9636\u5e73\u7a33\u70b9\u63d0\u4f9b\u4e86\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7ecf\u9a8c\u98ce\u9669\u548c\u603b\u4f53\u98ce\u9669\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6536\u655b\u7387\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2602.02185", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02185", "abs": "https://arxiv.org/abs/2602.02185", "authors": ["Yu Zeng", "Wenxuan Huang", "Zhen Fang", "Shuang Chen", "Yufan Shen", "Yishuo Cai", "Xiaoman Wang", "Zhenfei Yin", "Lin Chen", "Zehui Chen", "Shiting Huang", "Yiming Zhao", "Yao Hu", "Philip Torr", "Wanli Ouyang", "Shaosheng Cao"], "title": "Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Vision-DeepResearch\u57fa\u51c6\uff08VDR-Bench\uff09\uff0c\u5305\u542b2000\u4e2aVQA\u5b9e\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u6587\u672c\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u5de5\u4f5c\u6d41\u6765\u63d0\u5347\u89c6\u89c9\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4e0d\u662f\u4ee5\u89c6\u89c9\u641c\u7d22\u4e3a\u4e2d\u5fc3\uff0c\u7b54\u6848\u7ecf\u5e38\u901a\u8fc7\u6587\u672c\u95ee\u9898\u4e2d\u7684\u8de8\u6587\u672c\u7ebf\u7d22\u6cc4\u9732\uff0c\u6216\u53ef\u4ece\u5f53\u524dMLLMs\u7684\u5148\u9a8c\u4e16\u754c\u77e5\u8bc6\u4e2d\u63a8\u65ad\uff1b2\uff09\u8bc4\u4f30\u573a\u666f\u8fc7\u4e8e\u7406\u60f3\u5316\uff0c\u56fe\u50cf\u641c\u7d22\u53ef\u901a\u8fc7\u8fd1\u7cbe\u786e\u5339\u914d\u83b7\u5f97\u4fe1\u606f\uff0c\u6587\u672c\u641c\u7d22\u5219\u8fc7\u4e8e\u76f4\u63a5\u4e14\u6311\u6218\u6027\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86VDR-Bench\u57fa\u51c6\uff0c\u5305\u542b2000\u4e2aVQA\u5b9e\u4f8b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u9636\u6bb5\u7b56\u5212\u6d41\u7a0b\u548c\u4e25\u683c\u7684\u4e13\u5bb6\u8bc4\u5ba1\u521b\u5efa\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u5de5\u4f5c\u6d41\uff0c\u4ee5\u63d0\u5347\u5f53\u524dMLLMs\u5728\u771f\u5b9e\u89c6\u89c9\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u7b56\u7565\u88ab\u8bc1\u660e\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u89c6\u89c9\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "VDR-Bench\u57fa\u51c6\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bc4\u4f30\u89c6\u89c9-\u6587\u672c\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u591a\u8f6e\u88c1\u526a\u641c\u7d22\u5de5\u4f5c\u6d41\u4e3a\u63d0\u5347MLLMs\u7684\u89c6\u89c9\u68c0\u7d22\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01851", "abs": "https://arxiv.org/abs/2602.01851", "authors": ["Huanyu Zhang", "Xuehai Bai", "Chengzu Li", "Chen Liang", "Haochen Tian", "Haodong Li", "Ruichuan An", "Yifan Zhang", "Anna Korhonen", "Zhang Zhang", "Liang Wang", "Tieniu Tan"], "title": "How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing", "comment": "https://vibe-benchmark.github.io/", "summary": "Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VIBE\u89c6\u89c9\u6307\u4ee4\u7f16\u8f91\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u5c42\u4ea4\u4e92\u5c42\u6b21\uff0c\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u9075\u5faa\u89c6\u89c9\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u4f46\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\u6307\u5bfc\uff0c\u800c\u4eba\u7c7b\u4ea4\u6d41\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u89c6\u89c9\u6307\u4ee4\uff08\u5982\u8349\u56fe\uff09\u80fd\u66f4\u6709\u6548\u5730\u4f20\u8fbe\u7a7a\u95f4\u548c\u7ed3\u6784\u610f\u56fe\u3002\u9700\u8981\u586b\u8865\u89c6\u89c9\u6307\u4ee4\u7f16\u8f91\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faVIBE\u89c6\u89c9\u6307\u4ee4\u7f16\u8f91\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u5c42\u4ea4\u4e92\u5c42\u6b21\uff1a\u6307\u793a\u6027\u5b9a\u4f4d\u3001\u5f62\u6001\u64cd\u4f5c\u548c\u56e0\u679c\u63a8\u7406\u3002\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eLMM-as-a-judge\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u6307\u6807\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e8617\u4e2a\u4ee3\u8868\u6027\u5f00\u6e90\u548c\u4e13\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u53d1\u73b0\u4e13\u6709\u6a21\u578b\u5c55\u73b0\u51fa\u65e9\u671f\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u3002\u4f46\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\uff0c\u5373\u4f7f\u6700\u5f3a\u7cfb\u7edf\u7684\u6027\u80fd\u4e5f\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u89c6\u89c9\u6307\u4ee4\u7f16\u8f91\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e13\u6709\u6a21\u578b\u867d\u6709\u4f18\u52bf\u4f46\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002"}}
{"id": "2602.01357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01357", "abs": "https://arxiv.org/abs/2602.01357", "authors": ["Shangzhe Li", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang"], "title": "Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning", "comment": "35 pages, 5 tables, 3 figures", "summary": "Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $\u03c7^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u81ea\u535a\u5f08\u5fae\u8c03\u4e0e\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u57fa\u4e8e\u03c7\u00b2\u6563\u5ea6\u7684\u81ea\u535a\u5f08\u6a21\u4eff\u5fae\u8c03\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u535a\u5f08\u540e\u8bad\u7ec3\u65b9\u6cd5\u5df2\u6210\u4e3a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u81ea\u535a\u5f08\u5fae\u8c03\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u5fae\u8c03\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u6a21\u578b\u4e0e\u7531\u6a21\u578b\u53c2\u6570\u5316\u7684\u6b63\u5219\u5316\u9690\u5f0f\u5956\u52b1\u73a9\u5bb6\u4e4b\u95f4\u7684min-max\u535a\u5f08\uff0c\u63d0\u51fa\u57fa\u4e8e\u03c7\u00b2\u6563\u5ea6\u53d8\u5206\u76ee\u6807\u7684\u81ea\u535a\u5f08\u6a21\u4eff\u5fae\u8c03\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u81ea\u535a\u5f08\u5fae\u8c03\u4f1a\u6536\u655b\u5230\u5747\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u81ea\u535a\u5f08\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u81ea\u535a\u5f08\u5fae\u8c03\u4e0e\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u8054\u7cfb\u8d77\u6765\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.01854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01854", "abs": "https://arxiv.org/abs/2602.01854", "authors": ["A S M Sharifuzzaman Sagar", "Mohammed Bennamoun", "Farid Boussaid", "Naeha Sharif", "Lian Xu", "Shaaban Sahmoud", "Ali Kishk"], "title": "Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection", "comment": null, "summary": "In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u65f6\uff0c\u57fa\u4e8e\u50cf\u7d20\u7ea7\u522b\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u6548\u679c\u6709\u9650\uff0c\u751a\u81f3\u4f1a\u964d\u4f4e\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u800c\u57fa\u4e8e\u8bed\u4e49\u7406\u89e3\u548c\u5916\u90e8\u8bc1\u636e\u7684\u7cfb\u7edf\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u4e13\u6ce8\u4e8e\u50cf\u7d20\u7ea7\u522b\u7684\u7be1\u6539\u68c0\u6d4b\uff0c\u4f46\u5ffd\u7565\u4e86\u56fe\u50cf-\u6587\u672c\u5bf9\u8868\u8fbe\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4e3b\u5f20\u3002\u8fd9\u4e9b\u68c0\u6d4b\u5668\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7ba1\u9053\u4e2d\uff0c\u4f46\u5b83\u4eec\u5728\u9a8c\u8bc1\u56fe\u50cf-\u6587\u672c\u4e3b\u5f20\u65f6\u662f\u5426\u63d0\u4f9b\u6709\u7528\u4fe1\u53f7\uff0c\u8fd8\u662f\u5f15\u5165\u4e86\u8bef\u5bfc\u6027\u7684\u771f\u5b9e\u6027\u5148\u9a8c\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u79d1\u5b66\u548c\u5b9e\u8df5\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6MMFakeBench\u548cDGM4\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u8bc4\u4f30\uff1a(1)\u6700\u5148\u8fdb\u7684\u4ec5\u56fe\u50cf\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\uff1b(2)\u57fa\u4e8e\u8bc1\u636e\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u8fdb\u884c\u5de5\u5177\u5f15\u5bfc\u68c0\u7d22\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8fdb\u884c\u6df1\u601d\u719f\u8651\u63a8\u7406\uff1b(3)\u5c06\u68c0\u6d4b\u5668\u8f93\u51fa\u4f5c\u4e3a\u8f85\u52a9\u8bc1\u636e\u6ce8\u5165\u7684\u6df7\u5408\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u3002", "result": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u5355\u72ec\u4f7f\u7528\u65f6\u6548\u679c\u6709\u9650\uff08MMFakeBench\u4e0aF1\u5206\u65700.26-0.53\uff0cDGM4\u4e0a0.33-0.49\uff09\uff0c\u5c06\u5176\u96c6\u6210\u5230\u4e8b\u5b9e\u6838\u67e5\u7ba1\u9053\u4e2d\u53cd\u800c\u4f7f\u6027\u80fd\u4e0b\u964d0.04-0.08 F1\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u4e8e\u8bc1\u636e\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u8868\u73b0\u6700\u4f73\uff0c\u5728MMFakeBench\u4e0a\u8fbe\u5230\u7ea60.81 F1\uff0c\u5728DGM4\u4e0a\u8fbe\u52300.55 F1\u3002", "conclusion": "\u591a\u6a21\u6001\u4e3b\u5f20\u9a8c\u8bc1\u4e3b\u8981\u7531\u8bed\u4e49\u7406\u89e3\u548c\u5916\u90e8\u8bc1\u636e\u9a71\u52a8\uff0c\u50cf\u7d20\u7ea7\u522b\u7684\u4f2a\u5f71\u4fe1\u53f7\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u589e\u5f3a\u5bf9\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf-\u6587\u672c\u865a\u5047\u4fe1\u606f\u7684\u63a8\u7406\u3002\u8bc1\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4f18\u4e8e\u4f9d\u8d56\u50cf\u7d20\u7ea7\u522b\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.01359", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01359", "abs": "https://arxiv.org/abs/2602.01359", "authors": ["Jinju Park", "Seokho Kang"], "title": "PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection", "comment": "Accepted by the 14th International Conference on Learning Representations (ICLR 2026)", "summary": "Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.", "AI": {"tldr": "PaAno\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u65f6\u95f4\u7247\u6bb5\u5e76\u4f7f\u75281D\u5377\u79ef\u7f51\u7edc\u5b66\u4e60\u8868\u793a\uff0c\u7ed3\u5408\u4e09\u5143\u7ec4\u635f\u5931\u548c\u9884\u6587\u672c\u635f\u5931\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6bd4\u8f83\u7247\u6bb5\u5d4c\u5165\u4e0e\u6b63\u5e38\u7247\u6bb5\u6765\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u5728TSB-AD\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982Transformer\u548c\u57fa\u7840\u6a21\u578b\uff09\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u4e14\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7247\u6bb5\u8868\u793a\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5PaAno\uff1a1\uff09\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u77ed\u65f6\u95f4\u7247\u6bb5\uff1b2\uff09\u4f7f\u75281D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5c06\u6bcf\u4e2a\u7247\u6bb5\u5d4c\u5165\u4e3a\u5411\u91cf\u8868\u793a\uff1b3\uff09\u7ed3\u5408\u4e09\u5143\u7ec4\u635f\u5931\u548c\u9884\u6587\u672c\u635f\u5931\u8bad\u7ec3\u6a21\u578b\uff1b4\uff09\u63a8\u7406\u65f6\u901a\u8fc7\u6bd4\u8f83\u5f53\u524d\u7247\u6bb5\u5d4c\u5165\u4e0e\u8bad\u7ec3\u6b63\u5e38\u7247\u6bb5\u5d4c\u5165\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728TSB-AD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaAno\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u91cd\u578b\u67b6\u6784\u7684\u65b9\u6cd5\uff09\uff0c\u5728\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u5404\u79cd\u8303\u56f4\u578b\u548c\u70b9\u578b\u6027\u80fd\u6307\u6807\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PaAno\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02244", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02244", "abs": "https://arxiv.org/abs/2602.02244", "authors": ["Hao Wang", "Hao Gu", "Hongming Piao", "Kaixiong Gong", "Yuxiao Ye", "Xiangyu Yue", "Sirui Han", "Yike Guo", "Dapeng Wu"], "title": "Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models", "comment": null, "summary": "The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.", "AI": {"tldr": "CurioSFT\uff1a\u4e00\u79cd\u4fdd\u6301\u71b5\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5728\u597d\u5947\u5fc3\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u63d0\u4f9b\u66f4\u597d\u7684\u8d77\u70b9", "motivation": "\u4f20\u7edf\u7684SFT-then-RL\u6d41\u7a0b\u5b58\u5728\u95ee\u9898\uff1aSFT\u9636\u6bb5\u6a21\u4eff\u4e13\u5bb6\u6f14\u793a\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u548c\u751f\u6210\u591a\u6837\u6027\u964d\u4f4e\uff0c\u9650\u5236\u4e86RL\u9636\u6bb5\u7684\u63a2\u7d22\u7a7a\u95f4\u3002\u7b80\u5355\u7684\u71b5\u6b63\u5219\u5316\u4f1a\u5747\u5300\u5316token\u5206\u5e03\uff0c\u65e0\u6cd5\u771f\u6b63\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faCurioSFT\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u81ea\u6211\u63a2\u7d22\u84b8\u998f\uff1a\u5c06\u6a21\u578b\u84b8\u998f\u5230\u81ea\u8eab\u751f\u6210\u7684\u6e29\u5ea6\u7f29\u653e\u7248\u672c\uff0c\u9f13\u52b1\u5728\u80fd\u529b\u8303\u56f4\u5185\u63a2\u7d22\uff1b(2) \u71b5\u5f15\u5bfc\u6e29\u5ea6\u9009\u62e9\uff1a\u81ea\u9002\u5e94\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u5728\u63a8\u7406token\u4e0a\u589e\u5f3a\u63a2\u7d22\uff0c\u5728\u4e8b\u5b9etoken\u4e0a\u4fdd\u6301\u7a33\u5b9a\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cCurioSFT\u5728SFT\u9636\u6bb5\u6bd4\u4f20\u7edfSFT\u63d0\u53472.5\u4e2a\u70b9\uff08\u5206\u5e03\u5185\uff09\u548c2.9\u4e2a\u70b9\uff08\u5206\u5e03\u5916\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cSFT\u9636\u6bb5\u4fdd\u7559\u7684\u63a2\u7d22\u80fd\u529b\u6210\u529f\u8f6c\u5316\u4e3aRL\u9636\u6bb5\u7684\u5177\u4f53\u6536\u76ca\uff0c\u5e73\u5747\u63d0\u53475.0\u4e2a\u70b9\u3002", "conclusion": "CurioSFT\u901a\u8fc7\u4fdd\u6301\u71b5\u548c\u589e\u5f3a\u5185\u5728\u597d\u5947\u5fc3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfSFT\u7684\u9650\u5236\uff0c\u4e3a\u540e\u7eedRL\u9636\u6bb5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u63a2\u7d22\u8d77\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01864", "abs": "https://arxiv.org/abs/2602.01864", "authors": ["Yuan Wang", "Yuhao Wan", "Siming Zheng", "Bo Li", "Qibin Hou", "Peng-Tao Jiang"], "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling", "comment": "26 pages, 19 figures. Accepted to ICLR 2026", "summary": "Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a \"Trust but Verify\" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.", "AI": {"tldr": "Ada-RefSR\uff1a\u57fa\u4e8e\"\u4fe1\u4efb\u4f46\u9a8c\u8bc1\"\u539f\u5219\u7684\u5355\u6b65\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9690\u5f0f\u5173\u8054\u95e8\u63a7\u673a\u5236\uff0c\u5728\u53c2\u8003\u56fe\u50cf\u53ef\u9760\u65f6\u5229\u7528\u53c2\u8003\u4fe1\u606f\uff0c\u4e0d\u53ef\u9760\u65f6\u6291\u5236\u53c2\u8003\u4fe1\u606f\uff0c\u89e3\u51b3\u53c2\u8003\u56fe\u50cf\u4e0e\u4f4e\u8d28\u91cf\u8f93\u5165\u5bf9\u5e94\u5173\u7cfb\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u50cf\u9000\u5316\u4f7f\u5f97\u4f4e\u8d28\u91cf\u8f93\u5165\u4e0e\u53c2\u8003\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u4e0d\u53ef\u9760\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u8fd9\u79cd\u76f8\u5173\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u8106\u5f31\u7684\u663e\u5f0f\u5339\u914d\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4f9d\u8d56\u8bef\u5bfc\u6027\u53c2\u8003\u6216\u672a\u80fd\u5145\u5206\u5229\u7528\u6709\u4ef7\u503c\u7684\u7ebf\u7d22\u3002", "method": "\u63d0\u51faAda-RefSR\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u9690\u5f0f\u5173\u8054\u95e8\u63a7\u673a\u5236\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6458\u8981\u4ee4\u724c\u6765\u63d0\u53d6\u4e3b\u8981\u53c2\u8003\u6a21\u5f0f\uff0c\u5e76\u6355\u83b7\u4e0e\u4f4e\u8d28\u91cf\u7279\u5f81\u7684\u9690\u5f0f\u76f8\u5173\u6027\uff0c\u96c6\u6210\u5230\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u81ea\u9002\u5e94\u53c2\u8003\u5f15\u5bfc\u8c03\u8282\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAda-RefSR\u5728\u4fdd\u771f\u5ea6\u3001\u81ea\u7136\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u53c2\u8003\u5bf9\u9f50\u6761\u4ef6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "Ada-RefSR\u901a\u8fc7\"\u4fe1\u4efb\u4f46\u9a8c\u8bc1\"\u539f\u5219\u548c\u81ea\u9002\u5e94\u9690\u5f0f\u5173\u8054\u95e8\u63a7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53c2\u8003\u56fe\u50cf\u4e0e\u4f4e\u8d28\u91cf\u8f93\u5165\u5bf9\u5e94\u5173\u7cfb\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u53c2\u8003\u5f15\u5bfc\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2602.02285", "categories": ["cs.LG", "cs.CL", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02285", "abs": "https://arxiv.org/abs/2602.02285", "authors": ["Yuanhe Zhang", "Jason D. Lee", "Fanghui Liu"], "title": "Statistical Learning Theory in Lean 4: Empirical Processes from Scratch", "comment": "19 pages, 2 figures. Comments are welcome", "summary": "We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u8fc7\u7a0b\u7406\u8bba\u7684\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u5b8c\u6574Lean 4\u5f62\u5f0f\u5316\uff0c\u586b\u8865\u4e86Mathlib\u5e93\u7a7a\u767d\uff0c\u5305\u542b\u9ad8\u65afLipschitz\u96c6\u4e2d\u6027\u3001Dudley\u71b5\u79ef\u5206\u5b9a\u7406\u7b49\u5f62\u5f0f\u5316\uff0c\u5e94\u7528\u4e8e\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u3002", "motivation": "\u586b\u8865Lean 4 Mathlib\u5e93\u5728\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u5efa\u7acb\u53ef\u91cd\u7528\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u8fc7\u7a0b\u63ed\u793a\u548c\u89e3\u51b3\u6807\u51c6SLT\u6559\u79d1\u4e66\u4e2d\u7684\u9690\u542b\u5047\u8bbe\u548c\u7f3a\u5931\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\uff1a\u4eba\u7c7b\u8bbe\u8ba1\u8bc1\u660e\u7b56\u7565\uff0cAI\u4ee3\u7406\u6267\u884c\u6218\u672f\u8bc1\u660e\u6784\u9020\uff0c\u6700\u7ec8\u751f\u6210\u7ecf\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u7684Lean 4\u5de5\u5177\u7bb1\u3002\u5b9e\u73b0\u4e86\u9ad8\u65afLipschitz\u96c6\u4e2d\u6027\u3001Dudley\u71b5\u79ef\u5206\u5b9a\u7406\u7b49\u5f62\u5f0f\u5316\uff0c\u5e76\u5e94\u7528\u4e8e\u6700\u5c0f\u4e8c\u4e58\uff08\u7a00\u758f\uff09\u56de\u5f52\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u8fc7\u7a0b\u7406\u8bba\u7684\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u5b8c\u6574Lean 4\u5f62\u5f0f\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u586b\u8865\u4e86Mathlib\u5e93\u7a7a\u767d\uff0c\u83b7\u5f97\u4e86\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\u7684\u5c16\u9510\u901f\u7387\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u53ef\u91cd\u7528\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7684\u672a\u6765\u53d1\u5c55\u6253\u5f00\u4e86\u5927\u95e8\uff0c\u540c\u65f6\u901a\u8fc7\u5f62\u5f0f\u5316\u8fc7\u7a0b\u52a0\u6df1\u4e86\u5bf9\u7406\u8bba\u7684\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u6559\u79d1\u4e66\u4e2d\u7684\u9690\u542b\u95ee\u9898\u3002"}}
{"id": "2602.01881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01881", "abs": "https://arxiv.org/abs/2602.01881", "authors": ["Ye Chen", "Yupeng Zhu", "Xiongzhen Zhang", "Zhewen Wan", "Yingzhe Li", "Wenjun Zhang", "Bingbing Ni"], "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding", "comment": null, "summary": "Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u4ee3\u7406\u7684\u53c2\u91cf\u5316\u56fe\u50cf\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u7eb9\u7406\u5c5e\u6027\u89e3\u8026\u5230\u72ec\u7acb\u53ef\u64cd\u63a7\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u5b9e\u65f6\u7269\u7406\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u8868\u793a\u65b9\u6cd5\uff08\u5982\u5149\u6805\u56fe\u50cf\u3001\u9ad8\u65af\u57fa\u5143\u7b49\u663e\u5f0f\u8868\u793a\uff0c\u6216\u9690\u5f0f\u8868\u793a\uff09\u5b58\u5728\u8868\u793a\u5197\u4f59\u5bfc\u81f4\u7f16\u8f91\u56f0\u96be\uff0c\u6216\u7f3a\u4e4f\u4ece\u9690\u53d8\u91cf\u5230\u8bed\u4e49\u5b9e\u4f8b/\u90e8\u4ef6\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u96be\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u64cd\u63a7\uff0c\u963b\u788d\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u56fe\u50cf\u89c6\u9891\u7f16\u8f91\u3002", "method": "\u57fa\u4e8e\u8f93\u5165\u56fe\u50cf\u7684\u8bed\u4e49\u611f\u77e5\u5206\u89e3\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d1d\u585e\u5c14\u62df\u5408\u548c\u8fed\u4ee3\u5185\u90e8\u533a\u57df\u7ec6\u5206\u7f51\u683c\u5316\u6784\u5efa\u5206\u5c42\u4ee3\u7406\u51e0\u4f55\u3002\u5c06\u591a\u5c3a\u5ea6\u9690\u5f0f\u7eb9\u7406\u53c2\u6570\u5d4c\u5165\u5230\u51e0\u4f55\u611f\u77e5\u7684\u5206\u5e03\u5f0f\u4ee3\u7406\u8282\u70b9\u4e2d\uff0c\u5b9e\u73b0\u50cf\u7d20\u57df\u8fde\u7eed\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u5b9e\u4f8b/\u90e8\u4ef6\u72ec\u7acb\u7684\u8bed\u4e49\u7f16\u8f91\u3002\u5f15\u5165\u5c40\u90e8\u81ea\u9002\u5e94\u7279\u5f81\u7d22\u5f15\u673a\u5236\u786e\u4fdd\u7a7a\u95f4\u7eb9\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5728ImageNet\u3001OIR-Bench\u3001HumanEdit\u7b49\u56fe\u50cf\u91cd\u5efa\u548c\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u4fdd\u771f\u5ea6\uff0c\u652f\u6301\u76f4\u89c2\u3001\u4ea4\u4e92\u5f0f\u548c\u7269\u7406\u5408\u7406\u7684\u64cd\u63a7\u3002\u901a\u8fc7\u5c06\u4ee3\u7406\u8282\u70b9\u4e0e\u57fa\u4e8e\u4f4d\u7f6e\u7684\u52a8\u529b\u5b66\u7ed3\u5408\uff0c\u652f\u6301\u8f7b\u91cf\u7ea7\u9690\u5f0f\u6e32\u67d3\u7684\u5b9e\u65f6\u7269\u7406\u9a71\u52a8\u52a8\u753b\uff0c\u76f8\u6bd4\u751f\u6210\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u4ee3\u7406\u53c2\u91cf\u5316\u56fe\u50cf\u8868\u793a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8868\u793a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u52a8\u753b\u751f\u6210\uff0c\u4e3a\u56fe\u50cf\u8868\u793a\u548c\u64cd\u63a7\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01367", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01367", "abs": "https://arxiv.org/abs/2602.01367", "authors": ["Pinar Erbil", "Alberto Archetti", "Eugenio Lomurno", "Matteo Matteucci"], "title": "Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation", "comment": null, "summary": "Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.", "AI": {"tldr": "CONVERSE\u662f\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u6df1\u5ea6\u751f\u5b58\u5206\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6b65\u5b66\u4e60\u548c\u805a\u7c7b\u7279\u5b9a\u751f\u5b58\u5934\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u98ce\u9669\u5206\u5c42\u7684\u7edf\u4e00\u3002", "motivation": "\u751f\u5b58\u5206\u6790\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6839\u672c\u6743\u8861\u3002\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u51c6\u786e\u5ea6\u9ad8\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u805a\u7c7b\u7684\u65b9\u6cd5\u867d\u7136\u53ef\u89e3\u91ca\u4f46\u727a\u7272\u4e86\u9884\u6d4b\u80fd\u529b\u3002", "method": "CONVERSE\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u53d8\u5206\u5d4c\u5165\u548c\u591a\u79cd\u7c07\u5185\u7c07\u95f4\u5bf9\u6bd4\u635f\u5931\uff0c\u91c7\u7528\u81ea\u6b65\u5b66\u4e60\u4ece\u6613\u5230\u96be\u9010\u6b65\u7eb3\u5165\u6837\u672c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u652f\u6301\u805a\u7c7b\u7279\u5b9a\u751f\u5b58\u5934\u8fdb\u884c\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cCONVERSE\u76f8\u6bd4\u73b0\u6709\u6df1\u5ea6\u751f\u5b58\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u610f\u4e49\u7684\u60a3\u8005\u5206\u5c42\u3002", "conclusion": "CONVERSE\u6210\u529f\u5f25\u5408\u4e86\u6df1\u5ea6\u751f\u5b58\u5206\u6790\u4e2d\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u5206\u5c42\u5de5\u5177\u3002"}}
{"id": "2602.01901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01901", "abs": "https://arxiv.org/abs/2602.01901", "authors": ["Jiedong Zhuang", "Lu Lu", "Ming Dai", "Rui Hu", "Jian Chen", "Qiang Liu", "Haoji Hu"], "title": "Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model", "comment": "Accepted by AAAI26", "summary": "Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.", "AI": {"tldr": "\u63d0\u51faLazy Attention\u673a\u5236\uff0c\u901a\u8fc7\u8de8\u5c42\u5171\u4eab\u76f8\u4f3c\u6ce8\u610f\u529b\u6a21\u5f0f\u51cf\u5c11MLLMs\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u964d\u4f4eKV\u7f13\u5b5835%\u4ee5\u4e0a\uff0c\u63d0\u5347\u541e\u5410\u91cf1.5\u500d\uff0c\u6027\u80fd\u635f\u5931\u4ec5\u7ea61%", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u56e0\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u5927\u91cf\u89c6\u89c9token\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\u4f1a\u7834\u574fKV\u7f13\u5b58\u5b8c\u6574\u6027\uff0c\u5f71\u54cd\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1", "method": "\u63d0\u51faLazy Attention\u673a\u5236\uff0c\u53d1\u73b0\u8d85\u8fc7\u4e00\u534a\u89e3\u7801\u5c42\u7684\u6ce8\u610f\u529b\u8bed\u4e49\u76f8\u4f3c\uff0c\u901a\u8fc7\u8de8\u5c42\u5171\u4eab\u6ce8\u610f\u529b\u6a21\u5f0f\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7Q Cache\u652f\u6301\u76f8\u90bb\u5c42\u95f4\u67e5\u8be2\u91cd\u7528\uff0c\u517c\u5bb9\u73b0\u6709\u63a8\u7406\u6846\u67b6", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u80fd\u51cf\u5c11KV\u7f13\u5b58\u4f7f\u7528\u8d85\u8fc735%\uff0c\u5b9e\u73b01.5\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5404\u79cdMLLMs\u4e0a\u6027\u80fd\u635f\u5931\u4ec5\u7ea61%\uff0c\u4f18\u4e8e\u73b0\u6709SOTA token\u526a\u679d\u65b9\u6cd5", "conclusion": "Lazy Attention\u901a\u8fc7\u8de8\u5c42\u5171\u4eab\u76f8\u4f3c\u6ce8\u610f\u529b\u6a21\u5f0f\u6709\u6548\u51cf\u5c11MLLMs\u5197\u4f59\u8ba1\u7b97\uff0c\u4e0e\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\u6b63\u4ea4\u4e14\u517c\u5bb9\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387"}}
{"id": "2602.01399", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01399", "abs": "https://arxiv.org/abs/2602.01399", "authors": ["Fabian Fumagalli", "Landon Butler", "Justin Singh Kang", "Kannan Ramchandran", "R. Teal Witter"], "title": "An Odd Estimator for Shapley Values", "comment": null, "summary": "The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.", "AI": {"tldr": "\u63d0\u51faOddSHAP\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5947\u5b50\u7a7a\u95f4\u591a\u9879\u5f0f\u56de\u5f52\u6539\u8fdbShapley\u503c\u4f30\u8ba1\uff0c\u5229\u7528\u5085\u91cc\u53f6\u57fa\u5206\u79bb\u5947\u5076\u5206\u91cf\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "Shapley\u503c\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f52\u56e0\u6846\u67b6\uff0c\u4f46\u7cbe\u786e\u8ba1\u7b97\u901a\u5e38\u4e0d\u53ef\u884c\u3002\u73b0\u6709\u6700\u6709\u6548\u7684\u4f30\u8ba1\u5668\u4f7f\u7528\u914d\u5bf9\u91c7\u6837\u542f\u53d1\u5f0f\u65b9\u6cd5\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff0c\u4f46\u5176\u7406\u8bba\u673a\u5236\u4e00\u76f4\u4e0d\u660e\u786e\u3002", "method": "\u8bc1\u660eShapley\u503c\u4ec5\u4f9d\u8d56\u4e8e\u96c6\u5408\u51fd\u6570\u7684\u5947\u5206\u91cf\uff0c\u914d\u5bf9\u91c7\u6837\u901a\u8fc7\u6b63\u4ea4\u5316\u56de\u5f52\u76ee\u6807\u6765\u8fc7\u6ee4\u65e0\u5173\u7684\u5076\u5206\u91cf\u3002\u63d0\u51faOddSHAP\u4f30\u8ba1\u5668\uff0c\u5728\u5947\u5b50\u7a7a\u95f4\u4e0a\u8fdb\u884c\u591a\u9879\u5f0f\u56de\u5f52\uff0c\u5229\u7528\u5085\u91cc\u53f6\u57fa\u5206\u79bb\u5947\u5076\u5206\u91cf\uff0c\u5e76\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u8bc6\u522b\u9ad8\u5f71\u54cd\u4ea4\u4e92\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u57fa\u51c6\u8bc4\u4f30\uff0cOddSHAP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u514b\u670d\u4e86\u9ad8\u9636\u8fd1\u4f3c\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "conclusion": "\u4e3a\u914d\u5bf9\u91c7\u6837\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u5176\u901a\u8fc7\u6b63\u4ea4\u5316\u56de\u5f52\u76ee\u6807\u8fc7\u6ee4\u5076\u5206\u91cf\u6765\u6539\u8fdb\u4f30\u8ba1\u3002\u63d0\u51fa\u7684OddSHAP\u65b9\u6cd5\u5728\u5947\u5b50\u7a7a\u95f4\u4e0a\u8fdb\u884c\u591a\u9879\u5f0f\u56de\u5f52\uff0c\u663e\u8457\u63d0\u5347\u4e86Shapley\u503c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.01905", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01905", "abs": "https://arxiv.org/abs/2602.01905", "authors": ["Theodore Zhengde Zhao", "Sid Kiblawi", "Jianwei Yang", "Naoto Usuyama", "Reuben Tan", "Noel C Codella", "Tristan Naumann", "Hoifung Poon", "Mu Wei"], "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization", "comment": null, "summary": "Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.", "AI": {"tldr": "STELLAR\u6846\u67b6\u901a\u8fc7\u5c06\u89c6\u89c9\u7279\u5f81\u5206\u89e3\u4e3a\u8bed\u4e49\u6982\u5ff5\u4e0e\u5176\u7a7a\u95f4\u5206\u5e03\u7684\u4e58\u79ef\uff0c\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u8bed\u4e49\u7406\u89e3\u4e0e\u56fe\u50cf\u91cd\u5efa\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u652f\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u8bed\u4e49\u8bc6\u522b\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u9762\u4e34\u8bed\u4e49\u7406\u89e3\u4e0e\u56fe\u50cf\u91cd\u5efa\u4e4b\u95f4\u7684\u6839\u672c\u51b2\u7a81\uff1a\u8bed\u4e49SSL\uff08\u5982DINO\uff09\u4f9d\u8d56\u5168\u5c40\u4ee4\u724c\u4f46\u4e22\u5f03\u7a7a\u95f4\u5750\u6807\uff0c\u800c\u751f\u6210\u5f0fSSL\uff08\u5982MAE\uff09\u4fdd\u7559\u5bc6\u96c6\u7279\u5f81\u7f51\u683c\u4f46\u65e0\u6cd5\u4ea7\u751f\u9ad8\u7ea7\u62bd\u8c61\u3002", "method": "\u5c06\u89c6\u89c9\u7279\u5f81\u5206\u89e3\u4e3a\u8bed\u4e49\u6982\u5ff5\u548c\u7a7a\u95f4\u5206\u5e03\u7684\u4e58\u79ef\uff0c\u5728\u8bed\u4e49\u4ee4\u724c\u4e0a\u6267\u884cDINO\u98ce\u683c\u7684\u589e\u5f3a\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u5b9a\u4f4d\u77e9\u9635\u4e2d\u4fdd\u6301\u7cbe\u786e\u7684\u7a7a\u95f4\u6620\u5c04\u4ee5\u652f\u6301\u50cf\u7d20\u7ea7\u91cd\u5efa\u3002", "result": "\u4ec5\u970016\u4e2a\u7a00\u758f\u4ee4\u724c\u5373\u53ef\u540c\u65f6\u652f\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\uff082.60 FID\uff09\u548c\u5339\u914d\u5bc6\u96c6\u9aa8\u5e72\u7f51\u7edc\u7684\u8bed\u4e49\u6027\u80fd\uff0879.10% ImageNet\u51c6\u786e\u7387\uff09\u3002", "conclusion": "STELLAR\u4f5c\u4e3a\u4e00\u79cd\u591a\u529f\u80fd\u7a00\u758f\u8868\u793a\uff0c\u901a\u8fc7\u6218\u7565\u6027\u5730\u5206\u79bb\u8bed\u4e49\u8eab\u4efd\u548c\u7a7a\u95f4\u51e0\u4f55\uff0c\u5f25\u5408\u4e86\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u89c6\u89c9\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.01410", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01410", "abs": "https://arxiv.org/abs/2602.01410", "authors": ["Yunjie Pan", "Yongyi Yang", "Hanmei Yang", "Scott Mahlke"], "title": "SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training", "comment": "Accepted to ASPLOS 2026", "summary": "Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.", "AI": {"tldr": "SNIP\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u671f\u6536\u96c6\u7edf\u8ba1\u4fe1\u606f\u5e76\u5b9a\u4e49\u524d\u5411\u635f\u5931\u53d1\u6563\u548c\u540e\u5411\u6743\u91cd\u53d1\u6563\u4e24\u4e2a\u5173\u952e\u6307\u6807\uff0c\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4f18\u5316\u5c42\u95f4\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5f53\u524d\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6cd5\u8981\u4e48\u5bf9\u6240\u6709GEMM\u64cd\u4f5c\u5e94\u7528\u7edf\u4e00\u7cbe\u5ea6\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u6cdb\u5316\uff0c\u5bfc\u81f4\u6536\u655b\u6b21\u4f18\u548c\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u652f\u6301\u4e9a\u5b57\u8282\u7cbe\u5ea6\u3001\u81ea\u9002\u5e94\u4f18\u5316\u7cbe\u5ea6\u5206\u914d\u7684\u65b9\u6cd5\u6765\u9ad8\u6548\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u3002", "method": "SNIP\u6846\u67b6\u5b9a\u671f\u6536\u96c6\u6fc0\u6d3b\u3001\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u5b9a\u4e49\u524d\u5411\u635f\u5931\u53d1\u6563\uff08\u91cf\u5316\u5f15\u8d77\u7684\u8bad\u7ec3\u635f\u5931\u589e\u52a0\uff09\u548c\u540e\u5411\u6743\u91cd\u53d1\u6563\uff08\u68af\u5ea6\u8bef\u5dee\u4f20\u64ad\u5f71\u54cd\u6a21\u578b\u66f4\u65b0\uff09\u4e24\u4e2a\u6307\u6807\u3002\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u7cfb\u7edf\u4f18\u5316\u5c42\u95f4\u7cbe\u5ea6\uff0c\u5728\u6ee1\u8db3\u6548\u7387\u76ee\u6807\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6574\u4f53\u8d28\u91cf\u635f\u5931\u3002", "result": "\u57281B\u30013B\u30017B\u548c70B Llama-like\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSNIP\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u5c06FLOPs\u51cf\u5c11\u9ad8\u8fbe80%\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u9636\u6bb5\u90fd\u80fd\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "SNIP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u7cbe\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.01906", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01906", "abs": "https://arxiv.org/abs/2602.01906", "authors": ["Farhan Ullah", "Irfan Ullah", "Khalil Khan", "Giovanni Pau", "JaKeoung Koo"], "title": "DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification", "comment": null, "summary": "Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.", "AI": {"tldr": "DSXFormer\uff1a\u4e00\u79cd\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u65b0\u578b\u53cc\u6c60\u5316\u5149\u8c31\u6324\u538b-\u6269\u5c55Transformer\uff0c\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u5149\u8c31\u5f3a\u8c03\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u9ad8\u5149\u8c31\u7ef4\u5ea6\u3001\u590d\u6742\u5149\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u6709\u9650\u6807\u8bb0\u6837\u672c\u7684\u6311\u6218\u3002\u73b0\u6709Transformer\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u8db3\u591f\u7684\u5149\u8c31\u53ef\u533a\u5206\u6027\u3002", "method": "\u63d0\u51faDSXFormer\uff0c\u5305\u542b\uff1a1\uff09\u53cc\u6c60\u5316\u5149\u8c31\u6324\u538b-\u6269\u5c55\uff08DSX\uff09\u5757\uff0c\u5229\u7528\u5168\u5c40\u5e73\u5747\u548c\u6700\u5927\u6c60\u5316\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u5149\u8c31\u7279\u5f81\u901a\u9053\uff1b2\uff09\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\uff08DCA\uff09\u673a\u5236\uff0c\u5728\u57fa\u4e8e\u7a97\u53e3\u7684Transformer\u67b6\u6784\u4e2d\u52a8\u6001\u6355\u83b7\u5c40\u90e8\u5149\u8c31-\u7a7a\u95f4\u5173\u7cfb\uff1b3\uff09\u8865\u4e01\u63d0\u53d6\u3001\u5d4c\u5165\u548c\u5408\u5e76\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u5c3a\u5ea6\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u9ad8\u5149\u8c31\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff08Salinas\u3001Indian Pines\u3001Pavia University\u3001Kennedy Space Center\uff09\u5206\u522b\u8fbe\u523099.95%\u300198.91%\u300199.85%\u300198.52%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DSXFormer\u901a\u8fc7\u5149\u8c31\u53cc\u6c60\u5316\u6324\u538b-\u6269\u5c55\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u7684\u8054\u5408\u96c6\u6210\uff0c\u5728\u5149\u8c31\u5f3a\u8c03\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u8868\u793a\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u6548\u5e73\u8861\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01419", "abs": "https://arxiv.org/abs/2602.01419", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb", "Emmanuel Stathatos", "Panorios Benardos", "George-Christopher Vosniakos"], "title": "Semi-supervised CAPP Transformer Learning via Pseudo-labeling", "comment": null, "summary": "High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u57fa\u4e8eTransformer\u7684CAPP\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u901a\u8fc7\u8bad\u7ec3oracle\u7b5b\u9009\u6b63\u786e\u9884\u6d4b\u7528\u4e8e\u5355\u6b21\u91cd\u8bad\u7ec3\uff0c\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u9ad8\u7ea7\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u827a\u89c4\u5212(CAPP)\u4ece\u96f6\u4ef6\u89c4\u683c\u751f\u6210\u5236\u9020\u5de5\u827a\u8ba1\u5212\uff0c\u4f46\u5de5\u4e1a\u4e2d\u6570\u636e\u96c6\u53ef\u7528\u6027\u6709\u9650\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff1a\u8bad\u7ec3oracle\u6a21\u578b\u57fa\u4e8e\u53ef\u7528Transformer\u884c\u4e3a\u6570\u636e\u7b5b\u9009\u6b63\u786e\u9884\u6d4b\uff0c\u5c06\u7b5b\u9009\u51fa\u7684\u6b63\u786e\u9884\u6d4b\u7528\u4e8e\u5355\u6b21\u91cd\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8", "result": "\u5728\u5168\u6570\u636e\u5206\u5e03\u7684\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u5236\u9020\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347Transformer-based CAPP\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2602.02472", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02472", "abs": "https://arxiv.org/abs/2602.02472", "authors": ["Qifan Yu", "Xinyu Ma", "Zhijian Zhuo", "Minrui Wang", "Deyi Liu", "Shiyi Zhan", "Yiyuan Ma", "Liang Xiang", "Xingyan Bin", "Di He"], "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning", "comment": null, "summary": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\\times$ width expansion.", "AI": {"tldr": "SPARKLING\u662f\u4e00\u79cd\u7528\u4e8e\u6a21\u578b\u5bbd\u5ea6\u6269\u5c55\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7RMS\u5c3a\u5ea6\u4e00\u81f4\u6027\u548c\u975e\u5bf9\u79f0\u4f18\u5316\u5668\u72b6\u6001\u91cd\u7f6e\u89e3\u51b3\u4e2d\u9636\u6bb5\u5bbd\u5ea6\u6269\u5c55\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728MoE\u6a21\u578b\u4e0a\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u51cf\u5c1135%\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u6e10\u8fdb\u5b66\u4e60\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6765\u51cf\u5c11\u9884\u8bad\u7ec3\u8ba1\u7b97\u5f00\u9500\u3002\u867d\u7136\u6df1\u5ea6\u6269\u5c55\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5bbd\u5ea6\u6269\u5c55\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u3002\u7136\u800c\uff0c\u4e2d\u9636\u6bb5\u5bbd\u5ea6\u6269\u5c55\u5bf9\u4e8e\u6700\u5927\u5316\u8ba1\u7b97\u8282\u7701\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4e25\u91cd\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faSPARKLING\u6846\u67b6\uff0c\u901a\u8fc7RMS\u5c3a\u5ea6\u4e00\u81f4\u6027\u5b9e\u73b0\u4fe1\u53f7\u4fdd\u7559\uff0c\u7a33\u5b9a\u6269\u5c55\u8fc7\u7a0b\u4e2d\u7684\u6fc0\u6d3b\u7edf\u8ba1\uff1b\u901a\u8fc7\u975e\u5bf9\u79f0\u4f18\u5316\u5668\u72b6\u6001\u91cd\u7f6e\u548c\u5b66\u4e60\u7387\u91cd\u65b0\u9884\u70ed\u786e\u4fdd\u5bf9\u79f0\u6027\u6253\u7834\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u5bbd\u5ea6\u8f74\u548c\u4f18\u5316\u5668\u5bb6\u65cf\u3002", "result": "\u5728\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSPARKLING\u5728\u591a\u79cd\u5bbd\u5ea6\u8f74\u548c\u4f18\u5316\u5668\u5bb6\u65cf\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\uff0c\u57282\u500d\u5bbd\u5ea6\u6269\u5c55\u4e0b\u51cf\u5c11\u9ad8\u8fbe35%\u7684\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "SPARKLING\u6210\u529f\u89e3\u51b3\u4e86\u4e2d\u9636\u6bb5\u5bbd\u5ea6\u6269\u5c55\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u8282\u7701\uff0c\u4e3a\u6e10\u8fdb\u5b66\u4e60\u4e2d\u7684\u5bbd\u5ea6\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01951", "abs": "https://arxiv.org/abs/2602.01951", "authors": ["Shuyang Wu", "Yifu Qiu", "Ines P. Nearchou", "Sandrine Prost", "Jonathan A Fallowfield", "Hakan Bilen", "Timothy J Kendall"], "title": "Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network", "comment": null, "summary": "Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.", "AI": {"tldr": "MSPN\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7f51\u7edc\uff0c\u4f5c\u4e3a\u6ce8\u610f\u529bMIL\u7684\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u7f51\u683c\u91cd\u6620\u5c04\u548c\u7c97\u7c92\u5ea6\u5f15\u5bfc\u7f51\u7edc\u5b9e\u73b0\u6e10\u8fdb\u591a\u5c3a\u5ea6\u5206\u6790\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6846\u67b6\u4e2d\u4e00\u81f4\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u4e34\u5e8a\u7814\u7a76\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u5236\u9020\u5546\u5b9a\u4e49\u7684\u56fa\u5b9a\u653e\u5927\u500d\u6570\u8f93\u5165\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1b2\uff09\u91c7\u7528\u540e\u671f\u7279\u5f81\u878d\u5408\uff0c\u65e0\u6cd5\u4fdd\u7559\u8de8\u5c3a\u5ea6\u7279\u5f81\u95f4\u7684\u5173\u8054\uff1b3\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faMSPN\uff08\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7f51\u7edc\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u7f51\u683c\u7684\u91cd\u6620\u5c04\uff0c\u4f7f\u7528\u9ad8\u653e\u5927\u500d\u6570\u7279\u5f81\u63a8\u5bfc\u7c97\u7c92\u5ea6\u7279\u5f81\uff1b2\uff09\u7c97\u7c92\u5ea6\u5f15\u5bfc\u7f51\u7edc\uff08CGN\uff09\uff0c\u5b66\u4e60\u7c97\u7c92\u5ea6\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u8be5\u6a21\u5757\u53ef\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u5230\u57fa\u4e8e\u6ce8\u610f\u529b\u7684MIL\u6846\u67b6\u4e2d\u3002", "result": "\u57284\u4e2a\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\u30013\u79cd\u57fa\u7840\u6a21\u578b\u4ee5\u53ca\u9884\u8bad\u7ec3MIL\u6846\u67b6\u4e0a\uff0c\u5c06MSPN\u4f5c\u4e3a\u9644\u52a0\u6a21\u5757\u96c6\u6210\u52304\u79cd\u6ce8\u610f\u529b\u6846\u67b6\u4e2d\uff0c\u7ed3\u679c\u663e\u793aMSPN\u5728\u6240\u6709\u914d\u7f6e\u548c\u4efb\u52a1\u4e2d\u90fd\u80fd\u4e00\u81f4\u63d0\u5347MIL\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u548c\u6613\u7528\u6027\u3002", "conclusion": "MSPN\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u591a\u5c3a\u5ea6\u5206\u6790\u6a21\u5757\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u6ce8\u610f\u529b\u7684MIL\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u5c3a\u5ea6\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2602.01428", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.01428", "abs": "https://arxiv.org/abs/2602.01428", "authors": ["Weiqing He", "Xiang Li", "Li Shen", "Weijie Su", "Qi Long"], "title": "Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models", "comment": "Accepted at ICLR 2026", "summary": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u6c34\u5370\u6280\u672f\u662f\u8ffd\u8e2a\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u63a8\u6d4b\u91c7\u6837\u53ef\u4ee5\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u6d4b\u91c7\u6837\u63a5\u53d7\u7387\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u6c34\u5370\u5f3a\u5ea6\u8d8a\u9ad8\uff0c\u63a5\u53d7\u7387\u8d8a\u4f4e\uff0c\u4e24\u8005\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u6743\u8861\u5173\u7cfb\u3002", "method": "1. \u5f15\u5165\u4e00\u4e2a\u91cf\u5316\u6c34\u5370\u5f3a\u5ea6\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u8be5\u6807\u51c6\u63a7\u5236\u7edf\u8ba1\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u5728\u4ee4\u724c\u662f\u4f2a\u968f\u673a\u6570\u7684\u786e\u5b9a\u6027\u51fd\u6570\u65f6\u8fbe\u5230\u6700\u5927\u503c\uff1b2. \u5c06\u6743\u8861\u5173\u7cfb\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u4e24\u79cd\u73b0\u6709\u6c34\u5370\u65b9\u6848\u63a8\u5bfc\u51fa\u660e\u786e\u7684\u5e15\u7d2f\u6258\u66f2\u7ebf\uff1b3. \u63d0\u51fa\u4e00\u79cd\u539f\u5219\u6027\u673a\u5236\uff0c\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u786e\u4fdd\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\u540c\u65f6\u4fdd\u6301\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u53ef\u68c0\u6d4b\u6027\u3002\u901a\u8fc7\u5411\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u8fc7\u7a0b\u6ce8\u5165\u4f2a\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5927\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u7684\u534f\u540c\u4f18\u5316\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u6743\u8861\u9650\u5236\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u6d4b\u91c7\u6837\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5e76\u975e\u7edd\u5bf9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u63a8\u6d4b\u91c7\u6837\u4e0e\u6c34\u5370\u6280\u672f\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4e3a\u4e24\u8005\u7684\u9ad8\u6548\u5b9e\u7528\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f2a\u968f\u673a\u6027\u6ce8\u5165\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u6c34\u5370\u5f3a\u5ea6\u4e0e\u63a8\u7406\u6548\u7387\u7684\u53cc\u91cd\u4f18\u5316\u3002"}}
{"id": "2602.02488", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02488", "abs": "https://arxiv.org/abs/2602.02488", "authors": ["Yinjie Wang", "Tianbao Xie", "Ke Shen", "Mengdi Wang", "Ling Yang"], "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System", "comment": "Code: https://github.com/Gen-Verse/Open-AgentRL", "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "RLAnything\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u52a8\u6001\u6784\u5efa\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u589e\u5f3aLLM\u548c\u667a\u80fd\u4f53\u573a\u666f\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728LLM\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u9762\u4e34\u5b66\u4e60\u4fe1\u53f7\u4e0d\u8db3\u3001\u73af\u5883\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u52a8\u6001\u3001\u95ed\u73af\u7684\u4f18\u5316\u6846\u67b6\u6765\u63d0\u5347\u6574\u4f53\u5b66\u4e60\u6548\u679c\u3002", "method": "1) \u7b56\u7565\u8bad\u7ec3\u6574\u5408\u6b65\u8fdb\u548c\u7ed3\u679c\u4fe1\u53f7\u7684\u53cd\u9988\uff1b2) \u5956\u52b1\u6a21\u578b\u901a\u8fc7\u4e00\u81f4\u6027\u53cd\u9988\u8054\u5408\u4f18\u5316\uff1b3) \u57fa\u4e8e\u7406\u8bba\u7684\u81ea\u52a8\u73af\u5883\u9002\u5e94\u5229\u7528\u6279\u8bc4\u53cd\u9988\u6539\u8fdb\u5956\u52b1\u548c\u7b56\u7565\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6301\u7eed\u6539\u8fdb\u6574\u4f53\u7cfb\u7edf\uff0cRLAnything\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aQwen3-VL-8B-Thinking\u5728OSWorld\u4e0a\u63d0\u53479.1%\uff0cQwen2.5-7B-Instruct\u5728AlfWorld\u548cLiveBench\u4e0a\u5206\u522b\u63d0\u534718.7%\u548c11.9%\u3002\u4f18\u5316\u7684\u5956\u52b1\u6a21\u578b\u4fe1\u53f7\u4f18\u4e8e\u4f9d\u8d56\u4eba\u5de5\u6807\u7b7e\u7684\u7ed3\u679c\u3002", "conclusion": "RLAnything\u901a\u8fc7\u52a8\u6001\u95ed\u73af\u4f18\u5316\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u5b66\u4e60\u80fd\u529b\u548c\u6027\u80fd\uff0c\u4e3aLLM\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2602.01954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01954", "abs": "https://arxiv.org/abs/2602.01954", "authors": ["Shuai Yang", "Ziyue Huang", "Jiaxin Chen", "Qingjie Liu", "Yunhong Wang"], "title": "Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images", "comment": null, "summary": "Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.", "AI": {"tldr": "RS-MPOD\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u63d0\u793a\u548c\u6587\u672c\u63d0\u793a\u6765\u89e3\u51b3\u9065\u611f\u573a\u666f\u4e2d\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u5bfc\u81f4\u7684\u7c7b\u522b\u8bed\u4e49\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u901a\u5e38\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u6765\u6307\u5b9a\u76ee\u6807\u7c7b\u522b\uff0c\u5047\u8bbe\u63a8\u7406\u65f6\u7684\u7c7b\u522b\u67e5\u8be2\u53ef\u4ee5\u901a\u8fc7\u9884\u8bad\u7ec3\u8bf1\u5bfc\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u53ef\u9760\u5730\u63a5\u5730\u3002\u4f46\u5b9e\u9645\u4e0a\uff0c\u7531\u4e8e\u4efb\u52a1\u548c\u5e94\u7528\u7279\u5b9a\u7684\u7c7b\u522b\u8bed\u4e49\uff0c\u8fd9\u79cd\u5047\u8bbe\u5728\u9065\u611f\u573a\u666f\u4e2d\u7ecf\u5e38\u5931\u6548\uff0c\u5bfc\u81f4\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e0b\u7684\u7c7b\u522b\u6307\u5b9a\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faRS-MPOD\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u6846\u67b6\uff0c\u8d85\u8d8a\u4ec5\u6587\u672c\u63d0\u793a\u7684\u7c7b\u522b\u6307\u5b9a\u65b9\u5f0f\uff0c\u7ed3\u5408\u5b9e\u4f8b\u63a5\u5730\u7684\u89c6\u89c9\u63d0\u793a\u3001\u6587\u672c\u63d0\u793a\u53ca\u5176\u591a\u6a21\u6001\u96c6\u6210\u3002\u5f15\u5165\u89c6\u89c9\u63d0\u793a\u7f16\u7801\u5668\u4ece\u793a\u4f8b\u5b9e\u4f8b\u4e2d\u63d0\u53d6\u57fa\u4e8e\u5916\u89c2\u7684\u7c7b\u522b\u7ebf\u7d22\uff0c\u5b9e\u73b0\u65e0\u9700\u6587\u672c\u7684\u7c7b\u522b\u6307\u5b9a\uff1b\u4ee5\u53ca\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u5728\u4e24\u79cd\u6a21\u6001\u90fd\u53ef\u7528\u65f6\u96c6\u6210\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002", "result": "\u5728\u6807\u51c6\u3001\u8de8\u6570\u636e\u96c6\u548c\u7ec6\u7c92\u5ea6\u9065\u611f\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9\u63d0\u793a\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u7c7b\u522b\u6307\u5b9a\uff0c\u800c\u591a\u6a21\u6001\u63d0\u793a\u5728\u6587\u672c\u8bed\u4e49\u5bf9\u9f50\u826f\u597d\u65f6\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "RS-MPOD\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u8bed\u4e49\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u89c6\u89c9\u63d0\u793a\u5728\u8bed\u4e49\u6a21\u7cca\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u53ef\u9760\uff0c\u591a\u6a21\u6001\u63d0\u793a\u5219\u4e3a\u4e0d\u540c\u573a\u666f\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01433", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01433", "abs": "https://arxiv.org/abs/2602.01433", "authors": ["Muhammad Hasan Ferdous", "Md Osman Gani"], "title": "DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data", "comment": null, "summary": "Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u89e3\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u548c\u6b8b\u5dee\u6210\u5206\uff0c\u5206\u522b\u8fdb\u884c\u56e0\u679c\u5206\u6790\uff0c\u6700\u540e\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u591a\u5c3a\u5ea6\u56e0\u679c\u7ed3\u6784", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff08\u91d1\u878d\u3001\u6c14\u5019\u79d1\u5b66\u3001\u533b\u7597\u7b49\u9886\u57df\uff09\u5e38\u5448\u73b0\u957f\u671f\u8d8b\u52bf\u3001\u5b63\u8282\u6a21\u5f0f\u548c\u77ed\u671f\u6ce2\u52a8\uff0c\u5728\u975e\u5e73\u7a33\u6027\u548c\u81ea\u76f8\u5173\u4e0b\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5728\u539f\u59cb\u89c2\u6d4b\u4e0a\u64cd\u4f5c\uff0c\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8fb9\u548c\u9519\u8bef\u5f52\u56e0\u7684\u65f6\u95f4\u4f9d\u8d56", "method": "\u5206\u89e3\u6846\u67b6\uff1a1\uff09\u5c06\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u548c\u6b8b\u5dee\u6210\u5206\uff1b2\uff09\u5bf9\u8d8b\u52bf\u6210\u5206\u4f7f\u7528\u5e73\u7a33\u6027\u68c0\u9a8c\uff1b3\uff09\u5bf9\u5b63\u8282\u6210\u5206\u4f7f\u7528\u57fa\u4e8e\u6838\u7684\u4f9d\u8d56\u5ea6\u91cf\uff1b4\uff09\u5bf9\u6b8b\u5dee\u6210\u5206\u4f7f\u7528\u57fa\u4e8e\u7ea6\u675f\u7684\u56e0\u679c\u53d1\u73b0\uff1b5\uff09\u5c06\u6210\u5206\u7ea7\u56fe\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u591a\u5c3a\u5ea6\u56e0\u679c\u7ed3\u6784", "result": "\u5728\u5e7f\u6cdb\u7684\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u6c14\u5019\u6570\u636e\u4e0a\uff0c\u8be5\u6846\u67b6\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u66f4\u51c6\u786e\u5730\u6062\u590d\u771f\u5b9e\u56e0\u679c\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u5f3a\u975e\u5e73\u7a33\u6027\u548c\u65f6\u95f4\u81ea\u76f8\u5173\u60c5\u51b5\u4e0b", "conclusion": "\u5206\u89e3\u65b9\u6cd5\u80fd\u5206\u79bb\u957f\u671f\u548c\u77ed\u671f\u56e0\u679c\u6548\u5e94\uff0c\u51cf\u5c11\u865a\u5047\u5173\u8054\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5904\u7406\u975e\u5e73\u7a33\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01973", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01973", "abs": "https://arxiv.org/abs/2602.01973", "authors": ["Muli Yang", "Gabriel James Goenawan", "Henan Wang", "Huaiyuan Qin", "Chenghao Xu", "Yanhua Yang", "Fen Fang", "Ying Sun", "Joo-Hwee Lim", "Hongyuan Zhu"], "title": "Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated", "comment": "AAAI 2026. Code: https://github.com/muliyangm/AIGI-Det-Calib", "summary": "Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u7684\u540e\u5904\u7406\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6807\u91cf\u6821\u6b63\u6a21\u578blogits\uff0c\u89e3\u51b3AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u6d4b\u8bd5\u65f6\u7684\u7cfb\u7edf\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u6d4b\u8bd5\u65f6\u4ecd\u8868\u73b0\u51fa\u7cfb\u7edf\u504f\u5dee\uff0c\u7ecf\u5e38\u5c06\u5047\u56fe\u50cf\u8bef\u5206\u7c7b\u4e3a\u771f\u56fe\u50cf\u3002\u4f5c\u8005\u5047\u8bbe\u8fd9\u79cd\u884c\u4e3a\u6e90\u4e8e\u5047\u6837\u672c\u7684\u5206\u5e03\u504f\u79fb\u548c\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u7684\u9690\u5f0f\u5148\u9a8c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7406\u8bba\u7684\u540e\u5904\u7406\u6821\u51c6\u6846\u67b6\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6807\u91cf\u6821\u6b63\u6a21\u578blogits\uff0c\u5728\u76ee\u6807\u5206\u5e03\u7684\u5c0f\u9a8c\u8bc1\u96c6\u4e0a\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u5e72\u7f51\u7edc\u51bb\u7ed3\u3002\u8fd9\u79cd\u53c2\u6570\u8c03\u6574\u8865\u507f\u6a21\u578b\u8f93\u51fa\u7684\u5206\u5e03\u504f\u79fb\uff0c\u5373\u4f7f\u4e0d\u9700\u8981\u771f\u5b9e\u6807\u7b7e\u4e5f\u80fd\u91cd\u65b0\u5bf9\u9f50\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e3a\u5f00\u653e\u4e16\u754c\u4e2d\u53ef\u9760\u4e14\u81ea\u9002\u5e94\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u6709\u7406\u8bba\u4f9d\u636e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u540e\u5904\u7406\u6821\u51c6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u7684\u7cfb\u7edf\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u53c2\u6570\u8c03\u6574\u8865\u507f\u5206\u5e03\u504f\u79fb\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u63d0\u5347\u3002"}}
{"id": "2602.01434", "categories": ["cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.01434", "abs": "https://arxiv.org/abs/2602.01434", "authors": ["Andrea Montanari", "Zihao Wang"], "title": "Phase Transitions for Feature Learning in Neural Networks", "comment": "74 pages; 17 pdf figures", "summary": "According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\\boldsymbol x}_i,y_i)$, where the covariate vectors ${\\boldsymbol x}_i\\in\\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\\boldsymbol x}_i$ through a $k$-dimensional projection ${\\boldsymbol \u0398}_*^{\\sf T}{\\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\\boldsymbol \u0398}_*$.\n  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\\to\\infty$, $n/d\\to\u03b4$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $\u03b4> \u03b4_{\\text{alg}}$, for $\u03b4_{\\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $\u03b4_{\\text{alg}}$. Here we derive an analogous threshold $\u03b4_{\\text{NN}}$ for two-layer networks. Our characterization of $\u03b4_{\\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.\n  The threshold $\u03b4_{\\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $\u03b4_{\\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u5728\u6bd4\u4f8b\u6e10\u8fd1\u6761\u4ef6\u4e0b\u5b66\u4e60\u591a\u7d22\u5f15\u6a21\u578b\u7684\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\uff0c\u786e\u5b9a\u4e86\u7279\u5f81\u5b66\u4e60\u53ef\u80fd\u6027\u7684\u9608\u503c\u03b4_NN\uff0c\u8be5\u9608\u503c\u7531Hessian\u77e9\u9635\u8c31\u7684\u76f8\u53d8\u51b3\u5b9a\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u4f4e\u7ef4\u8868\u793a\uff0c\u7279\u522b\u662f\u5728\u591a\u7d22\u5f15\u6a21\u578b\u8bbe\u7f6e\u4e2d\uff0c\u7406\u89e3\u7279\u5f81\u5b66\u4e60\u7684\u6761\u4ef6\u9608\u503c\u4e0e\u7b97\u6cd5\u6548\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u5728\u6bd4\u4f8b\u6e10\u8fd1\u6846\u67b6\u4e0b\uff08n,d\u2192\u221e\uff0cn/d\u2192\u03b4\uff09\uff0c\u5206\u6790\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\uff0c\u5176\u4e2d\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6k\u548c\u9690\u85cf\u795e\u7ecf\u5143\u6570m\u56fa\u5b9a\u3002\u901a\u8fc7\u7814\u7a76\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u548cHessian\u77e9\u9635\u7684\u884c\u4e3a\u6765\u786e\u5b9a\u7279\u5f81\u5b66\u4e60\u9608\u503c\u3002", "result": "\u63a8\u5bfc\u51fa\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u5b66\u4e60\u9608\u503c\u03b4_NN\uff0c\u8be5\u9608\u503c\u5bf9\u5e94Hessian\u77e9\u9635\u8c31\u7684\u76f8\u53d8\u70b9\u3002\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u5b66\u4e60\u5927\u68af\u5ea6\u65b9\u5411\uff0c\u7136\u540e\u53d7Hessian\u8d1f\u65b9\u5411\u4e3b\u5bfc\u3002", "conclusion": "\u786e\u5b9a\u4e86\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u5b66\u4e60\u7684\u5173\u952e\u9608\u503c\u03b4_NN\uff0c\u4e3a\u7814\u7a76\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7b97\u6cd5\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u4f9d\u8d56\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u548cHessian\u76f8\u4e92\u4f5c\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.01984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01984", "abs": "https://arxiv.org/abs/2602.01984", "authors": ["Minyoung Lee", "Yeji Park", "Dongjun Hwang", "Yejin Kim", "Seong Joon Oh", "Junsuk Choe"], "title": "Enhancing Multi-Image Understanding through Delimiter Token Scaling", "comment": "Accepted at ICLR 2026", "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u7f29\u653e\u5206\u9694\u7b26\u6807\u8bb0\u7684\u9690\u85cf\u72b6\u6001\u6765\u589e\u5f3aLVLMs\u591a\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u63a8\u7406\u6210\u672c", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8de8\u56fe\u50cf\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\u3002\u867d\u7136\u5df2\u6709\u5206\u9694\u7b26\u6807\u8bb0\u6765\u6807\u8bb0\u56fe\u50cf\u8fb9\u754c\uff0c\u4f46\u8fd9\u4e9b\u6807\u8bb0\u672a\u80fd\u6709\u6548\u963b\u6b62\u8de8\u56fe\u50cf\u4fe1\u606f\u6cc4\u6f0f\u3002", "method": "\u63d0\u51fa\u7f29\u653e\u5206\u9694\u7b26\u6807\u8bb0\u7684\u9690\u85cf\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u5206\u9694\u7b26\u7684\u4f5c\u7528\u6765\u5f3a\u5316\u56fe\u50cf\u5185\u90e8\u4ea4\u4e92\uff0c\u9650\u5236\u4e0d\u5fc5\u8981\u7684\u8de8\u56fe\u50cf\u4ea4\u4e92\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4fdd\u7559\u56fe\u50cf\u7279\u5b9a\u4fe1\u606f\u3002", "result": "\u5728Mantis\u3001MuirBench\u3001MIRB\u3001QBench2\u7b49\u591a\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u3002\u5728TQABench\u3001MultiNews\u3001WCEP-10\u7b49\u591a\u6587\u6863\u548c\u591a\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u7f29\u653e\u5206\u9694\u7b26\u6807\u8bb0\u7684\u9690\u85cf\u72b6\u6001\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aLVLMs\u5728\u591a\u56fe\u50cf\u548c\u591a\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u63a8\u7406\u6210\u672c\uff0c\u4e3a\u89e3\u51b3\u8de8\u56fe\u50cf\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.01437", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01437", "abs": "https://arxiv.org/abs/2602.01437", "authors": ["Yinsong Wang", "Shahin Shahrampour"], "title": "Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data", "comment": null, "summary": "The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.\n  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.", "AI": {"tldr": "\u672c\u6587\u4ece\u795e\u7ecf\u7f51\u7edc\u8ddd\u79bb\u7684\u89d2\u5ea6\uff0c\u4e3a\u6d4b\u91cf\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08MCR\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u89e3\u91ca\u4e86\u5176\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6570\u636e\u4e0b\u63d0\u5347\u63d2\u8865\u8d28\u91cf\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5bf9\u5076\u95f4\u9699\u7684\u65e9\u671f\u505c\u6b62\u8bad\u7ec3\u534f\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u6d4b\u91cf\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08MCR\uff09\u5728\u56fe\u50cf\u4fee\u590d\u3001\u6570\u636e\u63d2\u8865\u548c\u534a\u76d1\u7763\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u7ecf\u9a8c\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4ece\u7406\u8bba\u89d2\u5ea6\u7406\u89e3MCR\u4e3a\u4f55\u3001\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u63d0\u5347\u90e8\u5206\u53ef\u89c2\u6d4b\u6570\u636e\u4e0b\u7684\u63d2\u8865\u8d28\u91cf\u3002", "method": "1. \u4ece\u795e\u7ecf\u7f51\u7edc\u8ddd\u79bb\u7684\u89d2\u5ea6\u5bf9MCR\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff1b2. \u8bc6\u522bMCR\u5e26\u6765\u6cdb\u5316\u4f18\u52bf\u7684\u5173\u952e\u9879\uff1b3. \u5c06\u5206\u6790\u6269\u5c55\u5230\u4e0d\u5b8c\u7f8e\u8bad\u7ec3\u673a\u5236\uff1b4. \u63d0\u51fa\u57fa\u4e8e\u5bf9\u5076\u95f4\u9699\u76d1\u6d4b\u7684\u65e9\u671f\u505c\u6b62\u8bad\u7ec3\u534f\u8bae\uff1b5. \u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u7406\u8bba\u4e3b\u5f20\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eMCR\u7684\u6cdb\u5316\u4f18\u52bf\u5e76\u975e\u603b\u662f\u4fdd\u8bc1\u7684\uff0c\u53d6\u51b3\u4e8e\u7279\u5b9a\u6761\u4ef6\u3002\u63d0\u51fa\u7684\u57fa\u4e8e\u5bf9\u5076\u95f4\u9699\u7684\u65e9\u671f\u505c\u6b62\u534f\u8bae\u80fd\u591f\u6709\u6548\u4fdd\u7559\u6cdb\u5316\u4f18\u52bf\u3002\u5b9e\u8bc1\u7814\u7a76\u652f\u6301\u4e86\u7406\u8bba\u4e3b\u5f20\uff0c\u5e76\u5c55\u793a\u4e86MCR\u5728\u4e0d\u540c\u6570\u636e\u6e90\u548c\u6a21\u578b\u67b6\u6784\u4e0b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aMCR\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u5176\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6570\u636e\u4e0b\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u8bad\u7ec3\u534f\u8bae\u3002\u7814\u7a76\u4e0d\u4ec5\u6df1\u5316\u4e86\u5bf9MCR\u7684\u7406\u89e3\uff0c\u8fd8\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5c55\u793a\u4e86MCR\u5728\u4e0d\u540c\u6570\u636e\u573a\u666f\u4e0b\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2602.01991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01991", "abs": "https://arxiv.org/abs/2602.01991", "authors": ["Pablo Domingo-Gregorio", "Javier Ruiz-Hidalgo"], "title": "Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models", "comment": null, "summary": "Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7528\u6237\u5b9a\u4e49\u7684\u56fe\u50cf\u533a\u57df\u5b9e\u73b0\u7cbe\u786e\u7684\u5c40\u90e8\u63a7\u5236\uff0c\u540c\u65f6\u8ba9\u6a21\u578b\u6839\u636e\u539f\u59cb\u63d0\u793a\u81ea\u4e3b\u751f\u6210\u5176\u4f59\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u9700\u8981\u53cd\u590d\u8bd5\u9519\u3002\u73b0\u6709\u56fe\u50cf\u7ea7\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u8fb9\u7f18\u3001\u5206\u5272\u3001\u6df1\u5ea6\u56fe\uff09\u5bf9\u6574\u4e2a\u56fe\u50cf\u5e94\u7528\u7edf\u4e00\u6761\u4ef6\uff0c\u7f3a\u4e4f\u5c40\u90e8\u63a7\u5236\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u63a9\u7801\u7279\u5f81\u548c\u989d\u5916\u7684\u635f\u5931\u9879\u3002\u8be5\u635f\u5931\u9879\u5229\u7528\u4efb\u4f55\u6269\u6563\u6b65\u9aa4\u4e2d\u521d\u59cb\u6f5c\u5728\u5411\u91cf\u7684\u9884\u6d4b\uff0c\u589e\u5f3a\u5f53\u524d\u6b65\u9aa4\u4e0e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6700\u7ec8\u6837\u672c\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5408\u6210\u5177\u6709\u53d7\u63a7\u5c40\u90e8\u6761\u4ef6\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u7528\u6237\u5b9a\u4e49\u56fe\u50cf\u533a\u57df\u7684\u7cbe\u786e\u5c40\u90e8\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u6269\u6563\u6a21\u578b\u6839\u636e\u539f\u59cb\u63d0\u793a\u81ea\u4e3b\u751f\u6210\u5176\u4f59\u533a\u57df\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c40\u90e8\u63a7\u5236\u7684\u95ee\u9898\u3002"}}
{"id": "2602.01439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01439", "abs": "https://arxiv.org/abs/2602.01439", "authors": ["Perry Dong", "Kuo-Han Hung", "Alexander Swerdlow", "Dorsa Sadigh", "Chelsea Finn"], "title": "TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse", "comment": null, "summary": "Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u4e2d\u96be\u4ee5\u6709\u6548\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u9886\u57df\u56e0\u89c4\u6a21\u6269\u5927\u800c\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ecd\u4e3b\u8981\u4f7f\u7528\u5c0f\u578b\u4ef7\u503c\u51fd\u6570\u3002\u7b80\u5355\u6269\u5c55\u4ef7\u503c\u51fd\u6570\uff08\u5305\u62ec\u4f7f\u7528\u5df2\u77e5\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\u7684Transformer\u67b6\u6784\uff09\u901a\u5e38\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76Transformer\u5728\u4ef7\u503c\u51fd\u6570\u4e2d\u96be\u4ee5\u6709\u6548\u6269\u5c55\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff1a\u968f\u7740\u5bb9\u91cf\u589e\u52a0\uff0c\u6ce8\u610f\u529b\u5206\u6570\u4f1a\u5d29\u6e83\u3002\u6838\u5fc3\u6d1e\u5bdf\u662f\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u7684\u71b5\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u8fd9\u79cd\u5d29\u6e83\u5e76\u7a33\u5b9a\u8bad\u7ec3\u3002\u57fa\u4e8e\u6b64\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u89e3\u9501Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\u3002", "result": "TQL\u65b9\u6cd5\u5728\u4ece\u6700\u5c0f\u5230\u6700\u5927\u7f51\u7edc\u89c4\u6a21\u6269\u5c55\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe43%\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u5219\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\uff0cTransformer\u53ef\u4ee5\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u4e2d\u6709\u6548\u6269\u5c55\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u65b9\u6cd5\u5728\u6269\u5c55\u65f6\u9047\u5230\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2602.02000", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02000", "abs": "https://arxiv.org/abs/2602.02000", "authors": ["Bing He", "Jingnan Gao", "Yunuo Chen", "Ning Cao", "Gang Chen", "Zhengxue Cheng", "Li Song", "Wenjun Zhang"], "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors", "comment": "ICLR 2026", "summary": "Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/", "AI": {"tldr": "SurfSplat\u662f\u4e00\u4e2a\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u524d\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u9762\u8fde\u7eed\u6027\u5148\u9a8c\u548c\u5f3a\u5236alpha\u6df7\u5408\u7b56\u7565\uff0c\u4ece\u7a00\u758f\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fd1\u8ddd\u79bb\u89c2\u5bdf\u65f6\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\u4ece\u7a00\u758f\u56fe\u50cf\u91cd\u5efa3D\u573a\u666f\u65f6\uff0c\u5f80\u5f80\u4ea7\u751f\u79bb\u6563\u3001\u989c\u8272\u504f\u5dee\u7684\u70b9\u4e91\uff0c\u5728\u6b63\u5e38\u5206\u8fa8\u7387\u4e0b\u770b\u4f3c\u5408\u7406\uff0c\u4f46\u5728\u8fd1\u8ddd\u79bb\u89c2\u5bdf\u65f6\u4f1a\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\uff0c\u65e0\u6cd5\u751f\u6210\u8fde\u7eed\u8868\u9762\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684SurfSplat\u6846\u67b6\uff0c\u5229\u75282DGS\u63d0\u4f9b\u66f4\u5f3a\u7684\u5404\u5411\u5f02\u6027\u548c\u66f4\u9ad8\u51e0\u4f55\u7cbe\u5ea6\u3002\u5f15\u5165\u8868\u9762\u8fde\u7eed\u6027\u5148\u9a8c\u548c\u5f3a\u5236alpha\u6df7\u5408\u7b56\u7565\u6765\u91cd\u5efa\u8fde\u8d2f\u51e0\u4f55\u548c\u5fe0\u5b9e\u7eb9\u7406\u3002\u8fd8\u63d0\u51fa\u4e86\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\u4e00\u81f4\u6027\uff08HRRC\uff09\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728RealEstate10K\u3001DL3DV\u548cScanNet\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSurfSplat\u5728\u6807\u51c6\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684HRRC\u6307\u6807\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a\u7a00\u758f\u8f93\u5165\u7684\u9ad8\u4fdd\u771f3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SurfSplat\u901a\u8fc72D\u9ad8\u65af\u6cfc\u6e85\u3001\u8868\u9762\u8fde\u7eed\u6027\u5148\u9a8c\u548c\u5f3a\u5236alpha\u6df7\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u56fe\u50cf3D\u91cd\u5efa\u4e2d\u7684\u8868\u9762\u4e0d\u8fde\u7eed\u548c\u8fd1\u8ddd\u79bb\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2602.02002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02002", "abs": "https://arxiv.org/abs/2602.02002", "authors": ["Guosheng Zhao", "Yaozeng Wang", "Xiaofeng Wang", "Zheng Zhu", "Tingdong Yu", "Guan Huang", "Yongchen Zai", "Ji Jiao", "Changliang Xue", "Xiaole Wang", "Zhen Yang", "Futang Zhu", "Xingang Wang"], "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving", "comment": "16 pages, 7 figures", "summary": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream", "AI": {"tldr": "UniDriveDreamer\uff1a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u5355\u9636\u6bb5\u7edf\u4e00\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u751f\u6210\u591a\u6a21\u6001\u672a\u6765\u89c2\u6d4b\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\u6216\u7ea7\u8054\u6a21\u5757", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u4e13\u6ce8\u4e8e\u5355\u6a21\u6001\u751f\u6210\uff08\u591a\u6444\u50cf\u5934\u89c6\u9891\u6216LiDAR\u5e8f\u5217\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u9650\u5236\u4e86\u6570\u636e\u5408\u6210\u7684\u5b8c\u6574\u6027\u548c\u5b9e\u7528\u6027", "method": "1. \u4f7f\u7528LiDAR\u4e13\u7528VAE\u7f16\u7801LiDAR\u5e8f\u5217\uff0c\u89c6\u9891VAE\u7f16\u7801\u591a\u6444\u50cf\u5934\u56fe\u50cf\uff1b2. \u63d0\u51fa\u7edf\u4e00\u6f5c\u5728\u951a\u5b9a(ULA)\u663e\u5f0f\u5bf9\u9f50\u4e24\u79cd\u6a21\u6001\u7684\u6f5c\u5728\u5206\u5e03\uff1b3. \u878d\u5408\u5bf9\u9f50\u7279\u5f81\u540e\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u8054\u5408\u5efa\u6a21\u51e0\u4f55\u5bf9\u5e94\u548c\u65f6\u95f4\u6f14\u5316\uff1b4. \u5c06\u7ed3\u6784\u5316\u573a\u666f\u5e03\u5c40\u4fe1\u606f\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u53f7\u6307\u5bfc\u5408\u6210", "result": "\u5728\u89c6\u9891\u548cLiDAR\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5e26\u6765\u53ef\u6d4b\u91cf\u7684\u6539\u8fdb", "conclusion": "UniDriveDreamer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5355\u9636\u6bb5\u7edf\u4e00\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u76f4\u63a5\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u6240\u9700\u7684\u591a\u6a21\u6001\u672a\u6765\u89c2\u6d4b\uff0c\u4e3a\u6570\u636e\u5408\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01445", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01445", "abs": "https://arxiv.org/abs/2602.01445", "authors": ["Ons Saadallah", "M\u00e1ty\u00e1s and\u00f3", "Tam\u00e1s G\u00e1bor Orosz"], "title": "A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting", "comment": null, "summary": "Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.", "AI": {"tldr": "LLM-AutoOpt\uff1a\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u4e0eLLM\u63a8\u7406\u7684\u6df7\u5408\u8d85\u53c2\u6570\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u63d0\u5347\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u3002\u867d\u7136\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u901a\u5e38\u5c06\u8c03\u4f18\u4efb\u52a1\u89c6\u4e3a\u72ec\u7acb\uff0c\u4e14\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u6d1e\u5bdf\u529b\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u5c06\u7ed3\u6784\u5316\u5148\u9a8c\u77e5\u8bc6\u548c\u63a8\u7406\u878d\u5165\u4f18\u5316\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51faLLM-AutoOpt\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u3002\u8be5\u6846\u67b6\u5c06\u6570\u636e\u96c6\u5143\u7279\u5f81\u3001\u6a21\u578b\u63cf\u8ff0\u3001\u5386\u53f2\u4f18\u5316\u7ed3\u679c\u548c\u76ee\u6807\u76ee\u6807\u7f16\u7801\u4e3aLLM\u63d0\u793a\u4e2d\u7684\u7ed3\u6784\u5316\u5143\u77e5\u8bc6\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u521d\u59cb\u5316\u641c\u7d22\u5e76\u7f13\u89e3\u51b7\u542f\u52a8\u6548\u5e94\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u7a33\u5b9a\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u540c\u65f6\u66b4\u9732\u4f18\u5316\u51b3\u7b56\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-AutoOpt\u76f8\u6bd4\u6ca1\u6709\u5143\u77e5\u8bc6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u66f4\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u884c\u4e3a\u3002", "conclusion": "LLM-AutoOpt\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548cLLM\u63a8\u7406\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u5143\u77e5\u8bc6\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.02004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02004", "abs": "https://arxiv.org/abs/2602.02004", "authors": ["Gongli Xi", "Kun Wang", "Zeming Gao", "Huahui Yi", "Haolang Lu", "Ye Tian", "Wendong Wang"], "title": "ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning", "comment": "20 pages, 7 figures", "summary": "Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \\emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\\rightarrow$ outputs $\\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \\textbf{without any additional training}, ClueTracer improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\emph{etc}.) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non-reasoning} settings, it yields a $\\mathbf{1.14\\times}$ gain.", "AI": {"tldr": "\u63d0\u51faClueTracer\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u5173\u952e\u7ebf\u7d22\u4f20\u64ad\u6765\u6291\u5236\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u751f\u6210\u4e0e\u8f93\u5165\u56fe\u50cf\u6216\u95ee\u9898\u65e0\u5173\u7684\u5185\u5bb9\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\"\u63a8\u7406\u6f02\u79fb\"\u73b0\u8c61\uff1a\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u4e0e\u95ee\u9898\u65e0\u5173\u7684\u5b9e\u4f53\uff0c\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\u88ab\u7a00\u91ca\uff0c\u63a8\u7406\u8f68\u8ff9\u9010\u6e10\u8131\u79bb\u89c6\u89c9\u57fa\u7840\u3002", "method": "\u63d0\u51faClueRecall\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u89c6\u89c9\u7ebf\u7d22\u68c0\u7d22\u80fd\u529b\uff0c\u5e76\u5f00\u53d1ClueTracer\u65b9\u6cd5\u3002ClueTracer\u4ece\u95ee\u9898\u51fa\u53d1\uff0c\u8ffd\u8e2a\u5173\u952e\u7ebf\u7d22\u5728\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u4f20\u64ad\uff08\u95ee\u9898\u2192\u8f93\u51fa\u2192\u89c6\u89c9\u6807\u8bb0\uff09\uff0c\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff0c\u540c\u65f6\u6291\u5236\u5bf9\u65e0\u5173\u533a\u57df\u7684\u865a\u5047\u5173\u6ce8\u3002", "result": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0cClueTracer\u5728\u6240\u6709\u63a8\u7406\u67b6\u6784\u4e0a\u5e73\u5747\u63d0\u53471.21\u500d\u6027\u80fd\u3002\u5728\u975e\u63a8\u7406\u8bbe\u7f6e\u4e2d\u4e5f\u80fd\u5e26\u67651.14\u500d\u7684\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u8ffd\u8e2a\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u7ebf\u7d22\u4f20\u64ad\uff0c\u53ef\u4ee5\u6709\u6548\u6291\u5236\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3001\u53c2\u6570\u65e0\u5173\u3001\u67b6\u6784\u65e0\u5173\u3002"}}
{"id": "2602.01453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01453", "abs": "https://arxiv.org/abs/2602.01453", "authors": ["Idan Barnea", "Orin Levy", "Yishay Mansour"], "title": "Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs", "comment": null, "summary": "We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.\n  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\\tilde{O}(S^6 H^6 A / \u03b5^2)$ agents to obtain an $\u03b5$ approximation of the dynamics (i.e., yields an $\u03b5$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $\u03c1< H$ phases requires at least $A^{H/\u03c1}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65e0\u5956\u52b1\u63a2\u7d22\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u8f83\u5c11\u65f6\u7684\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65e0\u5956\u52b1\u63a2\u7d22\u95ee\u9898\uff0c\u591a\u4e2a\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u5956\u52b1\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u5171\u540c\u63a2\u7d22\u672a\u77e5MDP\u4ee5\u5b66\u4e60\u5176\u52a8\u6001\u3002\u5173\u6ce8\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u8f83\u5c11\u65f6\u7684\u60c5\u51b5\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u5b66\u4e60\u9636\u6bb5\u4e2d\u591a\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u4e0e\u73af\u5883\u4ea4\u4e92\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u88ab\u5206\u914d\u4e00\u4e2a\u7b56\u7565\uff0c\u6267\u884c\u8be5\u7b56\u7565\u5e76\u89c2\u5bdf\u7ed3\u679c\u8f68\u8ff9\u3002\u7814\u7a76\u91cd\u70b9\u662f\u5206\u6790\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u7531\u65f6\u95f4\u6b65\u957fH\u63a7\u5236\u7684\u5c16\u9510\u8f6c\u53d8\uff1a\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u7b49\u4e8eH\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u4ec5\u9700\u00d5(S\u2076H\u2076A/\u03b5\u00b2)\u4e2a\u667a\u80fd\u4f53\u5373\u53ef\u83b7\u5f97\u52a8\u6001\u7684\u03b5\u8fd1\u4f3c\u3002\u4e0b\u754c\u8868\u660e\uff0c\u4efb\u4f55\u9650\u5236\u5728\u03c1<H\u9636\u6bb5\u7684\u7b97\u6cd5\u90fd\u9700\u8981\u81f3\u5c11A^(H/\u03c1)\u4e2a\u667a\u80fd\u4f53\u624d\u80fd\u8fbe\u5230\u6052\u5b9a\u7cbe\u5ea6\u3002", "conclusion": "\u5982\u679c\u8981\u5c06\u667a\u80fd\u4f53\u6570\u91cf\u9650\u5236\u5728\u591a\u9879\u5f0f\u7ea7\u522b\uff0c\u5fc5\u987b\u8981\u6709H\u4e2a\u5b66\u4e60\u9636\u6bb5\u3002\u8fd9\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u65e0\u5956\u52b1\u63a2\u7d22\u4e2d\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2602.01454", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01454", "abs": "https://arxiv.org/abs/2602.01454", "authors": ["Amirreza Shiralinasab Langari", "Leila Yeganeh", "Kim Khoa Nguyen"], "title": "Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs", "comment": null, "summary": "We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\\cdot \\mid v)$ and $P(\\cdot \\mid \\mathcal{G})$.\n  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ee3\u6570\u65b9\u6cd5\uff0c\u5c06\u56fe\u62d3\u6251\u4e0e\u8282\u70b9\u5c5e\u6027\u5206\u5e03\u7ed3\u5408\uff0c\u5f62\u6210\u62d3\u6251\u5f71\u54cd\u5206\u5e03\uff0c\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u56fe\u62d3\u6251\u5982\u4f55\u5f71\u54cd\u8282\u70b9\u5c5e\u6027\u7684\u5206\u5e03\uff0c\u5c06\u62d3\u6251\u548c\u5c5e\u6027\u89c6\u4e3a\u7ed3\u6784\u4e0d\u540c\u4f46\u76f8\u4e92\u4f5c\u7528\u7684\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u65b0\u7684\u5206\u6790\u89c6\u89d2\u3002", "method": "\u5f15\u5165\u4ee3\u6570\u65b9\u6cd5\u7ed3\u5408\u56fe\u62d3\u6251\u548c\u8282\u70b9\u5c5e\u6027\u6982\u7387\u5206\u5e03\uff1b\u5f00\u53d1\u8303\u7574\u6846\u67b6\u5f62\u5f0f\u5316\u8282\u70b9\u5bf9\u62d3\u6251\u7684\u611f\u77e5\uff1b\u91cf\u5316\u8be5\u89c6\u89d2\u5e76\u4e0e\u5c5e\u6027\u5206\u5e03\u6574\u5408\uff1b\u5efa\u7acb\u5145\u5206\u6027\u6761\u4ef6\u8bc1\u660e\u5728\u5b8c\u5168\u56fe\u4e0a\u6062\u590d\u539f\u59cb\u5206\u5e03\u3002", "result": "\u6784\u5efa\u4e86\u62d3\u6251\u6761\u4ef6\u5206\u5e03\u4f5c\u4e3a\u540e\u9a8c\u6982\u7387P(\u00b7|v)\u548cP(\u00b7|G)\u7684\u8fd1\u4f3c\uff1b\u5728\u7b80\u5355\u6d4b\u8bd5\u6a21\u578bID\u4e0a\u901a\u8fc7\u65e0\u76d1\u7763\u56fe\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u9a8c\u8bc1\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u6570\u6846\u67b6\u6210\u529f\u6355\u6349\u4e86\u56fe\u62d3\u6251\u5bf9\u8282\u70b9\u5c5e\u6027\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u4e3a\u7406\u89e3\u62d3\u6251\u4e0e\u5c5e\u6027\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.02033", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02033", "abs": "https://arxiv.org/abs/2602.02033", "authors": ["Shuo Lu", "Haohan Wang", "Wei Feng", "Weizhen Wang", "Shen Zhang", "Yaoyu Li", "Ao Ma", "Zheng Zhang", "Jingjing Lv", "Junjie Shen", "Ching Law", "Bing Zhan", "Yuan Xu", "Huizai Yao", "Yongcan Yu", "Chenyang Si", "Jian Liang"], "title": "One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation", "comment": null, "summary": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.", "AI": {"tldr": "OSMF\u6846\u67b6\u901a\u8fc7\u4ea7\u54c1\u611f\u77e5\u81ea\u9002\u5e94\u5206\u7ec4\u548c\u504f\u597d\u6761\u4ef6\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u73b0\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u5e7f\u544a\u56fe\u50cf\u4e2a\u6027\u5316\u751f\u6210\uff0c\u63d0\u5347\u7fa4\u4f53\u70b9\u51fb\u7387\u3002", "motivation": "\u73b0\u6709\u5e7f\u544a\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u91c7\u7528\"\u4e00\u5200\u5207\"\u7b56\u7565\uff0c\u4f18\u5316\u6574\u4f53\u70b9\u51fb\u7387\u800c\u5ffd\u89c6\u7528\u6237\u7fa4\u4f53\u7684\u504f\u597d\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u7279\u5b9a\u7fa4\u4f53\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9a\u5411\u8425\u9500\u6548\u679c\u3002", "method": "1) \u4ea7\u54c1\u611f\u77e5\u81ea\u9002\u5e94\u5206\u7ec4\uff1a\u57fa\u4e8e\u7528\u6237\u5c5e\u6027\u548c\u4ea7\u54c1\u7279\u5f81\u52a8\u6001\u7ec4\u7ec7\u7528\u6237\u7fa4\u4f53\uff1b2) \u504f\u597d\u6761\u4ef6\u56fe\u50cf\u751f\u6210\uff1a\u4f7f\u7528\u7fa4\u4f53\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(G-MLLM)\u4e3a\u6bcf\u4e2a\u7fa4\u4f53\u751f\u6210\u5b9a\u5236\u56fe\u50cf\uff1b3) \u4f7f\u7528Group-DPO\u5fae\u8c03G-MLLM\u8fdb\u884c\u7fa4\u4f53\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u7ea660\u4e07\u4e2a\u7fa4\u4f53\u3001\u57fa\u4e8e4000\u4e07\u7528\u6237\u7684GAIP\u6570\u636e\u96c6\u3002", "conclusion": "OSMF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7fa4\u4f53\u504f\u597d\u591a\u6837\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7fa4\u4f53\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u70b9\u51fb\u7387\u8868\u73b0\u3002"}}
{"id": "2602.01456", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01456", "abs": "https://arxiv.org/abs/2602.01456", "authors": ["Yilun Kuang", "Yash Dagade", "Tim G. J. Rudner", "Randall Balestriero", "Yann LeCun"], "title": "Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations", "comment": null, "summary": "Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Rectified Distribution Matching Regularization (RDMReg)\uff0c\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5339\u914d\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784(JEPA)\u4e2d\u5b66\u4e60\u7a00\u758f\u3001\u975e\u8d1f\u8868\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u9ad8\u65af\u5206\u5e03\u6b63\u5219\u5316\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u63a7\u5236\u7a00\u758f\u6027\u3002", "motivation": "\u73b0\u6709JEPA\u65b9\u6cd5\u4f7f\u7528\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\u6b63\u5219\u5316\u8868\u793a\uff0c\u4f46\u8fd9\u503e\u5411\u4e8e\u4ea7\u751f\u5bc6\u96c6\u8868\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u9ad8\u6548\u8868\u793a\u4e2d\u89c2\u5bdf\u5230\u7684\u5173\u952e\u7a00\u758f\u7279\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u63a7\u5236\u7a00\u758f\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRectified Distribution Matching Regularization (RDMReg)\uff0c\u8fd9\u662f\u4e00\u79cd\u5207\u7247\u53cc\u6837\u672c\u5206\u5e03\u5339\u914d\u635f\u5931\uff0c\u5c06\u8868\u793a\u5bf9\u9f50\u5230Rectified Generalized Gaussian (RGG)\u5206\u5e03\u3002RGG\u901a\u8fc7\u6574\u6d41\u64cd\u4f5c\u663e\u5f0f\u63a7\u5236\u671f\u671b\u2113\u2080\u8303\u6570\uff08\u7a00\u758f\u6027\uff09\uff0c\u540c\u65f6\u5728\u671f\u671b\u2113\u209a\u8303\u6570\u7ea6\u675f\u4e0b\u4fdd\u6301\u6700\u5927\u71b5\u7279\u6027\u3002", "result": "\u5c06RDMReg\u5e94\u7528\u4e8eJEPA\u5f97\u5230Rectified LpJEPA\uff0c\u80fd\u591f\u5b66\u4e60\u7a00\u758f\u3001\u975e\u8d1f\u8868\u793a\uff0c\u5728\u7a00\u758f\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u6743\u8861\u3002\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "RDMReg\u80fd\u6709\u6548\u5f3a\u5236\u7a00\u758f\u6027\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0cRectified LpJEPA\u4e25\u683c\u63a8\u5e7f\u4e86\u5148\u524d\u57fa\u4e8e\u9ad8\u65af\u7684JEPA\u65b9\u6cd5\uff0c\u4e3a\u5b66\u4e60\u9ad8\u6548\u7a00\u758f\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02043", "abs": "https://arxiv.org/abs/2602.02043", "authors": ["Cristian Sbrolli", "Matteo Matteucci", "Toshihiko Yamasaki"], "title": "Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models", "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).", "AI": {"tldr": "Auto-Comp\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5408\u6210\u57fa\u51c6\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u989c\u8272\u7ed1\u5b9a\u548c\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u666e\u904d\u5931\u8d25\uff0c\u5e76\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u4e0a\u4e0b\u6587\u5728\u5e2e\u52a9\u7a7a\u95f4\u63a8\u7406\u7684\u540c\u65f6\u4f1a\u963b\u788d\u5c5e\u6027\u7ed1\u5b9a\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff0c\u7ecf\u5e38\u6df7\u6dc6\"\u7ea2\u8272\u7acb\u65b9\u4f53\u548c\u84dd\u8272\u7403\u4f53\"\u4e0e\"\u84dd\u8272\u7acb\u65b9\u4f53\u548c\u7ea2\u8272\u7403\u4f53\"\u3002\u5206\u79bb\u8fd9\u4e9b\u5931\u8d25\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u6839\u6e90\u662f\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u5f15\u5165Auto-Comp\uff0c\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u548c\u5408\u6210\u7684\u57fa\u51c6\u751f\u6210\u6d41\u6c34\u7ebf\u3002\u5b83\u751f\u6210\u6700\u5c0f\u63cf\u8ff0\uff08\u5982\"\u767d\u8272\u80cc\u666f\u4e0a\u81ea\u884c\u8f66\u5de6\u4fa7\u7684\u663e\u793a\u5668\"\uff09\u548cLLM\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff08\u5982\"\u5728\u660e\u4eae\u6444\u5f71\u68da\u4e2d\uff0c\u663e\u793a\u5668\u4f4d\u4e8e\u81ea\u884c\u8f66\u5de6\u4fa7\"\uff09\u7684\u914d\u5bf9\u56fe\u50cf\uff0c\u901a\u8fc7\u53d7\u63a7A/B\u6d4b\u8bd5\u5206\u79bb\u6838\u5fc3\u7ed1\u5b9a\u80fd\u529b\u4e0e\u89c6\u89c9\u8bed\u8a00\u590d\u6742\u6027\u3002", "result": "\u572820\u4e2aVLM\u4e0a\u8bc4\u4f30\u989c\u8272\u7ed1\u5b9a\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u65b0\u57fa\u51c6\uff0c\u53d1\u73b0CLIP\u548cSigLIP\u6a21\u578b\u5bb6\u65cf\u90fd\u5b58\u5728\u666e\u904d\u7684\u7ec4\u5408\u63a8\u7406\u5931\u8d25\u3002\u65b0\u7684\"\u6df7\u6dc6\u57fa\u51c6\"\u63ed\u793a\u4e86\u66f4\u6df1\u5c42\u7f3a\u9677\uff1a\u6a21\u578b\u5bf9\u4f4e\u71b5\u5e72\u6270\u7269\uff08\u5982\u91cd\u590d\u5bf9\u8c61\u6216\u989c\u8272\uff09\u9ad8\u5ea6\u654f\u611f\uff0c\u8868\u660e\u5176\u7ec4\u5408\u5931\u8d25\u8d85\u51fa\u4e86\u5df2\u77e5\u7684\u8bcd\u888b\u9650\u5236\u3002\u53d1\u73b0\u4e86\u4e00\u4e2a\u4ee4\u4eba\u60ca\u8bb6\u7684\u6743\u8861\uff1a\u89c6\u89c9\u8bed\u8a00\u4e0a\u4e0b\u6587\u63d0\u4f9b\u5168\u5c40\u573a\u666f\u7ebf\u7d22\u6709\u52a9\u4e8e\u7a7a\u95f4\u63a8\u7406\uff0c\u4f46\u540c\u65f6\u4f1a\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u6742\u6ce2\u963b\u788d\u5c40\u90e8\u5c5e\u6027\u7ed1\u5b9a\u3002", "conclusion": "Auto-Comp\u6846\u67b6\u4e3a\u672a\u6765\u57fa\u51c6\u521b\u5efa\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u7279\u522b\u662f\u89c6\u89c9\u8bed\u8a00\u4e0a\u4e0b\u6587\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u5c5e\u6027\u7ed1\u5b9a\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.01468", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01468", "abs": "https://arxiv.org/abs/2602.01468", "authors": ["Viet Nguyen", "Tuan Minh Pham", "Thinh Cao", "Tan Dinh", "Huy Nguyen", "Nhat Ho", "Alessandro Rinaldo"], "title": "A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts", "comment": "Viet Nguyen, Tuan Minh Pham, and Thinh Cao contributed equally to this work", "summary": "Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.", "AI": {"tldr": "\u95e8\u63a7\u6ce8\u610f\u529b\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6539\u8fdb\u6807\u51c6\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\uff0c\u4ec5\u9700\u591a\u9879\u5f0f\u6570\u91cf\u6837\u672c\u5373\u53ef\u51c6\u786e\u4f30\u8ba1\u4e13\u5bb6\uff0c\u800c\u6807\u51c6\u6ce8\u610f\u529b\u9700\u8981\u6307\u6570\u7ea7\u6837\u672c\u3002", "motivation": "\u95e8\u63a7\u6ce8\u610f\u529b\u5df2\u88ab\u5b9e\u8bc1\u8bc1\u660e\u80fd\u63d0\u9ad8\u6807\u51c6\u6ce8\u610f\u529b\u7684\u8868\u8fbe\u80fd\u529b\u5e76\u6d88\u9664\u6ce8\u610f\u529b\u6c47\u73b0\u8c61\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u4f18\u52bf\u7684\u7406\u8bba\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5c06\u95e8\u63a7\u6ce8\u610f\u529b\u77e9\u9635\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u77e9\u9635\u7684\u6bcf\u4e2a\u6761\u76ee\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u5c42\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u5c06\u5b66\u4e60\u95ee\u9898\u8f6c\u5316\u4e3a\u4e13\u5bb6\u4f30\u8ba1\u95ee\u9898\uff0c\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u6837\u672c\u6548\u7387\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u95e8\u63a7\u6ce8\u610f\u529b\u6bd4\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5177\u6709\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\uff1a\u95e8\u63a7\u6ce8\u610f\u529b\u4ec5\u9700\u591a\u9879\u5f0f\u6570\u91cf\u6570\u636e\u70b9\u5373\u53ef\u51c6\u786e\u4f30\u8ba1\u4e13\u5bb6\uff0c\u800c\u6807\u51c6\u6ce8\u610f\u529b\u9700\u8981\u6307\u6570\u7ea7\u6570\u636e\u70b9\u624d\u80fd\u8fbe\u5230\u76f8\u540c\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "\u95e8\u63a7\u6ce8\u610f\u529b\u7684\u7406\u8bba\u4f18\u52bf\u89e3\u91ca\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e3a\u95e8\u63a7\u673a\u5236\u5728\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u8f93\u51fa\u6216\u503c\u6620\u5c04\u4f4d\u7f6e\u653e\u7f6e\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.02067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02067", "abs": "https://arxiv.org/abs/2602.02067", "authors": ["Nikola Cenikj", "\u00d6zg\u00fcn Turgut", "Alexander M\u00fcller", "Alexander Steger", "Jan Kehrer", "Marcus Brugger", "Daniel Rueckert", "Eimo Martens", "Philip M\u00fcller"], "title": "Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data", "comment": null, "summary": "Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.", "AI": {"tldr": "\u63d0\u51faSegmentMIL\uff0c\u4e00\u79cd\u57fa\u4e8etransformer\u7684\u591a\u89c6\u89d2\u591a\u793a\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u60a3\u8005\u7ea7\u522b\u7684\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u5206\u7c7b\uff0c\u65e0\u9700\u89c6\u89d2\u7ea7\u6807\u6ce8\uff0c\u4ec5\u9700\u60a3\u8005\u7ea7\u76d1\u7763\u5373\u53ef\u540c\u65f6\u9884\u6d4b\u72ed\u7a84\u5b58\u5728\u5e76\u5b9a\u4f4d\u53d7\u5f71\u54cd\u533a\u57df\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u662f\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u901a\u5e38\u9700\u8981\u4ece\u591a\u4e2a\u8840\u7ba1\u9020\u5f71\u89c6\u89d2\u8fdb\u884c\u5206\u6790\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u7684\u89c6\u89d2\u7ea7\u6807\u6ce8\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u591a\u89c6\u89d2\u95f4\u7684\u65f6\u5e8f\u52a8\u6001\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSegmentMIL\u6846\u67b6\uff0c\u57fa\u4e8etransformer\u7684\u591a\u89c6\u89d2\u591a\u793a\u4f8b\u5b66\u4e60\uff0c\u4ec5\u4f7f\u7528\u60a3\u8005\u7ea7\u76d1\u7763\uff08\u65e0\u9700\u89c6\u89d2\u7ea7\u6807\u6ce8\uff09\uff0c\u8054\u5408\u9884\u6d4b\u72ed\u7a84\u5b58\u5728\u5e76\u5b9a\u4f4d\u53d7\u5f71\u54cd\u89e3\u5256\u533a\u57df\uff08\u533a\u5206\u5de6\u53f3\u51a0\u72b6\u52a8\u8109\u53ca\u5176\u5206\u6bb5\uff09\u3002", "result": "\u5728\u5185\u90e8\u548c\u5916\u90e8\u8bc4\u4f30\u4e2d\u5747\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u4f18\u4e8e\u89c6\u89d2\u7ea7\u6a21\u578b\u548c\u7ecf\u5178MIL\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u4f5c\u4e3a\u4e34\u5e8a\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "SegmentMIL\u901a\u8fc7\u60a3\u8005\u7ea7\u76d1\u7763\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u51a0\u72b6\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\uff0c\u65e0\u9700\u6602\u8d35\u7684\u89c6\u89d2\u7ea7\u6807\u6ce8\uff0c\u80fd\u591f\u6355\u6349\u591a\u89c6\u89d2\u95f4\u7684\u52a8\u6001\u4f9d\u8d56\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01469", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01469", "abs": "https://arxiv.org/abs/2602.01469", "authors": ["Mude Hui", "Xin Huang", "Jaime Campos Salas", "Yue Sun", "Nathan Pemberton", "Xiang Song", "Ashish Khetan", "George Karypis"], "title": "P-EAGLE: Parallel-Drafting EAGLE with Scalable Training", "comment": null, "summary": "Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.", "AI": {"tldr": "P-EAGLE\u5c06EAGLE\u4ece\u81ea\u56de\u5f52\u8f6c\u6362\u4e3a\u5e76\u884c\u591atoken\u9884\u6d4b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5171\u4eab\u9690\u85cf\u72b6\u6001\u548c\u8bad\u7ec3\u4f18\u5316\u6846\u67b6\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406LLM\u4e2d\u5b9e\u73b01.10-1.36\u500d\u52a0\u901f", "motivation": "\u63a8\u7406LLM\u751f\u6210\u66f4\u957f\u8f93\u51fa\u9700\u8981\u8bad\u7ec3\u5728\u6269\u5c55\u5e8f\u5217\u4e0a\u7684\u63a8\u6d4b\u89e3\u7801\u8349\u7a3f\u5668\u3002\u5e76\u884c\u8349\u7a3f\uff08\u6bcf\u6b21\u524d\u5411\u4f20\u64ad\u9884\u6d4b\u591a\u4e2atoken\uff09\u76f8\u6bd4\u987a\u5e8f\u751f\u6210\u6709\u5ef6\u8fdf\u4f18\u52bf\uff0c\u4f46\u8bad\u7ec3\u590d\u6742\u5ea6\u968f\u5e8f\u5217\u957f\u5ea6\u548c\u5e76\u884c\u4f4d\u7f6e\u4e58\u79ef\u5448\u4e8c\u6b21\u65b9\u589e\u957f\uff0c\u4f7f\u5f97\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645", "method": "\u63d0\u51faP-EAGLE\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5171\u4eab\u9690\u85cf\u72b6\u6001\u5c06EAGLE\u4ece\u81ea\u56de\u5f52\u8f6c\u6362\u4e3a\u5e76\u884c\u591atoken\u9884\u6d4b\u3002\u4e3a\u6269\u5c55\u5230\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u6ce8\u610f\u529b\u63a9\u7801\u9884\u8ba1\u7b97\u548c\u5e8f\u5217\u5206\u533a\u6280\u672f\u7684\u6846\u67b6\uff0c\u652f\u6301\u5728\u5355\u4e2a\u5e8f\u5217\u5185\u8fdb\u884c\u68af\u5ea6\u7d2f\u79ef\u7684\u5e76\u884c\u9884\u6d4b\u8bad\u7ec3", "result": "\u5728vLLM\u4e2d\u5b9e\u73b0P-EAGLE\uff0c\u5728GPT-OSS 120B\u300120B\u548cQwen3-Coder 30B\u6a21\u578b\u4e0a\u76f8\u6bd4\u81ea\u56de\u5f52EAGLE-3\u5b9e\u73b0\u4e861.10-1.36\u500d\u7684\u52a0\u901f", "conclusion": "P-EAGLE\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u884c\u9884\u6d4b\u67b6\u6784\u548c\u8bad\u7ec3\u4f18\u5316\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5e76\u884c\u8349\u7a3f\u8bad\u7ec3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u63a8\u7406LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02089", "abs": "https://arxiv.org/abs/2602.02089", "authors": ["Changbai Li", "Haodong Zhu", "Hanlin Chen", "Xiuping Liang", "Tongfei Chen", "Shuwei Shao", "Linlin Yang", "Huobin Tan", "Baochang Zhang"], "title": "UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction", "comment": "ICLR 2026", "summary": "While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.", "AI": {"tldr": "UrbanGS\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u76843D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4D-Normal\u6b63\u5219\u5316\u548c\u7a7a\u95f4\u81ea\u9002\u5e94\u9ad8\u65af\u526a\u679d\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u6709\u9650\u573a\u666f\u4e2d\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u6e32\u67d3\uff0c\u4f46\u6269\u5c55\u5230\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u65f6\u9762\u4e34\u51e0\u4f55\u4e00\u81f4\u6027\u5dee\u3001\u5185\u5b58\u6548\u7387\u4f4e\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "1. \u6df1\u5ea6\u4e00\u81f4D-Normal\u6b63\u5219\u5316\u6a21\u5757\uff1a\u7ed3\u5408D-Normal\u7ea6\u675f\u4e0e\u5916\u90e8\u6df1\u5ea6\u76d1\u7763\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u4e00\u81f4\u6027\u548c\u9006\u6df1\u5ea6\u504f\u5dee\u7684\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u52a0\u6743\u673a\u5236\uff0c\u5168\u9762\u66f4\u65b0\u6240\u6709\u51e0\u4f55\u53c2\u6570\uff1b2. \u7a7a\u95f4\u81ea\u9002\u5e94\u9ad8\u65af\u526a\u679d\u7b56\u7565\uff1a\u6839\u636e\u5c40\u90e8\u51e0\u4f55\u590d\u6742\u5ea6\u548c\u53ef\u89c1\u6027\u52a8\u6001\u8c03\u6574\u9ad8\u65af\u5bc6\u5ea6\uff1b3. \u7edf\u4e00\u5206\u533a\u548c\u89c6\u56fe\u5206\u914d\u65b9\u6848\uff1a\u6d88\u9664\u8fb9\u754c\u4f2a\u5f71\u5e76\u4f18\u5316\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5728\u591a\u4e2a\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUrbanGS\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u5747\u8fbe\u5230\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "UrbanGS\u901a\u8fc7\u521b\u65b0\u7684\u6df1\u5ea6\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u526a\u679d\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e863DGS\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u5185\u5b58\u6548\u7387\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u57ce\u5e02\u89c4\u6a21\u91cd\u5efa\u3002"}}
{"id": "2602.01480", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01480", "abs": "https://arxiv.org/abs/2602.01480", "authors": ["Eric Regis", "Sinho Chewi"], "title": "Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability", "comment": null, "summary": "How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the \"Central Flow\", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a \"rod\"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRod Flow\u4f5c\u4e3a\u68af\u5ea6\u4e0b\u964d\u52a8\u529b\u5b66\u7684\u65b0ODE\u8fd1\u4f3c\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684Central Flow\u65b9\u6cd5\uff0cRod Flow\u57fa\u4e8e\u7269\u7406\"\u6746\"\u6a21\u578b\u63a8\u5bfc\uff0c\u80fd\u66f4\u597d\u6355\u6349GD\u52a8\u6001\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u5e76\u80fd\u9884\u6d4b\u4e34\u754c\u9510\u5ea6\u9608\u503c\u548c\u81ea\u7a33\u5b9a\u73b0\u8c61\u3002", "motivation": "\u7406\u89e3\u68af\u5ea6\u4e0b\u964d\u5728\u975e\u51f8\u666f\u89c2\u4e0a\u7684\u8bad\u7ec3\u52a8\u6001\u662f\u4e00\u4e2a\u6311\u6218\u3002Cohen\u7b49\u4eba(2021)\u53d1\u73b0\u7684\"\u7a33\u5b9a\u6027\u8fb9\u7f18\"\u73b0\u8c61\u8868\u660e\uff0c\u5927\u5b66\u4e60\u7387\u7684\u68af\u5ea6\u4e0b\u964d\u4f1a\u504f\u79bb\u68af\u5ea6\u6d41\u3002\u867d\u7136Central Flow(Cohen et al., 2025)\u63d0\u4f9b\u4e86ODE\u8fd1\u4f3c\uff0c\u4f46\u9700\u8981\u66f4\u539f\u5219\u6027\u3001\u66f4\u51c6\u786e\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRod Flow\u65b9\u6cd5\uff0c\u5c06\u68af\u5ea6\u4e0b\u964d\u8fed\u4ee3\u89c6\u4e3a\u4e00\u7ef4\u6269\u5c55\u7269\u4f53\u2014\u2014\"\u6746\"\uff0c\u57fa\u4e8e\u6b64\u7269\u7406\u56fe\u50cf\u8fdb\u884c\u539f\u5219\u6027\u63a8\u5bfc\u3002\u8be5\u65b9\u6cd5\u80fd\u663e\u5f0f\u8ba1\u7b97\u4e14\u6210\u672c\u4f4e\u5ec9\uff0c\u76f8\u6bd4Central Flow\u80fd\u66f4\u597d\u5730\u6355\u6349\u7b80\u5355\u73a9\u5177\u793a\u4f8b\u7684GD\u52a8\u6001\uff0c\u5e76\u5728\u4ee3\u8868\u6027\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86Rod Flow\u80fd\u6b63\u786e\u9884\u6d4b\u4e34\u754c\u9510\u5ea6\u9608\u503c\uff0c\u5e76\u89e3\u91ca\u56db\u6b21\u52bf\u4e2d\u7684\u81ea\u7a33\u5b9a\u73b0\u8c61\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\uff0cRod Flow\u5728\u7b80\u5355\u793a\u4f8b\u4e2d\u6bd4Central Flow\u8868\u73b0\u66f4\u597d\uff0c\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "Rod Flow\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u51c6\u786e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684ODE\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u7406\u89e3\u5927\u5b66\u4e60\u7387\u4e0b\u68af\u5ea6\u4e0b\u964d\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u6027\u8fb9\u7f18\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7269\u7406\"\u6746\"\u6a21\u578b\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349GD\u8fed\u4ee3\u7684\u52a8\u6001\u7279\u6027\u3002"}}
{"id": "2602.02092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02092", "abs": "https://arxiv.org/abs/2602.02092", "authors": ["FSVideo Team", "Qingyu Chen", "Zhiyuan Fang", "Haibin Huang", "Xinwei Huang", "Tong Jin", "Minxuan Lin", "Bo Liu", "Celong Liu", "Chongyang Ma", "Xing Mei", "Xiaohui Shen", "Yaojie Shen", "Fuwen Tan", "Angtian Wang", "Xiao Yang", "Yiding Yang", "Jiamin Yuan", "Lingxi Zhang", "Yuxin Zhang"], "title": "FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space", "comment": "Project Page: https://kingofprank.github.io/fsvideo/", "summary": "We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\\times64\\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.", "AI": {"tldr": "FSVideo\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5feb\u901f\u56fe\u50cf\u5230\u89c6\u9891\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u5ea6\u538b\u7f29\u7684\u6f5c\u5728\u7a7a\u95f4\u3001\u6539\u8fdb\u7684\u6269\u6563Transformer\u67b6\u6784\u548c\u591a\u5206\u8fa8\u7387\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6bd4\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u89c6\u9891\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5feb\u901f\u9ad8\u6548\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u7ed3\u679c\u3002", "method": "1) \u8bbe\u8ba1\u65b0\u7684\u89c6\u9891\u81ea\u7f16\u7801\u5668\uff0c\u5b9e\u73b064\u00d764\u00d74\u7684\u9ad8\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\uff1b2) \u91c7\u7528\u5e26\u6709\u5c42\u5185\u5b58\u8bbe\u8ba1\u7684\u6269\u6563Transformer\u67b6\u6784\uff0c\u589e\u5f3a\u5c42\u95f4\u4fe1\u606f\u6d41\u548c\u4e0a\u4e0b\u6587\u91cd\u7528\uff1b3) \u901a\u8fc7\u591a\u6b65DIT\u4e0a\u91c7\u6837\u5668\u5b9e\u73b0\u591a\u5206\u8fa8\u7387\u751f\u6210\u7b56\u7565\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5305\u542b140\u4ebf\u53c2\u6570\u7684DIT\u57fa\u7840\u6a21\u578b\u548c140\u4ebf\u53c2\u6570\u7684DIT\u4e0a\u91c7\u6837\u5668\uff0c\u5728\u4fdd\u6301\u4e0e\u5176\u4ed6\u6d41\u884c\u5f00\u6e90\u6a21\u578b\u7ade\u4e89\u6027\u6027\u80fd\u7684\u540c\u65f6\uff0c\u751f\u6210\u901f\u5ea6\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FSVideo\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u6548\u7684\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01483", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.01483", "abs": "https://arxiv.org/abs/2602.01483", "authors": ["Edwin V. Bonilla", "He Zhao", "Daniel M. Steinberg"], "title": "Causal Preference Elicitation", "comment": null, "summary": "We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.", "AI": {"tldr": "\u63d0\u51fa\u56e0\u679c\u504f\u597d\u83b7\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u4e13\u5bb6\u5bf9\u5c40\u90e8\u8fb9\u5173\u7cfb\u7684\u5224\u65ad\u6765\u52a0\u901f\u56e0\u679c\u56fe\u540e\u9a8c\u96c6\u4e2d\uff0c\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u63d0\u5347\u6709\u5411\u6548\u5e94\u6062\u590d\u6548\u679c", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u89c2\u6d4b\u6570\u636e\uff0c\u4f46\u4e13\u5bb6\u77e5\u8bc6\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5c40\u90e8\u7ed3\u6784\u4fe1\u606f\u3002\u5982\u4f55\u6709\u6548\u7ed3\u5408\u4e13\u5bb6\u5224\u65ad\u6765\u52a0\u901f\u56e0\u679c\u56fe\u5b66\u4e60\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4ece\u4efb\u610f\u9ed1\u76d2\u89c2\u6d4b\u540e\u9a8c\u51fa\u53d1\uff0c\u7528\u4e09\u5206\u7c7b\u4f3c\u7136\u5efa\u6a21\u4e13\u5bb6\u5bf9\u8fb9\u5b58\u5728\u548c\u65b9\u5411\u7684\u566a\u58f0\u5224\u65ad\uff0c\u4f7f\u7528\u7c92\u5b50\u8fd1\u4f3c\u8fdb\u884c\u540e\u9a8c\u63a8\u65ad\uff0c\u57fa\u4e8e\u671f\u671b\u4fe1\u606f\u589e\u76ca\u51c6\u5219\u9009\u62e9\u67e5\u8be2", "result": "\u5728\u5408\u6210\u56fe\u3001\u86cb\u767d\u8d28\u4fe1\u53f7\u6570\u636e\u548c\u4eba\u7c7b\u57fa\u56e0\u6270\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u540e\u9a8c\u96c6\u4e2d\u548c\u66f4\u597d\u7684\u6709\u5411\u6548\u5e94\u6062\u590d", "conclusion": "\u56e0\u679c\u504f\u597d\u83b7\u53d6\u6846\u67b6\u6709\u6548\u6574\u5408\u4e13\u5bb6\u5c40\u90e8\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u56e0\u679c\u53d1\u73b0\u6548\u7387\uff0c\u4e3a\u4e13\u5bb6\u53c2\u4e0e\u7684\u56e0\u679c\u5b66\u4e60\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5"}}
{"id": "2602.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02107", "abs": "https://arxiv.org/abs/2602.02107", "authors": ["Yu Wang", "Chuanguang Yang", "Zhulin An", "Weilun Feng", "Jiarui Zhao", "Chengqing Yu", "Libo Huang", "Boyu Diao", "Yongjun Xu"], "title": "Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model", "comment": null, "summary": "Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.", "AI": {"tldr": "\u63d0\u51faDSKD\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u5206\u7c7b\u5668\u5f15\u5bfc\u8f7b\u91cf\u7ea7\u6269\u6563\u6a21\u578b\u53bb\u566a\u5b66\u751f\u7279\u5f81\uff0c\u5e76\u4f7f\u7528LSH\u5f15\u5bfc\u7684\u7279\u5f81\u84b8\u998f\uff0c\u6d88\u9664\u5e08\u751f\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u76f4\u63a5\u5bf9\u9f50\u5e08\u751f\u7279\u5f81\uff0c\u4f46\u7531\u4e8e\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u5b66\u751f\u53ef\u80fd\u4ece\u6559\u5e08\u5b66\u4e60\u5230\u4e0d\u517c\u5bb9\u7684\u4fe1\u606f\uff0c\u9700\u8981\u89e3\u51b3\u5e08\u751f\u7279\u5f81\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6559\u5e08\u5f15\u5bfc\u7684\u5b66\u751f\u6269\u6563\u81ea\u84b8\u998f(DSKD)\uff1a1) \u5229\u7528\u6559\u5e08\u5206\u7c7b\u5668\u5f15\u5bfc\u8f7b\u91cf\u7ea7\u6269\u6563\u6a21\u578b\u5bf9\u5b66\u751f\u7279\u5f81\u8fdb\u884c\u53bb\u566a\u91c7\u6837\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c(LSH)\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u539f\u59cb\u5b66\u751f\u7279\u5f81\u548c\u53bb\u566a\u540e\u7279\u5f81\u4e4b\u95f4\u8fdb\u884c\u84b8\u998f\u3002", "result": "\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDSKD\u5728\u5404\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "DSKD\u901a\u8fc7\u6269\u6563\u6a21\u578b\u53bb\u566a\u5b66\u751f\u7279\u5f81\u5e76\u5229\u7528LSH\u5f15\u5bfc\u84b8\u998f\uff0c\u80fd\u6709\u6548\u6d88\u9664\u5e08\u751f\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u8ba9\u5b66\u751f\u4ece\u6559\u5e08\u5b66\u4e60\u5230\u66f4\u6709\u610f\u4e49\u7684\u77e5\u8bc6\u3002"}}
{"id": "2602.01485", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01485", "abs": "https://arxiv.org/abs/2602.01485", "authors": ["Muheng Li", "Jian Qian", "Wenlong Mou"], "title": "Predicting and improving test-time scaling laws via reward tail-guided search", "comment": "33 pages, 5 figures", "summary": "Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c3e\u90e8\u5206\u5e03\u4f30\u8ba1\u7684\u7f29\u653e\u5b9a\u5f8b\u9884\u6d4b\u65b9\u6cd5\u548c\u7f29\u653e\u5b9a\u5f8b\u5f15\u5bfc\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edfBest-of-N\u7b56\u7565\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1Best-of-N\u7b56\u7565\u5df2\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8eN\u9009\u62e9\u3001\u9884\u7b97\u5206\u914d\u548c\u591a\u9636\u6bb5\u51b3\u7b56\u7684\u539f\u5219\u6027\u6307\u5bfc\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u5956\u52b1\u7684\u5c3e\u90e8\u5206\u5e03\u6765\u9884\u6d4bLLM\u7f29\u653e\u5b9a\u5f8b\uff0c\u65e0\u9700\u7a77\u4e3e\u8bc4\u4f30\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u7f29\u653e\u5b9a\u5f8b\u5f15\u5bfc\u641c\u7d22\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u8bc6\u522b\u548c\u5229\u7528\u5177\u6709\u6700\u9ad8\u9884\u6d4b\u6f5c\u529b\u7684\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u7406\u8bba\u8bc1\u660eSLG\u76f8\u6bd4\u5b8c\u7f8e\u4fe1\u606f\u9884\u8a00\u673a\u5b9e\u73b0\u53ef\u5ffd\u7565\u7684\u9057\u61be\uff0c\u83b7\u5f97\u4e0eBest-of-N\u76f8\u540c\u9884\u671f\u5956\u52b1\u6240\u9700\u8ba1\u7b97\u9884\u7b97\u5448\u591a\u9879\u5f0f\u7ea7\u51cf\u5c11\u3002\u5b9e\u8bc1\u9a8c\u8bc1\u5728\u4e0d\u540cLLM\u548c\u5956\u52b1\u6a21\u578b\u4e0a\uff0c\u5c3e\u90e8\u5f15\u5bfc\u5206\u914d\u59cb\u7ec8\u6bd4\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684Best-of-N\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u4ea7\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u5c3e\u90e8\u5f15\u5bfc\u641c\u7d22\u6846\u67b6\u4e3a\u6d4b\u8bd5\u65f6\u7f29\u653e\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.02114", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02114", "abs": "https://arxiv.org/abs/2602.02114", "authors": ["Xin Ding", "Yun Chen", "Sen Zhang", "Kao Zhang", "Nenglun Chen", "Peibei Cao", "Yongwei Wang", "Fei Wu"], "title": "Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training", "comment": null, "summary": "Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \\textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\\times64$ to $256\\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.", "AI": {"tldr": "\u63d0\u51faiCCDM\u6539\u8fdb\u6846\u67b6\uff0c\u5c06\u5148\u8fdb\u7684EDM\u6269\u6563\u6a21\u578b\u4e0e\u81ea\u9002\u5e94\u90bb\u57df\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\uff0c\u5728\u8fde\u7eed\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ecStable Diffusion 3\u7b49\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u91c7\u6837\u6210\u672c\u3002", "motivation": "\u73b0\u6709CCDM\u6846\u67b6\u867d\u7136\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f9d\u8d56\u8fc7\u65f6\u7684\u6269\u6563\u6846\u67b6\u3001\u91c7\u6837\u6548\u7387\u4f4e\uff08\u91c7\u6837\u8f68\u8ff9\u957f\uff09\uff0c\u4e14\u6700\u8fd1\u88abGAN\u65b9\u6cd5CcGAN-AVAR\u8d85\u8d8a\u3002\u9700\u8981\u6539\u8fdb\u751f\u6210\u8d28\u91cf\u548c\u91c7\u6837\u6548\u7387\u3002", "method": "\u63d0\u51faiCCDM\u6846\u67b6\uff0c\u91c7\u7528\u5148\u8fdb\u7684Elucidated Diffusion Model (EDM)\u6846\u67b6\u5e76\u8fdb\u884c\u91cd\u5927\u4fee\u6539\uff0c\u5f15\u5165\u65b0\u9896\u7684\u77e9\u9635\u5f62\u5f0fEDM\u516c\u5f0f\u548c\u81ea\u9002\u5e94\u90bb\u57df\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u56fe\u50cf\u5206\u8fa8\u7387\u4ece64\u00d764\u5230256\u00d7256\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0ciCCDM\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion 3\u3001FLUX.1\u548cQwen-Image\uff09\uff0c\u5728\u5b9e\u73b0\u66f4\u9ad8\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u91c7\u6837\u6210\u672c\u3002", "conclusion": "iCCDM\u901a\u8fc7\u6574\u5408\u5148\u8fdb\u7684EDM\u6846\u67b6\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86CCDM\u7684\u5c40\u9650\u6027\uff0c\u5728\u8fde\u7eed\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\u548c\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2602.01486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01486", "abs": "https://arxiv.org/abs/2602.01486", "authors": ["Xuesong Wang", "Michael Groom", "Rafael Oliveira", "He Zhao", "Terence O'Kane", "Edwin V. Bonilla"], "title": "Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems", "comment": null, "summary": "Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u5668(MSWT)\uff0c\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\u6765\u89e3\u51b3\u795e\u7ecf\u7b97\u5b50\u4e2d\u7684\u9891\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u5728\u6df7\u6c8c\u7cfb\u7edf\u548c\u6c14\u5019\u518d\u5206\u6790\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u8bb8\u591a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u52a8\u6001\u7cfb\u7edf\u4ee3\u7406\u6a21\u578b\uff08\u5982\u795e\u7ecf\u7b97\u5b50\uff09\u5b58\u5728\u9891\u8c31\u504f\u5dee\uff0c\u4f1a\u8870\u51cf\u9ad8\u9891\u6210\u5206\uff0c\u800c\u8fd9\u4e9b\u9ad8\u9891\u6210\u5206\u901a\u5e38\u7f16\u7801\u4e86\u5c0f\u5c3a\u5ea6\u7ed3\u6784\u3002\u5728\u5929\u6c14\u9884\u62a5\u7b49\u5e94\u7528\u4e2d\uff0c\u9ad8\u9891\u6210\u5206\u7684\u8bef\u8868\u793a\u4f1a\u5bfc\u81f4\u957f\u671f\u9884\u6d4b\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u53d8\u6362\u5668(MSWT)\uff0c\u5728\u5c0f\u6ce2\u57df\u4e2d\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\u3002\u5c0f\u6ce2\u53d8\u6362\u660e\u786e\u5206\u79bb\u4e86\u4e0d\u540c\u5c3a\u5ea6\u7684\u4f4e\u9891\u548c\u9ad8\u9891\u5185\u5bb9\u3002MSWT\u91c7\u7528\u4fdd\u7559\u5c0f\u6ce2\u7279\u6027\u7684\u4e0b\u91c7\u6837\u65b9\u6848\u6765\u4fdd\u6301\u9ad8\u9891\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5c0f\u6ce2\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8de8\u5c3a\u5ea6\u548c\u9891\u5e26\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u6df7\u6c8c\u52a8\u6001\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\uff0cMSWT\u663e\u8457\u51cf\u5c11\u4e86\u8bef\u5dee\u5e76\u6539\u5584\u4e86\u957f\u671f\u9891\u8c31\u4fdd\u771f\u5ea6\u3002\u5728ERA5\u6c14\u5019\u518d\u5206\u6790\u6570\u636e\u4e0a\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c11\u4e86\u6c14\u5019\u5b66\u504f\u5dee\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MSWT\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u4e2d\u5b66\u4e60\u52a8\u6001\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7b97\u5b50\u7684\u9891\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u9891\u7279\u5f81\u7684\u540c\u65f6\u6539\u5584\u4e86\u957f\u671f\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02123", "abs": "https://arxiv.org/abs/2602.02123", "authors": ["Yangyi Cao", "Yuanhang Li", "Lan Chen", "Qi Mao"], "title": "MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos", "comment": null, "summary": "We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.", "AI": {"tldr": "MLV-Edit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u6d41\u7684\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u5206\u949f\u7ea7\u89c6\u9891\u7f16\u8f91\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u6bb5\u7f16\u8f91\u7b56\u7565\u548c\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff08Velocity Blend\u548cAttention Sink\uff09\u6765\u4fdd\u6301\u957f\u65f6\u95f4\u89c6\u9891\u7684\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u6280\u672f\u64c5\u957f\u77ed\u89c6\u9891\u5904\u7406\uff0c\u4f46\u5728\u6269\u5c55\u5230\u957f\u65f6\u95f4\u89c6\u9891\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u548c\u96be\u4ee5\u7ef4\u6301\u6570\u5343\u5e27\u95f4\u5168\u5c40\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7684\u5206\u6bb5\u7f16\u8f91\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aVelocity Blend\u901a\u8fc7\u5bf9\u9f50\u76f8\u90bb\u7247\u6bb5\u7684\u6d41\u573a\u6765\u7ea0\u6b63\u8fd0\u52a8\u4e0d\u4e00\u81f4\u6027\uff1bAttention Sink\u5c06\u5c40\u90e8\u7247\u6bb5\u7279\u5f81\u951a\u5b9a\u5230\u5168\u5c40\u53c2\u8003\u5e27\u6765\u6291\u5236\u7d2f\u79ef\u7ed3\u6784\u6f02\u79fb\u3002", "result": "\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0cMLV-Edit\u5728\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MLV-Edit\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u95f4\u89c6\u9891\u7f16\u8f91\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5206\u6bb5\u7f16\u8f91\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u957f\u89c6\u9891\u5904\u7406\u3002"}}
{"id": "2602.01493", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01493", "abs": "https://arxiv.org/abs/2602.01493", "authors": ["Zhuoyuan Wang", "Hanjiang Hu", "Xiyu Deng", "Saviz Mowlavi", "Yorie Nakahira"], "title": "OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference", "comment": null, "summary": "Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.", "AI": {"tldr": "OpInf-LLM\uff1a\u57fa\u4e8e\u7b97\u5b50\u63a8\u7406\u7684LLM\u53c2\u6570\u5316PDE\u6c42\u89e3\u6846\u67b6\uff0c\u5229\u7528\u5c11\u91cf\u89e3\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u6307\u5b9aPDE\u6c42\u89e3\u4efb\u52a1\uff0c\u5728\u5f02\u6784\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u4ecd\u9762\u4e34\u6267\u884c\u6210\u529f\u7387\u4e0e\u6570\u503c\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6cdb\u5316\u5230\u672a\u89c1\u53c2\u6570\u548c\u8fb9\u754c\u6761\u4ef6\u65f6\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faOpInf-LLM\u6846\u67b6\uff0c\u57fa\u4e8e\u7b97\u5b50\u63a8\u7406\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u89e3\u6570\u636e\u5b9e\u73b0\u53c2\u6570\u5316PDE\u6c42\u89e3\u3002\u8be5\u6846\u67b6\u4e0eLLM\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u6307\u5b9aPDE\u6c42\u89e3\u4efb\u52a1\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u9700\u6c42\u548c\u7edf\u4e00\u5de5\u5177\u63a5\u53e3\u3002", "result": "OpInf-LLM\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5305\u62ec\u672a\u89c1\u53c2\u6570\u548c\u914d\u7f6e\u5728\u5185\u7684\u591a\u6837\u5316PDE\u5b9e\u4f8b\uff0c\u5728\u5f02\u6784\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u6267\u884c\u6210\u529f\u7387\uff0c\u4e3aLLM-based PDE\u6c42\u89e3\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u6cdb\u5316\u964d\u9636\u5efa\u6a21\u53ef\u80fd\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7b97\u5b50\u63a8\u7406\u4e0eLLM\u80fd\u529b\uff0cOpInf-LLM\u4e3a\u57fa\u4e8eLLM\u7684PDE\u6c42\u89e3\u4e2d\u7684\u53ef\u6cdb\u5316\u964d\u9636\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u6267\u884c\u6210\u529f\u7387\u4e0e\u6570\u503c\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2602.02124", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02124", "abs": "https://arxiv.org/abs/2602.02124", "authors": ["Olga Graf", "Dhrupal Patel", "Peter Gro\u00df", "Charlotte Lempp", "Matthias Hein", "Fabian Heinemann"], "title": "Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies", "comment": null, "summary": "Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\\% of pathological tissue classified as healthy and 0.35\\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.", "AI": {"tldr": "AI\u6846\u67b6\u901a\u8fc7\u7ec4\u7ec7\u5206\u5272\u548c\u5f02\u5e38\u68c0\u6d4b\u8bc6\u522b\u6bd2\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u75c5\u7406\u7ec4\u7ec7\uff0c\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u7f55\u89c1\u75c5\u7406\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe99.8%\u4ee5\u4e0a", "motivation": "\u836f\u7269\u8bf1\u5bfc\u6bd2\u6027\u662f\u4e34\u5e8a\u524d\u5f00\u53d1\u548c\u65e9\u671f\u4e34\u5e8a\u8bd5\u9a8c\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u3002\u7ec4\u7ec7\u75c5\u7406\u5b66\u8bc4\u4f30\u4f9d\u8d56\u4e13\u5bb6\u75c5\u7406\u5b66\u5bb6\uff0c\u5728\u5927\u89c4\u6a21\u7b5b\u67e5\u4e2d\u5f62\u6210\u74f6\u9888\uff0c\u9700\u8981AI\u89e3\u51b3\u65b9\u6848\u6765\u52a0\u901f\u5b89\u5168\u836f\u7269\u5f00\u53d1", "method": "1) \u521b\u5efa\u5065\u5eb7\u7ec4\u7ec7\u548c\u5df2\u77e5\u75c5\u7406\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528DINOv2\u9884\u8bad\u7ec3Vision Transformer\u5e76\u901a\u8fc7LoRA\u5fae\u8c03\u8fdb\u884c\u7ec4\u7ec7\u5206\u5272\uff1b3) \u4f7f\u7528\u9a6c\u6c0f\u8ddd\u79bb\u63d0\u53d6OOD\u68c0\u6d4b\u7279\u5f81\uff1b4) \u63d0\u51fa\u7c7b\u522b\u7279\u5b9a\u9608\u503c\u6765\u8003\u8651\u7ec4\u7ec7\u5b66\u6570\u636e\u7684\u7c7b\u522b\u4f9d\u8d56\u6027\u53d8\u5f02", "result": "\u4ec50.16%\u7684\u75c5\u7406\u7ec4\u7ec7\u88ab\u8bef\u5206\u7c7b\u4e3a\u5065\u5eb7\uff0c0.35%\u7684\u5065\u5eb7\u7ec4\u7ec7\u88ab\u8bef\u5206\u7c7b\u4e3a\u75c5\u7406\u3002\u5728\u5df2\u77e5\u6bd2\u7406\u5b66\u53d1\u73b0\u7684\u9f20\u6807\u809d\u810fWSIs\u4e0a\uff0c\u6846\u67b6\u51c6\u786e\u68c0\u6d4b\u5f02\u5e38\uff0c\u5305\u62ec\u7f55\u89c1\u7684OOD\u5f62\u6001", "conclusion": "AI\u9a71\u52a8\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u6709\u6f5c\u529b\u652f\u6301\u4e34\u5e8a\u524d\u5de5\u4f5c\u6d41\u7a0b\uff0c\u51cf\u5c11\u540e\u671f\u5931\u8d25\uff0c\u63d0\u9ad8\u836f\u7269\u5f00\u53d1\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6bd2\u6027\u7b5b\u67e5\u63d0\u4f9b\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01505", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01505", "abs": "https://arxiv.org/abs/2602.01505", "authors": ["Navdeep Kumar", "Tehila Dahan", "Lior Cohen", "Ananyabrata Barua", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum", "comment": null, "summary": "We establish an optimal sample complexity of $O(\u03b5^{-2})$ for obtaining an $\u03b5$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(\u03b5^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5355\u65f6\u95f4\u5c3a\u5ea6actor-critic\u7b97\u6cd5\uff0c\u5728\u6709\u9650\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u7684\u65e0\u9650\u65f6\u57df\u6298\u6263MDP\u4e2d\uff0c\u5c06\u83b7\u5f97\u03b5\u6700\u4f18\u5168\u5c40\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u4eceO(\u03b5^{-3})\u6539\u8fdb\u5230O(\u03b5^{-2})", "motivation": "\u73b0\u6709actor-critic\u7b97\u6cd5\u5728\u975e\u5e73\u7a33\u5360\u7528\u5ea6\u91cf\u91c7\u6837\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(\u03b5^{-3})\uff0c\u9700\u8981\u6539\u8fdb\u5230\u7406\u8bba\u6700\u4f18\u7684O(\u03b5^{-2})", "method": "\u7ed3\u5408STORM\u65b9\u5dee\u51cf\u5c11\u6280\u672f\u548c\u7f13\u51b2\u533a\u673a\u5236\uff1a1) \u4f7f\u7528STORM\u51cf\u5c11critic\u66f4\u65b0\u7684\u65b9\u5dee\uff1b2) \u7ef4\u62a4\u6700\u8fd1\u6837\u672c\u7684\u5c0f\u7f13\u51b2\u533a\uff0c\u5747\u5300\u91c7\u6837\u7528\u4e8ecritic\u66f4\u65b0", "result": "\u5b9e\u73b0\u4e86O(\u03b5^{-2})\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u8fbe\u5230\u7406\u8bba\u6700\u4f18\uff0c\u4e14\u65b9\u6cd5\u517c\u5bb9\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u53ea\u9700\u5fae\u5c0f\u4fee\u6539", "conclusion": "\u901a\u8fc7STORM\u65b9\u5dee\u51cf\u5c11\u548c\u7f13\u51b2\u533a\u91c7\u6837\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5355\u65f6\u95f4\u5c3a\u5ea6AC\u7b97\u6cd5\u7684\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.02130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02130", "abs": "https://arxiv.org/abs/2602.02130", "authors": ["Lukas Zimmermann", "Michael Rauter", "Maximilian Schmid", "Dietmar Georg", "Barbara Kn\u00e4usl"], "title": "Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework", "comment": null, "summary": "Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684CBCT\u6a21\u62df\u751f\u6210\u51e0\u4f55\u5bf9\u9f50\u8bad\u7ec3\u5bf9\uff0c\u7ed3\u5408\u51e0\u4f55\u5bf9\u9f50\u6307\u6807\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u914d\u51c6\u7684\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u4e34\u5e8a\u504f\u597d", "motivation": "\u4f20\u7edf\u76d1\u7763\u5f0fCBCT\u5408\u6210\u9700\u8981\u914d\u51c6\u8bad\u7ec3\u5bf9\uff0c\u4f46\u5b8c\u7f8e\u914d\u51c6\u65e0\u6cd5\u5b9e\u73b0\uff0c\u914d\u51c6\u504f\u5dee\u4f1a\u4f20\u64ad\u5230\u8bad\u7ec3\u6a21\u578b\u4e2d\u5e76\u6c61\u67d3\u8bc4\u4f30\u6307\u6807\uff0c\u5bfc\u81f4\u66f4\u597d\u7684\u57fa\u51c6\u6027\u80fd\u53ef\u80fd\u53ea\u662f\u66f4\u597d\u5730\u590d\u5236\u4e86\u914d\u51c6\u4f2a\u5f71\u800c\u975e\u89e3\u5256\u4fdd\u771f\u5ea6", "method": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u7684CBCT\u6a21\u62df\u6765\u6784\u5efa\u51e0\u4f55\u5bf9\u9f50\u7684\u8bad\u7ec3\u5bf9\uff0c\u7ed3\u5408\u4f7f\u7528\u51e0\u4f55\u5bf9\u9f50\u6307\u6807\uff08\u5982\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\uff09\u5bf9\u8f93\u5165CBCT\u8fdb\u884c\u8bc4\u4f30\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u6709\u504f\u5dee\u7684ground truth", "result": "\u5728\u4e24\u4e2a\u72ec\u7acb\u76c6\u8154\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u5bf9\u9f50\uff08NMI: 0.31 vs 0.22\uff09\uff0c\u5c3d\u7ba1\u4f20\u7edf\u5f3a\u5ea6\u5206\u6570\u8f83\u4f4e\u3002NMI\u59cb\u7ec8\u9884\u6d4b\u89c2\u5bdf\u8005\u504f\u597d\uff0c\u4e34\u5e8a\u89c2\u5bdf\u8005\u572887%\u7684\u75c5\u4f8b\u4e2d\u504f\u597d\u5408\u6210\u8bad\u7ec3\u8f93\u51fa", "conclusion": "\u51e0\u4f55\u4fdd\u771f\u5ea6\u800c\u975e\u4e0e\u6709\u504f\u5deeground truth\u7684\u5f3a\u5ea6\u4e00\u81f4\u6027\u7b26\u5408\u4e34\u5e8a\u9700\u6c42\uff0c\u57fa\u4e8e\u7269\u7406\u6a21\u62df\u7684\u51e0\u4f55\u5bf9\u9f50\u8bad\u7ec3\u548c\u8bc4\u4f30\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u914d\u51c6\u65b9\u6cd5"}}
{"id": "2602.01510", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.01510", "abs": "https://arxiv.org/abs/2602.01510", "authors": ["Hengzhe Zhang", "Qi Chen", "Bing Xue", "Wolfgang Banzhaf", "Mengjie Zhang"], "title": "Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization", "comment": null, "summary": "Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7f16\u7a0b\u7684\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\u6765\u63a7\u5236\u8fc7\u62df\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u566a\u58f0\u4f30\u8ba1\u548c\u6d41\u5f62\u5165\u4fb5\u68c0\u6d4b\u673a\u5236\uff0c\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u8fc7\u62df\u5408\u95ee\u9898\u9650\u5236\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u9700\u8981\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\uff0c\u63a7\u5236\u8fc7\u62df\u5408\u3002", "method": "1) \u8bc1\u660evicinal risk\u53ef\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u52a0\u6b63\u5219\u5316\u9879\uff08\u6709\u9650\u5dee\u5206\u6216vicinal Jensen gap\uff09\u6765\u754c\u5b9a\uff1b2) \u63d0\u51fa\u8fdb\u5316\u7279\u5f81\u6784\u5efa\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u7ecf\u9a8c\u98ce\u9669\u548cvicinal Jensen gap\uff1b3) \u5f00\u53d1\u566a\u58f0\u4f30\u8ba1\u7b56\u7565\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff1b4) \u63d0\u51fa\u6d41\u5f62\u5165\u4fb5\u68c0\u6d4b\u673a\u5236\u9632\u6b62\u751f\u6210\u4e0d\u73b0\u5b9e\u7684\u589e\u5f3a\u6837\u672c\u3002", "result": "\u572858\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJensen gap\u6700\u5c0f\u5316\u6bd4\u5176\u4ed6\u590d\u6742\u5ea6\u5ea6\u91cf\u66f4\u6709\u6548\u3002\u4e0e15\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6bd4\u8f83\u663e\u793a\uff0c\u91c7\u7528\u6240\u63d0\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\u7684\u9057\u4f20\u7f16\u7a0b\u83b7\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u63d0\u51fa\u7684\u8fc7\u62df\u5408\u63a7\u5236\u7b56\u7565\uff08\u5305\u62ecvicinal Jensen gap\u4f18\u5316\u3001\u566a\u58f0\u4f30\u8ba1\u548c\u6d41\u5f62\u5165\u4fb5\u68c0\u6d4b\uff09\uff0c\u9057\u4f20\u7f16\u7a0b\u7279\u5f81\u6784\u5efa\u7684\u6cdb\u5316\u80fd\u529b\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.02154", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02154", "abs": "https://arxiv.org/abs/2602.02154", "authors": ["Sidi Wu", "Yizi Chen", "Maurizio Gribaudi", "Konrad Schindler", "Cl\u00e9ment Mallet", "Julien Perret", "Lorenz Hurni"], "title": "Deep learning enables urban change profiling through alignment of historical maps", "comment": "40 pages", "summary": "Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u81ea\u52a8\u6846\u67b6\uff0c\u4ece\u5386\u53f2\u5730\u56fe\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u57ce\u5e02\u53d8\u5316\u4fe1\u606f\uff0c\u901a\u8fc7\u5bc6\u96c6\u5bf9\u9f50\u3001\u591a\u65f6\u76f8\u76ee\u6807\u68c0\u6d4b\u548c\u53d8\u5316\u5206\u6790\u6a21\u5757\uff0c\u5b9e\u73b0\u7cfb\u7edf\u5316\u5b9a\u91cf\u5206\u6790\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u63d0\u4f9b\u4e86\u57ce\u5e02\u957f\u671f\u8f6c\u578b\u7684\u72ec\u7279\u8bb0\u5f55\uff0c\u4f46\u7531\u4e8e\u7a7a\u95f4\u9519\u4f4d\u3001\u5236\u56fe\u53d8\u5316\u548c\u6587\u6863\u8d28\u91cf\u9000\u5316\u7b49\u95ee\u9898\uff0c\u4ece\u5386\u53f2\u5730\u56fe\u7cfb\u5217\u4e2d\u63d0\u53d6\u4e00\u81f4\u4e14\u7ec6\u7c92\u5ea6\u7684\u53d8\u5316\u4fe1\u606f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u9650\u5236\u4e86\u5927\u591a\u6570\u5206\u6790\u53ea\u80fd\u91c7\u7528\u5c0f\u89c4\u6a21\u6216\u5b9a\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5168\u81ea\u52a8\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6574\u5408\u5bc6\u96c6\u5730\u56fe\u5bf9\u9f50\u3001\u591a\u65f6\u76f8\u76ee\u6807\u68c0\u6d4b\u548c\u53d8\u5316\u5206\u6790\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5c06\u5386\u53f2\u5730\u56fe\u5206\u6790\u4ece\u4e34\u65f6\u89c6\u89c9\u6bd4\u8f83\u8f6c\u5411\u7cfb\u7edf\u5316\u5b9a\u91cf\u8868\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6240\u63d0\u51fa\u7684\u5bf9\u9f50\u548c\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u80fd\u3002\u5e94\u7528\u4e8e1868-1937\u5e74\u7684\u5df4\u9ece\uff0c\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u57ce\u5e02\u8f6c\u578b\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5f02\u8d28\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u6587\u7814\u7a76\u4e2d\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u9002\u5e94\u4e0d\u540c\u7684\u5236\u56fe\u80cc\u666f\u548c\u4e0b\u6e38\u5e94\u7528\uff0c\u4e3a\u5386\u53f2\u5730\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u4ece\u5b9a\u6027\u5230\u5b9a\u91cf\u3001\u4ece\u4e34\u65f6\u5230\u7cfb\u7edf\u5316\u7684\u8f6c\u53d8\u3002"}}
{"id": "2602.01516", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01516", "abs": "https://arxiv.org/abs/2602.01516", "authors": ["Enzo Nicolas Spotorno", "Matheus Wagner", "Antonio Augusto Medeiros Frohlich"], "title": "White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC", "comment": "5 pages, 1 table, 1 figure, submitted to IEEE VTC 2026 Recent Results Track", "summary": "We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u767d\u76d2\u81ea\u9002\u5e94NMPC\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4e3b\u6743\u8303\u5f0f\u5728\u51bb\u7ed3\u7684\u7279\u5b9a\u5de5\u51b5\u795e\u7ecf\u7f51\u7edc\u4e13\u5bb6\u4e4b\u95f4\u8fdb\u884c\u4ef2\u88c1\uff0c\u89e3\u51b3\u4e86\u8f66\u8f86\u53ef\u5851\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u5de5\u51b5", "motivation": "\u89e3\u51b3\u8f66\u8f86\u63a7\u5236\u4e2d\u7684\u53ef\u5851\u6027\u95ee\u9898\uff0c\u5373\u8f66\u8f86\u9700\u8981\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u81ea\u9002\u5e94\u800c\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u767d\u76d2\u53ef\u89e3\u91ca\u6027\u548c\u8fd0\u884c\u65f6\u53ef\u5ba1\u8ba1\u6027", "method": "\u91c7\u7528\u6a21\u5757\u5316\u4e3b\u6743\u8303\u5f0f\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684\u7279\u5b9a\u5de5\u51b5\u795e\u7ecf\u7f51\u7edc\u4e13\u5bb6\u8fdb\u884c\u4ef2\u88c1\uff0c\u5c06\u96c6\u6210\u52a8\u6001\u7ef4\u62a4\u4e3aCasADi\u4e2d\u5b8c\u5168\u53ef\u904d\u5386\u7684\u7b26\u53f7\u56fe\uff0c\u5b9e\u73b0\u6700\u5927\u8fd0\u884c\u65f6\u53ef\u5ba1\u8ba1\u6027", "result": "\u540c\u6b65\u4eff\u771f\u9a8c\u8bc1\u4e86\u5feb\u901f\u9002\u5e94\u80fd\u529b\uff08\u7ea67.3\u6beb\u79d2\uff09\uff0c\u5728\u590d\u5408\u5de5\u51b5\u53d8\u5316\uff08\u6469\u64e6\u3001\u8d28\u91cf\u3001\u963b\u529b\uff09\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u7406\u60f3\u7684\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u800c\u975e\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5\u5931\u6548", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8f66\u8f86\u53ef\u5851\u6027\u95ee\u9898\uff0c\u4f46\u91cf\u5316\u4e86\u900f\u660e\u5ea6\u6210\u672c\uff1a\u7b26\u53f7\u56fe\u7ef4\u62a4\u4f7f\u6c42\u89e3\u5668\u5ef6\u8fdf\u589e\u52a0\u4e8672-102\u500d\uff0c\u76f8\u5bf9\u4e8e\u7f16\u8bd1\u7684\u53c2\u6570\u5316\u7269\u7406\u6a21\u578b\uff0c\u8fd9\u786e\u7acb\u4e86\u4e25\u683c\u767d\u76d2\u5b9e\u73b0\u7684\u6548\u7387\u4ee3\u4ef7"}}
{"id": "2602.02156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02156", "abs": "https://arxiv.org/abs/2602.02156", "authors": ["Wen-Jie Shu", "Xuerui Qiu", "Rui-Jie Zhu", "Harold Haodong Chen", "Yexin Liu", "Harry Yang"], "title": "LoopViT: Scaling Visual ARC with Looped Transformers", "comment": "8 pages, 11 figures", "summary": "Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes\" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.", "AI": {"tldr": "\u63d0\u51faLoop-ViT\u9012\u5f52\u67b6\u6784\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u7684\u5faa\u73af\u673a\u5236\u89e3\u8026\u63a8\u7406\u6df1\u5ea6\u4e0e\u6a21\u578b\u5bb9\u91cf\uff0c\u5728ARC-AGI\u57fa\u51c6\u4e0a\u4ee5\u66f4\u5c0f\u53c2\u6570\u91cf\u8d85\u8d8a\u5927\u6a21\u578b", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4f7f\u7528\u524d\u9988\u67b6\u6784\uff0c\u8ba1\u7b97\u6df1\u5ea6\u4e25\u683c\u53d7\u9650\u4e8e\u53c2\u6570\u89c4\u6a21\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u5f52\u7eb3\u63a8\u7406\u7684\u8fed\u4ee3\u7b97\u6cd5\u7279\u6027", "method": "\u63d0\u51faLoop-ViT\u9012\u5f52\u67b6\u6784\uff0c\u91c7\u7528\u6743\u91cd\u5171\u4eab\u7684\u6df7\u5408\u5757\uff08\u5c40\u90e8\u5377\u79ef+\u5168\u5c40\u6ce8\u610f\u529b\uff09\u8fed\u4ee3\u5f62\u6210\u6f5c\u5728\u601d\u7ef4\u94fe\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684\u52a8\u6001\u9000\u51fa\u673a\u5236", "result": "\u5728ARC-AGI-1\u57fa\u51c6\u4e0a\uff0c18M\u53c2\u6570\u7684Loop-ViT\u8fbe\u523065.8%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e73M\u53c2\u6570\u7684\u96c6\u6210\u6a21\u578b", "conclusion": "\u81ea\u9002\u5e94\u8fed\u4ee3\u8ba1\u7b97\u6bd4\u5355\u7eaf\u589e\u52a0\u7f51\u7edc\u5bbd\u5ea6\u4e3a\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6269\u5c55\u8f74"}}
{"id": "2602.01519", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01519", "abs": "https://arxiv.org/abs/2602.01519", "authors": ["Shiju Zhao", "Junhao Hu", "Jiaqi Zheng", "Guihai Chen"], "title": "You Need an Encoder for Native Position-Independent Caching", "comment": "12 pages, 10 figures. Welcome back, Encoder", "summary": "The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.", "AI": {"tldr": "\u63d0\u51fa\u539f\u751f\u4f4d\u7f6e\u65e0\u5173\u7f13\u5b58(PIC)\u65b9\u6cd5COMB\uff0c\u901a\u8fc7\u4e3a\u4ec5\u89e3\u7801\u5668LLM\u91cd\u65b0\u5f15\u5165\u7f16\u7801\u5668\u5e76\u663e\u5f0f\u8bad\u7ec3\u652f\u6301PIC\uff0c\u663e\u8457\u964d\u4f4e\u9996\u8bcd\u5ef6\u8fdf\u5e76\u63d0\u5347\u541e\u5410\u91cf", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u524d\u7f00\u7684KV\u7f13\u5b58\u5728\u5904\u7406\u4efb\u610f\u987a\u5e8f\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709PIC\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7684\u51c6\u786e\u7387\u4e0b\u964d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528", "method": "\u4e3a\u6d41\u884c\u7684\u4ec5\u89e3\u7801\u5668LLM\u91cd\u65b0\u5f15\u5165\u7f16\u7801\u5668\u5e76\u663e\u5f0f\u8bad\u7ec3\u4ee5\u652f\u6301PIC\uff0c\u5f00\u53d1COMB\u7f13\u5b58\u7cfb\u7edf\u4e0e\u73b0\u6709\u63a8\u7406\u6846\u67b6\u65e0\u7f1d\u96c6\u6210", "result": "COMB\u5c06\u9996\u8bcd\u5ef6\u8fdf\u964d\u4f4e51-94%\uff0c\u541e\u5410\u91cf\u63d0\u53473\u500d\u4e14\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u51c6\u786e\u7387\uff0c\u5728DeepSeek-V2-Lite-Chat\u4e0a\u7684\u8d28\u91cf\u6539\u8fdb\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027", "conclusion": "\u63d0\u51fa\u7684\u539f\u751fPIC\u65b9\u6cd5COMB\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfKV\u7f13\u5b58\u7684\u4f4d\u7f6e\u4f9d\u8d56\u9650\u5236\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u4f4d\u7f6e\u65e0\u5173\u7f13\u5b58\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.02163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02163", "abs": "https://arxiv.org/abs/2602.02163", "authors": ["Julian Wyatt", "Ronald Clark", "Irina Voiculescu"], "title": "Reg4Pru: Regularisation Through Random Token Routing for Token Pruning", "comment": "11 pages, 7 figures", "summary": "Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.", "AI": {"tldr": "Reg4Pru\u662f\u4e00\u79cd\u8bad\u7ec3\u6b63\u5219\u5316\u6280\u672f\uff0c\u901a\u8fc7\u7f13\u89e3token\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5206\u5272\u4efb\u52a1\u7684\u7cbe\u5ea6\u3002", "motivation": "Transformer\u5728\u89c6\u89c9\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u968ftoken\u6570\u91cf\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u3002\u73b0\u6709token\u526a\u679d\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6df1\u5c42\u8868\u793a\u4e0d\u7a33\u5b9a\uff0c\u964d\u4f4e\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faReg4Pru\u8bad\u7ec3\u6b63\u5219\u5316\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9token\u526a\u679d\u7b56\u7565\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u7f13\u89e3\u526a\u679d\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u7279\u522b\u662f\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u3002", "result": "\u5728FIVES\u8840\u7ba1\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0cReg4Pru\u76f8\u6bd4\u65e0\u8def\u7531\u8bad\u7ec3\u7684\u76f8\u540c\u6a21\u578b\uff0c\u5e73\u5747\u7cbe\u5ea6\u7edd\u5bf9\u63d0\u534746%\uff0c\u540c\u65f6\u5b9e\u73b029%\u7684\u76f8\u5bf9\u52a0\u901f\uff08\u76f8\u6bd4\u975e\u526a\u679d\u57fa\u7ebf\uff09\u3002", "conclusion": "Reg4Pru\u662ftoken\u7f29\u51cf\u7b56\u7565\u4e2d\u6709\u4ef7\u503c\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2602.01522", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01522", "abs": "https://arxiv.org/abs/2602.01522", "authors": ["Haoran Zhao", "Soyeon Caren Han", "Eduard Hovy"], "title": "When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant \"gap\" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGap-Init\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06rank-1 LoRA\u65b9\u5411\u4e0e\u6a21\u6001\u95f4\u9699\u5411\u91cf\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u6781\u4f4e\u79e9PEFT\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u662f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5728\u6781\u4f4e\u79e9\u8bbe\u7f6e\uff08\u7279\u522b\u662frank-1 LoRA\uff09\u4e0b\u8bad\u7ec3\u5f80\u5f80\u4e0d\u7a33\u5b9a\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u4e0d\u4ec5\u6e90\u4e8e\u6709\u9650\u5bb9\u91cf\uff0c\u66f4\u56e0\u4e3a\u4f18\u5316\u5bf9\u66f4\u65b0\u65b9\u5411\u9ad8\u5ea6\u654f\u611f\u3002", "method": "\u63d0\u51faGap-Init\u521d\u59cb\u5316\u65b9\u6cd5\uff1a1\uff09\u5206\u6790\u9884\u8bad\u7ec3\u8868\u793a\uff0c\u8bc6\u522b\u4e3b\u5bfc\u68af\u5ea6\u6d41\u7684\u6a21\u6001\u95f4\u9699\u8f74\uff1b2\uff09\u4f7f\u7528\u5c0f\u578b\u6821\u51c6\u96c6\u4f30\u8ba1\u6a21\u6001\u95f4\u9699\u5411\u91cf\uff1b3\uff09\u5c06rank-1 LoRA\u65b9\u5411\u4e0e\u8be5\u5411\u91cf\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u521d\u59cbLoRA\u66f4\u65b0\u4e3a\u96f6\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cGap-Init\u80fd\u7a33\u5b9arank-1\u8bad\u7ec3\uff0c\u6027\u80fd\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u5f3a\u5927\u7684rank-8\u57fa\u7ebf\u3002\u7ed3\u679c\u8868\u660e\u5728\u6781\u4f4e\u79e9\u6781\u9650\u4e0b\uff0c\u521d\u59cb\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u53ef\u4e0e\u79e9\u672c\u8eab\u76f8\u5f53\u3002", "conclusion": "\u6781\u4f4e\u79e9LoRA\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u6a21\u6001\u95f4\u9699\u5bfc\u81f4\u7684\u68af\u5ea6\u6d41\u95ee\u9898\uff0c\u800c\u975e\u5355\u7eaf\u5bb9\u91cf\u9650\u5236\u3002\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u521d\u59cb\u5316\u5bf9\u9f50\u6a21\u6001\u95f4\u9699\u65b9\u5411\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e3a\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.02171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02171", "abs": "https://arxiv.org/abs/2602.02171", "authors": ["Lu Cao", "Xiquan He", "Junying Zeng", "Chaoyun Mai", "Min Luo"], "title": "Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks", "comment": null, "summary": "The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.", "AI": {"tldr": "\u63d0\u51faTSGAN\u4e24\u9636\u6bb5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u901a\u8fc7\u89e3\u8026\u80ba\u7ed3\u8282\u5f62\u6001\u7ed3\u6784\u548c\u7eb9\u7406\u7279\u5f81\u6765\u589e\u5f3a\u5408\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u7a7a\u95f4\u53ef\u63a7\u6027\uff0c\u63d0\u5347\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u80ba\u7ed3\u8282CT\u6570\u636e\u96c6\u6837\u672c\u91cf\u6709\u9650\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u3001\u53ef\u63a7\u6027\u5dee\u3001\u7eb9\u7406\u7279\u5f81\u5355\u8c03\u548c\u89e3\u5256\u7ed3\u6784\u626d\u66f2\u7b49\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528StyleGAN\u751f\u6210\u8bed\u4e49\u5206\u5272\u63a9\u7801\u56fe\u50cf\uff0c\u7f16\u7801\u80ba\u7ed3\u8282\u548c\u7ec4\u7ec7\u80cc\u666f\u4ee5\u63a7\u5236\u89e3\u5256\u7ed3\u6784\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528DL-Pix2Pix\u6a21\u578b\u5c06\u63a9\u7801\u56fe\u8f6c\u6362\u4e3aCT\u56fe\u50cf\uff0c\u91c7\u7528\u5c40\u90e8\u91cd\u8981\u6027\u6ce8\u610f\u529b\u6355\u83b7\u5c40\u90e8\u7279\u5f81\uff0c\u540c\u65f6\u5229\u7528\u52a8\u6001\u6743\u91cd\u591a\u5934\u7a97\u53e3\u6ce8\u610f\u529b\u589e\u5f3a\u80ba\u7ed3\u8282\u7eb9\u7406\u548c\u80cc\u666f\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728LUNA16\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u539f\u59cb\u6570\u636e\u96c6\uff0c\u51c6\u786e\u7387\u63d0\u53474.6%\uff0cmAP\u63d0\u53474%\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eTSGAN\u80fd\u591f\u589e\u5f3a\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u548c\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "TSGAN\u901a\u8fc7\u89e3\u8026\u5f62\u6001\u7ed3\u6784\u548c\u7eb9\u7406\u7279\u5f81\u7684\u4e24\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5408\u6210\u80ba\u7ed3\u8282CT\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u7a7a\u95f4\u53ef\u63a7\u6027\uff0c\u4ece\u800c\u6539\u5584\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.02175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02175", "abs": "https://arxiv.org/abs/2602.02175", "authors": ["Xinquan Yu", "Wei Lu", "Xiangyang Luo"], "title": "CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization", "comment": null, "summary": "To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.", "AI": {"tldr": "\u63d0\u51faCIEC\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u7c97\u7c92\u5ea6\u56fe\u50cf/\u53e5\u5b50\u7ea7\u6807\u6ce8\u5b9e\u73b0\u591a\u6a21\u6001\u5f31\u76d1\u7763\u7be1\u6539\u5b9a\u4f4d\uff0c\u5305\u542b\u56fe\u50cf\u548c\u6587\u672c\u4e24\u4e2a\u5206\u652f\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff08\u5982\u8865\u4e01/\u6807\u8bb0\u7ea7\uff09\uff0c\u9700\u8981\u5f00\u53d1\u4ec5\u4f7f\u7528\u7c97\u7c92\u5ea6\u56fe\u50cf/\u53e5\u5b50\u7ea7\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCIEC\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a1\uff09\u56fe\u50cf\u5f31\u76d1\u7763\u5b9a\u4f4d\u5206\u652f\uff1a\u4f7f\u7528TRPS\u6a21\u5757\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4f2a\u9020\u7ebf\u7d22\uff0c\u7ed3\u5408\u80cc\u666f\u6291\u5236\u548c\u7a7a\u95f4\u5bf9\u6bd4\u7ea6\u675f\uff1b2\uff09\u6587\u672c\u5f31\u76d1\u7763\u5b9a\u4f4d\u5206\u652f\uff1a\u4f7f\u7528VCTG\u6a21\u5757\u5173\u6ce8\u5185\u5bb9\u8bcd\uff0c\u5229\u7528\u76f8\u5bf9\u89c6\u89c9\u504f\u5dee\u8f85\u52a9\u6807\u8bb0\u5b9a\u4f4d\uff0c\u7ed3\u5408\u975e\u5bf9\u79f0\u7a00\u758f\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCIEC\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "CIEC\u6846\u67b6\u4ec5\u4f7f\u7528\u7c97\u7c92\u5ea6\u6807\u6ce8\u5c31\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u7be1\u6539\u5b9a\u4f4d\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.01526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01526", "abs": "https://arxiv.org/abs/2602.01526", "authors": ["Jianqiao Zheng", "Hemanth Saratchandran", "Simon Lucey"], "title": "The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy", "comment": null, "summary": "Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u95f4NTK\u5206\u89e3\u53d1\u73b0\"\u5165\u53e3\u79e9\u5d29\u6e83\"\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u79e9\u6269\u5c55\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u4f7f\u6807\u51c6MLP\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u5728\u6709\u9650\u8bad\u7ec3\u9884\u7b97\u4e0b\u96be\u4ee5\u6062\u590d\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u867d\u7136\u7ecf\u9a8c\u6280\u672f\u5982\u4f4d\u7f6e\u7f16\u7801\u3001\u6b63\u5f26\u6fc0\u6d3b\u548c\u6279\u5f52\u4e00\u5316\u53ef\u4ee5\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u7684\u7406\u8bba\u89e3\u91ca\u5927\u591a\u662f\u540e\u9a8c\u7684\uff0c\u53ea\u5728\u4fee\u6539\u540e\u5173\u6ce8\u5168\u5c40NTK\u8c31\u3002\u9700\u8981\u4ece\u7ed3\u6784\u89d2\u5ea6\u7406\u89e3INRs\u7684\u8868\u8fbe\u74f6\u9888\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u95f4\u5206\u89e3NTK\uff0c\u6570\u5b66\u4e0a\u8bc6\u522b\"\u5165\u53e3\u79e9\u5d29\u6e83\"\u73b0\u8c61\u3002\u57fa\u4e8e\u6b64\u8bca\u65ad\uff0c\u63a8\u5bfc\u51fa\u79e9\u6269\u5c55\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u8868\u793a\u79e9\u968f\u5c42\u5bbd\u6269\u5c55\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u4f4d\u7f6e\u7f16\u7801\u3001\u6b63\u5f26\u6fc0\u6d3b\u548c\u6279\u5f52\u4e00\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u79e9\u6062\u590d\u89c6\u89d2\u3002\u79e9\u6269\u5c55\u521d\u59cb\u5316\u4f7f\u6807\u51c6MLP\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff0c\u8bc1\u660e\u4f18\u5316\u521d\u59cb\u79e9\u4f20\u64ad\u7ed3\u6784\u662f\u63d0\u5347INRs\u6027\u80fd\u7684\u5173\u952e\u3002", "conclusion": "INRs\u7684\u5173\u952e\u5728\u4e8e\u521d\u59cb\u79e9\u4f20\u64ad\u7684\u7ed3\u6784\u4f18\u5316\uff0c\u4ee5\u6709\u6548\u586b\u5145\u6f5c\u5728\u7a7a\u95f4\u3002\u79e9\u6269\u5c55\u521d\u59cb\u5316\u4f5c\u4e3a\u4e00\u79cd\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u89e3\u51b3\u5165\u53e3\u79e9\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2602.01553", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01553", "abs": "https://arxiv.org/abs/2602.01553", "authors": ["Quang Truong", "Yu Song", "Donald Loveland", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "title": "Plain Transformers are Surprisingly Powerful Link Predictors", "comment": null, "summary": "Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.", "AI": {"tldr": "PENCIL\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u94fe\u63a5\u9884\u6d4b\u7684\u7f16\u7801\u5668\u4e13\u7528Transformer\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u91c7\u6837\u5b50\u56fe\uff0c\u65e0\u9700\u624b\u5de5\u5148\u9a8c\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u6807\u51c6Transformer\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u8d85\u8d8a\u4e86\u542f\u53d1\u5f0fGNN\u548cID\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1aGNN\u4f9d\u8d56\u663e\u5f0f\u7ed3\u6784\u542f\u53d1\u5f0f\u6216\u5185\u5b58\u5bc6\u96c6\u578b\u8282\u70b9\u5d4c\u5165\uff0c\u96be\u4ee5\u6cdb\u5316\u548c\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56fe\uff1b\u56feTransformer\u56e0\u590d\u6742\u7ed3\u6784\u7f16\u7801\u800c\u5f00\u9500\u5de8\u5927\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6355\u83b7\u4e30\u5bcc\u62d3\u6251\u4f9d\u8d56\u53c8\u80fd\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPENCIL\u6a21\u578b\uff0c\u91c7\u7528\u7f16\u7801\u5668\u4e13\u7528\u666e\u901aTransformer\u67b6\u6784\uff0c\u7528\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u91c7\u6837\u7684\u5c40\u90e8\u5b50\u56fe\uff0c\u66ff\u4ee3\u624b\u5de5\u8bbe\u8ba1\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u6807\u51c6Transformer\u7684\u53ef\u6269\u5c55\u6027\u548c\u786c\u4ef6\u6548\u7387\uff0c\u80fd\u591f\u9690\u5f0f\u6cdb\u5316\u5e7f\u6cdb\u7684\u542f\u53d1\u5f0f\u548c\u57fa\u4e8e\u5b50\u56fe\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cPENCIL\u6bd4GNN\u63d0\u53d6\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u4fe1\u53f7\uff0c\u6027\u80fd\u4f18\u4e8e\u542f\u53d1\u5f0fGNN\uff0c\u53c2\u6570\u6548\u7387\u8fdc\u9ad8\u4e8e\u57fa\u4e8eID\u5d4c\u5165\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u8282\u70b9\u7279\u5f81\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "PENCIL\u6311\u6218\u4e86\u5f53\u524d\u4f9d\u8d56\u590d\u6742\u5de5\u7a0b\u6280\u672f\u7684\u8303\u5f0f\uff0c\u8bc1\u660e\u7b80\u5355\u7684\u8bbe\u8ba1\u9009\u62e9\u53ef\u80fd\u8db3\u4ee5\u5b9e\u73b0\u76f8\u540c\u80fd\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u94fe\u63a5\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02186", "abs": "https://arxiv.org/abs/2602.02186", "authors": ["Ziqiao Weng", "Jiancheng Yang", "Kangxian Xie", "Bo Zhou", "Weidong Cai"], "title": "Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision", "comment": "18 pages, 7 figures", "summary": "Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \\textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.", "AI": {"tldr": "TopoField\uff1a\u4e00\u79cd\u7528\u4e8e\u4fee\u590d\u4e0d\u5b8c\u6574\u80ba\u6811\u62d3\u6251\u7ed3\u6784\u7684\u9690\u5f0f\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u70b9\u4e91\u8868\u793a\u548c\u8fde\u7eed\u9690\u5f0f\u573a\u5b9e\u73b0\u62d3\u6251\u4fee\u590d\u3001\u89e3\u5256\u6807\u8bb0\u548c\u80ba\u6bb5\u91cd\u5efa\u7684\u591a\u4efb\u52a1\u7edf\u4e00\u63a8\u7406\u3002", "motivation": "CT\u56fe\u50cf\u63d0\u53d6\u7684\u80ba\u6811\u5e38\u5b58\u5728\u62d3\u6251\u4e0d\u5b8c\u6574\u95ee\u9898\uff08\u5982\u7f3a\u5931\u6216\u65ad\u5f00\u5206\u652f\uff09\uff0c\u4e25\u91cd\u5f71\u54cd\u4e0b\u6e38\u89e3\u5256\u5206\u6790\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u4f53\u7d20\u5904\u7406\u6216\u663e\u5f0f\u56fe\u63a8\u7406\uff0c\u6548\u7387\u6709\u9650\u4e14\u5bf9\u7ed3\u6784\u635f\u574f\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u8868\u9762\u548c\u9aa8\u67b6\u70b9\u4e91\u8868\u793a\u80ba\u89e3\u5256\u7ed3\u6784\uff0c\u5b66\u4e60\u8fde\u7eed\u9690\u5f0f\u573a\uff0c\u901a\u8fc7\u5728\u5df2\u4e0d\u5b8c\u6574\u7684\u6811\u4e0a\u5f15\u5165\u5408\u6210\u7ed3\u6784\u7834\u574f\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u5b8c\u6574\u6216\u663e\u5f0f\u65ad\u5f00\u6807\u6ce8\u3002\u57fa\u4e8e\u4fee\u590d\u7684\u9690\u5f0f\u8868\u793a\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9690\u5f0f\u51fd\u6570\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u8054\u5408\u63a8\u65ad\u89e3\u5256\u6807\u8bb0\u548c\u80ba\u6bb5\u91cd\u5efa\u3002", "result": "\u5728Lung3D+\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTopoField\u80fd\u6301\u7eed\u6539\u5584\u62d3\u6251\u5b8c\u6574\u6027\uff0c\u5728\u6311\u6218\u6027\u4e0d\u5b8c\u6574\u573a\u666f\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u89e3\u5256\u6807\u8bb0\u548c\u80ba\u6bb5\u91cd\u5efa\u3002\u7531\u4e8e\u5176\u9690\u5f0f\u516c\u5f0f\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u6bcf\u75c5\u4f8b\u4ec5\u97001\u79d2\u591a\u5b8c\u6210\u6240\u6709\u4efb\u52a1\u3002", "conclusion": "TopoField\u5c06\u62d3\u6251\u4fee\u590d\u4f5c\u4e3a\u9996\u8981\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u80ba\u6811\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u548c\u65f6\u95f4\u654f\u611f\u7684\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2602.01554", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01554", "abs": "https://arxiv.org/abs/2602.01554", "authors": ["Lv Tang", "Tianyi Zheng", "Bo Li", "Xingyu Li"], "title": "InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs", "comment": null, "summary": "Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.", "AI": {"tldr": "\u63d0\u51faInfoTok\uff1a\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684\u4fe1\u606f\u6b63\u5219\u5316\u89c6\u89c9\u5206\u8bcd\u673a\u5236\uff0c\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5171\u4eab\u5206\u8bcd\u8bbe\u8ba1\u4e2d\u5e73\u8861\u538b\u7f29\u4e0e\u4efb\u52a1\u76f8\u5173\u6027", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5171\u4eab\u5206\u8bcd\u8bbe\u8ba1\u5927\u591a\u662f\u67b6\u6784\u9a71\u52a8\u7684\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6807\u51c6\u6765\u786e\u5b9a\u5206\u8bcd\u5e94\u8be5\u4fdd\u7559\u54ea\u4e9b\u4fe1\u606f\u6765\u540c\u65f6\u652f\u6301\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1", "method": "\u5f15\u5165\u5bb9\u91cf\u7ea6\u675f\u89c6\u89d2\uff0c\u5c06\u89c6\u89c9\u5206\u8bcd\u5668\u89c6\u4e3a\u8ba1\u7b97\u53d7\u9650\u7684\u5b66\u4e60\u5668\uff0c\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684InfoTok\u673a\u5236\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u6b63\u5219\u5316\u63a7\u5236\u4ece\u56fe\u50cf\u5230\u5171\u4eab\u5206\u8bcd\u518d\u5230\u591a\u6a21\u6001\u8f93\u51fa\u7684\u4fe1\u606f\u6d41", "result": "\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u7edf\u4e00MLLM\u4e0a\u96c6\u6210InfoTok\uff08\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb", "conclusion": "\u4fe1\u606f\u6b63\u5219\u5316\u5206\u8bcd\u4e3a\u7edf\u4e00MLLM\u4e2d\u5b66\u4e60\u5171\u4eab\u5206\u8bcd\u7a7a\u95f4\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u8bc1\u660e\u5bb9\u91cf\u7ea6\u675f\u89c6\u89d2\u548c\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684\u6709\u6548\u6027"}}
{"id": "2602.02193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02193", "abs": "https://arxiv.org/abs/2602.02193", "authors": ["Chen Min", "Enze Jiang", "Jishen Peng", "Zheng Ma"], "title": "SSI-DM: Singularity Skipping Inversion of Diffusion Models", "comment": null, "summary": "Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.", "AI": {"tldr": "\u63d0\u51faSSI-DM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8df3\u8fc7\u6570\u5b66\u5947\u70b9\u533a\u57df\u89e3\u51b3\u6269\u6563\u6a21\u578b\u53cd\u6f14\u4e2d\u7684\u975e\u9ad8\u65af\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u56fe\u50cf\u7f16\u8f91\u6548\u679c", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u53cd\u6f14\u65b9\u6cd5\u5728\u65e9\u671f\u53bb\u566a\u6b65\u9aa4\u4e2d\u5b58\u5728\u4e0d\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u975e\u9ad8\u65af\u566a\u58f0\u7f16\u8f91\u6027\u5dee\uff0c\u6839\u6e90\u5728\u4e8e\u6570\u5b66\u5947\u70b9\u4f7f\u5f97\u53cd\u6f14\u95ee\u9898\u672c\u8d28\u4e0d\u9002\u5b9a", "method": "SSI-DM\u65b9\u6cd5\u901a\u8fc7\u5728\u6807\u51c6\u53cd\u6f14\u524d\u6dfb\u52a0\u5c11\u91cf\u566a\u58f0\u6765\u7ed5\u8fc7\u5947\u70b9\u533a\u57df\uff0c\u4ea7\u751f\u5177\u6709\u81ea\u7136\u9ad8\u65af\u7279\u6027\u7684\u53cd\u6f14\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u4fdd\u771f\u5ea6", "result": "\u5728\u516c\u5171\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u548c\u63d2\u503c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u53cd\u6f14\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "conclusion": "SSI-DM\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6280\u672f\uff0c\u517c\u5bb9\u901a\u7528\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8df3\u8fc7\u5947\u70b9\u533a\u57df\u89e3\u51b3\u4e86\u53cd\u6f14\u7684\u6839\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u6548\u679c"}}
{"id": "2602.01558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01558", "abs": "https://arxiv.org/abs/2602.01558", "authors": ["Yiming Ma", "Lixu Wang", "Lionel Z. Wang", "Hongkun Yang", "Haoming Sun", "Xin Xu", "Jiaqi Wu", "Bin Chen", "Wei Dong"], "title": "How Implicit Bias Accumulates and Propagates in LLM Long-term Memory", "comment": "Under review, and the first two authors contribute equally", "summary": "Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.", "AI": {"tldr": "\u7814\u7a76LLMs\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u4e2d\u7684\u9690\u6027\u504f\u89c1\u79ef\u7d2f\u4e0e\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u51fa\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0\u65b9\u6cd5\u8fdb\u884c\u7f13\u89e3", "motivation": "LLMs\u7684\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u867d\u7136\u80fd\u7ef4\u6301\u4ea4\u4e92\u8fde\u7eed\u6027\u548c\u4e2a\u6027\u5316\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u516c\u5e73\u6027\u98ce\u9669\uff0c\u7279\u522b\u662f\u9690\u6027\u504f\u89c1\u5728\u957f\u671f\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u79ef\u7d2f\u548c\u4f20\u64ad\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76", "method": "1) \u63d0\u51fa\u51b3\u7b56\u9690\u6027\u504f\u89c1\u57fa\u51c6(DIB)\uff0c\u5305\u542b3,776\u4e2a\u8de89\u4e2a\u793e\u4f1a\u9886\u57df\u7684\u51b3\u7b56\u573a\u666f\uff1b2) \u4f7f\u7528\u957f\u671f\u6a21\u62df\u6846\u67b6\u8bc4\u4f306\u4e2aSOTA LLMs\u4e0e3\u79cd\u8bb0\u5fc6\u67b6\u6784\uff1b3) \u63d0\u51fa\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0(DMT)\u65b9\u6cd5\uff0c\u5728\u8bb0\u5fc6\u5199\u5165\u65f6\u5f3a\u5236\u6267\u884c\u516c\u5e73\u7ea6\u675f", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) LLMs\u7684\u9690\u6027\u504f\u89c1\u4e0d\u4f1a\u4fdd\u6301\u9759\u6001\uff0c\u800c\u662f\u968f\u65f6\u95f4\u52a0\u5267\u5e76\u5728\u65e0\u5173\u9886\u57df\u95f4\u4f20\u64ad\uff1b2) \u9759\u6001\u7cfb\u7edf\u7ea7\u63d0\u793a\u7684\u7f13\u89e3\u6548\u679c\u6709\u9650\u4e14\u77ed\u6682\uff1b3) DMT\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u504f\u89c1\u79ef\u7d2f\uff0c\u6709\u6548\u6291\u5236\u8de8\u9886\u57df\u504f\u89c1\u4f20\u64ad", "conclusion": "LLMs\u957f\u671f\u8bb0\u5fc6\u4e2d\u7684\u9690\u6027\u504f\u89c1\u662f\u52a8\u6001\u79ef\u7d2f\u548c\u4f20\u64ad\u7684\u7cfb\u7edf\u6027\u95ee\u9898\uff0c\u9700\u8981\u52a8\u6001\u5e72\u9884\u673a\u5236\u5982DMT\u6765\u6709\u6548\u7f13\u89e3\uff0c\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u7684\u957f\u671f\u8bb0\u5fc6LLMs\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u548c\u65b9\u6cd5"}}
{"id": "2602.02212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02212", "abs": "https://arxiv.org/abs/2602.02212", "authors": ["Zheyuan Zhou", "Liang Du", "Zixun Sun", "Xiaoyu Zhou", "Ruimin Ye", "Qihao Chen", "Yinda Chen", "Lemiao Qiu"], "title": "MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models", "comment": null, "summary": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.", "AI": {"tldr": "MAIN-VLA\u901a\u8fc7\u5efa\u6a21\u610f\u56fe\u548c\u73af\u5883\u62bd\u8c61\uff0c\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\u7684\u51b3\u7b56\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709VLA\u65b9\u6cd5\u5728\u9ad8\u5ea6\u590d\u6742\u52a8\u6001\u73af\u5883\uff08\u59823D\u5f00\u653e\u4e16\u754c\u548c\u5927\u89c4\u6a21PvP\u6e38\u620f\uff09\u4e2d\uff0c\u96be\u4ee5\u4ece\u5197\u4f59\u4f20\u611f\u5668\u6d41\u4e2d\u63d0\u53d6\u52a8\u4f5c\u5173\u952e\u4fe1\u53f7\uff0c\u5bfc\u81f4\u51b3\u7b56\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMAIN-VLA\u6846\u67b6\uff1a1) \u610f\u56fe\u62bd\u8c61(IA)\u5c06\u5197\u957f\u8bed\u8a00\u6307\u4ee4\u538b\u7f29\u4e3a\u663e\u5f0f\u8bed\u4e49\u539f\u8bed\uff1b2) \u73af\u5883\u8bed\u4e49\u62bd\u8c61(ESA)\u5c06\u89c6\u89c9\u6d41\u6295\u5f71\u4e3a\u7ed3\u6784\u5316\u62d3\u6251\u53ef\u4f9b\u6027\u8868\u793a\uff1b3) \u5bf9\u9f50\u4e24\u79cd\u62bd\u8c61\u6a21\u6001\u4ea7\u751f\u6ce8\u610f\u529b\u96c6\u4e2d\u6548\u5e94\uff0c\u5b9e\u73b0\u65e0\u53c2\u6570token\u526a\u679d\u7b56\u7565\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u548c\u5927\u89c4\u6a21PvP\u73af\u5883\uff08Game for Peace\u548cValorant\uff09\u4e2d\uff0cMAIN-VLA\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u5728\u51b3\u7b56\u8d28\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MAIN-VLA\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u610f\u56fe\u548c\u73af\u5883\u62bd\u8c61\uff0c\u5728\u6df1\u5c42\u8bed\u4e49\u5bf9\u9f50\u800c\u975e\u8868\u9762\u6a21\u5f0f\u5339\u914d\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u51b3\u7b56\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684VLA\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01564", "categories": ["cs.LG", "math.AP", "math.OC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.01564", "abs": "https://arxiv.org/abs/2602.01564", "authors": ["Geuntaek Seo", "Minseop Shin", "Pierre Monmarch\u00e9", "Beomjun Choi"], "title": "Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space", "comment": null, "summary": "We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.", "AI": {"tldr": "\u8bc1\u660e\u4e86\u5747\u503c\u573aLangevin\u4e0b\u964d-\u4e0a\u5347\u52a8\u529b\u5b66\u5728\u71b5\u6b63\u5219\u5316\u4e8c\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\uff0c\u56de\u7b54\u4e86Wang\u548cChizat\u7684\u5f00\u653e\u95ee\u9898", "motivation": "\u5c3d\u7ba1\u5747\u503c\u573a\u76ee\u6807\u51fd\u6570\u5b58\u5728\u552f\u4e00\u7684\u6df7\u5408\u7eb3\u4ec0\u5747\u8861\uff0c\u4f46\u539f\u59cbMFL-DA\u52a8\u529b\u5b66\u5728\u4e00\u822c\u975e\u51f8-\u975e\u51f9\u6536\u76ca\u4e0b\u7684\u957f\u671f\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u56de\u7b54Wang\u548cChizat\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898", "method": "\u901a\u8fc7\u7ebf\u6027\u5316\u7b97\u5b50\u7684\u8c31\u5206\u6790\u5efa\u7acb\u71b5\u5728\u5747\u8861\u70b9\u9644\u8fd1\u7684\u5f3a\u5236\u6027\u4f30\u8ba1\uff0c\u63ed\u793a\u5c40\u90e8\u4f4d\u79fb\u51f8-\u51f9\u7ed3\u6784\uff0c\u8bc1\u660e\u52a8\u529b\u5b66\u6536\u7f29\u6027", "result": "\u8bc1\u660e\u4e86\u5747\u8861\u70b9\u662f\u5c40\u90e8\u6307\u6570\u7a33\u5b9a\u7684\uff1a\u5982\u679c\u521d\u59cb\u503c\u5728Wasserstein\u5ea6\u91cf\u4e0b\u8db3\u591f\u63a5\u8fd1\uff0c\u52a8\u529b\u5b66\u5c06\u4ee5\u6307\u6570\u901f\u7387\u6536\u655b\u5230\u5747\u8861\u70b9", "conclusion": "\u89e3\u51b3\u4e86Wang\u548cChizat\u63d0\u51fa\u7684\u5c40\u90e8\u7a33\u5b9a\u6027\u548c\u5b9a\u91cf\u901f\u7387\u95ee\u9898\uff0c\u4f46\u5168\u5c40\u6536\u655b\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218"}}
{"id": "2602.02214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02214", "abs": "https://arxiv.org/abs/2602.02214", "authors": ["Hongzhou Zhu", "Min Zhao", "Guande He", "Hang Su", "Chongxuan Li", "Jun Zhu"], "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation", "comment": "Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "summary": "To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: \\href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}", "AI": {"tldr": "\u63d0\u51faCausal Forcing\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0c\u89e3\u51b3\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u5230\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u7684\u67b6\u6784\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u7684\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u5b58\u5728\u67b6\u6784\u5dee\u8ddd\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7ODE\u84b8\u998f\u521d\u59cb\u5316\u81ea\u56de\u5f52\u5b66\u751f\u6a21\u578b\uff0c\u4f46\u8fd9\u9700\u8981\u5e27\u7ea7\u5355\u5c04\u6761\u4ef6\uff0c\u800c\u53cc\u5411\u6559\u5e08\u84b8\u998f\u8fdd\u53cd\u8be5\u6761\u4ef6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faCausal Forcing\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0c\u4ece\u800c\u5f25\u5408\u67b6\u6784\u5dee\u8ddd\uff0c\u786e\u4fdd\u6ee1\u8db3\u5e27\u7ea7\u5355\u5c04\u6761\u4ef6\uff0c\u80fd\u591f\u6062\u590d\u6559\u5e08\u7684\u6d41\u6620\u5c04\u3002", "result": "\u5728\u5404\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u76f8\u6bd4SOTA Self Forcing\u5728Dynamic Degree\u4e0a\u63d0\u534719.3%\uff0cVisionReward\u63d0\u53478.7%\uff0cInstruction Following\u63d0\u534716.7%\u3002", "conclusion": "Causal Forcing\u901a\u8fc7\u4f7f\u7528\u81ea\u56de\u5f52\u6559\u5e08\u8fdb\u884cODE\u521d\u59cb\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u84b8\u998f\u5230\u81ea\u56de\u5f52\u6a21\u578b\u65f6\u7684\u7406\u8bba\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01576", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01576", "abs": "https://arxiv.org/abs/2602.01576", "authors": ["Woosung Koh", "Sungjun Han", "Segyu Lee", "Se-Young Yun", "Jamin Shin"], "title": "Generative Visual Code Mobile World Models", "comment": "Pre-print (technical report)", "summary": "Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.", "AI": {"tldr": "gWorld\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u53ef\u89c6\u5316\u79fb\u52a8GUI\u4e16\u754c\u6a21\u578b\u65b0\u8303\u5f0f\uff0c\u4f7f\u7528\u5355\u4e00VLM\u9884\u6d4b\u53ef\u6267\u884c\u7f51\u9875\u4ee3\u7801\u800c\u975e\u76f4\u63a5\u751f\u6210\u50cf\u7d20\uff0c\u5728\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u5927\u5c0f\u4e4b\u95f4\u5efa\u7acb\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8GUI\u4e16\u754c\u6a21\u578b\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u57fa\u4e8e\u6587\u672c\u7684\u6a21\u578b\u727a\u7272\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u800c\u89c6\u89c9\u6a21\u578b\u65e0\u6cd5\u7cbe\u786e\u6e32\u67d3\u6587\u672c\uff0c\u4f9d\u8d56\u7f13\u6162\u590d\u6742\u7684\u591a\u6a21\u578b\u6d41\u6c34\u7ebf\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u65b0\u8303\u5f0f\uff1a\u4f7f\u7528\u5355\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2aGUI\u72b6\u6001\u4e3a\u53ef\u6267\u884c\u7684\u7f51\u9875\u4ee3\u7801\uff08\u800c\u975e\u76f4\u63a5\u751f\u6210\u50cf\u7d20\uff09\uff0c\u5e76\u5f00\u53d1\u4e86gWorld\u6570\u636e\u751f\u6210\u6846\u67b6\u81ea\u52a8\u5408\u6210\u57fa\u4e8e\u4ee3\u7801\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "gWorld\uff088B\u548c32B\u53c2\u6570\uff09\u57284\u4e2a\u5206\u5e03\u5185\u548c2\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u5927\u5c0f\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4f18\u4e8e8\u4e2a\u524d\u6cbf\u5f00\u6e90\u6a21\u578b\uff08\u6700\u5927\u6a21\u578b\u592750.25\u500d\uff09\u3002\u5206\u6790\u663e\u793a\u6570\u636e\u6269\u5c55\u3001\u6d41\u6c34\u7ebf\u7ec4\u4ef6\u548c\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u5747\u5e26\u6765\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u8303\u5f0f\u6210\u529f\u7ed3\u5408\u4e86\u6587\u672c\u7cbe\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0cgWorld\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u66f4\u5f3a\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u63d0\u5347\u4e0b\u6e38\u79fb\u52a8GUI\u7b56\u7565\u6027\u80fd\u3002"}}
{"id": "2602.02220", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02220", "abs": "https://arxiv.org/abs/2602.02220", "authors": ["Bo Miao", "Weijia Liu", "Jun Luo", "Lachlan Shinnick", "Jian Liu", "Thomas Hamilton-Smith", "Yuhe Yang", "Zijie Wu", "Vanja Videnovic", "Feras Dayoub", "Anton van den Hengel"], "title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation", "comment": null, "summary": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap", "AI": {"tldr": "HieraNav\u662f\u4e00\u4e2a\u591a\u7c92\u5ea6\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0cLangMap\u662f\u57fa\u4e8e\u771f\u5b9e3D\u5ba4\u5185\u626b\u63cf\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u56db\u4e2a\u8bed\u4e49\u7ea7\u522b\u7684\u5bfc\u822a\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u9a71\u52a8\u7684\u5177\u8eab\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u7269\u4f53\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb\u5bf9\u4e8e\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u6709\u610f\u4e49\u901a\u4fe1\u4ee5\u53ca\u5b9e\u9645\u6709\u7528\u7684\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5bfc\u822a\u57fa\u51c6\u5728\u8bed\u4e49\u7c92\u5ea6\u3001\u8bed\u8a00\u591a\u6837\u6027\u548c\u4efb\u52a1\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86HieraNav\u4efb\u52a1\u6846\u67b6\uff0c\u5305\u542b\u573a\u666f\u3001\u623f\u95f4\u3001\u533a\u57df\u548c\u5b9e\u4f8b\u56db\u4e2a\u8bed\u4e49\u7ea7\u522b\u3002\u6784\u5efa\u4e86LangMap\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e3D\u5ba4\u5185\u626b\u63cf\uff0c\u5305\u542b\u4eba\u7c7b\u9a8c\u8bc1\u7684\u6807\u6ce8\uff1a\u533a\u57df\u6807\u7b7e\u3001\u533a\u5206\u6027\u533a\u57df\u63cf\u8ff0\u3001\u8986\u76d6414\u4e2a\u5bf9\u8c61\u7c7b\u522b\u7684\u533a\u5206\u6027\u5b9e\u4f8b\u63cf\u8ff0\uff0c\u4ee5\u53ca\u8d85\u8fc718K\u4e2a\u5bfc\u822a\u4efb\u52a1\u3002\u6bcf\u4e2a\u76ee\u6807\u90fd\u6709\u7b80\u6d01\u548c\u8be6\u7ec6\u4e24\u79cd\u63cf\u8ff0\u98ce\u683c\u3002", "result": "LangMap\u5728\u6807\u6ce8\u8d28\u91cf\u4e0a\u4f18\u4e8eGOAT-Bench\uff0c\u533a\u5206\u6027\u51c6\u786e\u7387\u63d0\u9ad823.8%\uff0c\u540c\u65f6\u4f7f\u7528\u56db\u500d\u5c11\u7684\u8bcd\u6c47\u3002\u96f6\u6837\u672c\u548c\u76d1\u7763\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff1a\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u8bb0\u5fc6\u80fd\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u4f46\u957f\u5c3e\u3001\u5c0f\u578b\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u8fdc\u8ddd\u79bb\u76ee\u6807\u4ee5\u53ca\u591a\u76ee\u6807\u5b8c\u6210\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "HieraNav\u548cLangMap\u4e3a\u63a8\u8fdb\u8bed\u8a00\u9a71\u52a8\u7684\u5177\u8eab\u5bfc\u822a\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u591a\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u5f00\u653e\u8bcd\u6c47\u5bfc\u822a\u80fd\u529b\u7684\u8bc4\u4f30\u3002"}}
{"id": "2602.01581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01581", "abs": "https://arxiv.org/abs/2602.01581", "authors": ["Yao Zhao", "Kwang-Sung Jun"], "title": "Nearly Optimal Active Preference Learning and Its Application to LLM Alignment", "comment": null, "summary": "Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9LLM\u5bf9\u9f50\u4e2d\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4e13\u95e8\u9488\u5bf9\u504f\u597d\u5b66\u4e60\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u867d\u7136\u4e3b\u52a8\u5b66\u4e60\u5df2\u88ab\u7814\u7a76\u7528\u4e8e\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u91c7\u7528\u7ecf\u5178\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u51c6\u5219\uff08\u5982G-\u6216D-\u6700\u4f18\u6027\uff09\uff0c\u8fd9\u4e9b\u76ee\u6807\u5e76\u672a\u9488\u5bf9\u504f\u597d\u5b66\u4e60\u7684\u7279\u5b9a\u7ed3\u6784\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u95ee\u9898\u7279\u5b9a\u7684\u7b97\u6cd5\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc6\u522b\u4e86\u504f\u597d\u5b66\u4e60\u7279\u6709\u7684\u7b80\u5355\u76f4\u89c9\uff0c\u8d28\u7591\u73b0\u6709\u8bbe\u8ba1\u76ee\u6807\u7684\u9002\u7528\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff1a\u7b2c\u4e00\u79cd\u63d0\u4f9b\u4e86\u8be5\u8bbe\u7f6e\u4e0b\u9996\u4e2a\u5b9e\u4f8b\u4f9d\u8d56\u7684\u6807\u7b7e\u590d\u6742\u5ea6\u4fdd\u8bc1\uff1b\u7b2c\u4e8c\u79cd\u662f\u7b80\u5355\u5b9e\u7528\u7684\u8d2a\u5a6a\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u7b97\u6cd5\uff0c\u89c2\u5bdf\u5230\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6837\u672c\u6548\u7387\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u9488\u5bf9\u504f\u597d\u5b66\u4e60\u7ed3\u6784\u8bbe\u8ba1\u4e13\u95e8\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u964d\u4f4eLLM\u5bf9\u9f50\u4e2d\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u7684\u6210\u672c\u3002"}}
{"id": "2602.02222", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02222", "abs": "https://arxiv.org/abs/2602.02222", "authors": ["Ruiqi Liu", "Manni Cui", "Ziheng Qin", "Zhiyuan Yan", "Ruoxin Chen", "Yi Han", "Zhiheng Li", "Junkai Chen", "ZhiJin Chen", "Kaiqing Lin", "Jialiang Shen", "Lubin Weng", "Jing Dong", "Yan Wang", "Shu Wu"], "title": "MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection", "comment": null, "summary": "High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the \"superhuman crossover\" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR", "AI": {"tldr": "MIRROR\u5c06AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u91cd\u6784\u4e3a\u53c2\u8003\u6bd4\u8f83\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u79bb\u6563\u8bb0\u5fc6\u5e93\u7f16\u7801\u73b0\u5b9e\u5148\u9a8c\uff0c\u5229\u7528\u6d41\u5f62\u4e00\u81f4\u6027\u7406\u60f3\u53c2\u8003\u7684\u6b8b\u5dee\u4f5c\u4e3a\u68c0\u6d4b\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u63a5\u8fd1\u4eba\u7c7b\u611f\u77e5\u6781\u9650\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u4f9d\u8d56\u57fa\u4e8e\u4f2a\u5f71\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u65ad\u6f14\u5316\u7684\u751f\u6210\u75d5\u8ff9\u3002\u800c\u4eba\u7c7b\u5224\u65ad\u4f9d\u8d56\u4e8e\u7a33\u5b9a\u7684\u73b0\u5b9e\u4e16\u754c\u89c4\u5f8b\uff0c\u504f\u79bb\u4eba\u7c7b\u8ba4\u77e5\u6d41\u5f62\u53ef\u4f5c\u4e3a\u66f4\u901a\u7528\u7684\u4f2a\u9020\u4fe1\u53f7\u3002", "method": "\u63d0\u51faMIRROR\u6846\u67b6\uff0c\u5c06AIGI\u68c0\u6d4b\u91cd\u6784\u4e3a\u53c2\u8003\u6bd4\u8f83\u95ee\u9898\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u79bb\u6563\u8bb0\u5fc6\u5e93\u663e\u5f0f\u7f16\u7801\u73b0\u5b9e\u5148\u9a8c\uff0c\u901a\u8fc7\u7a00\u758f\u7ebf\u6027\u7ec4\u5408\u5c06\u8f93\u5165\u6295\u5f71\u5230\u6d41\u5f62\u4e00\u81f4\u6027\u7406\u60f3\u53c2\u8003\uff0c\u5229\u7528\u6b8b\u5dee\u4f5c\u4e3a\u9c81\u68d2\u68c0\u6d4b\u4fe1\u53f7\u3002", "result": "\u572814\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u57286\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u63d0\u53472.1%\uff0c\u57287\u4e2a\u91ce\u5916\u57fa\u51c6\u4e0a\u63d0\u53478.1%\u3002\u5728Human-AIGI\u57fa\u51c6\u4e0a\u8fbe\u523089.6%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u666e\u901a\u7528\u6237\u548c\u89c6\u89c9\u4e13\u5bb6\uff0c\u5e76\u968f\u7740\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u8fdb\u4e00\u6b65\u63a5\u8fd1\u4eba\u7c7b\u611f\u77e5\u6781\u9650\u3002", "conclusion": "MIRROR\u901a\u8fc7\u53c2\u8003\u6bd4\u8f83\u8303\u5f0f\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4eba\u7c7b\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5a92\u4f53\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u4e0d\u65ad\u6f14\u5316\u7684\u751f\u6210\u6a21\u578b\u5a01\u80c1\u3002"}}
{"id": "2602.01585", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01585", "abs": "https://arxiv.org/abs/2602.01585", "authors": ["Xu Zhang", "Qitong Wang", "Peng Wang", "Wei Wang"], "title": "A Lightweight Sparse Interaction Network for Time Series Forecasting", "comment": "The paper is published in AAAI Conference on Artificial Intelligence, AAAI 2025. The code is available at the link https://github.com/Meteor-Stars/LSINet", "summary": "Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.", "AI": {"tldr": "LSINet\u662f\u4e00\u4e2a\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8f7b\u91cf\u7ea7\u7a00\u758f\u4ea4\u4e92\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5934\u7a00\u758f\u4ea4\u4e92\u673a\u5236\u548c\u5171\u4eab\u4ea4\u4e92\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u7ebf\u6027\u6a21\u578b\u548cTransformer\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u6a21\u578b\u867d\u7136\u5728\u67d0\u4e9b\u957f\u65f6\u5e8f\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8eTransformer\uff0c\u4f46\u4ec5\u901a\u8fc7\u5806\u53e0MLP\u7ed3\u6784\u9690\u5f0f\u8fdb\u884c\u65f6\u95f4\u4ea4\u4e92\uff0c\u53ef\u80fd\u4e0d\u8db3\u4ee5\u6355\u6349\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51faLSINet\uff0c\u5305\u542b\u591a\u5934\u7a00\u758f\u4ea4\u4e92\u673a\u5236\uff08MSIM\uff09\u548c\u5171\u4eab\u4ea4\u4e92\u5b66\u4e60\uff08SIL\uff09\u3002MSIM\u901a\u8fc7\u7a00\u758f\u8bf1\u5bfc\u7684\u4f2f\u52aa\u5229\u5206\u5e03\u5b66\u4e60\u65f6\u95f4\u6b65\u4e4b\u95f4\u7684\u91cd\u8981\u8fde\u63a5\u6765\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\uff0c\u7a00\u758f\u6027\u7531\u81ea\u9002\u5e94\u6b63\u5219\u5316\u635f\u5931\u786e\u4fdd\u3002SIL\u5229\u7528\u65f6\u95f4\u4ea4\u4e92\u7684\u53ef\u5171\u4eab\u6027\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u548c\u6536\u655b\u6027\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLSINet\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u5148\u8fdb\u7684\u7ebf\u6027\u6a21\u578b\u548cTransformer\u6a21\u578b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u597d\u7684\u6548\u7387\u3002", "conclusion": "LSINet\u901a\u8fc7\u663e\u5f0f\u7684\u65f6\u95f4\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u4f4e\u5f00\u9500\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u957f\u65f6\u5e8f\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02223", "abs": "https://arxiv.org/abs/2602.02223", "authors": ["Junchi Feng", "Nikhil Ballem", "Mahya Beheshti", "Giles Hamilton-Fletcher", "Todd Hudson", "Maurizio Porfiri", "William H. Seiple", "John-Ross Rizzo"], "title": "Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type", "comment": null, "summary": "Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86OCR\u5728\u9759\u6001\u548c\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u884c\u8d70\u901f\u5ea6\u548c\u89c6\u89d2\u589e\u5927\u4f1a\u964d\u4f4e\u8bc6\u522b\u51c6\u786e\u7387\uff0cGoogle Vision\u8868\u73b0\u6700\u4f73\uff0c\u624b\u673a\u4e3b\u6444\u50cf\u5934\u548c\u80a9\u90e8\u4f69\u6234\u4f4d\u7f6e\u6548\u679c\u6700\u597d\u3002", "motivation": "\u5f53\u524dOCR\u8bc4\u4f30\u591a\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\uff0c\u672a\u80fd\u53cd\u6620\u79fb\u52a8\u4f7f\u7528\u4e2d\u7684\u771f\u5b9e\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u969c\u4eba\u58eb\u7684\u8f85\u52a9\u6280\u672f\u5e94\u7528\u573a\u666f\u3002", "method": "\u91c7\u7528\u9759\u6001\u6d4b\u8bd5\uff08\u8ddd\u79bb1-7\u7c73\uff0c\u6c34\u5e73\u89c6\u89d20-75\u5ea6\uff09\u548c\u52a8\u6001\u6d4b\u8bd5\uff08\u884c\u8d70\u901f\u5ea60.8-1.8 m/s\uff0c\u4e09\u79cd\u4f69\u6234\u4f4d\u7f6e\uff1a\u5934\u6234\u3001\u80a9\u6234\u3001\u624b\u6301\uff09\u3002\u8bc4\u4f30\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u773c\u955c\uff0c\u4f7f\u7528\u56db\u79cdOCR\u5f15\u64ce\uff08Google Vision\u3001PaddleOCR 3.0\u3001EasyOCR\u3001Tesseract\uff09\uff0c\u4ee5\u5b57\u7b26\u7ea7Levenshtein\u6bd4\u7387\u8ba1\u7b97\u51c6\u786e\u7387\u3002", "result": "\u8bc6\u522b\u51c6\u786e\u7387\u968f\u884c\u8d70\u901f\u5ea6\u589e\u52a0\u548c\u89c6\u89d2\u589e\u5927\u800c\u4e0b\u964d\u3002Google Vision\u6574\u4f53\u51c6\u786e\u7387\u6700\u9ad8\uff0cPaddleOCR\u662f\u6700\u4f73\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002\u624b\u673a\u4e3b\u6444\u50cf\u5934\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u80a9\u90e8\u4f69\u6234\u4f4d\u7f6e\u5e73\u5747\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4e0d\u540c\u8eab\u4f53\u4f4d\u7f6e\u95f4\u7684\u5dee\u5f02\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "conclusion": "\u79fb\u52a8OCR\u6027\u80fd\u53d7\u52a8\u6001\u6761\u4ef6\u663e\u8457\u5f71\u54cd\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8003\u8651\u884c\u8d70\u901f\u5ea6\u548c\u89c6\u89d2\u56e0\u7d20\uff0cGoogle Vision\u548cPaddleOCR\u662f\u53ef\u9760\u9009\u62e9\uff0c\u80a9\u90e8\u4f69\u6234\u4f4d\u7f6e\u53ef\u80fd\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2602.01588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01588", "abs": "https://arxiv.org/abs/2602.01588", "authors": ["Huu Hiep Nguyen", "Minh Hoang Nguyen", "Dung Nguyen", "Hung Le"], "title": "Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting", "comment": null, "summary": "Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.", "AI": {"tldr": "SpecTF\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u878d\u5408\u6587\u672c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u9891\u7387\u5e26\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u6587\u672c\u7279\u5f81\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u5bf9\u9f50\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u9010\u6b65\u5bf9\u9f50\u7684\u65b9\u5f0f\uff0c\u5ffd\u7565\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u5f71\u54cd\uff08\u5982\u65f6\u95f4\u5e8f\u5217\u5468\u671f\u548c\u52a8\u6001\u53d8\u5316\uff09\u3002\u5c40\u90e8\u5bf9\u9f50\u4e0e\u5168\u5c40\u6587\u672c\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u63d0\u51faSpecTF\u6846\u67b6\uff1a1) \u63d0\u53d6\u6587\u672c\u5d4c\u5165\uff1b2) \u5c06\u6587\u672c\u5d4c\u5165\u6295\u5f71\u5230\u9891\u57df\uff1b3) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6587\u672c\u7279\u5f81\u4e0e\u65f6\u95f4\u5e8f\u5217\u7684\u9891\u8c31\u5206\u91cf\u878d\u5408\uff1b4) \u57fa\u4e8e\u6587\u672c\u76f8\u5173\u6027\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u9891\u7387\u5e26\uff1b5) \u5c06\u7ed3\u679c\u6620\u5c04\u56de\u65f6\u57df\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSpecTF\u5728\u591a\u4e2a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u53c2\u6570\u6570\u91cf\u660e\u663e\u66f4\u5c11\u3002", "conclusion": "\u901a\u8fc7\u9891\u57df\u878d\u5408\u6587\u672c\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0cSpecTF\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u5f71\u54cd\uff0c\u89e3\u51b3\u4e86\u5c40\u90e8\u5bf9\u9f50\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02227", "abs": "https://arxiv.org/abs/2602.02227", "authors": ["Harold Haodong Chen", "Xinxiang Yin", "Wen-Jie Shu", "Hongfei Zhang", "Zixin Zhang", "Chenfei Liao", "Litao Guo", "Qifeng Chen", "Ying-Cong Chen"], "title": "Show, Don't Tell: Morphing Latent Reasoning into Image Generation", "comment": "Code: https://github.com/EnVision-Research/LatentMorph", "summary": "Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\\%$ on GenEval and $25\\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\\%$ and $11\\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\\%$ and token consumption by $51\\%$; and (IV) exhibits $71\\%$ cognitive alignment with human intuition on reasoning invocation.", "AI": {"tldr": "LatentMorph\uff1a\u4e00\u79cd\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9690\u5f0f\u63a8\u7406\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u8f7b\u91cf\u7ea7\u7ec4\u4ef6\u5b9e\u73b0\u9ad8\u6548\u81ea\u9002\u5e94\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u63a8\u7406\u548c\u7cbe\u70bc\u80fd\u529b\uff0c\u800c\u5f53\u524d\u57fa\u4e8e\u663e\u5f0f\u63a8\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u3001\u4fe1\u606f\u4e22\u5931\u548c\u8ba4\u77e5\u4e0d\u5339\u914d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u96c6\u6210\u65b9\u6848\u3002", "method": "\u63d0\u51faLatentMorph\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u51b7\u51dd\u5668\u7528\u4e8e\u538b\u7f29\u4e2d\u95f4\u751f\u6210\u72b6\u6001\u4e3a\u89c6\u89c9\u8bb0\u5fc6\uff1b2\uff09\u7ffb\u8bd1\u5668\u5c06\u6f5c\u5728\u601d\u60f3\u8f6c\u6362\u4e3a\u53ef\u64cd\u4f5c\u6307\u5bfc\uff1b3\uff09\u5851\u5f62\u5668\u52a8\u6001\u5f15\u5bfc\u4e0b\u4e00\u56fe\u50cf\u6807\u8bb0\u9884\u6d4b\uff1b4\uff09RL\u8bad\u7ec3\u8c03\u7528\u5668\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u8c03\u7528\u63a8\u7406\u3002\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u3002", "result": "1\uff09\u5c06Janus-Pro\u57fa\u7840\u6a21\u578b\u5728GenEval\u4e0a\u63d0\u534716%\uff0c\u5728T2I-CompBench\u4e0a\u63d0\u534725%\uff1b2\uff09\u5728WISE\u548cIPV-Txt\u7b49\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u663e\u5f0f\u63a8\u7406\u65b9\u6cd515%\u548c11%\uff1b3\uff09\u51cf\u5c1144%\u63a8\u7406\u65f6\u95f4\u548c51%\u6807\u8bb0\u6d88\u8017\uff1b4\uff09\u5728\u63a8\u7406\u8c03\u7528\u4e0a\u4e0e\u4eba\u7c7b\u76f4\u89c9\u7684\u8ba4\u77e5\u5bf9\u9f50\u8fbe\u523071%\u3002", "conclusion": "LatentMorph\u901a\u8fc7\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9690\u5f0f\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u663e\u5f0f\u63a8\u7406\u65b9\u6cd5\u7684\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u4e14\u8ba4\u77e5\u5bf9\u9f50\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u4e3a\u521b\u9020\u6027AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.01599", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01599", "abs": "https://arxiv.org/abs/2602.01599", "authors": ["Israel Adewuyi", "Solomon Okibe", "Vladmir Ivanov"], "title": "The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR", "comment": null, "summary": "The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u591a\u91cd\u5f69\u7968\u5047\u8bbe\"\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5305\u542b\u591a\u4e2a\u53ef\u884c\u7684\u7a00\u758f\u5b50\u7f51\u7edc\uff0c\u4ec5\u8bad\u7ec31%\u7684\u968f\u673a\u53c2\u6570\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u5168\u53c2\u6570RLVR\u5fae\u8c03\u6027\u80fd", "motivation": "\u5f69\u7968\u5047\u8bbe\u8868\u660e\u7a00\u758f\u5b50\u7f51\u7edc\u53ef\u5339\u914d\u5b8c\u6574\u6a21\u578b\u6027\u80fd\uff0cRLVR\u4e2d\u66f4\u65b0\u96c6\u4e2d\u5728\u7a00\u758f\u53c2\u6570\u5b50\u96c6\uff0c\u8fd9\u6697\u793a\u4e86\u53c2\u6570\u5197\u4f59\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u8fd9\u79cd\u5197\u4f59\u7684\u6700\u7b80\u5355\u65b9\u6cd5", "method": "\u91c7\u7528\u6781\u7aef\u7a00\u758f\u8bad\u7ec3\uff0c\u4ec5\u968f\u673a\u9009\u62e91%\u7684\u53c2\u6570\u8fdb\u884c\u8bad\u7ec3\uff0c\u57283\u4e2a\u6a21\u578b\u548c2\u4e2a\u4efb\u52a1\u9886\u57df\u9a8c\u8bc1\u6548\u679c\uff0c\u5206\u6790\u4e0d\u540c\u968f\u673a\u63a9\u7801\u7684\u91cd\u53e0\u5ea6", "result": "\u4ec5\u8bad\u7ec31%\u53c2\u6570\u5373\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u5168\u53c2\u6570RLVR\u5fae\u8c03\uff1b\u4e0d\u540c\u968f\u673a\u63a9\u7801\u91cd\u53e0\u5ea6\u6781\u4f4e\uff08Jaccard\u76f8\u4f3c\u5ea6\u22640.005\uff09\u4f46\u90fd\u80fd\u6210\u529f", "conclusion": "\u9884\u8bad\u7ec3\u6a21\u578b\u5305\u542b\u591a\u4e2a\u53ef\u884c\u7684\u7a00\u758f\u5b50\u7f51\u7edc\u800c\u975e\u5355\u4e00\u7279\u6743\u96c6\u5408\uff0cRLVR\u4e2d\u7684\u9690\u5f0fKL\u7ea6\u675f\u5c06\u66f4\u65b0\u9650\u5236\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u4f7f\u4efb\u610f\u7a00\u758f\u63a9\u7801\u90fd\u80fd\u6210\u529f"}}
{"id": "2602.02232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02232", "abs": "https://arxiv.org/abs/2602.02232", "authors": ["Andrea Matteazzi", "Dietmar Tutsch"], "title": "LiFlow: Flow Matching for 3D LiDAR Scene Completion", "comment": null, "summary": "In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.", "AI": {"tldr": "LiFlow\uff1a\u9996\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u76843D LiDAR\u573a\u666f\u8865\u5168\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u6846\u67b6\u89e3\u51b3\u6269\u6563\u65b9\u6cd5\u4e2d\u8bad\u7ec3\u4e0e\u63a8\u7406\u521d\u59cb\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cLiDAR\u70b9\u4e91\u5e38\u53d7\u906e\u6321\u548c\u8fdc\u8ddd\u79bb\u7a00\u758f\u6027\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u611f\u77e5\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u9884\u6d4b\u9ad8\u65af\u566a\u58f0\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u7684\u521d\u59cb\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5f71\u54cd\u573a\u666f\u8865\u5168\u6548\u679c\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u6d41\u5339\u914d\u76843D LiDAR\u573a\u666f\u8865\u5168\u6846\u67b6LiFlow\uff0c\u91c7\u7528\u6700\u8fd1\u90bb\u6d41\u5339\u914d\u635f\u5931\u548cChamfer\u8ddd\u79bb\u635f\u5931\uff0c\u540c\u65f6\u4f18\u5316\u70b9\u4e91\u7684\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u8986\u76d6\u5bf9\u9f50\u3002", "result": "LiFlow\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6269\u6563\u57fa\u65b9\u6cd5\u3002", "conclusion": "\u6d41\u5339\u914d\u6846\u67b6\u4e3a3D LiDAR\u573a\u666f\u8865\u5168\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u65b9\u6cd5\u4e2d\u8bad\u7ec3\u4e0e\u63a8\u7406\u521d\u59cb\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.02318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02318", "abs": "https://arxiv.org/abs/2602.02318", "authors": ["Xiang Li", "Yupeng Zheng", "Pengfei Li", "Yilun Chen", "Ya-Qin Zhang", "Wenchao Ding"], "title": "Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation", "comment": "Accepted by RA-L", "summary": "Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS\u2020. With depth integration, DiScene\u2020 attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.", "AI": {"tldr": "DiScene\uff1a\u4e00\u79cd\u5229\u7528\u591a\u7ea7\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u9c81\u68d2\u5360\u636e\u9884\u6d4b\u7684\u7a00\u758f\u67e5\u8be2\u6846\u67b6\uff0c\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5f53\u524d\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u9762\u4e34\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\uff1a\u5bc6\u96c6\u65b9\u6cd5\u5728\u7a7a\u4f53\u7d20\u4e0a\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u7a00\u758f\u67e5\u8be2\u65b9\u6cd5\u5728\u590d\u6742\u5ba4\u5185\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u53c8\u80fd\u9002\u5e94\u591a\u6837\u590d\u6742\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDiScene\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u591a\u7ea7\u4e00\u81f4\u6027\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u901a\u8fc7\u7f16\u7801\u5668\u7ea7\u7279\u5f81\u5bf9\u9f50\u3001\u67e5\u8be2\u7ea7\u7279\u5f81\u5339\u914d\u3001\u5148\u9a8c\u7ea7\u7a7a\u95f4\u6307\u5bfc\u548c\u951a\u70b9\u7ea7\u9ad8\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u56db\u4e2a\u5c42\u6b21\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u7684\u5c42\u6b21\u8868\u793a\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff1b2\uff09\u6559\u5e08\u5f15\u5bfc\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4f7f\u7528\u4f18\u5316\u7684\u53c2\u6570\u9884\u70ed\u52a0\u901f\u6a21\u578b\u6536\u655b\u3002", "result": "\u5728Occ-Scannet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiScene\u8fbe\u523023.2 FPS\uff08\u65e0\u6df1\u5ea6\u5148\u9a8c\uff09\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5OPUS\u63d0\u534736.1%\uff0c\u751a\u81f3\u4f18\u4e8e\u6df1\u5ea6\u589e\u5f3a\u7248\u672cOPUS\u2020\u3002\u96c6\u6210\u6df1\u5ea6\u540e\uff0cDiScene\u2020\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u8d85\u8d8aEmbodiedOcc 3.7%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb1.62\u500d\u3002\u5728Occ3D-nuScenes\u57fa\u51c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiScene\u901a\u8fc7\u521b\u65b0\u7684\u591a\u7ea7\u84b8\u998f\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5360\u636e\u9884\u6d4b\u4e2d\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u5404\u79cd\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.01605", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01605", "abs": "https://arxiv.org/abs/2602.01605", "authors": ["Anthony Bao", "Venkata Hasith Vattikuti", "Jeffrey Lai", "William Gilpin"], "title": "Universal Redundancies in Time Series Foundation Models", "comment": null, "summary": "Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5b58\u5728\u5197\u4f59\u7ec4\u4ef6\uff0c\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u5c42\u526a\u679d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u51fa\u5bfc\u81f4\u9000\u5316\u73b0\u8c61\uff08\u5982\u6a21\u5f0f\u91cd\u590d\u548c\u5b63\u8282\u6027\u504f\u5dee\uff09\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b9e\u73b0\u96f6\u6837\u672c\u9884\u6d4b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4e86\u89e3\u6709\u9650\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63ed\u793a\u8fd9\u4e9b\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u539f\u7406\u548c\u5197\u4f59\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u5957\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u5305\u62ec\u7279\u5b9a\u7ec4\u4ef6\u6d88\u878d\u548c\u6b8b\u5dee\u6d41\u4e0a\u7684\u76f4\u63a5\u5bf9\u6570\u5f52\u56e0\u3002\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5c06Transformer\u89c6\u4e3a\u6838\u56de\u5f52\u5668\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u6295\u5f71\u77e9\u9635\u7684\u7a33\u5b9a\u79e9\u8fdb\u884c\u5185\u5728\u6d88\u878d\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5bf9\u6574\u5c42\u6d88\u878d\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u7a33\u5b9a\u79e9\u65b9\u6cd5\u8bc6\u522b\u51fa\u5bfc\u81f4\u9000\u5316\u73b0\u8c61\uff08\u5982\u4e0a\u4e0b\u6587\u6a21\u5f0f\u91cd\u590d\u548c\u5b63\u8282\u6027\u504f\u5dee\uff09\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002\u8fd9\u4e9b\u53d1\u73b0\u5728\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u666e\u904d\u7279\u6027\uff0c\u5305\u62ec\u5c42\u5197\u4f59\u548c\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u5bf9\u9000\u5316\u73b0\u8c61\u7684\u8d23\u4efb\u3002\u63d0\u51fa\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u548c\u7406\u8bba\u6846\u67b6\u4e3a\u7406\u89e3\u8fd9\u4e00\u65b0\u5174\u67b6\u6784\u7c7b\u522b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.02334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02334", "abs": "https://arxiv.org/abs/2602.02334", "authors": ["Fatemeh Zargarbashi", "Dhruv Agrawal", "Jakob Buhmann", "Martin Guay", "Stelian Coros", "Robert W. Sumner"], "title": "VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations", "comment": null, "summary": "Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u8026\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u4e2d\u7684\u98ce\u683c\u4e0e\u5185\u5bb9\uff0c\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u5b9e\u65f6\u98ce\u683c\u8fc1\u79fb", "motivation": "\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u540c\u65f6\u5305\u542b\u8bed\u4e49\u5185\u5bb9\u548c\u7ec6\u5fae\u98ce\u683c\u7279\u5f81\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u8026\u8fd9\u4e24\u8005\u3002\u98ce\u683c\u8fc1\u79fb\u9700\u8981\u80fd\u591f\u5206\u79bb\u5185\u5bb9\u548c\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027", "method": "\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u4ece\u7c97\u5230\u7ec6\u7684\u8fd0\u52a8\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u4fe1\u606f\u6cc4\u9732\u635f\u5931\uff0c\u901a\u8fc7\u91cf\u5316\u4ee3\u7801\u4ea4\u6362\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u7684\u98ce\u683c\u8fc1\u79fb", "result": "\u6846\u67b6\u5728\u98ce\u683c\u8fc1\u79fb\u3001\u98ce\u683c\u79fb\u9664\u548c\u8fd0\u52a8\u6df7\u5408\u7b49\u591a\u4e2a\u63a8\u7406\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u5904\u7406\u672a\u89c1\u8fc7\u7684\u98ce\u683c\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u8026\u4e86\u8fd0\u52a8\u6570\u636e\u4e2d\u7684\u98ce\u683c\u4e0e\u5185\u5bb9\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u63a8\u7406\u65f6\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u4e3a\u8fd0\u52a8\u7f16\u8f91\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.01606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01606", "abs": "https://arxiv.org/abs/2602.01606", "authors": ["Zeqiao Li", "Yijing Wang", "Haoyu Wang", "Zheng Li", "Zhiqiang Zuo"], "title": "Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching", "comment": null, "summary": "Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \\textbf{F}low-based \\textbf{L}og-likelihood-\\textbf{A}ware \\textbf{M}aximum \\textbf{E}ntropy RL (\\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.", "AI": {"tldr": "FLAME\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91cd\u52a0\u6743\u7ed5\u8fc7\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff0c\u8bbe\u8ba1\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\u7ea0\u6b63\u504f\u5dee\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u548c\u4e00\u6b65\u63a8\u7406\u63a7\u5236\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u800c\u6d41\u5339\u914d\u867d\u7136\u652f\u6301\u4e00\u6b65\u751f\u6210\uff0c\u4f46\u96be\u4ee5\u96c6\u6210\u5230\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\uff1a\u6700\u4f18\u7b56\u7565\u662f\u96be\u5904\u7406\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u5e03\uff0c\u4e14\u9ad8\u6548\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u5b58\u5728\u4e25\u91cd\u79bb\u6563\u5316\u504f\u5dee\u3002", "method": "1) \u63a8\u5bfcQ\u91cd\u52a0\u6743\u6d41\u5339\u914d\u76ee\u6807\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91cd\u52a0\u6743\u7ed5\u8fc7\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff1b2) \u8bbe\u8ba1\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\u4e25\u683c\u7ea0\u6b63\u504f\u5dee\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff1b3) \u96c6\u6210MeanFlow\u516c\u5f0f\u5b9e\u73b0\u8868\u8fbe\u6027\u5f3a\u4e14\u9ad8\u6548\u7684\u4e00\u6b65\u63a7\u5236\u3002", "result": "\u5728MuJoCo\u5b9e\u9a8c\u4e2d\uff0cFLAME\u8d85\u8d8a\u9ad8\u65af\u57fa\u7ebf\uff0c\u5339\u914d\u591a\u6b65\u6269\u6563\u7b56\u7565\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "FLAME\u4e3a\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u96c6\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u6027\u5f3a\u3001\u63a8\u7406\u9ad8\u6548\u7684\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2602.02341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02341", "abs": "https://arxiv.org/abs/2602.02341", "authors": ["Zhenpeng Huang", "Jiaqi Li", "Zihan Jia", "Xinhao Li", "Desen Meng", "Lingxue Song", "Xi Chen", "Liang Li", "Limin Wang"], "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization", "comment": "NeurIPS 2025", "summary": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.", "AI": {"tldr": "LongVPO\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u8ba9\u77ed\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7406\u89e3\u8d85\u957f\u89c6\u9891\uff0c\u65e0\u9700\u957f\u89c6\u9891\u6807\u6ce8\uff0c\u4ec5\u752816K\u5408\u6210\u6570\u636e\u5c31\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u77ed\u89c6\u9891\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u957f\u89c6\u9891\uff0c\u800c\u83b7\u53d6\u957f\u89c6\u9891\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u957f\u89c6\u9891\u6807\u6ce8\u5c31\u80fd\u8ba9\u6a21\u578b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5408\u6210\u504f\u597d\u4e09\u5143\u7ec4\uff0c\u5c06\u95ee\u9898\u951a\u5b9a\u5230\u5355\u4e2a\u77ed\u7247\u6bb5\uff0c\u63d2\u5165\u5e72\u6270\u7247\u6bb5\uff0c\u5e94\u7528\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u95ee\u9898\u7279\u5f02\u6027\u8fc7\u6ee4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u9012\u5f52\u5b57\u5e55\u751f\u6210\u573a\u666f\u7ea7\u5143\u6570\u636e\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u591a\u7247\u6bb5\u63a8\u7406\u67e5\u8be2\u548c\u4e0d\u53d7\u504f\u597d\u7684\u54cd\u5e94\uff0c\u901a\u8fc7\u591a\u7247\u6bb5\u63a8\u7406\u4efb\u52a1\u5bf9\u9f50\u6a21\u578b\u504f\u597d\u3002", "result": "\u4ec5\u752816K\u5408\u6210\u793a\u4f8b\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0cLongVPO\u5728\u591a\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u540c\u65f6\u5728\u77ed\u89c6\u9891\u6027\u80fd\uff08\u5982MVBench\uff09\u4e0a\u4fdd\u6301\u5f3a\u5927\u8868\u73b0\u3002", "conclusion": "LongVPO\u4e3a\u9ad8\u6548\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u4e24\u9636\u6bb5\u4f18\u5316\uff0c\u4f7f\u77ed\u4e0a\u4e0b\u6587\u6a21\u578b\u80fd\u591f\u7a33\u5065\u7406\u89e3\u8d85\u957f\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u6807\u6ce8\u7a00\u7f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2602.01611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01611", "abs": "https://arxiv.org/abs/2602.01611", "authors": ["Weizheng Gu", "Chengze Li", "Zhuohao Yu", "Mengyuan Sun", "Zhibang Yang", "Wei Wang", "Hongrui Jia", "Shikun Zhang", "Wei Ye"], "title": "What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?", "comment": null, "summary": "Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.", "AI": {"tldr": "PIPE\u534f\u8bae\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u5199\u73af\u5883\u63a5\u53e3\u6765\u8bca\u65ad\u667a\u80fd\u4f53\u5bf9\u7279\u5b9a\u63a5\u53e3\u7684\u4f9d\u8d56\uff0c\u63ed\u793a\u8f68\u8ff9SFT\u8bad\u7ec3\u4f1a\u653e\u5927\u63a5\u53e3\u6377\u5f84\u5b66\u4e60\uff0c\u800c\u975e\u8bed\u4e49\u80fd\u529b", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u8bc4\u4f30\u5c06\u8bed\u4e49\u5de5\u5177\u4f7f\u7528\u548c\u63a5\u53e3\u7279\u5b9a\u4ea4\u4e92\u6a21\u5f0f\u8bb0\u5fc6\u6df7\u4e3a\u4e00\u8c08\uff0c\u57fa\u51c6\u5206\u6570\u65e0\u6cd5\u533a\u5206\u73af\u5883\u4e0d\u53d8\u80fd\u529b\uff0c\u9700\u8981\u8bca\u65ad\u63a5\u53e3\u4f9d\u8d56\u7684\u65b9\u6cd5", "method": "\u63d0\u51faPIPE\u534f\u8bae\u7ea7\u8bc4\u4f30\u589e\u5f3a\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u91cd\u5199\u73af\u5883\u63a5\u53e3\u4f46\u4fdd\u6301\u4efb\u52a1\u8bed\u4e49\u548c\u6267\u884c\u884c\u4e3a\u4e0d\u53d8\uff0c\u5f15\u5165\u63a5\u53e3\u4f9d\u8d56\u5ea6(IR)\u6307\u6807\u91cf\u5316\u5bf9\u8bad\u7ec3\u63a5\u53e3\u7684\u504f\u597d", "result": "\u572816\u4e2a\u73af\u5883\u4e2d\u53d1\u73b0\u8f68\u8ff9SFT\u8bad\u7ec3\u663e\u8457\u653e\u5927\u63a5\u53e3\u6377\u5f84\uff1a\u8bad\u7ec3\u667a\u80fd\u4f53\u5728\u6700\u5c0f\u63a5\u53e3\u91cd\u5199\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u800c\u975e\u8f68\u8ff9\u8bad\u7ec3\u6a21\u578b\u4fdd\u6301\u7a33\u5b9a\uff1b\u63a5\u53e3\u6377\u5f84\u5448\u73b0\u73af\u5883\u4f9d\u8d56\u3001\u975e\u5355\u8c03\u7684\u8bad\u7ec3\u52a8\u6001", "conclusion": "\u6807\u51c6\u8bc4\u4f30\u63a9\u76d6\u4e86\u667a\u80fd\u4f53\u5bf9\u7279\u5b9a\u63a5\u53e3\u7684\u4f9d\u8d56\uff0cPIPE\u80fd\u6709\u6548\u8bca\u65ad\u63a5\u53e3\u6377\u5f84\u5b66\u4e60\uff0c\u8f68\u8ff9SFT\u8bad\u7ec3\u53ef\u80fd\u8fc7\u5ea6\u62df\u5408\u63a5\u53e3\u6a21\u5f0f\u800c\u975e\u5b66\u4e60\u8bed\u4e49\u80fd\u529b"}}
{"id": "2602.02354", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02354", "abs": "https://arxiv.org/abs/2602.02354", "authors": ["Albert Kwok", "Zheyuan Hu", "Dounia Hammou"], "title": "Implicit neural representation of textures", "comment": "Albert Kwok and Zheyuan Hu contributed equally to this work", "summary": "Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5c06\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u4e3a\u65b0\u578b\u7eb9\u7406\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\uff0c\u5728\u8fde\u7eedUV\u5750\u6807\u7a7a\u95f4\u800c\u975e\u79bb\u6563\u7a7a\u95f4\u4e0a\u64cd\u4f5c\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u4f7f\u7528\u548c\u6e32\u67d3\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u7814\u7a76\u4e86\u5b9e\u65f6\u6e32\u67d3\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u5e94\u7528\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5df2\u5728\u591a\u4e2a\u9886\u57df\u8bc1\u660e\u5176\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u4e3a\u65b0\u578b\u7eb9\u7406INR\uff0c\u4f7f\u5176\u5728\u8fde\u7eedUV\u5750\u6807\u7a7a\u95f4\u800c\u975e\u79bb\u6563\u7a7a\u95f4\u4e0a\u64cd\u4f5c\uff0c\u4ee5\u6539\u8fdb\u7eb9\u7406\u8868\u793a\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u65b0\u578b\u7eb9\u7406INR\uff0c\u5728\u8fde\u7eedUV\u5750\u6807\u7a7a\u95f4\u4e0a\u8fdb\u884c\u64cd\u4f5c\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\u8fd9\u4e9bINR\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u4f7f\u7528\u548c\u6e32\u67d3\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u76ee\u6807\u4e4b\u95f4\u7684\u5e73\u8861\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9bINR\u5728\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u5177\u6709\u53ef\u89c2\u7684\u5185\u5b58\u4f7f\u7528\u6548\u7387\u548c\u6e32\u67d3\u63a8\u7406\u65f6\u95f4\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u8fd9\u4e9b\u76ee\u6807\u4e4b\u95f4\u7684\u5e73\u8861\u5173\u7cfb\uff0c\u5e76\u63a2\u7d22\u4e86\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982mipmap\u62df\u5408\u548cINR\u7a7a\u95f4\u751f\u6210\uff09\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c55\u793a\u4e86\u5c06\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u4e3a\u8fde\u7eedUV\u5750\u6807\u7a7a\u95f4\u7eb9\u7406INR\u7684\u53ef\u884c\u6027\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5185\u5b58\u6548\u7387\u548c\u6e32\u67d3\u901f\u5ea6\u65b9\u9762\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u548c\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u5e94\u7528\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.01613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01613", "abs": "https://arxiv.org/abs/2602.01613", "authors": ["Sergii Kozyrev", "Davyd Maiboroda"], "title": "A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models", "comment": "13 pages, 5 figures", "summary": "Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.", "AI": {"tldr": "Minima\u662f\u4e00\u4e2a\u751f\u4ea7\u7ea7\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7ed3\u6784\u538b\u7f29Transformer\u6a21\u578b\u6765\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u7ed3\u5408\u591a\u79cd\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u548c\u81ea\u5b9a\u4e49\u5185\u6838\uff0c\u652f\u6301\u63a8\u6d4b\u89e3\u7801\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u65f6\u53d7\u5230GPU\u5185\u5b58\u548c\u63a8\u7406\u5ef6\u8fdf\u7684\u9650\u5236\uff0c\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5e76\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5377\u79ef\u9884\u6d4b\u5668\u8bc4\u4f30\u5c42\u548c\u8865\u4e01\u7ea7\u522b\u7684\u654f\u611f\u6027\uff1b\u5bf9\u4f4e\u654f\u611f\u6027\u533a\u57df\u5e94\u7528Tucker\u3001\u5f20\u91cf\u94fe\u548c\u5f20\u91cf\u73af\u5206\u89e3\uff1b\u8fdb\u884c\u77ed\u671f\u4fee\u590d\u5fae\u8c03\uff1b\u4f7f\u7528\u81ea\u5b9a\u4e49Triton\u548cCUDA\u5185\u6838\u6267\u884c\u64cd\u4f5c\uff1b\u901a\u8fc7\u51cf\u5c11\u5185\u5b58\u5360\u7528\u5b9e\u73b0\u63a8\u6d4b\u89e3\u7801\u3002", "result": "\u5728Qwen3-32B\u6a21\u578b\u4e0a\uff0c\u5cf0\u503cVRAM\u4ece64GiB\u964d\u81f340GiB\uff1b\u5355\u8bf7\u6c42\u541e\u5410\u91cf\u4ece40tps\u63d0\u5347\u81f350tps\uff08Minima\uff09\u548c75tps\uff08\u5e26\u63a8\u6d4b\u89e3\u7801\uff09\uff1b50\u4e2a\u5e76\u884c\u8bf7\u6c42\u4e0b\u541e\u5410\u91cf\u5206\u522b\u4e3a34\u300144\u300153tps\uff0c\u663e\u793a\u5728\u9ad8\u5e76\u53d1\u4e0b\u4ecd\u6709\u6548\u3002", "conclusion": "Minima\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u7ed3\u6784\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u5f20\u91cf\u4e3b\u5e72\u548c\u5fae\u5c0f\u5c42\u9002\u914d\u5668\uff0c\u4e3a\u66f4\u6fc0\u8fdb\u7684\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.02356", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02356", "abs": "https://arxiv.org/abs/2602.02356", "authors": ["Wangduo Xie", "Matthew B. Blaschko"], "title": "NAB: Neural Adaptive Binning for Sparse-View CT reconstruction", "comment": null, "summary": "Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \\textbf{N}eural \\textbf{A}daptive \\textbf{B}inning (\\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.", "AI": {"tldr": "\u63d0\u51faNAB\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u7bb1\u673a\u5236\u5c06\u77e9\u5f62\u5148\u9a8c\u6574\u5408\u5230\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u4e2d\uff0c\u63d0\u9ad8\u5de5\u4e1a\u7269\u4f53\u91cd\u5efa\u8d28\u91cf", "motivation": "\u5de5\u4e1aCT\u4e2d\u8bb8\u591a\u7269\u4f53\u5177\u6709\u77e9\u5f62\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5f62\u72b6\u5148\u9a8c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6574\u5408\u77e9\u5f62\u5148\u9a8c\u7684\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u65b9\u6cd5", "method": "\u63d0\u51fa\u795e\u7ecf\u81ea\u9002\u5e94\u5206\u7bb1(NAB)\u65b9\u6cd5\uff1a1) \u5c06\u5750\u6807\u7a7a\u95f4\u6620\u5c04\u5230\u5206\u7bb1\u5411\u91cf\u7a7a\u95f4\uff1b2) \u4f7f\u7528\u57fa\u4e8e\u79fb\u4f4d\u53cc\u66f2\u6b63\u5207\u51fd\u6570\u5dee\u503c\u7684\u521b\u65b0\u5206\u7bb1\u673a\u5236\uff1b3) \u6269\u5c55\u652f\u6301\u7ed5\u8f93\u5165\u5e73\u9762\u6cd5\u5411\u91cf\u7684\u65cb\u8f6c\uff1b4) \u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u8868\u793a\u9884\u6d4bCT\u8870\u51cf\u7cfb\u6570\uff1b5) \u7aef\u5230\u7aef\u4f18\u5316\u7f16\u7801\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5927\u5c0f\u3001\u9661\u5ea6\u3001\u65cb\u8f6c\uff09", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u901a\u8fc7\u8c03\u6574\u5206\u7bb1\u51fd\u6570\u5e73\u6ed1\u5ea6\u53ef\u6cdb\u5316\u5230\u66f4\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u5728\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u4e5f\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "NAB\u4e3a\u5c06\u5f62\u72b6\u5148\u9a8c\u6574\u5408\u5230\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u80fd\u6709\u6548\u5229\u7528\u77e9\u5f62\u5148\u9a8c\u63d0\u9ad8\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u8d28\u91cf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90"}}
{"id": "2602.01614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01614", "abs": "https://arxiv.org/abs/2602.01614", "authors": ["Qi Cheng", "Licheng Liu", "Yao Zhang", "Mu Hong", "Yiqun Xie", "Xiaowei Jia"], "title": "AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems", "comment": null, "summary": "Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u9996\u4e2a\u65f6\u7a7a\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7269\u7406\u6a21\u578b\u6a21\u62df\u548c\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u78b3\u6c2e\u901a\u91cf\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4e86\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u5bf9\u5168\u7403\u6c14\u5019\u53d8\u5316\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u571f\u58e4\u91c7\u6837\u3001\u8fc7\u7a0b\u6a21\u578b\u548c\u9ed1\u7bb1\u673a\u5668\u5b66\u4e60\uff09\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u65f6\u7a7a\u5f02\u8d28\u6027\u9ad8\u3001\u5730\u4e0b\u8fc7\u7a0b\u590d\u6742\u7b49\u6311\u6218\uff0c\u7f3a\u4e4fAI\u5c31\u7eea\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u534f\u8bae\uff0c\u963b\u788d\u4e86\u53ef\u4fe1AI\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "1\uff09\u521b\u5efa\u9996\u4e2a\u65f6\u7a7a\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6574\u5408Ecosys\u548cDayCent\u7269\u7406\u6a21\u578b\u6a21\u62df\u6570\u636e\u3001\u6da1\u5ea6\u534f\u65b9\u5dee\u901a\u91cf\u5854\u548c\u53d7\u63a7\u73af\u5883\u8bbe\u65bd\u7684\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\uff1b2\uff09\u8bc4\u4f30\u591a\u79cd\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08LSTM\u3001\u65f6\u5e8fCNN\u3001Transformer\uff09\u7684\u78b3\u6c2e\u901a\u91cf\u9884\u6d4b\u6027\u80fd\uff1b3\uff09\u63a2\u7d22\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u62df\u6570\u636e\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6e29\u5ba4\u6c14\u4f53\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u78b3\u6c2e\u901a\u91cf\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u5982\u4f55\u5229\u7528\u6a21\u62df\u6570\u636e\u6539\u5584\u6a21\u578b\u5728\u771f\u5b9e\u89c2\u6d4b\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u51c6\u786e\u3001\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u519c\u4e1a\u751f\u6001\u7cfb\u7edf\u6a21\u578b\uff0c\u63a8\u8fdb\u5bf9\u751f\u6001\u7cfb\u7edf-\u6c14\u5019\u76f8\u4e92\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u4e3a\u519c\u4e1a\u6e29\u5ba4\u6c14\u4f53\u51cf\u6392\u7b56\u7565\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2602.02370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02370", "abs": "https://arxiv.org/abs/2602.02370", "authors": ["Uma Meleti", "Jeffrey J. Nirschl"], "title": "Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes", "comment": "Accepted for publication at the IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.", "AI": {"tldr": "SNGP\u6a21\u578b\u901a\u8fc7\u8c31\u5f52\u4e00\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u5c42\u6539\u8fdb\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548cOOD\u68c0\u6d4b\uff0c\u63d0\u5347\u4e34\u5e8a\u53ef\u4fe1\u5ea6", "motivation": "\u5f53\u524d\u6570\u5b57\u75c5\u7406\u5b66\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\u901a\u5e38\u8fc7\u4e8e\u81ea\u4fe1\u4e14\u6821\u51c6\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u4fe1\u4efb\u548c\u91c7\u7528\u3002\u533b\u7597\u5f71\u50cf\u5de5\u4f5c\u6d41\u7a0b\u9700\u8981\u5185\u5728\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7279\u6027\u6765\u51c6\u786e\u62d2\u7eddOOD\u8f93\u5165\u3002", "method": "\u5b9e\u73b0\u8c31\u5f52\u4e00\u5316\u795e\u7ecf\u9ad8\u65af\u8fc7\u7a0b\uff08SNGP\uff09\uff0c\u901a\u8fc7\u8c31\u5f52\u4e00\u5316\u548c\u7528\u9ad8\u65af\u8fc7\u7a0b\u5c42\u66ff\u6362\u6700\u7ec8\u5bc6\u96c6\u5c42\uff0c\u6539\u8fdb\u5355\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548cOOD\u68c0\u6d4b\u3002\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\uff08\u767d\u7ec6\u80de\u3001\u6dc0\u7c89\u6837\u6591\u5757\u3001\u7ed3\u76f4\u80a0\u7ec4\u7ec7\u75c5\u7406\u5b66\uff09\u7684\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30SNGP\u4e0e\u786e\u5b9a\u6027\u6a21\u578b\u548c\u8499\u7279\u5361\u6d1bdropout\u7684\u5bf9\u6bd4\u3002", "result": "SNGP\u5728\u5206\u5e03\u5185\u6027\u80fd\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548cOOD\u68c0\u6d4b\u3002\u5728\u4e09\u4e2a\u4efb\u52a1\u7684\u6240\u6709\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684OOD\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "SNGP\u53ca\u76f8\u5173\u6a21\u578b\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u7528\u6846\u67b6\uff0c\u652f\u6301\u5b89\u5168\u90e8\u7f72\u5e76\u5efa\u7acb\u4e0e\u75c5\u7406\u5b66\u5bb6\u7684\u4fe1\u4efb\u3002"}}
{"id": "2602.01619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01619", "abs": "https://arxiv.org/abs/2602.01619", "authors": ["Seyed Mohammad Hadi Hosseini", "Mahdieh Soleymani Baghshah"], "title": "SUSD: Structured Unsupervised Skill Discovery through State Factorization", "comment": "Accepted as a conference paper at ICLR 2026", "summary": "Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.", "AI": {"tldr": "SUSD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u56e0\u5b50\u5206\u89e3\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\u5e76\u4e3a\u4e0d\u540c\u56e0\u5b50\u5206\u914d\u6280\u80fd\u53d8\u91cf\uff0c\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u6280\u80fd\u53d1\u73b0\u548c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u53d1\u73b0\u7b80\u5355\u9759\u6001\u6280\u80fd\uff0c\u800c\u57fa\u4e8e\u8ddd\u79bb\u6700\u5927\u5316\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u4fc3\u8fdb\u52a8\u6001\u6280\u80fd\uff0c\u4f46\u65e0\u6cd5\u786e\u4fdd\u8986\u76d6\u6240\u6709\u53ef\u63a7\u56e0\u5b50\u6216\u5b9e\u4f53\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u73af\u5883\u7ec4\u5408\u7ed3\u6784\u3001\u53d1\u73b0\u66f4\u4e30\u5bcc\u591a\u6837\u6280\u80fd\u7684\u65b9\u6cd5\u3002", "method": "SUSD\u5c06\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u56e0\u5b50\uff08\u5982\u5bf9\u8c61\u6216\u53ef\u63a7\u5b9e\u4f53\uff09\uff0c\u4e3a\u4e0d\u540c\u56e0\u5b50\u5206\u914d\u72ec\u7acb\u7684\u6280\u80fd\u53d8\u91cf\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6280\u80fd\u53d1\u73b0\u3002\u91c7\u7528\u52a8\u6001\u6a21\u578b\u8ddf\u8e2a\u5404\u56e0\u5b50\u7684\u5b66\u4e60\u8fdb\u5ea6\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u8f6c\u5411\u672a\u5145\u5206\u63a2\u7d22\u7684\u56e0\u5b50\u3002", "result": "\u57281-10\u4e2a\u56e0\u5b50\u7684\u4e09\u4e2a\u73af\u5883\u4e2d\uff0cSUSD\u80fd\u591f\u53d1\u73b0\u591a\u6837\u590d\u6742\u7684\u6280\u80fd\uff0c\u5728\u56e0\u5b50\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u3002\u540c\u65f6\u83b7\u5f97\u56e0\u5b50\u5316\u7684\u6280\u80fd\u8868\u793a\uff0c\u652f\u6301\u5bf9\u5355\u4e2a\u5b9e\u4f53\u7684\u7ec6\u7c92\u5ea6\u89e3\u8026\u63a7\u5236\u3002", "conclusion": "SUSD\u901a\u8fc7\u5229\u7528\u73af\u5883\u7ec4\u5408\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\uff0c\u5176\u56e0\u5b50\u5316\u6280\u80fd\u8868\u793a\u4fbf\u4e8e\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u9ad8\u6548\u8bad\u7ec3\u7ec4\u5408\u4e0b\u6e38\u4efb\u52a1\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.02380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02380", "abs": "https://arxiv.org/abs/2602.02380", "authors": ["Yibin Wang", "Yuhang Zang", "Feng Han", "Jiazi Bu", "Yujie Zhou", "Cheng Jin", "Jiaqi Wang"], "title": "Unified Personalized Reward Model for Vision Generation", "comment": "Website: https://codegoat24.github.io/UnifiedReward/flex", "summary": "Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.", "AI": {"tldr": "\u63d0\u51faUnifiedReward-Flex\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u4e2a\u6027\u5316\u89c6\u89c9\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63a8\u7406\u89e3\u51b3\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5bf9\u5185\u5bb9\u7279\u5b9a\u89c6\u89c9\u7ebf\u7d22\u4e0d\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u901a\u5e38\u91c7\u7528\u5355\u4e00\u504f\u597d\u5206\u5e03\u6216\u56fa\u5b9a\u8bc4\u4f30\u6807\u51c6\uff0c\u5bf9\u5185\u5bb9\u7279\u5b9a\u7684\u89c6\u89c9\u7ebf\u7d22\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u4e0e\u4e3b\u89c2\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4eba\u7c7b\u504f\u597d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "method": "1. \u9996\u5148\u89e3\u91ca\u8bed\u4e49\u610f\u56fe\u5e76\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\uff1b2. \u52a8\u6001\u6784\u5efa\u5206\u5c42\u8bc4\u4f30\uff0c\u5728\u9884\u5b9a\u4e49\u548c\u81ea\u751f\u6210\u7684\u9ad8\u7ef4\u5ea6\u4e0b\u5b9e\u4f8b\u5316\u7ec6\u7c92\u5ea6\u6807\u51c6\uff1b3. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u7528\u95ed\u6e90VLM\u84b8\u998f\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u8fdb\u884cSFT\uff0c\u518d\u7528DPO\u4f18\u5316\u504f\u597d\u5bf9\u3002", "result": "\u5c06UnifiedReward-Flex\u96c6\u6210\u5230GRPO\u6846\u67b6\u4e2d\u8fdb\u884c\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\uff0c\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "UnifiedReward-Flex\u901a\u8fc7\u8026\u5408\u5956\u52b1\u5efa\u6a21\u4e0e\u7075\u6d3b\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5956\u52b1\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e2a\u6027\u5316\u89c6\u89c9\u751f\u6210\u5bf9\u9f50\u3002"}}
{"id": "2602.01626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01626", "abs": "https://arxiv.org/abs/2602.01626", "authors": ["Mehdi Setayesh", "Mahdi Beitollahi", "Yasser H. Khalil", "Hongliang Li"], "title": "Toward Enhancing Representation Learning in Federated Multi-Task Settings", "comment": "This paper has been accepted at ICLR 2026", "summary": "Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.", "AI": {"tldr": "FedMuscle\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Muscle\u635f\u5931\u51fd\u6570\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u5b66\u4e60\uff0c\u6709\u6548\u5904\u7406\u6a21\u578b\u548c\u4efb\u52a1\u7684\u5f02\u6784\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6a21\u578b\u540c\u6784\u6027\uff08\u5b8c\u5168\u6216\u90e8\u5206\u540c\u8d28\u6a21\u578b\uff09\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9650\u5236\u6027\u8f83\u5f3a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6a21\u578b\u548c\u4efb\u52a1\u5f02\u6784\u6027\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u9690\u79c1\u3002", "method": "\u63d0\u51faMuscle\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u80fd\u591f\u540c\u65f6\u5bf9\u9f50\u6240\u6709\u53c2\u4e0e\u6a21\u578b\u7684\u8868\u793a\u3002\u4e0e\u73b0\u6709\u7684\u6210\u5bf9\u5bf9\u9f50\u65b9\u6cd5\u4e0d\u540c\uff0cMuscle\u635f\u5931\u901a\u8fc7\u6700\u5927\u5316\u6240\u6709\u6a21\u578b\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u6355\u6349\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86FedMuscle\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedMuscle\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u548c\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "FedMuscle\u901a\u8fc7\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u800c\u975e\u5171\u4eab\u6a21\u578b\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u548c\u4efb\u52a1\u5f02\u6784\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02388", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02388", "abs": "https://arxiv.org/abs/2602.02388", "authors": ["Rajalaxmi Rajagopalan", "Debottam Dutta", "Yu-Lin Wei", "Romit Roy Choudhury"], "title": "Personalized Image Generation via Human-in-the-loop Bayesian Optimization", "comment": null, "summary": "Imagine Alice has a specific image $x^\\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\\ast$, even though the generative model has no information about $x^\\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMultiBO\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u6765\u4f18\u5316\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u5f25\u8865\u8bed\u8a00\u63d0\u793a\u7684\u5c40\u9650\u6027", "motivation": "\u5f53\u7528\u6237\u6709\u7279\u5b9a\u76ee\u6807\u56fe\u50cf\u65f6\uff0c\u4ec5\u9760\u8bed\u8a00\u63d0\u793a\u96be\u4ee5\u7cbe\u786e\u751f\u6210\uff0c\u4f46\u4eba\u7c7b\u4ecd\u80fd\u5224\u65ad\u54ea\u4e2a\u56fe\u50cf\u66f4\u63a5\u8fd1\u76ee\u6807\u3002\u9700\u8981\u5229\u7528\u8fd9\u79cd\u504f\u597d\u53cd\u9988\u6765\u7f29\u5c0f\u751f\u6210\u56fe\u50cf\u4e0e\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u8ddd", "method": "\u63d0\u51faMultiBO\uff08\u591a\u9009\u62e9\u504f\u597d\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u5f53\u524d\u6700\u4f73\u56fe\u50cf\u751f\u6210K\u4e2a\u65b0\u56fe\u50cf\uff1b2\uff09\u83b7\u53d6\u7528\u6237\u504f\u597d\u53cd\u9988\uff1b3\uff09\u7528\u53cd\u9988\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff1b4\uff09\u8fed\u4ee3B\u8f6e\u4f18\u5316", "result": "30\u540d\u7528\u6237\u7684\u5b9a\u6027\u8bc4\u5206\u548c5\u4e2a\u57fa\u7ebf\u7684\u5b9a\u91cf\u6307\u6807\u663e\u793a\uff0cMultiBO\u80fd\u5728\u6709\u9650\u53cd\u9988\u8f6e\u6b21\u5185\u663e\u8457\u63a5\u8fd1\u76ee\u6807\u56fe\u50cf\uff0c\u8bc1\u660e\u4eba\u7c7b\u591a\u9009\u62e9\u53cd\u9988\u80fd\u6709\u6548\u7528\u4e8e\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210", "conclusion": "\u901a\u8fc7\u591a\u8f6e\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u8d28\u91cf\uff0c\u5f25\u8865\u8bed\u8a00\u63d0\u793a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7cbe\u786e\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.01629", "categories": ["cs.LG", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.01629", "abs": "https://arxiv.org/abs/2602.01629", "authors": ["Renukanandan Tumu", "Aditya Singh", "Rahul Mangharam"], "title": "AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments", "comment": null, "summary": "Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \\textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.", "AI": {"tldr": "AdaptNC\uff1a\u8054\u5408\u5728\u7ebf\u81ea\u9002\u5e94\u975e\u5171\u5f62\u5206\u6570\u53c2\u6570\u548c\u5171\u5f62\u9608\u503c\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u73af\u5883\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4fdd\u5b88\u9884\u6d4b\u533a\u57df\u95ee\u9898", "motivation": "\u73b0\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b58\u5728\u5206\u5e03\u504f\u79fb\uff0c\u8fdd\u53cd\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u7684\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u73b0\u6709\u5728\u7ebfCP\u65b9\u6cd5\u4ec5\u8c03\u6574\u9608\u503c\u5bfc\u81f4\u9884\u6d4b\u533a\u57df\u8fc7\u4e8e\u4fdd\u5b88\u548c\u4f53\u79ef\u4f4e\u6548", "method": "\u63d0\u51faAdaptNC\u6846\u67b6\uff0c\u8054\u5408\u5728\u7ebf\u81ea\u9002\u5e94\u975e\u5171\u5f62\u5206\u6570\u53c2\u6570\u548c\u5171\u5f62\u9608\u503c\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u4f18\u5316\u5206\u6570\u51fd\u6570\uff0c\u5f15\u5165\u91cd\u653e\u7f13\u51b2\u673a\u5236\u7f13\u89e3\u5206\u6570\u8f6c\u6362\u671f\u95f4\u7684\u8986\u76d6\u4e0d\u7a33\u5b9a\u6027", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaptNC\u76f8\u6bd4\u4ec5\u8c03\u6574\u9608\u503c\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u533a\u57df\u4f53\u79ef\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u8986\u76d6\u6c34\u5e73", "conclusion": "AdaptNC\u901a\u8fc7\u8054\u5408\u81ea\u9002\u5e94\u975e\u5171\u5f62\u5206\u6570\u548c\u9608\u503c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u73af\u5883\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9884\u6d4b\u533a\u57df\u4fdd\u5b88\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6548\u7387"}}
{"id": "2602.02393", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02393", "abs": "https://arxiv.org/abs/2602.02393", "authors": ["Ruiqi Wu", "Xuanhua He", "Meng Cheng", "Tianyu Yang", "Yong Zhang", "Zhuoliang Kang", "Xunliang Cai", "Xiaoming Wei", "Chunle Guo", "Chongyi Li", "Ming-Ming Cheng"], "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory", "comment": "14 pages, 8 figures", "summary": "We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.", "AI": {"tldr": "Infinite-World\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u4fdd\u6301\u8d85\u8fc71000\u5e27\u7684\u8fde\u8d2f\u89c6\u89c9\u8bb0\u5fc6\uff0c\u901a\u8fc7\u5206\u5c42\u65e0\u59ff\u6001\u8bb0\u5fc6\u538b\u7f29\u5668\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u4f5c\u6807\u6ce8\u6a21\u5757\u89e3\u51b3\u771f\u5b9e\u89c6\u9891\u8bad\u7ec3\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u6548\u679c\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u9762\u4e34\u59ff\u6001\u4f30\u8ba1\u566a\u58f0\u548c\u89c6\u89d2\u91cd\u8bbf\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "method": "1. \u5206\u5c42\u65e0\u59ff\u6001\u8bb0\u5fc6\u538b\u7f29\u5668(HPMC)\uff1a\u9012\u5f52\u5c06\u5386\u53f2\u6f5c\u5728\u8868\u793a\u84b8\u998f\u4e3a\u56fa\u5b9a\u9884\u7b97\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\uff1b2. \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u4f5c\u6807\u6ce8\u6a21\u5757\uff1a\u5c06\u8fde\u7eed\u8fd0\u52a8\u79bb\u6563\u5316\u4e3a\u4e09\u6001\u903b\u8f91\uff0c\u6700\u5927\u5316\u5229\u7528\u539f\u59cb\u89c6\u9891\u6570\u636e\uff1b3. \u91cd\u8bbf\u5bc6\u96c6\u5fae\u8c03\u7b56\u7565\uff1a\u4f7f\u752830\u5206\u949f\u6570\u636e\u96c6\u6fc0\u6d3b\u957f\u8ddd\u79bb\u95ed\u73af\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\uff08\u5ba2\u89c2\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\uff09\u8868\u660e\uff0cInfinite-World\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u52a8\u4f5c\u53ef\u63a7\u6027\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "Infinite-World\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u538b\u7f29\u548c\u52a8\u4f5c\u6807\u6ce8\u673a\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u957f\u671f\u8fde\u8d2f\u89c6\u89c9\u8bb0\u5fc6\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7684\u4ea4\u4e92\u5f0f\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01635", "abs": "https://arxiv.org/abs/2602.01635", "authors": ["Jinwoo Park", "Hyeongwon Kang", "Seung Hun Han", "Pilsung Kang"], "title": "COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.", "AI": {"tldr": "COMET\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7801\u672c\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u5d4c\u5165\u65b9\u6cd5\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8865\u4e01\u7f16\u7801\u3001\u5411\u91cf\u91cf\u5316\u6838\u5fc3\u96c6\u548c\u5728\u7ebf\u7801\u672c\u9002\u5e94\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u5728\u8865\u4e01\u7ea7\u8868\u793a\u5b66\u4e60\u4e2d\u672a\u80fd\u5145\u5206\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u591a\u53d8\u91cf\u76f8\u5173\u6027\uff1b2) \u4f9d\u8d56\u5355\u4e00\u5c3a\u5ea6\u6a21\u5f0f\u9650\u5236\u4e86\u8de8\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u7684\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff1b3) \u4e13\u6ce8\u4e8e\u6b63\u5e38\u6570\u636e\u8868\u793a\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5bb9\u6613\u53d7\u5230\u5206\u5e03\u504f\u79fb\u7684\u5f71\u54cd\u3002", "method": "COMET\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u591a\u5c3a\u5ea6\u8865\u4e01\u7f16\u7801\uff1a\u5728\u591a\u4e2a\u8865\u4e01\u5c3a\u5ea6\u4e0a\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u53d8\u91cf\u95f4\u76f8\u5173\u6027\uff1b2) \u5411\u91cf\u91cf\u5316\u6838\u5fc3\u96c6\uff1a\u901a\u8fc7\u7801\u672c\u5b66\u4e60\u4ee3\u8868\u6027\u6b63\u5e38\u6a21\u5f0f\uff0c\u7ed3\u5408\u91cf\u5316\u8bef\u5dee\u548c\u8bb0\u5fc6\u8ddd\u79bb\u7684\u53cc\u91cd\u8bc4\u5206\u68c0\u6d4b\u5f02\u5e38\uff1b3) \u5728\u7ebf\u7801\u672c\u9002\u5e94\uff1a\u57fa\u4e8e\u7801\u672c\u6761\u76ee\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9002\u5e94\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCOMET\u572845\u4e2a\u8bc4\u4f30\u6307\u6807\u4e2d\u768436\u4e2a\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "COMET\u901a\u8fc7\u591a\u5c3a\u5ea6\u8868\u793a\u5b66\u4e60\u3001\u7801\u672c\u9a71\u52a8\u7684\u6b63\u5e38\u6a21\u5f0f\u5b66\u4e60\u548c\u5728\u7ebf\u81ea\u9002\u5e94\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.02401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02401", "abs": "https://arxiv.org/abs/2602.02401", "authors": ["Xinshun Wang", "Peiming Li", "Ziyi Wang", "Zhongbin Fang", "Zhichao Deng", "Songtao Wu", "Jason Li", "Mengyuan Liu"], "title": "Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation", "comment": null, "summary": "Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.", "AI": {"tldr": "Superman\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u7684\u8fd0\u52a8\u6807\u8bb0\u5668\u548c\u5355\u4e00MLLM\u67b6\u6784\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u4e0e\u57fa\u4e8e\u9aa8\u67b6\u7684\u65f6\u95f4\u8fd0\u52a8\u751f\u6210\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u8fd0\u52a8\u5206\u6790\u9886\u57df\u7684\u5206\u5272\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8fd0\u52a8\u5206\u6790\u9886\u57df\u5b58\u5728\u4e25\u91cd\u5206\u5272\uff1a1\uff09\u611f\u77e5\u6a21\u578b\u53ea\u80fd\u4ece\u89c6\u9891\u7406\u89e3\u8fd0\u52a8\u4f46\u4ec5\u8f93\u51fa\u6587\u672c\uff0c\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u4ece\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u611f\u77e5\uff1b2\uff09\u751f\u6210\u5f0fMLLM\u901a\u5e38\u5c40\u9650\u4e8e\u4f7f\u7528\u5bc6\u96c6\u53c2\u6570\u5316SMPL\u6a21\u578b\u7684\u5355\u5e27\u9759\u6001\u59ff\u6001\uff0c\u65e0\u6cd5\u5904\u7406\u65f6\u95f4\u8fd0\u52a8\uff1b3\uff09\u73b0\u6709\u8fd0\u52a8\u8bcd\u6c47\u4ec5\u57fa\u4e8e\u9aa8\u67b6\u6570\u636e\u6784\u5efa\uff0c\u5207\u65ad\u4e86\u4e0e\u89c6\u89c9\u9886\u57df\u7684\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u89c6\u89c9\u5f15\u5bfc\u7684\u8fd0\u52a8\u6807\u8bb0\u5668\uff0c\u5229\u75283D\u9aa8\u67b6\u4e0e\u89c6\u89c9\u6570\u636e\u4e4b\u95f4\u7684\u81ea\u7136\u51e0\u4f55\u5bf9\u9f50\uff0c\u4ece\u4e24\u79cd\u6a21\u6001\u8fdb\u884c\u8054\u5408\u5b66\u4e60\uff0c\u521b\u5efa\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u8fd0\u52a8\u8bcd\u6c47\uff1b2\uff09\u57fa\u4e8e\u8be5\u8fd0\u52a8\u8bed\u8a00\uff0c\u8bad\u7ec3\u5355\u4e00\u7edf\u4e00\u7684MLLM\u67b6\u6784\u5904\u7406\u6240\u6709\u4efb\u52a1\uff0c\u7075\u6d3b\u5904\u7406\u591a\u6837\u5316\u65f6\u95f4\u8f93\u5165\uff0c\u7edf\u4e00\u4ece\u89c6\u9891\u76843D\u9aa8\u67b6\u59ff\u6001\u4f30\u8ba1\uff08\u611f\u77e5\uff09\u4e0e\u57fa\u4e8e\u9aa8\u67b6\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u4e2d\u95f4\u5e27\u751f\u6210\uff08\u751f\u6210\uff09\u3002", "result": "\u5728Human3.6M\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7edf\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u8fd0\u52a8\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6216\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Superman\u5c55\u793a\u4e86\u4f7f\u7528\u9aa8\u67b6\u8fdb\u884c\u751f\u6210\u5f0f\u8fd0\u52a8\u5206\u6790\u7684\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u6846\u67b6\u89e3\u51b3\u4e86\u5f53\u524d\u8fd0\u52a8\u5206\u6790\u9886\u57df\u7684\u5206\u5272\u95ee\u9898\u3002"}}
{"id": "2602.01637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01637", "abs": "https://arxiv.org/abs/2602.01637", "authors": ["Sreenivasan Mohandas"], "title": "Chance-Constrained Inference for Hallucination Risk Control in Large Language Models", "comment": null, "summary": "Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \\emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \\emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.", "AI": {"tldr": "\u63d0\u51fa\u673a\u4f1a\u7ea6\u675f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u9a8c\u8bc1\u65b9\u6cd5\u5728\u90e8\u7f72\u65f6\u63a7\u5236\u5e7b\u89c9\u98ce\u9669\uff0c\u786e\u4fdd\u91cd\u590d\u4f7f\u7528\u4e2d\u7684\u6982\u7387\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u53ea\u964d\u4f4e\u5e73\u5747\u9519\u8bef\u7387\uff0c\u65e0\u6cd5\u63a7\u5236\u91cd\u590d\u4f7f\u7528\u4e2d\u7684\u5931\u8d25\u9891\u7387\uff0c\u9700\u8981\u63d0\u4f9b\u660e\u786e\u7684\u6982\u7387\u98ce\u9669\u4fdd\u8bc1", "method": "\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u90e8\u7f72\u65f6\u98ce\u9669\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u673a\u4f1a\u7ea6\u675f\u63a8\u7406\u6846\u67b6\uff0c\u4f7f\u7528\u987a\u5e8f\u3001\u968f\u65f6\u6709\u6548\u7684\u63a8\u7406\u7a0b\u5e8f\u81ea\u9002\u5e94\u9a8c\u8bc1\u53ef\u884c\u6027", "result": "\u5728NaturalQuestions\u98ce\u683c\u95ee\u9898\u548c\u53d7\u63a7\u591a\u8df3\u95ee\u7b54\u4e0a\u5b9e\u73b0\u53ef\u9760\u7684\u98ce\u9669\u63a7\u5236\uff0c\u65e9\u671f\u68c0\u6d4b\u4e0d\u53ef\u884c\u8f93\u5165\uff0c\u786e\u4fdd\u91cd\u590d\u4f7f\u7528\u4e0b\u7684\u5b89\u5168\u7ec4\u5408", "conclusion": "\u673a\u4f1a\u7ea6\u675f\u63a8\u7406\u4e3a\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u6982\u7387\u98ce\u9669\u4fdd\u8bc1\uff0c\u4f18\u4e8e\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u91cd\u590d\u4f7f\u7528"}}
{"id": "2602.02408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02408", "abs": "https://arxiv.org/abs/2602.02408", "authors": ["Jiaxing Qiu", "Kaihua Hou", "Roxana Daneshjou", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "ReasonEdit: Editing Vision-Language Models using Human Reasoning", "comment": null, "summary": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.", "AI": {"tldr": "ReasonEdit\uff1a\u9996\u4e2a\u652f\u6301\u7528\u6237\u89e3\u91ca\u63a8\u7406\u8fc7\u7a0b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u5668\uff0c\u901a\u8fc7\u4ee3\u7801\u672c\u5b58\u50a8\u4eba\u7c7b\u63a8\u7406\uff0c\u4f7f\u7528\u62d3\u6251\u5e73\u8861\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6cd5\u68c0\u7d22\u76f8\u5173\u4e8b\u5b9e\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u7f16\u8f91\u6027\u80fd", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u5668\u65e0\u6cd5\u5904\u7406\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u9700\u8981\u4eba\u7c7b\u548c\u6a21\u578b\u5bf9\u56fe\u50cf\u8fdb\u884c\u63a8\u7406\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u8fdb\u884c\u6a21\u578b\u7f16\u8f91\u7684\u65b0\u65b9\u6cd5", "method": "\u63d0\u51faReasonEdit\u6846\u67b6\uff1a1\uff09\u5141\u8bb8\u7528\u6237\u5728\u7f16\u8f91\u65f6\u63d0\u4f9b\u63a8\u7406\u89e3\u91ca\uff1b2\uff09\u5c06\u4eba\u7c7b\u63a8\u7406\u5b58\u50a8\u5728\u4ee3\u7801\u672c\u4e2d\uff1b3\uff09\u63a8\u7406\u65f6\u4f7f\u7528\u53d7\u7f51\u7edc\u79d1\u5b66\u542f\u53d1\u7684\u62d3\u6251\u5e73\u8861\u591a\u6a21\u6001\u5d4c\u5165\u65b9\u6cd5\u68c0\u7d22\u76f8\u5173\u4e8b\u5b9e", "result": "\u5728\u56db\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cReasonEdit\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u6027\u80fd\uff0c\u8bc1\u660e\u5728\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u4eba\u7c7b\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u7f16\u8f91\u6cdb\u5316\u80fd\u529b", "conclusion": "ReasonEdit\u662f\u9996\u4e2a\u652f\u6301\u63a8\u7406\u89e3\u91ca\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u63a8\u7406\u548c\u521b\u65b0\u7684\u5d4c\u5165\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7f16\u8f91\u6548\u679c\uff0c\u4e3a\u6a21\u578b\u7f16\u8f91\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2602.01642", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01642", "abs": "https://arxiv.org/abs/2602.01642", "authors": ["Matias D. Cattaneo", "Boris Shigida"], "title": "The Effect of Mini-Batch Noise on the Implicit Bias of Adam", "comment": null, "summary": "With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(\u03b2_1, \u03b2_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $\u03b2_1$, $\u03b2_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $\u03b2_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $\u03b2_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $\u03b2_1$. In particular, the commonly \"default\" pair $(\u03b2_1, \u03b2_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $\u03b2_1$ closer to $\u03b2_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.", "AI": {"tldr": "\u7814\u7a76Adam\u4f18\u5316\u5668\u4e2d\u52a8\u91cf\u8d85\u53c2\u6570(\u03b2\u2081, \u03b2\u2082)\u548c\u6279\u6b21\u5927\u5c0f\u5bf9\u591a\u8f6e\u8bad\u7ec3\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6279\u6b21\u5927\u5c0f\u4f1a\u6539\u53d8\u52a8\u91cf\u53c2\u6570\u5bf9\u6b63\u5219\u5316\u6548\u679c\u7684\u5f71\u54cd\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u9650\u800c\u8ba1\u7b97\u8d44\u6e90\u589e\u957f\uff0c\u591a\u8f6e\u8bad\u7ec3\u5728\u6df1\u5ea6\u5b66\u4e60\u5404\u9886\u57df\u91cd\u65b0\u53d8\u5f97\u91cd\u8981\u3002Adam\u4f18\u5316\u5668\u4f5c\u4e3a\u8bb8\u591a\u4efb\u52a1\uff08\u5982\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\uff09\u7684\u9996\u9009\uff0c\u5176\u52a8\u91cf\u8d85\u53c2\u6570(\u03b2\u2081, \u03b2\u2082)\u63a7\u5236\u8bb0\u5fc6\uff0c\u6279\u6b21\u5927\u5c0f\u63a7\u5236\u5c0f\u6279\u91cf\u566a\u58f0\u3002\u9700\u8981\u7406\u89e3\u5c0f\u6279\u91cf\u566a\u58f0\u5982\u4f55\u5f71\u54cdAdam\u4e2d\u8bb0\u5fc6\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u4ee5\u53ca\u8fd9\u4e0e\u6cdb\u5316\u5dee\u8ddd\u7684\u5173\u7cfb\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u5206\u6790\u5c0f\u6279\u91cf\u566a\u58f0\u5982\u4f55\u5f71\u54cdAdam\u4f18\u5316\u5668\u4e2d\u8bb0\u5fc6\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u7814\u7a76\u52a8\u91cf\u53c2\u6570(\u03b2\u2081, \u03b2\u2082)\u548c\u6279\u6b21\u5927\u5c0f\u5bf9\u635f\u5931\u51fd\u6570\u666f\u89c2\u4e2d\u66f4\u5c16\u9510\u6216\u66f4\u5e73\u5766\u533a\u57df\u7684\u504f\u7f6e\u5f71\u54cd\u3002\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u8fde\u63a5\u6279\u6b21\u89c4\u6a21\u53d8\u5316\u4e0e\u4e34\u754c\u6279\u6b21\u89c4\u6a21\u7684\u5173\u7cfb\uff0c\u5e76\u5728\u5373\u5c06\u8fc7\u62df\u5408\u7684\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u6279\u6b21\u5927\u5c0f\u4f1a\u6539\u53d8\u52a8\u91cf\u53c2\u6570\u5bf9\u6b63\u5219\u5316\u6548\u679c\u7684\u5f71\u54cd\u65b9\u5411\uff1a\u5728\u5927\u6279\u6b21\u65f6\uff0c\u8f83\u9ad8\u7684\u03b2\u2082\u4f1a\u589e\u52a0\u8bb0\u5fc6\u7684\u53cd\u6b63\u5219\u5316\u6548\u5e94\uff08\u635f\u5bb3\u6cdb\u5316\uff09\uff1b\u4f46\u968f\u7740\u6279\u6b21\u53d8\u5c0f\uff0c\u03b2\u2082\u5bf9\uff08\u53cd\uff09\u6b63\u5219\u5316\u7684\u4f9d\u8d56\u5173\u7cfb\u4f1a\u53cd\u8f6c\u3002\u03b2\u2081\u4e5f\u51fa\u73b0\u7c7b\u4f3c\u4f46\u65b9\u5411\u76f8\u53cd\u7684\u5355\u8c03\u6027\u53d8\u5316\u3002\u9ed8\u8ba4\u53c2\u6570(0.9, 0.999)\u5728\u5c0f\u6279\u6b21\u65f6\u8868\u73b0\u826f\u597d\uff1b\u5bf9\u4e8e\u5927\u6279\u6b21\uff0c\u5c06\u03b2\u2081\u8c03\u6574\u5230\u66f4\u63a5\u8fd1\u03b2\u2082\u7684\u503c\u5728\u591a\u8f6e\u8bad\u7ec3\u4e2d\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "\u6279\u6b21\u5927\u5c0f\u662f\u5f71\u54cdAdam\u4f18\u5316\u5668\u52a8\u91cf\u53c2\u6570\u6b63\u5219\u5316\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u6279\u6b21\u89c4\u6a21\u53d8\u5316\u4f1a\u5bfc\u81f4\u52a8\u91cf\u53c2\u6570\u5bf9\u6cdb\u5316\u6027\u80fd\u5f71\u54cd\u7684\u5355\u8c03\u6027\u53cd\u8f6c\u3002\u7406\u8bba\u63a8\u5bfc\u8868\u660e\u8fd9\u79cd\u53cd\u8f6c\u53d1\u751f\u7684\u6279\u6b21\u89c4\u6a21\u4e0e\u4e34\u754c\u6279\u6b21\u89c4\u6a21\u76f8\u5173\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e94\u6839\u636e\u6279\u6b21\u5927\u5c0f\u8c03\u6574Adam\u7684\u52a8\u91cf\u8d85\u53c2\u6570\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.02409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02409", "abs": "https://arxiv.org/abs/2602.02409", "authors": ["Abid Hassan", "Tuan Ngo", "Saad Shafiq", "Nenad Medvidovic"], "title": "Catalyst: Out-of-Distribution Detection via Elastic Scaling", "comment": null, "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($\u03b3$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $\u03b3$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.", "AI": {"tldr": "Catalyst\u662f\u4e00\u4e2a\u540e\u5904\u7406OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u5168\u5c40\u5e73\u5747\u6c60\u5316\u524d\u7279\u5f81\u56fe\u7684\u539f\u59cb\u901a\u9053\u7edf\u8ba1\u4fe1\u606f\uff0c\u901a\u8fc7\u52a8\u6001\u8ba1\u7b97\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u56e0\u5b50\u03b3\u6765\u5f39\u6027\u7f29\u653e\u73b0\u6709\u57fa\u7ebf\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347ID\u548cOOD\u5206\u5e03\u7684\u53ef\u5206\u79bb\u6027\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8f93\u51falogits\u6216\u5168\u5c40\u5e73\u5747\u6c60\u5316\u540e\u7684\u7279\u5f81\u5411\u91cf\uff0c\u4f46\u5ffd\u7565\u4e86\u5168\u5c40\u5e73\u5747\u6c60\u5316\u524d\u7279\u5f81\u56fe\u7684\u4e30\u5bcc\u539f\u59cb\u901a\u9053\u7edf\u8ba1\u4fe1\u606f\u3002\u8fd9\u4e9b\u88ab\u4e22\u5f03\u7684\u7edf\u8ba1\u4fe1\u606f\u5305\u542b\u91cd\u8981\u7684\u4e92\u8865\u4fe1\u53f7\uff0c\u53ef\u4ee5\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd\u3002", "method": "Catalyst\u6846\u67b6\u4ece\u5168\u5c40\u5e73\u5747\u6c60\u5316\u524d\u7684\u7279\u5f81\u56fe\u4e2d\u63d0\u53d6\u539f\u59cb\u901a\u9053\u7edf\u8ba1\u4fe1\u606f\uff08\u5982\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u6700\u5927\u6fc0\u6d3b\u503c\uff09\uff0c\u52a8\u6001\u8ba1\u7b97\u8f93\u5165\u76f8\u5173\u7684\u7f29\u653e\u56e0\u5b50\u03b3\uff0c\u7136\u540e\u5c06\u03b3\u4e0e\u73b0\u6709\u57fa\u7ebf\u5206\u6570\u8fdb\u884c\u4e58\u6cd5\u878d\u5408\uff0c\u5b9e\u73b0\"\u5f39\u6027\u7f29\u653e\"\uff0c\u4ece\u800c\u589e\u5f3aID\u548cOOD\u5206\u5e03\u7684\u53ef\u5206\u79bb\u6027\u3002", "result": "Catalyst\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\uff0c\u5728CIFAR-10\uff08ResNet-18\uff09\u4e0a\u5e73\u5747\u8bef\u62a5\u7387\u964d\u4f4e32.87%\uff0c\u5728CIFAR-100\uff08ResNet-18\uff09\u4e0a\u964d\u4f4e27.94%\uff0c\u5728ImageNet\uff08ResNet-50\uff09\u4e0a\u964d\u4f4e22.25%\u3002\u8be5\u6846\u67b6\u4e0elogit-based\u65b9\u6cd5\uff08\u5982Energy\u3001ReAct\u3001SCALE\uff09\u548c\u8ddd\u79bb\u68c0\u6d4b\u5668\uff08\u5982KNN\uff09\u90fd\u80fd\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "\u5168\u5c40\u5e73\u5747\u6c60\u5316\u524d\u7684\u7edf\u8ba1\u4fe1\u606f\u5177\u6709\u672a\u5f00\u53d1\u7684\u6f5c\u529b\uff0cCatalyst\u6846\u67b6\u4e0e\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u4e92\u8865\uff0c\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u88ab\u5ffd\u7565\u7684\u4fe1\u53f7\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2602.01643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01643", "abs": "https://arxiv.org/abs/2602.01643", "authors": ["Xichen Sun", "Wentao Wei", "Jiahua Rao", "Jiancong Xie", "Yuedong Yang"], "title": "De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion", "comment": null, "summary": "Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.", "AI": {"tldr": "MBGen\uff1a\u57fa\u4e8e\u591a\u4f53\u589e\u5f3a\u6269\u6563\u6846\u67b6\uff0c\u4ece\u8d28\u8c31\u6570\u636e\u751f\u6210\u5206\u5b50\u7ed3\u6784\uff0c\u901a\u8fc7\u591a\u4f53\u6ce8\u610f\u529b\u673a\u5236\u548c\u9ad8\u9636\u8fb9\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u5206\u5b50\u751f\u6210\u548c\u5f02\u6784\u4f53\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8d28\u8c31\u5206\u5b50\u7ed3\u6784\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u539f\u5b50\u4e2d\u5fc3\u548c\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\uff0c\u5ffd\u7565\u4e86\u9ad8\u9636\u8fb9\u76f8\u4e92\u4f5c\u7528\uff0c\u65e0\u6cd5\u7cfb\u7edf\u6355\u6349\u591a\u4f53\u7279\u5f81\uff0c\u9650\u5236\u4e86\u590d\u6742\u5f02\u6784\u4f53\u548c\u975e\u5c40\u90e8\u788e\u88c2\u673a\u5236\u89e3\u6790\u80fd\u529b\u3002", "method": "\u63d0\u51faMBGen\u591a\u4f53\u589e\u5f3a\u6269\u6563\u6846\u67b6\uff0c\u96c6\u6210\u591a\u4f53\u6ce8\u610f\u529b\u673a\u5236\u548c\u9ad8\u9636\u8fb9\u5efa\u6a21\uff0c\u5145\u5206\u5229\u7528MS/MS\u8c31\u56fe\u4e2d\u7f16\u7801\u7684\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\uff0c\u5b9e\u73b0\u51c6\u786e\u7684\u65b0\u5206\u5b50\u4ece\u5934\u751f\u6210\u548c\u5f02\u6784\u4f53\u533a\u5206\u3002", "result": "\u5728NPLIB1\u548cMassSpecGym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMBGen\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u5e45\u5ea6\u9ad8\u8fbe230%\uff0c\u6709\u6548\u6355\u6349\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\uff0c\u5bf9\u590d\u6742\u5f02\u6784\u4f53\u548c\u975e\u5c40\u90e8\u788e\u88c2\u4fe1\u606f\u8868\u73b0\u51fa\u589e\u5f3a\u654f\u611f\u6027\u3002", "conclusion": "\u591a\u4f53\u5efa\u6a21\u5728\u8d28\u8c31\u5206\u5b50\u7ed3\u6784\u751f\u6210\u4e2d\u5177\u6709\u91cd\u8981\u79d1\u5b66\u4ef7\u503c\u548c\u5b9e\u9645\u6548\u7528\uff0cMBGen\u6846\u67b6\u901a\u8fc7\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u751f\u6210\u51c6\u786e\u6027\u548c\u5f02\u6784\u4f53\u5206\u8fa8\u80fd\u529b\u3002"}}
{"id": "2602.02426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02426", "abs": "https://arxiv.org/abs/2602.02426", "authors": ["Simon-Olivier Duguay", "Hugo Baudchon", "Etienne Lalibert\u00e9", "Helene Muller-Landau", "Gonzalo Rivas-Torres", "Arthur Ouaknine"], "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond", "comment": "22 pages, 8 figures", "summary": "Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.", "AI": {"tldr": "\u63d0\u51faSelvaMask\u70ed\u5e26\u68ee\u6797\u6570\u636e\u96c6\u548c\u68c0\u6d4b-\u5206\u5272\u6d41\u7a0b\uff0c\u5728\u70ed\u5e26\u68ee\u6797\u6811\u51a0\u5206\u5272\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u70ed\u5e26\u68ee\u6797\u5bf9\u5168\u7403\u751f\u6001\u5e73\u8861\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6811\u51a0\u5206\u5272\u65b9\u6cd5\u5728\u70ed\u5e26\u68ee\u6797\u4e2d\u6027\u80fd\u4f4e\u4e0b\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6", "method": "1) \u521b\u5efa\u5305\u542b8,800\u591a\u4e2a\u624b\u5de5\u6807\u6ce8\u6811\u51a0\u7684SelvaMask\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u6a21\u5757\u5316\u68c0\u6d4b-\u5206\u5272\u6d41\u7a0b\uff0c\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7684\u68c0\u6d4b\u63d0\u793a\u5668\u9002\u914d\u89c6\u89c9\u57fa\u7840\u6a21\u578b", "result": "\u5728\u5bc6\u96c6\u70ed\u5e26\u68ee\u6797\u4e2d\u8d85\u8d8a\u96f6\u6837\u672c\u901a\u7528\u6a21\u578b\u548c\u5168\u76d1\u7763\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u5728\u5916\u90e8\u70ed\u5e26\u548c\u6e29\u5e26\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b", "conclusion": "SelvaMask\u65e2\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u4e5f\u662f\u5b9e\u73b0\u5e7f\u4e49\u68ee\u6797\u76d1\u6d4b\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00"}}
{"id": "2602.01644", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01644", "abs": "https://arxiv.org/abs/2602.01644", "authors": ["Gloria Felicia", "Nolan Bryant", "Handi Putra", "Ayaan Gazali", "Eliel Lobo", "Esteban Rojas"], "title": "From Perception to Action: Spatial AI Agents and World Models", "comment": "61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models", "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u63a5\u667a\u80fd\u4f53\u80fd\u529b\u548c\u7a7a\u95f4\u4efb\u52a1\u7684\u4e09\u8f74\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5f3a\u8c03\u7a7a\u95f4\u667a\u80fd\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\u548c\u516d\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5173\u6ce8\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u8981\u4e48\u5173\u6ce8\u7a7a\u95f4\u9886\u57df\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e24\u79cd\u4e92\u8865\u80fd\u529b\u7edf\u4e00\u8d77\u6765\u7684\u6846\u67b6\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u9886\u57df\u7684\u6210\u529f\u65e0\u6cd5\u76f4\u63a5\u8fc1\u79fb\u5230\u7269\u7406\u4e16\u754c\uff0c\u7a7a\u95f4\u667a\u80fd\uff08\u611f\u77e53D\u7ed3\u6784\u3001\u63a8\u7406\u7269\u4f53\u5173\u7cfb\u3001\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u884c\u52a8\uff09\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf92000\u591a\u7bc7\u8bba\u6587\u7684\u5168\u9762\u7efc\u8ff0\uff08\u5f15\u7528742\u7bc7\u9876\u7ea7\u4f1a\u8bae\u8bba\u6587\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u63a5\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u7a7a\u95f4\u4efb\u52a1\u7684\u4e09\u8f74\u5206\u7c7b\u6cd5\u3002\u533a\u5206\u4e86\u7a7a\u95f4\u57fa\u7840\uff08\u51e0\u4f55\u548c\u7269\u7406\u7684\u5ea6\u91cf\u7406\u89e3\uff09\u4e0e\u7b26\u53f7\u57fa\u7840\uff08\u56fe\u50cf\u4e0e\u6587\u672c\u5173\u8054\uff09\uff0c\u5e76\u8ba4\u4e3a\u4ec5\u611f\u77e5\u65e0\u6cd5\u8d4b\u4e88\u667a\u80fd\u4f53\u80fd\u529b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u5bf9\u957f\u65f6\u7a0b\u7a7a\u95f4\u4efb\u52a1\u5f88\u91cd\u8981\uff1b(2) GNN-LLM\u6574\u5408\u662f\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff1b(3) \u4e16\u754c\u6a21\u578b\u5bf9\u4e8e\u8de8\u5fae\u89c2\u5230\u5b8f\u89c2\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u63d0\u51fa\u4e86\u516d\u5927\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u9700\u8981\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6807\u51c6\u5316\u8de8\u9886\u57df\u8bc4\u4f30\u3002\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7edf\u4e00\u788e\u7247\u5316\u7684\u7814\u7a76\u52aa\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u7b49\u9886\u57df\u4e0b\u4e00\u4ee3\u7a7a\u95f4\u611f\u77e5\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.02437", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02437", "abs": "https://arxiv.org/abs/2602.02437", "authors": ["Dianyi Wang", "Chaofan Ma", "Feng Han", "Size Wu", "Wei Song", "Yibin Wang", "Zhixiong Zhang", "Tianhang Wang", "Siyuan Wang", "Zhongyu Wei", "Jiaqi Wang"], "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing", "comment": null, "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.", "AI": {"tldr": "UniReason\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u63a8\u7406\u8303\u5f0f\u5c06\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u6574\u5408\u8d77\u6765\uff0c\u6a21\u4eff\u4eba\u7c7b\u5148\u89c4\u5212\u540e\u7ec6\u5316\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u7684\u590d\u6742\u5408\u6210\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u901a\u5e38\u5c06\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u89c6\u4e3a\u5b64\u7acb\u7684\u80fd\u529b\u800c\u975e\u76f8\u4e92\u5173\u8054\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u63d0\u51faUniReason\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u63a8\u7406\u8303\u5f0f\uff1a1) \u5c06\u751f\u6210\u89c6\u4e3a\u4e16\u754c\u77e5\u8bc6\u589e\u5f3a\u7684\u89c4\u5212\uff0c\u6ce8\u5165\u9690\u5f0f\u7ea6\u675f\uff1b2) \u5229\u7528\u7f16\u8f91\u80fd\u529b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u5316\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u7ea0\u6b63\u89c6\u89c9\u9519\u8bef\u3002\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u63a8\u7406\u4e2d\u5fc3\u6570\u636e\u96c6\uff08\u7ea630\u4e07\u6837\u672c\uff09\uff0c\u6db5\u76d6\u4e94\u4e2a\u4e3b\u8981\u77e5\u8bc6\u9886\u57df\uff0c\u4ee5\u53ca\u4ee3\u7406\u751f\u6210\u7684\u89c6\u89c9\u81ea\u6211\u7ea0\u6b63\u8bed\u6599\u5e93\u3002", "result": "UniReason\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982WISE\u3001KrisBench\u548cUniREditBench\uff09\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u901a\u7528\u5408\u6210\u80fd\u529b\u3002", "conclusion": "UniReason\u901a\u8fc7\u7edf\u4e00\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4e3a\u590d\u6742\u591a\u6a21\u6001\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u63a8\u7406\u6846\u67b6\u3002"}}
{"id": "2602.01651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01651", "abs": "https://arxiv.org/abs/2602.01651", "authors": ["Zichao Wei"], "title": "On the Spatiotemporal Dynamics of Generalization in Neural Networks", "comment": null, "summary": "Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSEAD\u67b6\u6784\uff0c\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u957f\u5ea6\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u52a0\u6cd5\u7b49\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5b8c\u7f8e\u6cdb\u5316", "motivation": "\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u50cf\u4eba\u7c7b\u90a3\u6837\u5c06\u52a0\u6cd5\u89c4\u5219\u4ece16\u4f4d\u6570\u6cdb\u5316\u523032\u4f4d\u6570\uff0c\u8fd9\u4e0d\u4ec5\u662f\u5de5\u7a0b\u95ee\u9898\uff0c\u800c\u662f\u8fdd\u53cd\u4e86\u7269\u7406\u57fa\u672c\u5047\u8bbe\u3002\u9700\u8981\u4ece\u7269\u7406\u89d2\u5ea6\u91cd\u65b0\u601d\u8003\u8ba1\u7b97\u7cfb\u7edf\u7684\u57fa\u672c\u7ea6\u675f\u3002", "method": "\u4ece\u7269\u7406\u89d2\u5ea6\u63d0\u51fa\u4e09\u4e2a\u7ea6\u675f\u6761\u4ef6\uff1a\u5c40\u90e8\u6027\u3001\u5bf9\u79f0\u6027\u3001\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u51faSEAD\u67b6\u6784\u2014\u2014\u4e00\u79cd\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff0c\u901a\u8fc7\u5c40\u90e8\u5377\u79ef\u89c4\u5219\u8fed\u4ee3\u76f4\u5230\u6536\u655b\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff1a1) \u5947\u5076\u6027\u4efb\u52a1\u5b9e\u73b0\u5b8c\u7f8e\u957f\u5ea6\u6cdb\u5316\uff1b2) \u52a0\u6cd5\u4efb\u52a1\u4ece16\u4f4d\u5230100\u4e07\u4f4d\u5b9e\u73b0100%\u51c6\u786e\u7387\uff1b3) Rule 110\u4efb\u52a1\u5b66\u4e60\u56fe\u7075\u5b8c\u5907\u7ec6\u80de\u81ea\u52a8\u673a\u800c\u65e0\u8f68\u8ff9\u53d1\u6563\u3002", "conclusion": "\u7edf\u8ba1\u5b66\u4e60\u4e0e\u903b\u8f91\u63a8\u7406\u4e4b\u95f4\u7684\u9e3f\u6c9f\u53ef\u4ee5\u901a\u8fc7\u5c0a\u91cd\u8ba1\u7b97\u7684\u7269\u7406\u7279\u6027\u6765\u5f25\u5408\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u6269\u5927\u53c2\u6570\u89c4\u6a21\u3002SEAD\u67b6\u6784\u4e3a\u6784\u5efa\u53ef\u6cdb\u5316\u7684\u8ba1\u7b97\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2602.02471", "categories": ["cs.CV", "cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.02471", "abs": "https://arxiv.org/abs/2602.02471", "authors": ["Edwin Kys", "Febian Febian"], "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network", "comment": "8 pages, 3 figures, 1 table", "summary": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \\pm 0.036$ versus $0.732 \\pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSwin U-Net\u7684\u95e8\u63a7\u591a\u5934Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5207\u7247\u7ea7\u7ed3\u6784\u68c0\u6d4b\u95e8\u63a7\u5206\u5272\u9884\u6d4b\uff0c\u6709\u6548\u6291\u5236\u653e\u7597\u81ea\u52a8\u5206\u5272\u4e2d\u7684\u89e3\u5256\u5b66\u4e0d\u5408\u7406\u5047\u9633\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u5206\u5272\u5728\u653e\u7597\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u5728\u7f3a\u4e4f\u76ee\u6807\u7ed3\u6784\u7684\u5207\u7247\u4e2d\u4f1a\u4ea7\u751f\u89e3\u5256\u5b66\u4e0d\u5408\u7406\u7684\u5047\u9633\u6027\uff08\u5e7b\u89c9\uff09\u3002\u9700\u8981\u63d0\u9ad8\u5206\u5272\u7684\u89e3\u5256\u5b66\u5408\u7406\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8eSwin U-Net\u7684\u95e8\u63a7\u591a\u5934Transformer\u67b6\u6784\uff0c\u589e\u5f3a\u5207\u7247\u95f4\u4e0a\u4e0b\u6587\u96c6\u6210\u548c\u5e76\u884c\u68c0\u6d4b\u5934\u3002\u8054\u5408\u6267\u884c\u5207\u7247\u7ea7\u7ed3\u6784\u68c0\u6d4b\uff08\u901a\u8fc7MLP\uff09\u548c\u50cf\u7d20\u7ea7\u5206\u5272\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u6d41\uff09\u3002\u68c0\u6d4b\u8f93\u51fa\u95e8\u63a7\u5206\u5272\u9884\u6d4b\u4ee5\u6291\u5236\u89e3\u5256\u65e0\u6548\u5207\u7247\u4e2d\u7684\u5047\u9633\u6027\uff0c\u8bad\u7ec3\u4f7f\u7528\u5207\u7247\u7ea7Tversky\u635f\u5931\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "\u5728Prostate-Anatomical-Edge-Cases\u6570\u636e\u96c6\u4e0a\uff0c\u95e8\u63a7\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u95e8\u63a7\u57fa\u7ebf\uff0c\u5e73\u5747Dice\u635f\u5931\u4e3a0.013\u00b10.036 vs 0.732\u00b10.314\u3002\u68c0\u6d4b\u6982\u7387\u4e0e\u89e3\u5256\u5b58\u5728\u5f3a\u76f8\u5173\uff0c\u6709\u6548\u6d88\u9664\u865a\u5047\u5206\u5272\u3002\u975e\u95e8\u63a7\u6a21\u578b\u5728\u6240\u6709\u5207\u7247\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u53d8\u5f02\u6027\u548c\u6301\u7eed\u5047\u9633\u6027\u3002", "conclusion": "\u68c0\u6d4b\u95e8\u63a7\u589e\u5f3a\u4e86\u81ea\u52a8\u5206\u5272\u5e94\u7528\u7684\u9c81\u68d2\u6027\u548c\u89e3\u5256\u5b66\u5408\u7406\u6027\uff0c\u5728\u4e0d\u5f71\u54cd\u6709\u6548\u5207\u7247\u5206\u5272\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u5e7b\u89c9\u9884\u6d4b\uff0c\u4e3a\u63d0\u9ad8\u4e34\u5e8a\u653e\u7597\u81ea\u52a8\u52fe\u753b\u5de5\u4f5c\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.01658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01658", "abs": "https://arxiv.org/abs/2602.01658", "authors": ["Seyed Mohammad Hadi Hosseini", "Amir Najafi", "Mahdieh Soleymani Baghshah"], "title": "Efficient Adversarial Attacks on High-dimensional Offline Bandits", "comment": "Accepted at ICLR 2026 Conference", "summary": "Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...", "AI": {"tldr": "\u7814\u7a76\u79bb\u7ebfbandit\u8bc4\u4f30\u5728\u5956\u52b1\u6a21\u578b\u88ab\u5bf9\u6297\u6027\u6270\u52a8\u65f6\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u9ad8\u7ef4\u5e94\u7528\u4e2d\u5373\u4f7f\u5fae\u5c0f\u6270\u52a8\u4e5f\u80fd\u5b8c\u5168\u6539\u53d8bandit\u884c\u4e3a", "motivation": "\u867d\u7136\u79bb\u7ebfbandit\u8bc4\u4f30\u5df2\u6210\u4e3a\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u653b\u51fb\u8005\u6270\u52a8\u5956\u52b1\u6a21\u578b\u800c\u975e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b", "method": "\u63d0\u51fa\u65b0\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u4ece\u7406\u8bba\u5206\u6790\u7ebf\u6027\u5956\u52b1\u51fd\u6570\u6269\u5c55\u5230\u975e\u7ebf\u6027ReLU\u795e\u7ecf\u7f51\u7edc\uff0c\u5728Hugging Face\u7684\u4e24\u4e2a\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u5668\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u663e\u793a\u5fae\u5c0f\u4e14\u96be\u4ee5\u5bdf\u89c9\u7684\u6743\u91cd\u6270\u52a8\u5c31\u80fd\u663e\u8457\u6539\u53d8bandit\u884c\u4e3a\uff0c\u7406\u8bba\u8bc1\u660e\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u653b\u51fb\u6240\u9700\u6270\u52a8\u8303\u6570\u4f1a\u964d\u4f4e\uff0c\u4f7f\u56fe\u50cf\u8bc4\u4f30\u7b49\u73b0\u4ee3\u5e94\u7528\u7279\u522b\u8106\u5f31", "conclusion": "\u79bb\u7ebfbandit\u8bc4\u4f30\u5bf9\u5956\u52b1\u6a21\u578b\u7684\u5bf9\u6297\u6027\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u5b89\u5168\u5a01\u80c1"}}
{"id": "2602.02493", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02493", "abs": "https://arxiv.org/abs/2602.02493", "authors": ["Zehong Ma", "Ruihan Xu", "Shiliang Zhang"], "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss", "comment": "Project Pages: https://zehong-ma.github.io/PixelGen/", "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.", "AI": {"tldr": "PixelGen\uff1a\u4e00\u79cd\u7b80\u5355\u7684\u50cf\u7d20\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u76d1\u7763\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u56fe\u50cf\uff0c\u65e0\u9700VAE\u6216\u6f5c\u5728\u8868\u793a\uff0c\u5728ImageNet-256\u4e0a\u8fbe\u5230FID 5.11", "motivation": "\u73b0\u6709\u7684\u50cf\u7d20\u6269\u6563\u65b9\u6cd5\u96be\u4ee5\u4f18\u5316\u5305\u542b\u8bb8\u591a\u611f\u77e5\u65e0\u5173\u4fe1\u53f7\u7684\u9ad8\u7ef4\u50cf\u7d20\u6d41\u5f62\uff0c\u5bfc\u81f4\u6027\u80fd\u843d\u540e\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u7b80\u5355\u4f46\u66f4\u5f3a\u5927\u7684\u751f\u6210\u8303\u5f0f\uff0c\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u5de5\u4f5c\uff0c\u907f\u514dVAE\u5f15\u5165\u7684\u4f2a\u5f71\u548c\u74f6\u9888\u3002", "method": "\u63d0\u51faPixelGen\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u4e92\u8865\u7684\u611f\u77e5\u635f\u5931\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u5b66\u4e60\u66f4\u6709\u610f\u4e49\u7684\u611f\u77e5\u6d41\u5f62\uff1aLPIPS\u635f\u5931\u4fc3\u8fdb\u5b66\u4e60\u66f4\u597d\u7684\u5c40\u90e8\u6a21\u5f0f\uff0c\u57fa\u4e8eDINO\u7684\u611f\u77e5\u635f\u5931\u589e\u5f3a\u5168\u5c40\u8bed\u4e49\u3002\u8be5\u65b9\u6cd5\u65e0\u9700VAE\u3001\u6f5c\u5728\u8868\u793a\u6216\u8f85\u52a9\u9636\u6bb5\u3002", "result": "PixelGen\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u6f5c\u5728\u6269\u6563\u57fa\u7ebf\uff0c\u5728ImageNet-256\u4e0a\u4e0d\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u4ec5\u752880\u4e2a\u8bad\u7ec3\u5468\u671f\u5c31\u8fbe\u5230\u4e86FID 5.11\uff0c\u5728\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u663e\u793a\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u80fd\uff0cGenEval\u5f97\u5206\u4e3a0.79\u3002", "conclusion": "PixelGen\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u4f46\u66f4\u5f3a\u5927\u7684\u751f\u6210\u8303\u5f0f\uff0c\u65e0\u9700VAE\u3001\u6f5c\u5728\u8868\u793a\u6216\u8f85\u52a9\u9636\u6bb5\uff0c\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u5de5\u4f5c\uff0c\u901a\u8fc7\u611f\u77e5\u76d1\u7763\u5b9e\u73b0\u4e86\u4f18\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.01667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01667", "abs": "https://arxiv.org/abs/2602.01667", "authors": ["Siu Lun Chau", "Soroush H. Zargarbashi", "Yusuf Sale", "Michele Caprio"], "title": "Quantifying Epistemic Predictive Uncertainty in Conformal Prediction", "comment": "42 pages", "summary": "We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-\u03b1$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \\emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u4e2d\u91cf\u5316\u8ba4\u77e5\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u5e73\u5747\u4e0d\u7cbe\u786e\u5ea6\u7684\u8ba1\u7b97\u9ad8\u6548\u65b9\u6cd5\uff0c\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u9009\u62e9\u6027\u5206\u7c7b\u4e2d\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u9884\u6d4b\u533a\u57df\u5927\u5c0f\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u533a\u57df\u5927\u5c0f\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u91cf\u5316\u6a21\u578b\u591a\u91cd\u6027\u5e26\u6765\u7684\u8ba4\u77e5\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6d4b\u91cf\u9884\u6d4b\u5206\u5e03\u96c6\u5408\u4e2d\u4fe1\u606f\u51b2\u7a81\u7a0b\u5ea6\u7684\u66f4\u7cbe\u7ec6\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u8bf1\u5bfc\u7684\u9884\u6d4b\u5206\u5e03\u96c6\u5408\uff08credal set\uff09\uff0c\u63d0\u51fa\u6700\u5927\u5e73\u5747\u4e0d\u7cbe\u786e\u5ea6\u4f5c\u4e3a\u8ba4\u77e5\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u6307\u6807\u3002\u8bc1\u660e\u5728split CP\u4e2d\u9884\u6d4b\u533a\u57df\u4e0ecredal set\u4e2d\u6240\u6709\u5206\u5e03\u7684\u6982\u7387\u4e0b\u754c\u7b49\u4ef7\uff0c\u5e76\u5f00\u53d1\u8ba1\u7b97\u9ad8\u6548\u7684\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u9009\u62e9\u6027\u5206\u7c7b\u5b9e\u9a8c\u4e2d\uff0c\u63d0\u51fa\u7684\u8ba4\u77e5\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u6bd4\u4ec5\u4f9d\u8d56\u9884\u6d4b\u533a\u57df\u5927\u5c0f\u7684\u65b9\u6cd5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u53ef\u4ee5\u4f5c\u4e3a\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u7684\u6700\u5927\u5e73\u5747\u4e0d\u7cbe\u786e\u5ea6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u91cf\u5316\u6a21\u578b\u591a\u91cd\u6027\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u63d0\u4f9b\u66f4\u7cbe\u7ec6\u7684\u6307\u5bfc\u3002"}}
{"id": "2602.01668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01668", "abs": "https://arxiv.org/abs/2602.01668", "authors": ["Qianyang Li", "Xingjun Zhang", "Shaoxun Wang", "Jia Wei", "Yueqi Xing"], "title": "ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting", "comment": null, "summary": "Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba", "AI": {"tldr": "ASGMamba\u662f\u4e00\u4e2a\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8d85\u7b97\u73af\u5883\u7684\u9ad8\u6548\u957f\u65f6\u5e8f\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u8c31\u95e8\u63a7\u673a\u5236\u548cMamba\u4e3b\u5e72\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1aTransformer\u6a21\u578b\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u957f\u5e8f\u5217\u7684\u53ef\u6269\u5c55\u6027\uff1b\u800c\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u96be\u4ee5\u533a\u5206\u6709\u4ef7\u503c\u4fe1\u53f7\u548c\u9ad8\u9891\u566a\u58f0\uff0c\u5bfc\u81f4\u72b6\u6001\u5bb9\u91cf\u6d6a\u8d39\u3002\u9700\u8981\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8d85\u7b97\u73af\u5883\u8bbe\u8ba1\u9ad8\u6548\u9884\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51faASGMamba\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u8c31\u95e8\u63a7\u673a\u5236\uff0c\u57fa\u4e8e\u5c40\u90e8\u8c31\u80fd\u91cf\u52a8\u6001\u8fc7\u6ee4\u566a\u58f0\uff1b2\uff09Mamba\u4e3b\u5e72\u4e13\u6ce8\u4e8e\u9c81\u68d2\u7684\u65f6\u95f4\u52a8\u6001\uff1b3\uff09\u5206\u5c42\u591a\u5c3a\u5ea6\u67b6\u6784\u548c\u53d8\u91cf\u7279\u5b9a\u7684\u8282\u70b9\u5d4c\u5165\uff0c\u6355\u6349\u591a\u6837\u7269\u7406\u7279\u6027\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u4e25\u683c\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u957f\u65f6\u5e8f\u4efb\u52a1\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u6210\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u541e\u5410\u91cf\u9884\u6d4b\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ASGMamba\u4e3a\u8d44\u6e90\u53d7\u9650\u8d85\u7b97\u73af\u5883\u4e2d\u7684\u957f\u65f6\u5e8f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c31\u95e8\u63a7\u548cMamba\u67b6\u6784\u7684\u534f\u540c\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.01682", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01682", "abs": "https://arxiv.org/abs/2602.01682", "authors": ["Taihei Oki", "Shinsaku Sakaue"], "title": "Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets", "comment": null, "summary": "We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\\log T)$, as well as a finite but exponentially large bound of $\\exp(O(d\\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $\u03a9(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u9006\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5f53\u53ef\u884c\u96c6\u4e3aM-\u51f8\u96c6\u65f6\uff0c\u53ef\u4ee5\u83b7\u5f97O(d log d)\u7684\u6709\u9650\u9057\u61be\u754c\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u9006\u7ebf\u6027\u4f18\u5316\uff08\u4e5f\u79f0\u4e3a\u4e0a\u4e0b\u6587\u63a8\u8350\uff09\u4e2d\uff0c\u5b66\u4e60\u8005\u9700\u8981\u4ece\u968f\u65f6\u95f4\u53d8\u5316\u7684\u53ef\u884c\u96c6\u4e2d\u89c2\u5bdf\u6700\u4f18\u52a8\u4f5c\uff0c\u63a8\u65ad\u4ee3\u7406\u7684\u9690\u85cf\u76ee\u6807\u5411\u91cf\u3002\u4e4b\u524d\u7684\u7814\u7a76\u5efa\u7acb\u4e86O(d log T)\u7684\u9057\u61be\u754c\u548c\u6307\u6570\u7ea7\u7684\u6709\u9650\u9057\u61be\u754c\uff0c\u4f46\u662f\u5426\u5b58\u5728\u591a\u9879\u5f0f\u7ea7\u522b\u7684\u6709\u9650\u9057\u61be\u754c\u4e00\u76f4\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408M-\u51f8\u96c6\u4e0a\u6700\u4f18\u89e3\u7684\u7ed3\u6784\u7279\u5f81\u548c\u51e0\u4f55\u4f53\u79ef\u8bba\u8bc1\u65b9\u6cd5\u3002\u5bf9\u4e8e\u5bf9\u6297\u6027\u53cd\u9988\uff0c\u901a\u8fc7\u76d1\u6d4b\u89c2\u5bdf\u53cd\u9988\u8bf1\u5bfc\u7684\u6709\u5411\u56fe\u6765\u81ea\u9002\u5e94\u68c0\u6d4b\u8150\u8d25\uff0c\u65e0\u9700\u4e8b\u5148\u77e5\u9053\u8150\u8d25\u8f6e\u6570C\u3002", "result": "\u5f53\u53ef\u884c\u96c6\u4e3aM-\u51f8\u96c6\u65f6\uff0c\u83b7\u5f97\u4e86O(d log d)\u7684\u6709\u9650\u9057\u61be\u754c\u3002\u5728\u6700\u591aC\u8f6e\u5bf9\u6297\u6027\u8150\u8d25\u53cd\u9988\u4e0b\uff0c\u83b7\u5f97\u4e86O((C+1)d log d)\u7684\u9057\u61be\u754c\uff0c\u4e14\u65e0\u9700\u4e8b\u5148\u77e5\u9053C\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u90e8\u5206\u89e3\u51b3\u4e86\u5728\u7ebf\u9006\u7ebf\u6027\u4f18\u5316\u4e2d\u662f\u5426\u5b58\u5728\u591a\u9879\u5f0f\u6709\u9650\u9057\u61be\u754c\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5bf9\u4e8eM-\u51f8\u96c6\u8fd9\u4e00\u5e7f\u6cdb\u7c7b\u522b\uff0c\u53ef\u4ee5\u83b7\u5f97O(d log d)\u7684\u6709\u9650\u9057\u61be\u754c\uff0c\u5e76\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u5bf9\u6297\u6027\u53cd\u9988\u573a\u666f\u3002"}}
{"id": "2602.01685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01685", "abs": "https://arxiv.org/abs/2602.01685", "authors": ["Byeonghu Na", "Hyungho Na", "Yeongmin Kim", "Suhyeon Jo", "HeeSun Bae", "Mina Kang", "Il-Chul Moon"], "title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment", "comment": "Accepted at ICLR 2026", "summary": "Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.", "AI": {"tldr": "\u63d0\u51faWasserstein Policy Regularization (WPR)\uff0c\u4e00\u79cd\u57fa\u4e8e\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\u7684\u8bed\u4e49\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbRLHF\u6846\u67b6\u4e2d\u7684\u7b56\u7565\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edfRLHF\u65b9\u6cd5\u4f7f\u7528KL\u6563\u5ea6\u6216\u5176f-\u6563\u5ea6\u53d8\u4f53\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u6bd4\u8f83\u76f8\u540c\u4f4d\u7f6e\u7684token\u6982\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651token\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u3001\u8bed\u4e49\u611f\u77e5\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWasserstein Policy Regularization (WPR)\uff0c\u57fa\u4e8e\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\uff0c\u8be5\u8ddd\u79bb\u8003\u8651\u4e86token\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u901a\u8fc7\u8ddd\u79bb\u7684\u5bf9\u5076\u516c\u5f0f\uff0c\u5c06\u6b63\u5219\u5316\u8868\u793a\u4e3a\u901a\u8fc7\u6700\u4f18\u5bf9\u5076\u53d8\u91cf\u5e94\u7528\u4e8e\u5956\u52b1\u7684\u60e9\u7f5a\u9879\uff0c\u5f97\u5230\u4e00\u4e2a\u4e0e\u6807\u51c6RL\u7b97\u6cd5\u517c\u5bb9\u7684\u53ef\u5904\u7406\u76ee\u6807\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cWPR\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8eKL\u6563\u5ea6\u548cf-\u6563\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u7b56\u7565\u8ddd\u79bb\u5bf9\u4e8e\u5bf9\u9f50\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "Wasserstein Policy Regularization (WPR) \u63d0\u4f9b\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8bed\u8a00\u6a21\u578b\u7b56\u7565\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5728RLHF\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u3002"}}
{"id": "2602.01705", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01705", "abs": "https://arxiv.org/abs/2602.01705", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Yi-An Ma", "Lianhui Qin"], "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner", "comment": null, "summary": "Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.", "AI": {"tldr": "LaDi-RL\uff1a\u4e00\u79cd\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fdb\u884c\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u79bb\u6563token\u7ea7RL\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4f18\u5316\u79bb\u6563\u601d\u7ef4\u94fe\u751f\u6210\u65f6\u5b58\u5728\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u56e0\u4e3a\u7b56\u7565\u71b5\u964d\u4f4e\u5bfc\u81f4\u6a21\u5f0f\u6fc0\u53d1\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faLaDi-RL\u6846\u67b6\uff1a1\uff09\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a2\u7d22\uff0c\u6f5c\u5728\u53d8\u91cf\u7f16\u7801\u8bed\u4e49\u7ea7\u63a8\u7406\u8f68\u8ff9\uff1b2\uff09\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u63a2\u7d22\uff0c\u591a\u6b65\u53bb\u566a\u5206\u5e03\u968f\u673a\u6027\u5e76\u4fdd\u7559\u591a\u4e2a\u5171\u5b58\u89e3\u6a21\u5f0f\uff1b3\uff09\u5c06\u6f5c\u5728\u7a7a\u95f4\u63a2\u7d22\u4e0e\u6587\u672c\u7a7a\u95f4\u751f\u6210\u89e3\u8026\uff0c\u7ed3\u5408\u6f5c\u5728\u63a2\u7d22\u548c\u6587\u672c\u7b56\u7565\u83b7\u5f97\u989d\u5916\u589e\u76ca\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u79bb\u6563RL\u57fa\u7ebf\uff0cpass@1\u548cpass@k\u5747\u6709\u6301\u7eed\u6539\u8fdb\uff1a\u4ee3\u7801\u751f\u6210pass@1\u7edd\u5bf9\u63d0\u5347+9.4%\uff0c\u6570\u5b66\u63a8\u7406pass@1\u7edd\u5bf9\u63d0\u5347+5.7%\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u5728RL\u4e3a\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6bd4\u79bb\u6563token\u7ea7RL\u66f4\u6709\u6548\u7684\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u63a2\u7d22\u89e3\u51b3\u4e86\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2602.01718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01718", "abs": "https://arxiv.org/abs/2602.01718", "authors": ["Sora Nakai", "Youssef Fadhloun", "Kacem Mathlouthi", "Kotaro Yoshida", "Ganesh Talluri", "Ioannis Mitliagkas", "Hiroki Naganuma"], "title": "Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift", "comment": null, "summary": "Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u8d85\u8fc740\u79cd\u6cdb\u5316\u5ea6\u91cf\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u8bb8\u591a\u65b9\u6cd5\u5728IID\u548cOOD\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4f46\u5c11\u6570\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u9884\u6d4b\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u7814\u7a76\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u548c\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u6cdb\u5316\u5ea6\u91cf\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u8d85\u8fc710,000\u4e2a\u8d85\u53c2\u6570\u914d\u7f6e\u8bad\u7ec3\u4e2d\u5c0f\u578b\u6a21\u578b\uff0c\u8bc4\u4f3040\u591a\u79cd\u4ec5\u4ece\u8bad\u7ec3\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u53ef\u8ba1\u7b97\u7684\u6cdb\u5316\u5ea6\u91cf\u65b9\u6cd5\u3002\u6269\u5c55\u5b9e\u9a8c\u8303\u56f4\u5305\u62ec\uff1a(i) \u4ece\u6807\u51c6IID\u8bbe\u7f6e\u6269\u5c55\u5230\u591a\u6837\u5206\u5e03\u504f\u79fb\uff0c(ii) \u8bc4\u4f30\u591a\u79cd\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6848\uff0c(iii) \u65b0\u7eb3\u5165\u57fa\u4e8e\u6821\u51c6\u548c\u4fe1\u606f\u51c6\u5219\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u5206\u5e03\u504f\u79fb\u663e\u8457\u6539\u53d8\u4e86\u8bb8\u591a\u6cdb\u5316\u5ea6\u91cf\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u800c\u8f83\u5c0f\u7684\u5b50\u96c6\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u3002\u8fd9\u8868\u660e\u8bb8\u591a\u73b0\u6709\u5ea6\u91cf\u65b9\u6cd5\u5728IID\u548cOOD\u6cdb\u5316\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u6cdb\u5316\u5ea6\u91cf\u65b9\u6cd5\u5bf9\u5206\u5e03\u504f\u79fb\u654f\u611f\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u5c11\u6570\u7a33\u5b9a\u7684\u5ea6\u91cf\u65b9\u6cd5\u53ef\u80fd\u4e3a\u6a21\u578b\u6027\u80fd\u9884\u6d4b\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u975eIID\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.01734", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01734", "abs": "https://arxiv.org/abs/2602.01734", "authors": ["Lianhai Ren", "Yucheng Ding", "Xiao Liu", "Qianxiao Li", "Peng Cheng", "Yeyun Gong"], "title": "MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration", "comment": null, "summary": "Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $\u03bc$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMSign\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5468\u671f\u6027\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6765\u6062\u590d\u6743\u91cd\u77e9\u9635\u7684\u7a33\u5b9a\u79e9\uff0c\u6709\u6548\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u8868\u73b0\u4e3a\u7a81\u7136\u7684\u68af\u5ea6\u7206\u70b8\uff0c\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u8bad\u7ec3\u5931\u8d25\u73b0\u8c61\uff0c\u53d1\u73b0\u5d29\u6e83\u524d\u4f1a\u51fa\u73b0\u6743\u91cd\u77e9\u9635\u7a33\u5b9a\u79e9\u6025\u5267\u4e0b\u964d\u548c\u76f8\u90bb\u5c42\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u5ea6\u589e\u52a0\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\u3002", "method": "\u63d0\u51faMSign\u4f18\u5316\u5668\uff0c\u5468\u671f\u6027\u5e94\u7528\u77e9\u9635\u7b26\u53f7\u64cd\u4f5c\u6765\u6062\u590d\u6743\u91cd\u77e9\u9635\u7684\u7a33\u5b9a\u79e9\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7406\u8bba\u8bc1\u660e\uff1a\u7a33\u5b9a\u79e9\u4e0b\u964d\u548c\u96c5\u53ef\u6bd4\u77e9\u9635\u5bf9\u9f50\u5171\u540c\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u968f\u7f51\u7edc\u6df1\u5ea6\u6307\u6570\u589e\u957f\u3002", "result": "\u57285M\u52303B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMSign\u80fd\u6709\u6548\u9632\u6b62\u8bad\u7ec3\u5931\u8d25\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u4e8e7.0%\u3002", "conclusion": "MSign\u901a\u8fc7\u6253\u7834\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u673a\u5236\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a33\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2602.01736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01736", "abs": "https://arxiv.org/abs/2602.01736", "authors": ["Qinwei Ma", "Jingzhe Shi", "Jiahao Qiu", "Zaiwen Yang"], "title": "Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting", "comment": "14 pages, 3 figures, 2 tables", "summary": "Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8d28\u7591\u901a\u7528\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8ba4\u4e3a\u5176\u4e0e\u7279\u5b9a\u9886\u57dfSOTA\u5b58\u5728\u4e0d\u53ef\u8c03\u548c\u7684\u51b2\u7a81\uff0c\u547c\u5401\u7814\u7a76\u91cd\u70b9\u8f6c\u5411\u7279\u5b9a\u9886\u57df\u6df1\u5ea6\u5b66\u4e60\u6216\u901a\u7528\u9886\u57df\u7684\u5143\u5b66\u4e60\u65b9\u6cd5", "motivation": "\u8fd1\u5e74\u6765\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u4f46\u6027\u80fd\u5df2\u8d8b\u4e8e\u9971\u548c\u3002\u901a\u7528\u9886\u57df\u67b6\u6784\u4e0e\u7279\u5b9a\u9886\u57df\uff08\u91d1\u878d\u3001\u5929\u6c14\u3001\u4ea4\u901a\u7b49\uff09\u7684\u5b9e\u9645\u9700\u6c42\u8131\u8282\uff0c\u7279\u5b9a\u9886\u57df\u5f88\u5c11\u91c7\u7528\u8fd12-3\u5e74\u65f6\u95f4\u5e8f\u5217\u793e\u533a\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u5c55\u3002\u4f5c\u8005\u8d28\u7591\u901a\u7528\u9886\u57df\u67b6\u6784\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u603b\u7ed3\u73b0\u6709\u7814\u7a76\u7684\u5173\u6ce8\u70b9\uff0c\u5206\u6790\u901a\u7528\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5185\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5355\u4e00/\u5c11\u6570\u76f8\u4f3c\u9886\u57dfSOTA\u4e0e\u901a\u7528\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u4e0d\u53ef\u8c03\u548c\u51b2\u7a81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7814\u7a76\u5df2\u8d8b\u4e8e\u9971\u548c\uff0c\u4e14\u4e0e\u7279\u5b9a\u9886\u57df\u5b9e\u9645\u9700\u6c42\u8131\u8282\u3002\u7279\u5b9a\u9886\u57df\u5f00\u53d1\u81ea\u5df1\u7684\u65b9\u6cd5\uff0c\u5f88\u5c11\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u793e\u533a\u8fd1\u5e74\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u5c55\u3002", "conclusion": "\u547c\u5401\u65f6\u95f4\u5e8f\u5217\u793e\u533a\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u901a\u7528\u9886\u57df\u7684\u65f6\u95f4\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8f6c\u5411\uff1a(1) \u7279\u5b9a\u9886\u57df\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6216 (2) \u901a\u7528\u9886\u57df\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5f00\u53d1\u3002\u901a\u7528\u9886\u57df\u67b6\u6784\u7814\u7a76\u5df2\u9971\u548c\u4e14\u8fdc\u79bb\u7279\u5b9a\u9886\u57dfSOTA\u3002"}}
{"id": "2602.01744", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01744", "abs": "https://arxiv.org/abs/2602.01744", "authors": ["Mingwei Xu", "Xuan Lin", "Xinnan Guo", "Wanqing Xu", "Wanyun Cui"], "title": "Softmax Linear Attention: Reclaiming Global Competition", "comment": "11 pages,4 figures", "summary": "While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \\emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \\textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.", "AI": {"tldr": "SLA\uff08Softmax Linear Attention\uff09\u901a\u8fc7\u5c06softmax\u64cd\u4f5c\u4ecetoken\u7ea7\u522b\u63d0\u5347\u5230head\u7ea7\u522b\uff0c\u5728\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u6062\u590d\u4e86\u5168\u5c40\u7ade\u4e89\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u5c06Transformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u964d\u4f4e\u5230\u7ebf\u6027\uff0c\u4f46\u7531\u4e8e\u79fb\u9664\u4e86softmax\u5f52\u4e00\u5316\uff0c\u5931\u53bb\u4e86\u5168\u5c40\u7ade\u4e89\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u8868\u8fbe\u80fd\u529b\u548c\u957f\u4e0a\u4e0b\u6587\u566a\u58f0\u5904\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSoftmax Linear Attention\uff08SLA\uff09\u6846\u67b6\uff0c\u5c06softmax\u64cd\u4f5c\u4ecetoken\u7ea7\u522b\u63d0\u5347\u5230head\u7ea7\u522b\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5934\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u8bed\u4e49\u69fd\uff0c\u901a\u8fc7\u7ade\u4e89\u95e8\u63a7\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684\u5b50\u7a7a\u95f4\u3002", "result": "SLA\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u4e86\u73b0\u6709\u7ebf\u6027\u57fa\u7ebf\u6a21\u578b\uff08RetNet\u3001GLA\u3001GDN\uff09\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u68c0\u7d22\u573a\u666f\u4e2d\u663e\u8457\u589e\u5f3a\u4e86\u6297\u566a\u58f0\u9c81\u68d2\u6027\u3002", "conclusion": "SLA\u6210\u529f\u5728\u7ebf\u6027\u6ce8\u610f\u529b\u4e2d\u6062\u590d\u4e86\u7cbe\u786e\u7684\"\u8d62\u5bb6\u901a\u5403\"\u52a8\u6001\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u89c6\u89d2\u3002"}}
{"id": "2602.01745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01745", "abs": "https://arxiv.org/abs/2602.01745", "authors": ["Wenhao Yu", "Shaohang Wei", "Jiahong Liu", "Yifan Li", "Minda Hu", "Aiwei Liu", "Hao Zhang", "Irwin King"], "title": "Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning", "comment": null, "summary": "Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.", "AI": {"tldr": "RankTuner\u63d0\u51fa\u57fa\u4e8e\u6982\u7387-\u71b5\u6821\u51c6\u7684\u76f8\u5bf9\u6392\u5e8f\u6307\u6807\uff0c\u901a\u8fc7\u6bd4\u8f83\u771f\u5b9e\u6807\u8bb0\u5728\u9884\u6d4b\u5206\u5e03\u4e2d\u7684\u76f8\u5bf9\u6392\u540d\u6765\u91cd\u65b0\u52a0\u6743\u5fae\u8c03\u76ee\u6807\uff0c\u4e13\u6ce8\u4e8e\u771f\u6b63\u672a\u5145\u5206\u5b66\u4e60\u7684\u6807\u8bb0\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6982\u7387\u6216\u71b5\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6807\u8bb0\u7ea7\u91cd\u65b0\u52a0\u6743\u65b9\u6cd5\u4e3b\u8981\u662f\u4e00\u7ef4\u7684\uff1a\u57fa\u4e8e\u771f\u5b9e\u6982\u7387\u53cd\u6620\u4e0b\u6e38\u5bf9\u9f50\uff0c\u6216\u57fa\u4e8e\u6807\u8bb0\u71b5\u53cd\u6620\u9884\u8bad\u7ec3\u5148\u9a8c\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u3002\u5ffd\u7565\u71b5\u4f1a\u8bef\u5c06\u566a\u58f0\u6216\u6613\u66ff\u6362\u6807\u8bb0\u8bc6\u522b\u4e3a\u5b66\u4e60\u5173\u952e\uff0c\u800c\u5ffd\u7565\u6982\u7387\u5219\u65e0\u6cd5\u53cd\u6620\u76ee\u6807\u7279\u5b9a\u5bf9\u9f50\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u6821\u51c6\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u76f8\u5bf9\u6392\u5e8f\u6307\u6807\uff0c\u6bd4\u8f83\u771f\u5b9e\u6807\u8bb0\u5728\u9884\u6d4b\u5206\u5e03\u4e2d\u7684\u6392\u540d\u4e0e\u5176\u671f\u671b\u6392\u540d\u3002\u4f7f\u7528\u8be5\u6307\u6807\u7684\u5012\u6570\u4f5c\u4e3a\u76f8\u5bf9\u5c3a\u5ea6\uff0c\u5bf9\u5fae\u8c03\u76ee\u6807\u8fdb\u884c\u6807\u8bb0\u7ea7\u91cd\u65b0\u52a0\u6743\uff0c\u4e13\u6ce8\u4e8e\u771f\u6b63\u672a\u5145\u5206\u5b66\u4e60\u7684\u6807\u8bb0\uff0c\u907f\u514d\u8fc7\u5ea6\u60e9\u7f5a\u5185\u5728\u4e0d\u786e\u5b9a\u7684\u4f4d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1b\u5728\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u83b7\u5f97\u8fc1\u79fb\u589e\u76ca\uff1b\u5728\u4ee3\u7801\u751f\u6210\u6027\u80fd\u4e0a\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6982\u7387\u6216\u71b5\u7684\u91cd\u65b0\u52a0\u6743\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RankTuner\u901a\u8fc7\u6982\u7387-\u71b5\u6821\u51c6\u7684\u76f8\u5bf9\u6392\u5e8f\u6307\u6807\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u771f\u6b63\u9700\u8981\u5b66\u4e60\u7684\u6807\u8bb0\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u590d\u6742\u4efb\u52a1\u4e0a\u3002"}}
{"id": "2602.01746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01746", "abs": "https://arxiv.org/abs/2602.01746", "authors": ["Hongyi Peng", "Han Yu", "Xiaoxiao Li", "Qiang Yang"], "title": "Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.", "AI": {"tldr": "FedGaLore\uff1a\u9488\u5bf9\u975eIID\u8054\u90a6\u5b66\u4e60\u4e2dLoRA\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u5408\u5ba2\u6237\u7aefGaLore\u68af\u5ea6\u5b50\u7a7a\u95f4\u4f18\u5316\u548c\u670d\u52a1\u5668\u7aef\u6f02\u79fb\u9c81\u68d2\u540c\u6b65\u7684\u65b9\u6cd5", "motivation": "\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u8bbe\u7f6e\u4e0b\uff0c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5728\u8054\u90a6\u5fae\u8c03\u4e2d\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u5b58\u5728\u6027\u80fd\u5dee\u8ddd", "method": "\u63d0\u51faFedGaLore\u65b9\u6cd5\uff1a\u5ba2\u6237\u7aef\u91c7\u7528GaLore\u98ce\u683c\u7684\u68af\u5ea6\u5b50\u7a7a\u95f4\u4f18\u5316\uff0c\u670d\u52a1\u5668\u7aef\u901a\u8fc7\u8c31\u5171\u4eab\u4fe1\u53f7\u63d0\u53d6\u5b9e\u73b0\u6295\u5f71\u4e8c\u9636\u77e9\u72b6\u6001\u7684\u6f02\u79fb\u9c81\u68d2\u540c\u6b65", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedGaLore\u5728\u975eIID\u8bbe\u7f6e\u4e0b\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8054\u90a6LoRA\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u66f4\u65b0\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u4f18\u5316\u5668\u72b6\u6001\u4e0d\u5339\u914d\u4e24\u4e2a\u8026\u5408\u95ee\u9898\uff0cFedGaLore\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u4e2dLoRA\u5728\u975eIID\u573a\u666f\u4e0b\u7684\u6027\u80fd"}}
{"id": "2602.01751", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.01751", "abs": "https://arxiv.org/abs/2602.01751", "authors": ["Kunyi Fan", "Mengjie Chen", "Longlong Li", "Cunquan Qu"], "title": "MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network", "comment": "Submitted to ICASSP 2026", "summary": "Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.", "AI": {"tldr": "MGKAN\uff1a\u57fa\u4e8e\u56feKolmogorov-Arnold\u7f51\u7edc\u7684\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u57fa\u51fd\u6570\u548c\u975e\u5bf9\u79f0\u7f51\u7edc\u5efa\u6a21\u63d0\u5347\u9884\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7ebf\u6027\u805a\u5408\u548c\u5bf9\u79f0\u5047\u8bbe\uff0c\u96be\u4ee5\u6355\u6349\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u4e2d\u7684\u975e\u7ebf\u6027\u6a21\u5f0f\u548c\u5f02\u8d28\u6027\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u51c6\u786e\u6027", "method": "\u63d0\u51faMGKAN\u6a21\u578b\uff0c\u5c06KAN\u9a71\u52a8\u7684\u57fa\u51fd\u6570\u5f15\u5165\u975e\u5bf9\u79f0DDI\u9884\u6d4b\uff0c\u66ff\u4ee3\u4f20\u7edfMLP\u53d8\u6362\uff1b\u6574\u5408\u4e09\u79cd\u7f51\u7edc\u89c6\u56fe\uff08\u975e\u5bf9\u79f0DDI\u7f51\u7edc\u3001\u5171\u76f8\u4e92\u4f5c\u7528\u7f51\u7edc\u3001\u751f\u5316\u76f8\u4f3c\u6027\u7f51\u7edc\uff09\u548c\u89d2\u8272\u7279\u5b9a\u5d4c\u5165\uff1b\u91c7\u7528\u878d\u5408\u6a21\u5757\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u975e\u7ebf\u6027\u53d8\u6362", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMGKAN\u4f18\u4e8e7\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff1b\u6d88\u878d\u7814\u7a76\u548c\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u9884\u6d4b\u51c6\u786e\u6027\u4ee5\u53ca\u5728\u5efa\u6a21\u65b9\u5411\u6027\u836f\u7269\u6548\u5e94\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "MGKAN\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u57fa\u51fd\u6570\u548c\u975e\u5bf9\u79f0\u7f51\u7edc\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u836f\u7269\u5173\u7cfb\u4e2d\u7684\u975e\u7ebf\u6027\u6a21\u5f0f\u548c\u65b9\u5411\u6027\u8bed\u4e49"}}
{"id": "2602.01763", "categories": ["cs.LG", "cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.01763", "abs": "https://arxiv.org/abs/2602.01763", "authors": ["Xiaowei Ye", "Xiaoyu He", "Chao Liao", "Chen Wu", "Pinyan Lu"], "title": "A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention", "comment": null, "summary": "Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u6807\u51c6\u5168\u6ce8\u610f\u529b\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4e25\u683c\u5206\u79bb\uff1a\u5bf9\u4e8e\u591a\u6b65\u63a8\u7406\u4efb\u52a1\uff0cL+1\u5c42\u5168\u6ce8\u610f\u529b\u7f51\u7edc\u5373\u53ef\u89e3\u51b3\uff0c\u800c\u6df7\u5408\u7f51\u7edc\u5373\u4f7f\u5305\u542b\u5927\u91cf\u7ebf\u6027\u6ce8\u610f\u529b\u5c42\u4e5f\u65e0\u6cd5\u5b8c\u6210\u3002", "motivation": "\u867d\u7136\u5404\u79cd\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982\u7ebf\u6027\u6ce8\u610f\u529b\u3001\u6df7\u5408\u6ce8\u610f\u529b\uff09\u88ab\u5f00\u53d1\u4ee5\u964d\u4f4e\u5168\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u76f8\u5bf9\u4e8e\u5168\u6ce8\u610f\u529b\u7684\u8868\u8fbe\u80fd\u529b\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u523b\u753b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u8868\u8fbe\u80fd\u529b\u5dee\u5f02\u3002\u7279\u522b\u5173\u6ce8\u80fd\u591f\u8868\u793a\u4e3a\u9012\u5f52\u5f62\u5f0f\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u53d8\u4f53\uff08\u5305\u62ecMamba\u3001DeltaNet\u7b49\uff09\uff0c\u5e76\u7814\u7a76\u5b83\u4eec\u5728\u5e8f\u5217\u51fd\u6570\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc1\u660e\u4e86\u8868\u8fbe\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\uff1a\u5bf9\u4e8e\u591a\u6b65\u63a8\u7406\u4efb\u52a1\uff0cL+1\u5c42\u5168\u6ce8\u610f\u529b\u7f51\u7edc\u8db3\u591f\u89e3\u51b3\uff0c\u800c\u4efb\u4f55\u6df7\u5408\u7f51\u7edc\uff08\u5305\u542bL-1\u5c42\u5168\u6ce8\u610f\u529b\u548c\u6307\u6570\u7ea7\u5927\u91cf\u7ebf\u6027\u6ce8\u610f\u529b\u5c42\uff09\u90fd\u65e0\u6cd5\u89e3\u51b3\u8be5\u4efb\u52a1\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u4e0e\u5168\u6ce8\u610f\u529b\u7684\u53ef\u8bc1\u660e\u5206\u79bb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u4e0e\u6807\u51c6\u5168\u6ce8\u610f\u529b\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4e25\u683c\u5206\u79bb\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u57fa\u672c\u80fd\u529b\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89c6\u89d2\uff0c\u63ed\u793a\u4e86\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u6839\u672c\u9650\u5236\u3002"}}
{"id": "2602.01766", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01766", "abs": "https://arxiv.org/abs/2602.01766", "authors": ["Runsong Zhao", "Shilei Liu", "Jiwei Tang", "Langming Liu", "Haibin Chen", "Weidong Zhang", "Yujin Yuan", "Tong Xiao", "Jingbo Zhu", "Wenbo Su", "Bo Zheng"], "title": "CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling", "comment": null, "summary": "The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/", "AI": {"tldr": "CoMeT\u662f\u4e00\u79cd\u521b\u65b0\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5206\u5757\u5904\u7406\u5b9e\u73b0\u5e38\u6570\u5185\u5b58\u548c\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u957f\u5e8f\u5217\u5904\u7406\uff0c\u53ef\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u3002", "motivation": "\u6807\u51c6Transformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u65e0\u9650\u589e\u957f\u7684KV\u7f13\u5b58\u662f\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u4e3b\u8981\u969c\u788d\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u5904\u7406\u4efb\u610f\u957f\u5e8f\u5217\u7684\u65b9\u6cd5\u3002", "method": "CoMeT\u91c7\u7528\u5206\u5757\u5904\u7406\u5e8f\u5217\uff0c\u4f7f\u7528\u53cc\u8bb0\u5fc6\u7cfb\u7edf\uff1aFIFO\u961f\u5217\u7684\u4e34\u65f6\u8bb0\u5fc6\u5904\u7406\u8fd1\u671f\u4e8b\u4ef6\uff0c\u95e8\u63a7\u66f4\u65b0\u7684\u5168\u5c40\u8bb0\u5fc6\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\uff0c\u8fd9\u4e9b\u8bb0\u5fc6\u4f5c\u4e3a\u52a8\u6001\u8f6f\u63d0\u793a\u6307\u5bfc\u4e0b\u4e00\u5757\u5904\u7406\u3002\u5f15\u5165\u5c42\u7ea7\u6d41\u6c34\u7ebf\u5e76\u884c\u7b56\u7565\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "CoMeT\u572832k\u4e0a\u4e0b\u6587\u5fae\u8c03\u540e\uff0c\u80fd\u57281M\u4ee4\u724c\u5e8f\u5217\u4e2d\u51c6\u786e\u68c0\u7d22\u4efb\u610f\u4f4d\u7f6e\u7684\u5bc6\u94a5\u3002\u5728SCROLLS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5176\u4ed6\u9ad8\u6548\u65b9\u6cd5\uff0c\u5728\u6458\u8981\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u4ee3\u7406\u548c\u7528\u6237\u884c\u4e3aQA\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "CoMeT\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5206\u5757\u5904\u7406\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86Transformer\u5904\u7406\u957f\u5e8f\u5217\u7684\u5185\u5b58\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2602.01769", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01769", "abs": "https://arxiv.org/abs/2602.01769", "authors": ["Yuanshuai Li", "Yuping Yan", "Jirui Han", "Fei Ming", "Lingjuan Lv", "Yaochu Jin"], "title": "IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination", "comment": null, "summary": "Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.\n  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.", "AI": {"tldr": "IRIS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u5956\u52b1\u7684\u5185\u90e8\u7b5b\u9009\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u9690\u5f0f\u5956\u52b1\u5728\u539f\u751f\u5bf9\u6570\u6982\u7387\u7a7a\u95f4\u4e2d\u6355\u6349\u6a21\u6001\u7ade\u4e89\uff0c\u65e0\u9700\u5916\u90e8\u53cd\u9988\u5373\u53ef\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDPO\u7684\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u8bc4\u4f30\u5668\u8fdb\u884c\u8bc4\u5206\u6216\u91cd\u5199\uff0c\u5b58\u5728\u79bb\u7b56\u7565\u5b66\u4e60\u5dee\u8ddd\u548c\u79bb\u6563\u5316\u635f\u5931\uff0c\u4e14\u65e0\u6cd5\u8bbf\u95ee\u5185\u90e8\u72b6\u6001\uff0c\u5ffd\u7565\u4e86\u5bfc\u81f4\u5e7b\u89c9\u7684\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u7ec6\u7c92\u5ea6\u51b2\u7a81\u3002", "method": "IRIS\u5229\u7528\u8fde\u7eed\u9690\u5f0f\u5956\u52b1\u5728\u539f\u751f\u5bf9\u6570\u6982\u7387\u7a7a\u95f4\u4e2d\u4fdd\u6301\u5b8c\u6574\u4fe1\u606f\u5bc6\u5ea6\uff0c\u6355\u6349\u5185\u90e8\u6a21\u6001\u7ade\u4e89\u3002\u8fd9\u79cd\u5728\u7b56\u7565\u8303\u5f0f\u901a\u8fc7\u81ea\u751f\u6210\u504f\u597d\u5bf9\u6d88\u9664\u5b66\u4e60\u5dee\u8ddd\uff0c\u57fa\u4e8e\u591a\u6a21\u6001\u9690\u5f0f\u5956\u52b1\u7b5b\u9009\u8fd9\u4e9b\u504f\u597d\u5bf9\uff0c\u786e\u4fdd\u4f18\u5316\u76f4\u63a5\u89e3\u51b3\u6a21\u6001\u51b2\u7a81\u3002", "result": "\u5728\u5173\u952e\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIRIS\u4ec5\u4f7f\u75285.7k\u6837\u672c\u5c31\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u504f\u597d\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u5b8c\u5168\u4e0d\u9700\u8981\u4efb\u4f55\u5916\u90e8\u53cd\u9988\u3002", "conclusion": "IRIS\u4e3a\u7f13\u89e3MLLM\u5e7b\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u539f\u5219\u6027\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u9690\u5f0f\u5956\u52b1\u5f15\u5bfc\u7684\u5185\u90e8\u7b5b\u9009\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2602.01772", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.01772", "abs": "https://arxiv.org/abs/2602.01772", "authors": ["Yucheng Liao", "Han Wen", "Weinan E", "Weijie Zhang"], "title": "DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics", "comment": "21 pages, 5 figures", "summary": "Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.", "AI": {"tldr": "DIA-CLIP\u662f\u4e00\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u5b9e\u73b0\u96f6\u6837\u672c\u80bd\u6bb5-\u8c31\u56fe\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u86cb\u767d\u8d28\u9274\u5b9a\u6df1\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dDIA-MS\u5206\u6790\u6846\u67b6\u9700\u8981\u6bcf\u4e2a\u5b9e\u9a8c\u7684\u534a\u76d1\u7763\u8bad\u7ec3\u8fdb\u884cPSM\u91cd\u8bc4\u5206\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u7f3a\u4e4f\u8de8\u7269\u79cd\u548c\u5b9e\u9a8c\u6761\u4ef6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e0e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5efa\u7acb\u80bd\u6bb5\u548c\u5bf9\u5e94\u8c31\u56fe\u7279\u5f81\u7684\u7edf\u4e00\u8de8\u6a21\u6001\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u96f6\u6837\u672cPSM\u63a8\u65ad\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDIA-CLIP\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u5de5\u5177\uff0c\u86cb\u767d\u8d28\u9274\u5b9a\u589e\u52a0\u9ad8\u8fbe45%\uff0c\u540c\u65f6\u8bf1\u9975\u9274\u5b9a\u51cf\u5c1112%\u3002", "conclusion": "DIA-CLIP\u5c06DIA\u5206\u6790\u8303\u5f0f\u4ece\u534a\u76d1\u7763\u8bad\u7ec3\u8f6c\u5411\u901a\u7528\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u5728\u5355\u7ec6\u80de\u548c\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.01776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01776", "abs": "https://arxiv.org/abs/2602.01776", "authors": ["Mingyue Cheng", "Xiaoyu Tao", "Qi Liu", "Ze Guo", "Enhong Chen"], "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "comment": null, "summary": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.", "AI": {"tldr": "\u63d0\u51fa\"\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b(ATSF)\"\u65b0\u8303\u5f0f\uff0c\u5c06\u9884\u6d4b\u4ece\u4f20\u7edf\u7684\u6a21\u578b\u4e2d\u5fc3\u3001\u9759\u6001\u3001\u5355\u6b21\u9884\u6d4b\u8f6c\u53d8\u4e3a\u5305\u542b\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u548c\u8bb0\u5fc6\u7684\u4ee3\u7406\u5f0f\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u81ea\u9002\u5e94\u3001\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u4fe1\u606f\u63d0\u53d6\u3001\u63a8\u7406\u9a71\u52a8\u63a8\u65ad\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u6301\u7eed\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b(ATSF)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u5b9e\u73b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u8bbe\u8ba1\u3001\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u6df7\u5408\u4ee3\u7406\u5f0f\u5de5\u4f5c\u6d41\u8303\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u4ee3\u7406\u5f0f\u9884\u6d4b\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7814\u7a76\u7684\u65b0\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u6027\u6307\u5bfc\u3002", "conclusion": "\u4ece\u6a21\u578b\u4e2d\u5fc3\u9884\u6d4b\u8f6c\u5411\u4ee3\u7406\u5f0f\u9884\u6d4b\u662f\u5fc5\u8981\u7684\u6f14\u8fdb\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5de5\u5177\u4ea4\u4e92\u3001\u7ed3\u679c\u53cd\u9988\u548c\u7ecf\u9a8c\u79ef\u7d2f\u65b9\u9762\u3002"}}
{"id": "2602.01777", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01777", "abs": "https://arxiv.org/abs/2602.01777", "authors": ["M. Arashi", "M. Amintoosi"], "title": "Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions", "comment": null, "summary": "Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eStein\u6536\u7f29\u7684\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u6536\u7f29\u5c0f\u6279\u91cf\u68af\u5ea6\u5411\u52a8\u91cf\u65b9\u5411\uff0c\u5728\u7ef4\u5ea6\u22653\u65f6\u4f18\u4e8e\u6807\u51c6\u968f\u673a\u68af\u5ea6\uff0c\u5e76\u96c6\u6210\u5230Adam\u4f18\u5316\u5668\u4e2d\u63d0\u5347\u5927\u6279\u6b21\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u968f\u673a\u68af\u5ea6\u65b9\u6cd5\u5c06\u5c0f\u6279\u91cf\u68af\u5ea6\u89c6\u4e3a\u65e0\u504f\u4f30\u8ba1\uff0c\u4f46\u7edf\u8ba1\u51b3\u7b56\u7406\u8bba\u8868\u660e\u5728\u4e8c\u6b21\u635f\u5931\u4e0b\u65e0\u504f\u4f30\u8ba1\u901a\u5e38\u4e0d\u53ef\u63a5\u53d7\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u573a\u666f\u4e2d\uff0c\u6807\u51c6\u968f\u673a\u68af\u5ea6\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002", "method": "\u5c06\u968f\u673a\u68af\u5ea6\u8ba1\u7b97\u6784\u5efa\u4e3a\u9ad8\u7ef4\u4f30\u8ba1\u95ee\u9898\uff0c\u57fa\u4e8eStein\u89c4\u5219\u6536\u7f29\u6784\u5efa\u6536\u7f29\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u5c06\u566a\u58f0\u5c0f\u6279\u91cf\u68af\u5ea6\u81ea\u9002\u5e94\u6536\u7f29\u5230\u5386\u53f2\u52a8\u91cf\u63a8\u5bfc\u7684\u7a33\u5b9a\u53d7\u9650\u4f30\u8ba1\u5668\u65b9\u5411\uff0c\u6536\u7f29\u5f3a\u5ea6\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u68af\u5ea6\u566a\u58f0\u65b9\u5dee\u6570\u636e\u9a71\u52a8\u786e\u5b9a\u3002", "result": "\u5728\u7ef4\u5ea6p\u22653\u65f6\uff0c\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u5728\u5e73\u65b9\u8bef\u5dee\u635f\u5931\u4e0b\u4e00\u81f4\u4f18\u4e8e\u6807\u51c6\u968f\u673a\u68af\u5ea6\uff0c\u4e14\u5177\u6709\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6027\uff1b\u96c6\u6210\u5230Adam\u540e\u5728CIFAR10/100\u4e0a\uff0c\u7279\u522b\u662f\u5728\u5927\u6279\u6b21\u548c\u6807\u7b7e\u566a\u58f0\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4\u539f\u59cbAdam\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u7ecf\u5178\u6536\u7f29\u539f\u5219\u4e3a\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u968f\u673a\u68af\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u4e14\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5e94\u7528\u4e8e\u9ad8\u7ef4\u5377\u79ef\u5c42\u53ef\u83b7\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5168\u53c2\u6570\u6536\u7f29\u5219\u4f1a\u964d\u4f4e\u6027\u80fd\u3002"}}
{"id": "2602.01791", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01791", "abs": "https://arxiv.org/abs/2602.01791", "authors": ["Zheng Zhang", "Ao Lu", "Yuanhao Zeng", "Ziwei Shan", "Jinjin Guo", "Lufei Li", "Yexin Li", "Kan Ren"], "title": "Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.", "AI": {"tldr": "Grad2Reward\uff1a\u901a\u8fc7\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u4eceJudge\u6a21\u578b\u4e2d\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u89e3\u51b3\u5f00\u653e\u4efb\u52a1\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u5347LLM\u63a8\u7406\u8bad\u7ec3\u6548\u7387", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u5f00\u653e\u4efb\u52a1\u4e2d\u4f7f\u7528LLM-as-a-Judge\u63d0\u4f9b\u5e8f\u5217\u7ea7\u5956\u52b1\uff0c\u4f46\u5956\u52b1\u7a00\u758f\u4e14\u65e0\u6cd5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b\u540c\u65f6\u5c06Judge\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u7565\u4e86\u5176\u4e2d\u95f4\u53cd\u9988\u4fe1\u53f7", "method": "\u63d0\u51faGrad2Reward\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u4eceJudge\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\uff1b\u5f15\u5165\u81ea\u5224\u65ad\u673a\u5236\uff0c\u8ba9\u7b56\u7565\u901a\u8fc7\u81ea\u8eab\u8bc4\u4f30\u4fe1\u53f7\u6539\u8fdb", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Grad2Reward\u4f18\u5316\u7684\u7b56\u7565\u5728\u591a\u79cd\u5f00\u653e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b", "conclusion": "Grad2Reward\u901a\u8fc7\u63d0\u53d6Judge\u6a21\u578b\u7684\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u4efb\u52a1\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2602.01826", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01826", "abs": "https://arxiv.org/abs/2602.01826", "authors": ["Yaxiang Zhang", "Yingru Li", "Jiacai Liu", "Jiawei Xu", "Ziniu Li", "Qian Liu", "Haoyuan Li"], "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It", "comment": null, "summary": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to \"training inference mismatch stemming\" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u7a33\u5b9a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u89e3\u51b3\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u91cd\u8981\u6027\u91c7\u6837\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u53ef\u80fd\u5931\u6548\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4f1a\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u800c\u52a0\u5267\uff0c\u9700\u8981\u65b0\u7684\u7a33\u5b9a\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e13\u95e8\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u4e0d\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u8870\u51cf\u8ba1\u5212\uff0c\u800c\u662f\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u52a8\u6001\u89e6\u53d1\u5b66\u4e60\u7387\u8870\u51cf\u3002\u54cd\u5e94\u957f\u5ea6\u88ab\u8bc6\u522b\u4e3a\u5373\u5c06\u53d1\u751f\u4e0d\u7a33\u5b9a\u7684\u53ef\u9760\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u3002", "result": "\u901a\u8fc7\u5728\u5b66\u4e60\u7387\u4e0a\u5347\u65f6\u964d\u4f4e\u5b66\u4e60\u7387\uff0c\u80fd\u591f\u6301\u7eed\u7a33\u5b9aRL\u8bad\u7ec3\uff0c\u5e76\u5c06\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4fdd\u6301\u5728\u5b89\u5168\u6c34\u5e73\u3002\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4e0d\u662f\u9759\u6001\u6570\u503c\u5dee\u5f02\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u4f18\u5316\u8026\u5408\u7684\u52a8\u6001\u6545\u969c\u3002\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7a33\u5b9aRL\u8bad\u7ec3\u3002"}}
{"id": "2602.01949", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01949", "abs": "https://arxiv.org/abs/2602.01949", "authors": ["Leonardo Stoppani", "Davide Bacciu", "Shahab Mokarizadeh"], "title": "Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity", "comment": "Accepted at ESANN 2026", "summary": "Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fr\u00e9chet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiversity Score (DS)\u6307\u6807\u91cf\u5316\u5e03\u5c40\u591a\u6837\u6027\uff0c\u5e76\u5f15\u5165Boundary Cross-Attention (BCA)\u6a21\u5757\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u5e73\u9762\u56fe\u751f\u6210\u7cfb\u7edf\u867d\u7136\u80fd\u4ea7\u751f\u903c\u771f\u7684\u5e03\u5c40\uff0c\u4f46\u8fc7\u5ea6\u4f18\u5316FID\u7b49\u611f\u77e5\u6307\u6807\u4f1a\u5bfc\u81f4\u8bbe\u8ba1\u591a\u6837\u6027\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u597d\u7684\u8fb9\u754c\u7ea6\u675f\u673a\u5236\u3002", "method": "1) \u63d0\u51faDiversity Score (DS)\u6307\u6807\u6765\u91cf\u5316\u56fa\u5b9a\u7ea6\u675f\u4e0b\u7684\u5e03\u5c40\u591a\u6837\u6027\uff1b2) \u5f15\u5165Boundary Cross-Attention (BCA)\u6a21\u5757\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u5bf9\u5efa\u7b51\u8fb9\u754c\u7684\u6761\u4ef6\u7ea6\u675f\u3002", "result": "BCA\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u754c\u4f9d\u4ece\u6027\uff0c\u5ef6\u957f\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u591a\u6837\u6027\u5d29\u6e83\u4f46FID\u65e0\u6cd5\u68c0\u6d4b\uff0c\u63ed\u793a\u4e86\u771f\u5b9e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002OOD\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u96c6\u5148\u9a8c\u3002", "conclusion": "\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u7684\u751f\u6210\u7cfb\u7edf\u9700\u8981\u660e\u786e\u5e73\u8861\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002DS\u6307\u6807\u80fd\u6709\u6548\u8bca\u65ad\u591a\u6837\u6027\u95ee\u9898\uff0cBCA\u6a21\u5757\u80fd\u6539\u5584\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e3a\u66f4\u5e73\u8861\u7684\u8bbe\u8ba1\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.01828", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01828", "abs": "https://arxiv.org/abs/2602.01828", "authors": ["Dionisia Naddeo", "Jonas Linkerh\u00e4gner", "Nicola Toschi", "Geri Skenderi", "Veronica Lachi"], "title": "Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment", "comment": null, "summary": "Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking \"Is the graph hyperbolic?\" to also questioning \"Is the task aligned with hyperbolic geometry?\", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.", "AI": {"tldr": "HGNNs\u4ec5\u5728\u4efb\u52a1\u4e0e\u53cc\u66f2\u51e0\u4f55\u5bf9\u9f50\u65f6\u624d\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u56fe\u672c\u8eab\u5177\u6709\u53cc\u66f2\u7ed3\u6784", "motivation": "\u8d28\u7591\u5f53\u524dHGNNs\u4f5c\u4e3a\u6811\u72b6\u56fe\u8868\u793a\u5b66\u4e60\u9996\u9009\u8303\u5f0f\u7684\u5408\u7406\u6027\uff0c\u63d0\u51fa\u51e0\u4f55-\u4efb\u52a1\u5bf9\u9f50\u7684\u989d\u5916\u6761\u4ef6", "method": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff1a1\uff09\u5728\u4e24\u4e2a\u5408\u6210\u56de\u5f52\u95ee\u9898\u4e0a\u6d4b\u8bd5HGNNs\u6062\u590d\u4f4e\u5931\u771f\u8868\u793a\u7684\u80fd\u529b\uff1b2\uff09\u5728\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u8054\u5408\u5206\u6790\u9884\u6d4b\u6027\u80fd\u548c\u5d4c\u5165\u5931\u771f", "result": "HGNNs\u4ec5\u5728\u9700\u8981\u4fdd\u6301\u5ea6\u91cf\u7ed3\u6784\u7684\u4efb\u52a1\u4e2d\u53d1\u6325\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u4f18\u52bf\uff1b\u94fe\u63a5\u9884\u6d4b\u662f\u51e0\u4f55\u5bf9\u9f50\u7684\uff0c\u800c\u8282\u70b9\u5206\u7c7b\u4e0d\u662f\uff1bHGNNs\u5728\u51e0\u4f55\u5bf9\u9f50\u65f6\u59cb\u7ec8\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\uff0c\u5426\u5219\u4f18\u52bf\u6d88\u5931", "conclusion": "\u7814\u7a76\u91cd\u70b9\u5e94\u4ece\"\u56fe\u662f\u5426\u53cc\u66f2\uff1f\"\u8f6c\u5411\"\u4efb\u52a1\u662f\u5426\u4e0e\u53cc\u66f2\u51e0\u4f55\u5bf9\u9f50\uff1f\"\uff0c\u51e0\u4f55-\u4efb\u52a1\u5bf9\u9f50\u662fHGNNs\u6709\u6548\u6027\u7684\u5173\u952e\u6761\u4ef6"}}
{"id": "2602.01976", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01976", "abs": "https://arxiv.org/abs/2602.01976", "authors": ["Hongwei Yan", "Guanglong Sun", "Kanglei Zhou", "Qian Li", "Liyuan Wang", "Yi Zhong"], "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning", "comment": "33 pages. Accepted by ICLR 2026", "summary": "General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.", "AI": {"tldr": "FlyPrompt\u662f\u4e00\u4e2a\u53d7\u679c\u8747\u5927\u8111\u542f\u53d1\u7684\u901a\u7528\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u8def\u7531\u548c\u4e13\u5bb6\u80fd\u529b\u6539\u8fdb\u89e3\u51b3\u5355\u6b21\u975e\u5e73\u7a33\u6570\u636e\u6d41\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u901a\u7528\u6301\u7eed\u5b66\u4e60\uff08GCL\uff09\u8981\u6c42\u667a\u80fd\u7cfb\u7edf\u4ece\u5355\u6b21\u901a\u8fc7\u3001\u975e\u5e73\u7a33\u7684\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\uff0c\u6ca1\u6709\u660e\u786e\u7684\u4efb\u52a1\u8fb9\u754c\u3002\u73b0\u6709\u7684\u6301\u7eed\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\uff08PET\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u591a\u8f6e\u8bad\u7ec3\u548c\u660e\u786e\u7684\u4efb\u52a1\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5728GCL\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u6027\u8bbe\u8ba1\uff0c\u672a\u80fd\u89e3\u51b3\u6301\u7eedPET\u4e2d\u7684\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u5982\u4f55\u5c06\u4e13\u5bb6\u53c2\u6570\u5206\u914d\u7ed9\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u6709\u9650\u76d1\u7763\u4e0b\u63d0\u9ad8\u5176\u8868\u793a\u80fd\u529b\u3002", "method": "\u53d7\u679c\u8747\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\uff08\u7a00\u758f\u6269\u5c55\u548c\u6a21\u5757\u5316\u96c6\u6210\uff09\u542f\u53d1\uff0cFlyPrompt\u5c06GCL\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff1a\u4e13\u5bb6\u8def\u7531\u548c\u4e13\u5bb6\u80fd\u529b\u6539\u8fdb\u3002\u6846\u67b6\u5f15\u5165\u968f\u673a\u6269\u5c55\u7684\u5206\u6790\u8def\u7531\u5668\u8fdb\u884c\u5b9e\u4f8b\u7ea7\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u4ee5\u53ca\u8f93\u51fa\u5934\u7684\u65f6\u95f4\u96c6\u6210\u6765\u52a8\u6001\u9002\u5e94\u968f\u65f6\u95f4\u53d8\u5316\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5e7f\u6cdb\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660eFlyPrompt\u5177\u6709\u5353\u8d8a\u6027\u80fd\uff0c\u5728CIFAR-100\u3001ImageNet-R\u548cCUB-200\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8611.23%\u300112.43%\u548c7.62%\u3002", "conclusion": "FlyPrompt\u662f\u4e00\u4e2a\u53d7\u751f\u7269\u542f\u53d1\u7684\u6709\u6548\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u901a\u7528\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e13\u5bb6\u8def\u7531\u548c\u52a8\u6001\u9002\u5e94\u673a\u5236\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2602.01839", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.01839", "abs": "https://arxiv.org/abs/2602.01839", "authors": ["Ru Zhang", "Xunkai Li", "Yaxin Deng", "Sicheng Liu", "Daohan Su", "Qiangqiang Dai", "Hongchao Qin", "Rong-Hua Li", "Guoren Wang", "Jia Li"], "title": "DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis", "comment": "12 pages, 4 figures", "summary": "Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.\n  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.", "AI": {"tldr": "DOGMA\u662f\u4e00\u4e2a\u6570\u636e\u4e2d\u5fc3\u7684\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5c42\u6b21\u751f\u7269\u5148\u9a8c\u77e5\u8bc6\u6765\u91cd\u5851\u6570\u636e\u7ed3\u6784\u5e76\u589e\u5f3a\u8bed\u4e49\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u968f\u673a\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\uff0c\u5728\u8de8\u7269\u79cd\u548c\u591a\u5668\u5b98\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u65e9\u671f\u5e8f\u5217\u65b9\u6cd5\u5c06\u7ec6\u80de\u89c6\u4e3a\u72ec\u7acb\u5b9e\u4f53\uff0c\u5ffd\u7565\u4e86\u751f\u7269\u7cfb\u7edf\u529f\u80fd\u673a\u5236\u9a71\u52a8\u7684\u6f5c\u5728\u7ec6\u80de\u95f4\u5173\u7cfb\uff1b2\uff09\u7ed3\u6784\u5316\u65b9\u6cd5\u867d\u7136\u6355\u6349\u7ec6\u80de\u95f4\u5173\u7cfb\u5e76\u589e\u5f3a\u539f\u59cb\u6570\u636e\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u751f\u7269\u5148\u9a8c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u56fe\u8868\u793a\u6548\u679c\u4e0d\u4f73\u3002", "method": "DOGMA\u662f\u4e00\u4e2a\u6574\u4f53\u6027\u6570\u636e\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u751f\u7269\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u6570\u636e\u7ed3\u6784\u91cd\u5851\u548c\u8bed\u4e49\u589e\u5f3a\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u6574\u5408\u7edf\u8ba1\u951a\u70b9\u3001\u7ec6\u80de\u672c\u4f53\u548c\u7cfb\u7edf\u53d1\u80b2\u6811\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u7ed3\u6784\u53d1\u73b0\u548c\u9c81\u68d2\u7684\u8de8\u7269\u79cd\u5bf9\u9f50\uff1b2\uff09\u5229\u7528\u57fa\u56e0\u672c\u4f53\u901a\u8fc7\u529f\u80fd\u5148\u9a8c\u77e5\u8bc6\u5f25\u5408\u7279\u5f81\u7ea7\u8bed\u4e49\u9e3f\u6c9f\u3002", "result": "\u5728\u590d\u6742\u7684\u591a\u7269\u79cd\u548c\u591a\u5668\u5b98\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDOGMA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u4ee5\u663e\u8457\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u8fd0\u884c\u3002", "conclusion": "DOGMA\u901a\u8fc7\u6574\u5408\u751f\u7269\u5148\u9a8c\u77e5\u8bc6\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u968f\u673a\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\uff0c\u4e3a\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u6570\u636e\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u786e\u5b9a\u6027\u7ed3\u6784\u53d1\u73b0\u548c\u8de8\u7269\u79cd\u5bf9\u9f50\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2602.02110", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02110", "abs": "https://arxiv.org/abs/2602.02110", "authors": ["Zhongqian Fu", "Tianyi Zhao", "Kai Han", "Hang Zhou", "Xinghao Chen", "Yunhe Wang"], "title": "An Empirical Study of World Model Quantization", "comment": null, "summary": "World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4e16\u754c\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u91cf\u5316\uff0c\u4f7f\u7528DINO-WM\u4f5c\u4e3a\u6848\u4f8b\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u91cf\u5316\u65b9\u6cd5\u5728\u4e0d\u540c\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u91cf\u5316\u5bf9\u4e16\u754c\u6a21\u578b\u89c4\u5212\u7684\u7279\u6b8a\u5f71\u54cd\u6a21\u5f0f\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u5728\u73af\u5883\u52a8\u6001\u8868\u793a\u548c\u667a\u80fd\u4f53\u89c4\u5212\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\u5f88\u9ad8\uff0c\u9700\u8981\u91cf\u5316\u6765\u9ad8\u6548\u90e8\u7f72\u3002\u7136\u800c\uff0c\u540e\u8bad\u7ec3\u91cf\u5316\u5bf9\u4e16\u754c\u6a21\u578b\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528DINO-WM\u4f5c\u4e3a\u4ee3\u8868\u6027\u4e16\u754c\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4ec5\u6743\u91cd\u91cf\u5316\u548c\u8054\u5408\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u3002\u5728\u4e0d\u540c\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u3001\u91cf\u5316\u7c92\u5ea6\u548c\u957f\u8fbe50\u6b65\u7684\u89c4\u5212\u89c6\u91ce\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5206\u7ec4\u6743\u91cd\u91cf\u5316\u80fd\u7a33\u5b9a\u4f4e\u6bd4\u7279\u5c55\u5f00\uff1b\u6fc0\u6d3b\u91cf\u5316\u7c92\u5ea6\u6548\u679c\u4e0d\u4e00\u81f4\uff1b\u7f16\u7801\u5668\u548c\u9884\u6d4b\u5668\u6a21\u5757\u7684\u91cf\u5316\u654f\u611f\u6027\u9ad8\u5ea6\u4e0d\u5bf9\u79f0\uff1b\u6fc0\u8fdb\u4f4e\u6bd4\u7279\u91cf\u5316\u4f1a\u663e\u8457\u964d\u4f4e\u89c4\u5212\u76ee\u6807\u4e0e\u4efb\u52a1\u6210\u529f\u7684\u5bf9\u9f50\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e16\u754c\u6a21\u578b\u89c4\u5212\u4e2d\u72ec\u7279\u7684\u91cf\u5316\u8bf1\u5bfc\u5931\u6548\u6a21\u5f0f\uff0c\u4e3a\u5728\u4e25\u683c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u90e8\u7f72\u91cf\u5316\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2602.01842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01842", "abs": "https://arxiv.org/abs/2602.01842", "authors": ["Jinbin Bai", "Yixuan Li", "Yuchen Zhu", "Yi Xin", "Qingyu Shi", "Aosong Feng", "Xiaohong Liu", "Molei Tao", "Jianru Xue", "Xiangtai Li", "Ming-Hsuan Yang"], "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models", "comment": null, "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.", "AI": {"tldr": "Prism\u662f\u4e00\u4e2a\u9488\u5bf9\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8f68\u8ff9\u641c\u7d22\u3001\u5c40\u90e8\u5206\u652f\u4e0e\u90e8\u5206\u91cd\u63a9\u7801\u3001\u81ea\u9a8c\u8bc1\u53cd\u9988\u7b49\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b97\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u4e0d\u9002\u5408\u5e76\u884c\u89e3\u7801\u6574\u4e2a\u5e8f\u5217\u7684\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u91ca\u653edLLMs\u7684\u751f\u6210\u6f5c\u529b\u3002", "method": "\u63d0\u51faPrism\u6846\u67b6\uff1a1)\u5206\u5c42\u8f68\u8ff9\u641c\u7d22\uff0c\u5728\u65e9\u671f\u5230\u4e2d\u671f\u7684\u53bb\u566a\u7a97\u53e3\u52a8\u6001\u526a\u679d\u548c\u91cd\u65b0\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff1b2)\u5c40\u90e8\u5206\u652f\u4e0e\u90e8\u5206\u91cd\u63a9\u7801\uff0c\u63a2\u7d22\u591a\u6837\u5316\u5b9e\u73b0\u540c\u65f6\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6token\uff1b3)\u81ea\u9a8c\u8bc1\u53cd\u9988\uff0c\u901a\u8fc7\u81ea\u8bc4\u4f30\u63d0\u793a\u66ff\u4ee3\u5916\u90e8\u9a8c\u8bc1\u5668\u3002", "result": "\u5728\u4e09\u4e2adLLMs\uff08LLaDA 8B Instruct\u3001Dream 7B Instruct\u3001LLaDA 2.0-mini\uff09\u7684\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPrism\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u5339\u914d\u6700\u4f73N\u9009\u6027\u80fd\u3002", "conclusion": "Prism\u4e3a\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5\u4e0d\u9002\u7528\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.02259", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02259", "abs": "https://arxiv.org/abs/2602.02259", "authors": ["Hamza Adnan", "Matthew T. Jackson", "Alexey Zakharov"], "title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors", "comment": null, "summary": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.", "AI": {"tldr": "MaskLAM\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u667a\u80fd\u4f53\u5206\u5272\u6765\u6539\u8fdb\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff0c\u6709\u6548\u8fc7\u6ee4\u52a8\u4f5c\u76f8\u5173\u566a\u58f0\uff0c\u5728MuJoCo\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u7684\u5956\u52b1\u63d0\u5347\u3002", "motivation": "\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff08LAMs\uff09\u80fd\u591f\u4ece\u539f\u59cb\u89c2\u5bdf\u4e2d\u5b66\u4e60\u63d0\u53d6\u52a8\u4f5c\u76f8\u5173\u8868\u793a\uff0c\u4f46\u9762\u4e34\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff1a\u96be\u4ee5\u5c06\u52a8\u4f5c\u76f8\u5173\u7279\u5f81\u4e0e\u52a8\u4f5c\u76f8\u5173\u566a\u58f0\uff08\u5982\u80cc\u666f\u8fd0\u52a8\uff09\u5206\u79bb\u3002\u5982\u679c\u65e0\u6cd5\u8fc7\u6ee4\u8fd9\u4e9b\u5e72\u6270\u56e0\u7d20\uff0cLAMs\u4f1a\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u5e76\u6784\u5efa\u6b21\u4f18\u7684\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "MaskLAM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684LAM\u8bad\u7ec3\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u89c6\u89c9\u667a\u80fd\u4f53\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5206\u5272\u63a9\u7801\u5bf9LAM\u91cd\u5efa\u635f\u5931\u8fdb\u884c\u52a0\u6743\uff0c\u4ece\u800c\u4f18\u5148\u8003\u8651\u663e\u8457\u4fe1\u606f\u800c\u975e\u80cc\u666f\u5143\u7d20\uff0c\u540c\u65f6\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002", "result": "\u5728\u6dfb\u52a0\u4e86\u52a8\u4f5c\u76f8\u5173\u80cc\u666f\u566a\u58f0\u7684\u8fde\u7eed\u63a7\u5236MuJoCo\u4efb\u52a1\u4e2d\uff0cMaskLAM\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u500d\u7684\u7d2f\u79ef\u5956\u52b1\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u8bc4\u4f30\u663e\u793a\u6f5c\u5728\u52a8\u4f5c\u8d28\u91cf\u63d0\u9ad8\u4e863\u500d\u3002", "conclusion": "MaskLAM\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u5206\u5272\u6709\u6548\u89e3\u51b3\u4e86LAMs\u4e2d\u7684\u52a8\u4f5c\u76f8\u5173\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\u7684\u8d28\u91cf\u548c\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u67b6\u6784\u4fee\u6539\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2602.01845", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.01845", "abs": "https://arxiv.org/abs/2602.01845", "authors": ["Furkan Eris"], "title": "No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation", "comment": null, "summary": "Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \\textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $\u03c1= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference", "AI": {"tldr": "Proust\u662f\u4e00\u4e2a309M\u53c2\u6570\u7684\u56e0\u679c\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u5728\u9002\u5e94\u5ea6\u9884\u6d4b\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5728\u591a\u4e2a\u86cb\u767d\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u5206\u6b67\uff1a\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u9002\u5e94\u5ea6\u9884\u6d4b\uff0c\u800c\u56e0\u679c\u6a21\u578b\u652f\u6301\u751f\u6210\uff0c\u8feb\u4f7f\u7814\u7a76\u4eba\u5458\u7ef4\u62a4\u4e0d\u540c\u7684\u67b6\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u517c\u987e\u4e24\u79cd\u80fd\u529b\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u5f15\u5165Proust\u6a21\u578b\uff0c\u91c7\u7528\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u501f\u9274\u7684\u67b6\u6784\u521b\u65b0\uff1a\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff08\u5171\u4eabK/V\u6295\u5f71\uff09\u3001\u8de8\u5c42\u503c\u6b8b\u5dee\u548c\u6df1\u5ea6\u56e0\u679c\u5377\u79ef\u3002\u6a21\u578b\u572833B tokens\u4e0a\u8bad\u7ec3\uff0c\u8017\u65f640 B200 GPU\u5c0f\u65f6\u3002", "result": "\u5728ProteinGym\u66ff\u6362\u4efb\u52a1\u4e0a\u8fbe\u5230Spearman \u03c1=0.390\uff0c\u4e0e\u9700\u898150-200\u500d\u8ba1\u7b97\u91cf\u7684\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7ade\u4e89\uff1b\u5728indels\u4efb\u52a1\u4e0a\u521b\u4e0b\u65b0SOTA\uff0c\u8d85\u8d8a\u592720\u500d\u7684\u6a21\u578b\uff1b\u5728EVEREST\u75c5\u6bd2\u9002\u5e94\u5ea6\u57fa\u51c6\u4e0a\u63a5\u8fd1\u7ed3\u6784\u611f\u77e5\u65b9\u6cd5\u3002", "conclusion": "Proust\u5728\u9002\u5e94\u5ea6\u9884\u6d4b\u548c\u751f\u6210\u80fd\u529b\u4e4b\u95f4\u627e\u5230\u4e86\u5e73\u8861\u70b9\uff0c\u5176\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\u4e3a\u86cb\u767d\u8d28\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u663e\u793a\u4f4d\u7f6e\u71b5\u65b9\u5dee\u80fd\u9884\u6d4b\u68c0\u7d22\u589e\u5f3a\u7684\u6548\u679c\uff0c\u8fd9\u4e9b\u6d1e\u5bdf\u53ef\u5728\u89c4\u6a21\u6269\u5927\u65f6\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2602.01849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01849", "abs": "https://arxiv.org/abs/2602.01849", "authors": ["Ziwei Luo", "Ziqi Jin", "Lei Wang", "Lidong Bing", "Thomas B. Sch\u00f6n"], "title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models", "comment": "Project page: https://algolzw.github.io/sr-smc", "summary": "This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u5956\u52b1\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u6269\u6563\u7c92\u5b50\u4ea4\u4e92\u548c\u8f68\u8ff9\u7ea7\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u81ea\u5956\u52b1\u4fe1\u53f7\uff0c\u6539\u8fdb\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u91c7\u6837\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8d2a\u5a6a\u91c7\u6837\u7b56\u7565\uff0c\u53ea\u4fdd\u7559\u6700\u9ad8\u7f6e\u4fe1\u5ea6token\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u8def\u5f84\u591a\u6837\u6027\u53d7\u9650\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u63d0\u51fa\u81ea\u5956\u52b1SMC\u7b97\u6cd5\uff1a1\uff09\u5e76\u884c\u542f\u52a8\u591a\u4e2a\u4ea4\u4e92\u6269\u6563\u8fc7\u7a0b\uff08\u7c92\u5b50\uff09\u63a2\u7d22\u8f68\u8ff9\uff1b2\uff09\u5f15\u5165\u8f68\u8ff9\u7ea7\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u81ea\u5956\u52b1\u4fe1\u53f7\u5206\u914d\u7c92\u5b50\u91cd\u8981\u6027\u6743\u91cd\uff1b3\uff09\u8fed\u4ee3\u52a0\u6743\u548c\u91cd\u91c7\u6837\uff0c\u7cfb\u7edf\u5f15\u5bfc\u751f\u6210\u5411\u5168\u5c40\u7f6e\u4fe1\u5ea6\u9ad8\u3001\u8d28\u91cf\u597d\u7684\u6837\u672c\u3002", "result": "\u5728\u591a\u79cd\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5956\u52b1\u6307\u5bfc\u5373\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\uff0c\u6709\u6548\u5c06\u5e76\u884c\u63a8\u7406\u80fd\u529b\u8f6c\u5316\u4e3a\u91c7\u6837\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "\u81ea\u5956\u52b1SMC\u7b97\u6cd5\u89e3\u51b3\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u591a\u6837\u6027\u53d7\u9650\u95ee\u9898\uff0c\u901a\u8fc7\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u6846\u67b6\u5b9e\u73b0\u63a8\u7406\u65f6\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.01852", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01852", "abs": "https://arxiv.org/abs/2602.01852", "authors": ["Zeyan Wang", "Zhengmao Liu", "Yongxin Cai", "Chi Li", "Xiaoying Tang", "Jingchao Chen", "Zibin Pan", "Jing Qiu"], "title": "FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization", "comment": null, "summary": "Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.", "AI": {"tldr": "FUPareto\uff1a\u57fa\u4e8e\u5e15\u7d2f\u6258\u4f18\u5316\u7684\u8054\u90a6\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u8fb9\u754c\u504f\u79fb\u635f\u5931\u548c\u96f6\u7a7a\u95f4\u6295\u5f71\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u89e3\u51b3\u8054\u90a6\u9057\u5fd8\u4e2d\u7684\u6548\u7528-\u9057\u5fd8\u51b2\u7a81\u548c\u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u9057\u5fd8\u76ee\u6807\u5e38\u635f\u5bb3\u6a21\u578b\u6548\u7528\u6216\u589e\u52a0\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff1b2) \u9057\u5fd8\u4e0e\u6548\u7528\u5b58\u5728\u56fa\u6709\u51b2\u7a81\uff0c\u8fdb\u4e00\u6b65\u9057\u5fd8\u5fc5\u7136\u635f\u5bb3\u4fdd\u7559\u6027\u80fd\uff1b3) \u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\u652f\u6301\u5dee\uff0c\u68af\u5ea6\u51b2\u7a81\u964d\u4f4e\u9057\u5fd8\u8d28\u91cf\u3002", "method": "\u63d0\u51faFUPareto\u6846\u67b6\uff1a1) \u6700\u5c0f\u8fb9\u754c\u504f\u79fb\u635f\u5931\u5f3a\u5236\u76ee\u6807\u7c7blogit\u4f4e\u4e8e\u6700\u9ad8\u975e\u76ee\u6807\u7c7blogit\uff0c\u63d0\u9ad8\u9057\u5fd8\u6548\u7387\u5e76\u964d\u4f4eMIA\u98ce\u9669\uff1b2) \u5e15\u7d2f\u6258\u6539\u8fdb\u6b65\u9aa4\u4fdd\u6301\u6a21\u578b\u6548\u7528\uff1b3) \u5e15\u7d2f\u6258\u6269\u5c55\u4fdd\u8bc1\u9057\u5fd8\uff0c\u96c6\u6210\u96f6\u7a7a\u95f4\u6295\u5f71\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u89e3\u8026\u68af\u5ea6\u51b2\u7a81\uff0c\u5b9e\u73b0\u591a\u5ba2\u6237\u7aef\u516c\u5e73\u5e76\u53d1\u9057\u5fd8\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFUPareto\u5728\u9057\u5fd8\u6548\u679c\u548c\u4fdd\u7559\u6548\u7528\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "FUPareto\u901a\u8fc7\u5e15\u7d2f\u6258\u589e\u5f3a\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u9057\u5fd8\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u516c\u5e73\u7684\u591a\u5ba2\u6237\u7aef\u5e76\u53d1\u9057\u5fd8\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6548\u7528\u635f\u5931\u3002"}}
{"id": "2602.01853", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01853", "abs": "https://arxiv.org/abs/2602.01853", "authors": ["Xiangkun Wu", "Qianglin Wen", "Yingying Zhang", "Hongtu Zhu", "Ting Li", "Chengchun Shi"], "title": "Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning", "comment": null, "summary": "A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217A/B\u6d4b\u8bd5\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5b8c\u6574\u5386\u53f2\u4fe1\u606f\u548c\u76f4\u63a5\u4f18\u5316MSE\u6765\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5b9e\u9a8c\u4e2d\u7684A/B\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\uff1a\u73b0\u6709\u8bbe\u8ba1\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5b8c\u6574\u5386\u53f2\u4fe1\u606f\uff0c\u4e14\u4f9d\u8d56\u5f3a\u5047\u8bbe\u6765\u8fd1\u4f3c\u76ee\u6807\u51fd\u6570\uff08\u5982\u5904\u7406\u6548\u5e94\u4f30\u8ba1\u7684\u5747\u65b9\u8bef\u5dee\uff09", "method": "\u9996\u5148\u5efa\u7acb\u4e0d\u53ef\u80fd\u5b9a\u7406\uff0c\u8bc1\u660e\u4e0d\u57fa\u4e8e\u5b8c\u6574\u5386\u53f2\u4f1a\u5bfc\u81f4\u6b21\u4f18\u8bbe\u8ba1\uff1b\u7136\u540e\u63d0\u51faTransformer\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528Transformer\u57fa\u4e8e\u5b8c\u6574\u5386\u53f2\u8fdb\u884c\u5206\u914d\uff0c\u4f7f\u7528RL\u76f4\u63a5\u4f18\u5316MSE\u800c\u4e0d\u4f9d\u8d56\u9650\u5236\u6027\u5047\u8bbe", "result": "\u5728\u5408\u6210\u6570\u636e\u3001\u516c\u5f00\u8c03\u5ea6\u6a21\u62df\u5668\u548c\u771f\u5b9e\u4e16\u754c\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u8bbe\u8ba1", "conclusion": "\u63d0\u51fa\u7684Transformer RL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217A/B\u6d4b\u8bd5\u4e2d\u7684\u5386\u53f2\u5229\u7528\u548c\u76ee\u6807\u51fd\u6570\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u6027\u80fd"}}
{"id": "2602.01855", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.01855", "abs": "https://arxiv.org/abs/2602.01855", "authors": ["Blagoj Hristov", "Hristijan Gjoreski", "Vesna Ojleska Latkoska", "Gorjan Nadzinski"], "title": "Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG", "comment": null, "summary": "Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\\pm$ 2.98% to 96.9% $\\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6700\u5c11\u4f20\u611f\u5668\u5b9e\u73b0\u7cbe\u786e\u808c\u7535\u63a7\u5236\uff0c\u901a\u8fc7\u6df7\u5408Transformer\u548c\u53ef\u5b66\u4e60\u65f6\u95f4\u5d4c\u5165\uff0c\u5728\u7a00\u758f\u53cc\u901a\u9053sEMG\u4e0a\u8fbe\u523095.7%\u7684F1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u808c\u7535\u5047\u80a2\u63a7\u5236\u4f9d\u8d56\u590d\u6742\u5bc6\u96c6\u4f20\u611f\u5668\u9635\u5217\uff0c\u9650\u5236\u4e86\u6d88\u8d39\u8005\u53ef\u53ca\u6027\u3002\u9700\u8981\u5f00\u53d1\u4f7f\u7528\u6700\u5c11\u4f20\u611f\u5668\u786c\u4ef6\u4f46\u4ecd\u80fd\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u96c6\u6210Time2Vec\u53ef\u5b66\u4e60\u65f6\u95f4\u5d4c\u5165\u6355\u83b7\u751f\u7269\u4fe1\u53f7\u7684\u65f6\u95f4\u626d\u66f2\u7279\u6027\uff1b\u4f7f\u7528\u5f52\u4e00\u5316\u52a0\u6027\u878d\u5408\u7b56\u7565\u5bf9\u9f50\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\u7684\u6f5c\u5728\u5206\u5e03\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u534f\u8bae\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u572810\u7c7b\u52a8\u4f5c\u96c6\u4e0a\u8fbe\u523095.7% \u00b1 0.20%\u7684\u591a\u53d7\u8bd5\u8005F1\u5206\u6570\uff0c\u4f18\u4e8e\u6807\u51c6Transformer\u548cCNN-LSTM\u6a21\u578b\u3002\u5feb\u901f\u6821\u51c6\u534f\u8bae\uff08\u6bcf\u4e2a\u624b\u52bf\u4ec5\u97002\u6b21\u8bd5\u9a8c\uff09\u53ef\u5c06\u65b0\u53d7\u8bd5\u8005\u6027\u80fd\u4ece21.0%\u63d0\u5347\u81f396.9%\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u65f6\u95f4\u5d4c\u5165\u53ef\u4ee5\u8865\u507f\u4f4e\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u6311\u6218\u4e86\u9ad8\u5bc6\u5ea6\u4f20\u611f\u7684\u5fc5\u8981\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u5feb\u901f\u4e2a\u6027\u5316\u7684\u5047\u80a2\u63a5\u53e3\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u6709\u6548\u7684\u84dd\u56fe\u3002"}}
{"id": "2602.01877", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.01877", "abs": "https://arxiv.org/abs/2602.01877", "authors": ["Zichun Wang", "Gar Goei Loke", "Ruiting Zuo"], "title": "Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal", "comment": null, "summary": "Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.", "AI": {"tldr": "\u63d0\u51faA-OVE\u6a21\u578b\u7528\u4e8e\u81ea\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u7684\u6570\u636e\u9a71\u52a8\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edf\u9884\u6d4b-\u4f18\u5316\u65b9\u6cd5\u5728\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u66f4\u4f18", "motivation": "\u4f20\u7edf\u4f30\u8ba1-\u4f18\u5316\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e0b\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\uff0c\u7279\u522b\u662f\u5728\u81ea\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\uff08VARMA\u8fc7\u7a0b\uff09\u7684\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u76f4\u63a5\u4f18\u5316\u6837\u672c\u5916\u6027\u80fd\u7684\u6a21\u578b", "method": "\u63d0\u51fa\u81ea\u76f8\u5173\u4f18\u5316-\u4f30\u8ba1\uff08A-OVE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5145\u5206\u7edf\u8ba1\u91cf\u83b7\u5f97\u6837\u672c\u5916\u6700\u4f18\u89e3\uff0c\u5e76\u5f00\u53d1\u9012\u5f52\u5f62\u5f0f\u8ba1\u7b97\u5145\u5206\u7edf\u8ba1\u91cf", "result": "A-OVE\u5728\u5e26\u4ea4\u6613\u6210\u672c\u7684\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\u76f8\u5bf9\u4e8e\u5b8c\u7f8e\u4fe1\u606f\u57fa\u51c6\u5b9e\u73b0\u4f4e\u9057\u61be\uff0c\u4f18\u4e8e\u9884\u6d4b-\u4f18\u5316\u7684\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff1b\u9ad8\u7cbe\u5ea6ML\u6a21\u578b\u53ef\u80fd\u51b3\u7b56\u8d28\u91cf\u66f4\u5dee", "conclusion": "A-OVE\u6a21\u578b\u5728\u81ea\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u4e0b\u6709\u6548\uff0c\u6027\u80fd\u5728\u5c0f\u8bef\u8bbe\u4e0b\u4fdd\u6301\u7a33\u5065\uff0c\u547c\u5e94\u4e86\u6570\u636e\u9a71\u52a8\u4f18\u5316\u4e2d\u51b3\u7b56\u8d28\u91cf\u4e0e\u9884\u6d4b\u7cbe\u5ea6\u53ef\u80fd\u4e0d\u4e00\u81f4\u7684\u53d1\u73b0"}}
{"id": "2602.01897", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01897", "abs": "https://arxiv.org/abs/2602.01897", "authors": ["Sungheon Jeong", "Sanggeon Yun", "Ryozo Masukawa", "Wenjun Haung", "Hanning Chen", "Mohsen Imani"], "title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs", "comment": null, "summary": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \\emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \\emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \\emph{Code is available at} \\texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.", "AI": {"tldr": "\u63d0\u51fa\u5185\u90e8\u6d41\u7b7e\u540d\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7LLM\u5185\u90e8\u6df1\u5ea6\u52a8\u6001\u6765\u68c0\u6d4b\u548c\u4fee\u6b63\u4e0d\u5fe0\u5b9e\u751f\u6210\uff0c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u6a21\u578b\u6216\u5916\u90e8\u9a8c\u8bc1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u751f\u6210\u6d41\u7545\u4f46\u4e0d\u5fe0\u5b9e\u4e8e\u4e0a\u4e0b\u6587\u7684\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u6216\u751f\u6210\u540e\u5355\u72ec\u5224\u65ad\uff0c\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u51b3\u7b56\u52a8\u6001\u7684\u81ea\u6211\u68c0\u67e5\u673a\u5236\u3002", "method": "\u5f15\u5165\u5185\u90e8\u6d41\u7b7e\u540d\uff1a1) \u5728\u56fa\u5b9a\u5757\u95f4\u76d1\u63a7\u8fb9\u754c\u8fdb\u884c\u6df1\u5ea6\u52a8\u6001\u5ba1\u8ba1\uff1b2) \u901a\u8fc7\u504f\u7f6e\u4e2d\u5fc3\u76d1\u63a7\u7a33\u5b9atoken\u7ea7\u8fd0\u52a8\uff1b3) \u5728\u6bcf\u4e2a\u6df1\u5ea6\u7a97\u53e3\u5185\u6784\u5efa\u7d27\u51d1\u7684\u79fb\u52a8\u8bfb\u53d6\u5bf9\u9f50\u5b50\u7a7a\u95f4\uff1b4) \u4f7f\u7528\u6b63\u4ea4\u4f20\u8f93\u5bf9\u9f50\u76f8\u90bb\u7a97\u53e3\u5e27\uff0c\u83b7\u5f97\u6df1\u5ea6\u53ef\u6bd4\u7684\u4f20\u8f93\u6b65\u957f\u3001\u8f6c\u5411\u89d2\u548c\u5b50\u7a7a\u95f4\u6f02\u79fb\u6458\u8981\uff1b5) \u8bad\u7ec3\u8f7b\u91cfGRU\u9a8c\u8bc1\u5668\u8fdb\u884c\u81ea\u6211\u68c0\u67e5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u4e0d\u5fe0\u5b9e\u751f\u6210\uff0c\u8fd8\u80fd\u5b9a\u4f4d\u95ee\u9898\u6df1\u5ea6\u4e8b\u4ef6\uff0c\u5e76\u652f\u6301\u9488\u5bf9\u6027\u4fee\u6b63\uff1a\u6a21\u578b\u56de\u6eda\u5230\u95ee\u9898token\uff0c\u5728\u8bc6\u522b\u5757\u5904\u94b3\u5236\u5f02\u5e38\u4f20\u8f93\u6b65\u957f\uff0c\u540c\u65f6\u4fdd\u7559\u6b63\u4ea4\u6b8b\u5dee\u3002", "conclusion": "\u5185\u90e8\u6d41\u7b7e\u540d\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u90e8\u51b3\u7b56\u52a8\u6001\u7684\u53ef\u64cd\u4f5c\u5b9a\u4f4d\u548c\u4f4e\u5f00\u9500\u81ea\u6211\u68c0\u67e5\u7ba1\u9053\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.01898", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01898", "abs": "https://arxiv.org/abs/2602.01898", "authors": ["Sanna Jarl", "Maria B\u00e5nkestad", "Jonathan J. S. Scragg", "Jens Sj\u00f6lund"], "title": "Observation-dependent Bayesian active learning via input-warped Gaussian processes", "comment": "13 pages", "summary": "Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5355\u8c03\u91cd\u53c2\u6570\u5316\u626d\u66f2\u8f93\u5165\u7a7a\u95f4\uff0c\u4f7f\u8d1d\u53f6\u65af\u4e3b\u52a8\u5b66\u4e60\u7684\u63a2\u7d22\u7b56\u7565\u80fd\u591f\u6839\u636e\u89c2\u6d4b\u53cd\u9988\u52a8\u6001\u8c03\u6574\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387", "motivation": "\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u7684\u540e\u9a8c\u65b9\u5dee\u4ec5\u901a\u8fc7\u8d85\u53c2\u6570\u4f9d\u8d56\u89c2\u6d4b\u8f93\u51fa\uff0c\u5bfc\u81f4\u63a2\u7d22\u7b56\u7565\u5bf9\u5b9e\u9645\u6d4b\u91cf\u503c\u4e0d\u654f\u611f\u3002\u9700\u8981\u5f15\u5165\u89c2\u6d4b\u4f9d\u8d56\u7684\u53cd\u9988\u673a\u5236\u6765\u6539\u8fdb\u4e3b\u52a8\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387", "method": "\u901a\u8fc7\u5b66\u4e60\u5355\u8c03\u91cd\u53c2\u6570\u5316\u51fd\u6570\u626d\u66f2\u8f93\u5165\u7a7a\u95f4\uff0c\u4f7f\u8bbe\u8ba1\u7b56\u7565\u80fd\u591f\u6839\u636e\u89c2\u6d4b\u5230\u7684\u53d8\u5f02\u6027\u6269\u5c55\u6216\u538b\u7f29\u8f93\u5165\u7a7a\u95f4\u533a\u57df\uff0c\u4ece\u800c\u5f71\u54cd\u57fa\u4e8e\u65b9\u5dee\u7684\u83b7\u53d6\u51fd\u6570\u884c\u4e3a\u3002\u63d0\u51fa\u65b0\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u66ff\u4ee3\u4f20\u7edf\u7684\u8fb9\u7f18\u4f3c\u7136\u8bad\u7ec3", "result": "\u8be5\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u4e3b\u52a8\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u7a33\u6027\u6311\u6218\u4f20\u7edf\u65b9\u6cd5\u7684\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18", "conclusion": "\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u626d\u66f2\u5f15\u5165\u89c2\u6d4b\u4f9d\u8d56\u53cd\u9988\u662f\u6539\u8fdb\u8d1d\u53f6\u65af\u4e3b\u52a8\u5b66\u4e60\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u81ea\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u6bd4\u4f20\u7edf\u8fb9\u7f18\u4f3c\u7136\u8bad\u7ec3\u6548\u679c\u66f4\u597d"}}
{"id": "2602.01903", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01903", "abs": "https://arxiv.org/abs/2602.01903", "authors": ["Mingyi Li", "Taira Tsuchiya", "Kenji Yamanishi"], "title": "Data- and Variance-dependent Regret Bounds for Online Tabular MDPs", "comment": "80pages, 4tables", "summary": "This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u7ebf\u8868\u683cMDP\u95ee\u9898\uff0c\u63d0\u51fa\"\u4e24\u5168\u5176\u7f8e\"\u7b97\u6cd5\uff0c\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u6570\u636e\u4f9d\u8d56\u7684\u9057\u61be\u754c\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0\u65b9\u5dee\u4f9d\u8d56\u7684\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709MDP\u7b97\u6cd5\u901a\u5e38\u5728\u5bf9\u6297\u6027\u6216\u968f\u673a\u6027\u5047\u8bbe\u4e0b\u5206\u522b\u4f18\u5316\uff0c\u7f3a\u4e4f\u80fd\u540c\u65f6\u9002\u5e94\u4e24\u79cd\u73af\u5883\u7684\u7edf\u4e00\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e2\u80fd\u5904\u7406\u5bf9\u6297\u6027\u73af\u5883\u53c8\u80fd\u5229\u7528\u968f\u673a\u6027\u7ed3\u6784\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5168\u5c40\u4f18\u5316\u548c\u7b56\u7565\u4f18\u5316\u7684\u4e24\u79cd\u7b97\u6cd5\u6846\u67b6\uff0c\u90fd\u5efa\u7acb\u5728\u5e26\u6709\u5bf9\u6570\u969c\u788d\u6b63\u5219\u5316\u7684\u4e50\u89c2\u8ddf\u968f\u6b63\u5219\u5316\u9886\u5bfc\u8005\u65b9\u6cd5\u4e0a\u3002\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u4f7f\u7528\u6807\u51c6\u6280\u672f\uff0c\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5219\u5229\u7528\u65b0\u7684\u4e50\u89c2Q\u51fd\u6570\u4f30\u8ba1\u5668\u3002", "result": "\u5168\u5c40\u4f18\u5316\u7b97\u6cd5\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u4e00\u9636\u3001\u4e8c\u9636\u548c\u8def\u5f84\u957f\u5ea6\u9057\u61be\u754c\uff0c\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0\u65b9\u5dee\u611f\u77e5\u7684\u65e0\u95f4\u9699\u754c\u548c\u95f4\u9699\u4f9d\u8d56\u754c\uff08\u5bf9\u6570\u591a\u9879\u5f0f\uff09\u3002\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u8fbe\u5230\u7c7b\u4f3c\u7684\u81ea\u9002\u5e94\u6027\uff08\u76f8\u5dee\u4e00\u4e2a\u65f6\u95f4\u6b65\u56e0\u5b50\uff09\u3002\u5efa\u7acb\u4e86\u6570\u636e\u4f9d\u8d56\u590d\u6742\u5ea6\u5ea6\u91cf\u7684\u9057\u61be\u4e0b\u754c\uff0c\u8bc1\u660e\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u672c\u6587\u5f00\u53d1\u4e86\u80fd\u540c\u65f6\u9002\u5e94\u5bf9\u6297\u6027\u548c\u968f\u673a\u6027\u73af\u5883\u7684MDP\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u590d\u6742\u5ea6\u5ea6\u91cf\u5b9e\u73b0\u4e86\u7cbe\u7ec6\u7684\u6570\u636e\u548c\u65b9\u5dee\u4f9d\u8d56\u9057\u61be\u754c\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5176\u63a5\u8fd1\u6700\u4f18\u6027\u3002"}}
{"id": "2602.01914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01914", "abs": "https://arxiv.org/abs/2602.01914", "authors": ["Wenbo Pan", "Zhichao Liu", "Xianlong Wang", "Haining Yu", "Xiaohua Jia"], "title": "Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs", "comment": "ICML 2025 submission", "summary": "Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.", "AI": {"tldr": "FlashTrace\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u591atoken\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8span\u805a\u5408\u548c\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u4e2d\u7684\u6548\u7387\u74f6\u9888\u548c\u5fe0\u5b9e\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709token\u5f52\u56e0\u65b9\u6cd5\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u6548\u7387\u74f6\u9888\uff1a\u5f52\u56e0M\u4e2a\u76ee\u6807token\u5728N\u957f\u5ea6\u4e0a\u4e0b\u6587\u4e2d\u9700\u8981O(M*N)\u64cd\u4f5c\uff0c\u957f\u4e0a\u4e0b\u6587\u5f52\u56e0\u6781\u6162\uff1b2) \u5fe0\u5b9e\u5ea6\u4e0b\u964d\uff1a\u4e2d\u95f4\u63a8\u7406token\u5438\u6536\u5f52\u56e0\u6743\u91cd\uff0c\u963b\u6b62\u91cd\u8981\u6027\u4f20\u64ad\u56de\u539f\u59cb\u8f93\u5165\u3002", "method": "\u63d0\u51faFlashTrace\u65b9\u6cd5\uff1a1) \u4f7f\u7528span-wise\u805a\u5408\u5728\u5355\u6b21\u8ba1\u7b97\u4e2d\u5b8c\u6210\u591atoken\u76ee\u6807\u7684\u5f52\u56e0\uff1b2) \u8bbe\u8ba1\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u901a\u8fc7\u4e2d\u95f4\u63a8\u7406\u94fe\u8ffd\u8e2a\u91cd\u8981\u6027\u56de\u5230\u6e90\u8f93\u5165\uff1b3) \u4fdd\u6301\u5f52\u56e0\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\uff08RULER\uff09\u548c\u591a\u6b65\u63a8\u7406\uff08MATH, MorehopQA\uff09\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlashTrace\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u83b7\u5f97\u8d85\u8fc7130\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u4f18\u7684\u5fe0\u5b9e\u5ea6\u3002\u9012\u5f52\u5f52\u56e0\u5206\u6790\u663e\u793a\uff0c\u5373\u4f7f\u5355\u6b21\u9012\u5f52\u8df3\u8f6c\u4e5f\u80fd\u901a\u8fc7\u8ffd\u8e2a\u63a8\u7406\u94fe\u63d0\u5347\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "FlashTrace\u901a\u8fc7\u9ad8\u6548\u7684span-wise\u805a\u5408\u548c\u9012\u5f52\u5f52\u56e0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u4e2d\u7684\u5f52\u56e0\u6548\u7387\u548c\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u4e3a\u73b0\u4ee3LLM\u7684\u590d\u6742\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u91ca\u5de5\u5177\u3002"}}
{"id": "2602.01915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01915", "abs": "https://arxiv.org/abs/2602.01915", "authors": ["Elad Sharony", "Tom Jurgenson", "Orr Krupnik", "Dotan Di Castro", "Shie Mannor"], "title": "VLM-Guided Experience Replay", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u81ea\u52a8\u8bc4\u4f30\u548c\u4f18\u5148\u5904\u7406\u6709\u6f5c\u529b\u7684\u5b50\u8f68\u8ff9\uff0c\u5728\u6e38\u620f\u548c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5df2\u6210\u529f\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u4e2a\u7ec4\u4ef6\u4e2d\uff0c\u4f46\u6838\u5fc3\u7684\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5229\u7528VLM\u7684\u8bed\u4e49\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u6765\u6539\u8fdb\u7ecf\u9a8c\u4f18\u5148\u7ea7\u6392\u5e8f\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u667a\u80fd\u4f53\u7ecf\u9a8c\u4e2d\u6709\u6f5c\u529b\u7684\u5b50\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u79bb\u6563\u548c\u8fde\u7eed\u9886\u57df\uff0c\u6db5\u76d6\u6e38\u620f\u548c\u673a\u5668\u4eba\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u573a\u666f\u4e2d\uff0c\u4f7f\u7528\u8be5\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u6cd5\u7684\u667a\u80fd\u4f53\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534711-52%\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad819-45%\u3002", "conclusion": "VLM\u6307\u5bfc\u7684\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u4f18\u5148\u7ea7\u6392\u5e8f\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.01920", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01920", "abs": "https://arxiv.org/abs/2602.01920", "authors": ["Abdul Joseph Fofanah", "Lian Wen", "David Chen"], "title": "PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks", "comment": null, "summary": "Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \\textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\\%) and balanced accuracy (up to +8.3\\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \\texttt{https://github.com/afofanah/PIMPC-GNN}.", "AI": {"tldr": "\u63d0\u51faPIMPC-GNN\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u542f\u53d1\u7684\u591a\u76f8\u5171\u8bc6\u673a\u5236\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5c11\u6570\u7c7b\u53ec\u56de\u7387\u548c\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c11\u6570\u7c7b\u6837\u672c\u4e0d\u8db3\u5bfc\u81f4\u9884\u6d4b\u504f\u5411\u591a\u6570\u7c7b\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u5584\u5c11\u6570\u7c7b\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u4e92\u8865\u52a8\u6001\uff1a\u70ed\u529b\u5b66\u6269\u6563\u4f20\u64ad\u5c11\u6570\u7c7b\u6807\u7b7e\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\uff0cKuramoto\u540c\u6b65\u901a\u8fc7\u632f\u8361\u5171\u8bc6\u5bf9\u9f50\u5c11\u6570\u7c7b\u8282\u70b9\uff0c\u8c31\u5d4c\u5165\u901a\u8fc7\u7ed3\u6784\u6b63\u5219\u5316\u5206\u79bb\u7c7b\u522b\u3002\u4f7f\u7528\u7c7b\u522b\u81ea\u9002\u5e94\u96c6\u6210\u6743\u91cd\u548c\u7ed3\u5408\u5e73\u8861\u4ea4\u53c9\u71b5\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u4e0d\u5e73\u8861\u611f\u77e5\u635f\u5931\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c5-100\u7684\u4e0d\u5e73\u8861\u6bd4\u4f8b\u4e0b\uff0cPIMPC-GNN\u4f18\u4e8e16\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c11\u6570\u7c7b\u53ec\u56de\u7387\u63d0\u5347\u9ad8\u8fbe12.7%\uff0c\u5e73\u8861\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe8.3%\u3002", "conclusion": "PIMPC-GNN\u6846\u67b6\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u8282\u70b9\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u8fd8\u4e3a\u56fe\u5b66\u4e60\u4e2d\u7684\u5171\u8bc6\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.01922", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01922", "abs": "https://arxiv.org/abs/2602.01922", "authors": ["Orell Trautmann", "Olaf Wolkenhauer", "Cl\u00e9mence R\u00e9da"], "title": "Embedding Learning on Multiplex Networks for Link Prediction", "comment": null, "summary": "Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u591a\u91cd\u7f51\u7edc\u5d4c\u5165\u5b66\u4e60\u7528\u4e8e\u94fe\u8def\u9884\u6d4b\u7684\u7efc\u8ff0\u6587\u7ae0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u6cd5\u3001\u516c\u5e73\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u9488\u5bf9\u6709\u5411\u591a\u91cd\u7f51\u7edc\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u6d4b\u8bd5\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u590d\u6742\u6027\u589e\u52a0\uff08\u8fde\u63a5\u6570\u91cf\u548c\u4ea4\u4e92\u7c7b\u578b\u589e\u591a\uff09\uff0c\u591a\u91cd\u7f51\u7edc\u4e0a\u7684\u5d4c\u5165\u5b66\u4e60\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u5206\u7c7b\u548c\u516c\u5e73\u8bc4\u4f30\u6807\u51c6\uff0c\u7279\u522b\u662f\u5728\u6709\u5411\u591a\u91cd\u7f51\u7edc\u4e0a\u7684\u8bc4\u4f30\u5b58\u5728\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u6539\u8fdb\u7684\u5206\u7c7b\u6cd5\uff0c\u6839\u636e\u5d4c\u5165\u7c7b\u578b\u548c\u6280\u672f\u5bf9\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u6bd4\u8f83\uff1b2. \u5ba1\u67e5\u5e76\u89e3\u51b3\u591a\u91cd\u7f51\u7edc\u4e0a\u5d4c\u5165\u5b66\u4e60\u5728\u94fe\u8def\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u95ee\u9898\uff1b3. \u9488\u5bf9\u6709\u5411\u591a\u91cd\u7f51\u7edc\u63d0\u51fa\u65b0\u9896\u4e14\u516c\u5e73\u7684\u6d4b\u8bd5\u6d41\u7a0b\u3002", "result": "\u8be5\u7efc\u8ff0\u4e3a\u591a\u91cd\u7f51\u7edc\u5d4c\u5165\u5b66\u4e60\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u516c\u5e73\u8bc4\u4f30\u6307\u5357\uff0c\u5e76\u4e3a\u6709\u5411\u591a\u91cd\u7f51\u7edc\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u66f4\u9ad8\u6548\u3001\u53ef\u8ffd\u8e2a\u7684\u5d4c\u5165\u5b66\u4e60\u65b9\u6cd5\u53d1\u5c55\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u662f\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u8ffd\u8e2a\u7684\u591a\u91cd\u7f51\u7edc\u5d4c\u5165\u5b66\u4e60\u65b9\u6cd5\u53ca\u5176\u516c\u5e73\u8bc4\u4f30\u7684\u5173\u952e\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u6a21\u578b\u8bc4\u4f30\u6307\u5357\uff0c\u5e76\u5bf9\u5f53\u524d\u53ef\u7528\u4e8e\u591a\u91cd\u7f51\u7edc\u4e0b\u6e38\u5206\u6790\u7684\u6311\u6218\u548c\u5de5\u5177\u63d0\u4f9b\u4e86\u6709\u89c1\u5730\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.01924", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01924", "abs": "https://arxiv.org/abs/2602.01924", "authors": ["Luc\u00eda Gonz\u00e1lez-Zamorano", "Nuria Balb\u00e1s-Esteban", "Vanessa G\u00f3mez-Verdejo", "Albert Belenguer-Llorens", "Carlos Sevilla-Salcedo"], "title": "Bayesian Integration of Nonlinear Incomplete Clinical Data", "comment": null, "summary": "Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.", "AI": {"tldr": "BIONIC\u662f\u4e00\u4e2a\u8d1d\u53f6\u65af\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u9ad8\u7ef4\u5f02\u6784\u7684\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff0c\u80fd\u5904\u7406\u7ed3\u6784\u5316\u7f3a\u5931\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u4f9b\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u5177\u6709\u9ad8\u7ef4\u5ea6\u3001\u5f02\u6784\u8868\u793a\u548c\u7ed3\u6784\u5316\u7f3a\u5931\u7684\u7279\u70b9\uff0c\u8fd9\u7ed9\u9884\u6d4b\u5efa\u6a21\u3001\u6570\u636e\u6574\u5408\u548c\u53ef\u89e3\u91ca\u6027\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u6027\u3002", "method": "\u63d0\u51faBIONIC\u6846\u67b6\uff0c\u91c7\u7528\u8054\u5408\u751f\u6210-\u5224\u522b\u5f0f\u6f5c\u5728\u67b6\u6784\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u591a\u6a21\u6001\u516c\u5f0f\u6574\u5408\u5f02\u6784\u6570\u636e\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u5d4c\u5165\u5904\u7406\u590d\u6742\u6a21\u6001\uff08\u5982\u533b\u5b66\u56fe\u50cf\u548c\u4e34\u5e8a\u6587\u672c\uff09\uff0c\u540c\u65f6\u5c06\u7ed3\u6784\u5316\u4e34\u5e8a\u53d8\u91cf\u76f4\u63a5\u7eb3\u5165\u6a21\u578b\u3002\u663e\u5f0f\u5efa\u6a21\u6a21\u6001\u7ea7\u548c\u53d8\u91cf\u7ea7\u7f3a\u5931\u4ee5\u53ca\u7f3a\u5931\u6807\u7b7e\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4e34\u5e8a\u548c\u751f\u7269\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4ee3\u8868\u6027\u591a\u6a21\u6001\u57fa\u7ebf\u65b9\u6cd5\uff0cBIONIC\u5c55\u73b0\u51fa\u5f3a\u5927\u4e14\u4e00\u81f4\u7684\u5224\u522b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5b8c\u6574\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BIONIC\u4e0d\u4ec5\u63d0\u4f9b\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u5176\u6f5c\u5728\u7ed3\u6784\u63d0\u4f9b\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u6a21\u6001\u76f8\u5173\u6027\u7684\u7fa4\u4f53\u7ea7\u5206\u6790\u548c\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\uff0c\u4e3a\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2602.01935", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.01935", "abs": "https://arxiv.org/abs/2602.01935", "authors": ["Annabelle Sujun Tang", "Christopher Priebe", "Lianhui Qin", "Hadi Esmaeilzadeh"], "title": "COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation", "comment": null, "summary": "Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.", "AI": {"tldr": "COLT\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eabMCTS\u6811\u5b9e\u73b0\u7f16\u8bd1\u5668\u4f18\u5316\uff0c\u8ba9\u5c0f\u6a21\u578b\u534f\u4f5c\u8fbe\u5230\u5927\u6a21\u578b\u6027\u80fd", "motivation": "\u6a21\u578b\u670d\u52a1\u6210\u672c\u4e3b\u5bfcAI\u7cfb\u7edf\uff0c\u9700\u8981\u7f16\u8bd1\u5668\u4f18\u5316\u3002\u5927LLM\u6307\u5bfc\u7f16\u8bd1\u5668\u641c\u7d22\u6548\u679c\u597d\u4f46\u6602\u8d35\uff0c\u5c0f\u6a21\u578b\u5355\u72ec\u4f7f\u7528\u4e0d\u53ef\u9760\u3002\u7814\u7a76\u591aLLM\u534f\u4f5c\u80fd\u5426\u8fbe\u5230\u6216\u8d85\u8fc7\u5355\u4e2a\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faCOLT\u6846\u67b6\uff0c\u5728\u5355\u4e2aMCTS\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u591a\u6a21\u578b\u534f\u8c03\u63a8\u7406\u3002\u4f7f\u7528\u5171\u4eabMCTS\u6811\u4f5c\u4e3a\u534f\u4f5c\u57fa\u7840\uff0c\u91cd\u7528\u8f6c\u6362\u524d\u7f00\u548c\u8de8\u6a21\u578b\u4ef7\u503c\u4f20\u64ad\u3002\u6bcf\u8f6e\u8fed\u4ee3\u4e2d\uff0c\u6267\u884cLLM\u63d0\u51fa\u8054\u5408\u52a8\u4f5c\uff08\u7f16\u8bd1\u5668\u8f6c\u6362\uff0c\u4e0b\u4e00\u4e2a\u67e5\u8be2\u7684\u6a21\u578b\uff09\u3002\u5f15\u5165\u6a21\u578b\u611f\u77e5\u6811\u7b56\u7565\u504f\u5411\u5c0f\u6a21\u578b\uff0c\u4ee5\u53ca\u5f53\u641c\u7d22\u6301\u7eed\u9000\u5316\u65f6\u5347\u7ea7\u5230\u5927\u6a21\u578b\u7684\u822a\u5411\u4fee\u6b63\u673a\u5236\u3002", "result": "\u8bba\u6587\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u80fd\u591f\u964d\u4f4e\u63a8\u7406\u6210\u672c\u540c\u65f6\u4fdd\u6301\u4f18\u5316\u6027\u80fd\u7684\u6846\u67b6\u3002", "conclusion": "COLT\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u907f\u514d\u4e86\u91cd\u578b\u5185\u90e8\u63a8\u7406\u673a\u5236\u548c\u4f20\u7edf\u4ee3\u7406\u673a\u5236\uff0c\u6709\u671b\u8ba9\u5c0f\u6a21\u578b\u534f\u4f5c\u8fbe\u5230\u5927\u6a21\u578b\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2602.01936", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01936", "abs": "https://arxiv.org/abs/2602.01936", "authors": ["Abdul Joseph Fofanah", "Lian Wen", "David Chen"], "title": "PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting", "comment": null, "summary": "Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.", "AI": {"tldr": "MCPST\u662f\u4e00\u4e2a\u7528\u4e8e\u5c11\u6837\u672c\u4ea4\u901a\u9884\u6d4b\u7684\u591a\u9636\u6bb5\u5171\u8bc6\u65f6\u7a7a\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u4ea4\u901a\u52a8\u6001\u7684\u6269\u6563\u3001\u540c\u6b65\u548c\u8c31\u5d4c\u5165\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5171\u8bc6\u673a\u5236\u548c\u7ed3\u6784\u5316\u5143\u5b66\u4e60\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u7684\u8de8\u57df\u573a\u666f\u4e2d\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\uff0c\u8de8\u57df\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u51c6\u786e\u4ea4\u901a\u6d41\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u6709\u9650\u7684\u5386\u53f2\u6570\u636e\u963b\u788d\u6a21\u578b\u8bad\u7ec3\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u590d\u6742\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u975e\u7ebf\u6027\u52a8\u6001\u4f7f\u4e0d\u540c\u57ce\u5e02\u95f4\u7684\u5c11\u6837\u672c\u5b66\u4e60\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u63d0\u51faMCPST\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u591a\u9636\u6bb5\u5f15\u64ce\u901a\u8fc7\u6269\u6563\u3001\u540c\u6b65\u548c\u8c31\u5d4c\u5165\u5efa\u6a21\u4ea4\u901a\u52a8\u6001\uff1b2\uff09\u81ea\u9002\u5e94\u5171\u8bc6\u673a\u5236\u52a8\u6001\u878d\u5408\u9636\u6bb5\u7279\u5b9a\u9884\u6d4b\u5e76\u5f3a\u5236\u4e00\u81f4\u6027\uff1b3\uff09\u7ed3\u6784\u5316\u5143\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u65b0\u57ce\u5e02\u7684\u5feb\u901f\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cMCPST\u8d85\u8d8a\u4e8614\u79cd\u6700\u5148\u8fdb\u7684\u65f6\u7a7a\u56fe\u5b66\u4e60\u65b9\u6cd5\u3001\u52a8\u6001\u56fe\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u65f6\u7a7a\u9884\u6d4b\u65b9\u6cd5\u548c\u8de8\u57df\u5c11\u6837\u672c\u8bbe\u7f6e\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u6240\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u3002", "conclusion": "MCPST\u901a\u8fc7\u5c06\u4ea4\u901a\u9884\u6d4b\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u591a\u9636\u6bb5\u5171\u8bc6\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u8de8\u57df\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2602.01937", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01937", "abs": "https://arxiv.org/abs/2602.01937", "authors": ["Suhan Guo", "Bingxu Wang", "Shaodan Zhang", "Furao Shen"], "title": "T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation", "comment": null, "summary": "Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.", "AI": {"tldr": "T-LLM\uff1a\u901a\u8fc7\u65f6\u95f4\u84b8\u998f\u6846\u67b6\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\u8d4b\u4e88\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6559\u5e08\u6a21\u578b\u5728\u8bad\u7ec3\u671f\u95f4\u4f20\u9012\u9884\u6d4b\u884c\u4e3a\uff0c\u63a8\u7406\u65f6\u4ec5\u4f7f\u7528LLM\u8fdb\u884c\u9884\u6d4b\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0e\u5e95\u5c42\u8fc7\u7a0b\u6f14\u5316\u7d27\u5bc6\u76f8\u5173\uff0c\u53ea\u80fd\u968f\u65f6\u95f4\u63a8\u79fb\u79ef\u7d2f\uff0c\u8fd9\u9650\u5236\u4e86\u4ec5\u9760\u89c4\u6a21\u9a71\u52a8\u7684\u9884\u8bad\u7ec3\u6548\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8868\u793a\u7ea7\u5bf9\u9f50\u6216\u63a8\u7406\u65f6\u7684\u65f6\u95f4\u6a21\u5757\uff0c\u800c\u975e\u660e\u786e\u6559\u6388LLM\u9884\u6d4b\u884c\u4e3a\u3002", "method": "\u63d0\u51faT-LLM\u65f6\u95f4\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6559\u5e08\u6a21\u578b\uff08\u7ed3\u5408\u8d8b\u52bf\u5efa\u6a21\u548c\u9891\u57df\u5206\u6790\uff09\u5728\u8bad\u7ec3\u671f\u95f4\u5411LLM\u63d0\u4f9b\u7ed3\u6784\u5316\u65f6\u95f4\u76d1\u7763\uff0c\u63a8\u7406\u65f6\u5b8c\u5168\u79fb\u9664\u6559\u5e08\u6a21\u578b\uff0c\u4ec5\u4fdd\u7559LLM\u4f5c\u4e3a\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4f20\u67d3\u75c5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cT-LLM\u5728\u5168\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eLLM\u7684\u9884\u6d4b\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u90e8\u7f72\u6d41\u7a0b\u3002", "conclusion": "T-LLM\u6846\u67b6\u6210\u529f\u5730\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\u8d4b\u4e88\u901a\u7528LLM\uff0c\u901a\u8fc7\u65f6\u95f4\u84b8\u998f\u65b9\u6cd5\u89e3\u51b3\u4e86\u65f6\u95f4\u7ea6\u675f\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.01953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01953", "abs": "https://arxiv.org/abs/2602.01953", "authors": ["Dmitrij Schlesinger", "Boris Flach", "Alexander Shekhovtsov"], "title": "Deep Multivariate Models with Parametric Conditionals", "comment": null, "summary": "We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u6df1\u5ea6\u591a\u53d8\u91cf\u6a21\u578b\uff0c\u901a\u8fc7\u8bad\u7ec3\u53c2\u6570\u5316\u9a6c\u5c14\u53ef\u592b\u94fe\u6838\u6765\u5b66\u4e60\u8054\u5408\u5206\u5e03\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u548c\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u591a\u53d8\u91cf\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u4efb\u52a1\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5728\u5176\u4ed6\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6761\u4ef6\u6982\u7387\u5206\u5e03\u8868\u793a\u8054\u5408\u6982\u7387\u5206\u5e03\uff0c\u5c06\u6bcf\u4e2a\u53d8\u91cf\u7ec4\u4ee5\u5176\u4f59\u53d8\u91cf\u4e3a\u6761\u4ef6\u5efa\u6a21\u3002\u901a\u8fc7\u6700\u5927\u5316\u6570\u636e\u4f3c\u7136\u8bad\u7ec3\u53c2\u6570\u5316\u9a6c\u5c14\u53ef\u592b\u94fe\u6838\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6df1\u5ea6\u591a\u53d8\u91cf\u6a21\u578b\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5f02\u6784\u53d8\u91cf\u96c6\u5408\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u534a\u76d1\u7763\u5b66\u4e60\u573a\u666f\u3002", "conclusion": "\u57fa\u4e8e\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u5efa\u6a21\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6df1\u5ea6\u591a\u53d8\u91cf\u6a21\u578b\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2602.01956", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01956", "abs": "https://arxiv.org/abs/2602.01956", "authors": ["Seonghyeon Park", "Jewon Yeom", "Jaewon Sok", "Jeongjae Park", "Heejun Kim", "Taesup Kim"], "title": "Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation", "comment": null, "summary": "Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u5c0f\u578b\u8349\u7a3f\u6a21\u578b\u9ad8\u6548\u4f30\u8ba1LLM\u4e2d\u6807\u8bb0\u7ea7\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\uff0c\u907f\u514d\u5927\u89c4\u6a21\u96c6\u6210\u8ba1\u7b97\u5f00\u9500", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u51cf\u5c11\u5e7b\u89c9\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u8fc7\u6df1\u5ea6\u96c6\u6210\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5728\u6a21\u578b\u89c4\u6a21\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8", "method": "\u57fa\u4e8e\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\u7406\u8bba\uff0c\u901a\u8fc7\u8349\u7a3f\u6a21\u578b\u95f4\u7684Jensen-Shannon\u6563\u5ea6\uff08\u65b9\u5dee\u4ee3\u7406\uff09\u548c\u8349\u7a3f\u6df7\u5408\u4e0e\u76ee\u6807\u95f4\u7684KL\u6563\u5ea6\uff08\u504f\u5dee\u4ee3\u7406\uff09\u8fd1\u4f3c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff1b\u5f15\u5165\u5728\u7ebf\u968f\u673a\u84b8\u998f\u9ad8\u6548\u8fd1\u4f3c\u76ee\u6807\u805a\u5408\uff0c\u4ee5\u53ca\u6570\u636e\u591a\u6837\u5316\u8349\u7a3f\u7b56\u7565\u589e\u5f3a\u8349\u7a3f\u591a\u6837\u6027", "result": "\u5728GSM8K\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4f30\u8ba1\u8bef\u5dee\uff08RMSE\uff09\u964d\u4f4e\u8fbe37%\uff1b\u5e7b\u89c9\u68c0\u6d4b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6270\u52a8\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u53ef\u5ffd\u7565", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u800c\u4e0d\u663e\u8457\u589e\u52a0\u63a8\u7406\u5f00\u9500"}}
{"id": "2602.01960", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01960", "abs": "https://arxiv.org/abs/2602.01960", "authors": ["Christos Ziakas", "Amir Bar", "Alessandra Russo"], "title": "Grounding Generated Videos in Feasible Plans via World Models", "comment": null, "summary": "Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.", "AI": {"tldr": "GVP-WM\uff1a\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u5c06\u89c6\u9891\u751f\u6210\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u884c\u52a8\u4f5c\u5e8f\u5217\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u89c6\u9891\u751f\u6210\u8ba1\u5212\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u95ee\u9898", "motivation": "\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u96f6\u6837\u672c\u89c6\u89c9\u89c4\u5212\u5668\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u751f\u6210\u7684\u89c6\u9891\u8ba1\u5212\u7ecf\u5e38\u8fdd\u53cd\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u6620\u5c04\u5230\u53ef\u6267\u884c\u52a8\u4f5c\u65f6\u5931\u8d25", "method": "\u63d0\u51faGVP-WM\u65b9\u6cd5\uff1a1\uff09\u4ece\u521d\u59cb\u548c\u76ee\u6807\u89c2\u6d4b\u751f\u6210\u89c6\u9891\u8ba1\u5212\uff1b2\uff09\u901a\u8fc7\u89c6\u9891\u5f15\u5bfc\u7684\u6f5c\u5728\u5171\u4f4d\u6cd5\u5c06\u89c6\u9891\u6307\u5bfc\u6295\u5f71\u5230\u52a8\u6001\u53ef\u884c\u7684\u6f5c\u5728\u8f68\u8ff9\u6d41\u5f62\u4e0a\uff1b3\uff09\u5c06\u63a5\u5730\u95ee\u9898\u8868\u8ff0\u4e3a\u76ee\u6807\u6761\u4ef6\u6f5c\u5728\u7a7a\u95f4\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e0e\u89c6\u9891\u8ba1\u5212\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\u4f18\u5316\u6f5c\u5728\u72b6\u6001\u548c\u52a8\u4f5c", "result": "GVP-WM\u80fd\u591f\u4ece\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u548c\u8fd0\u52a8\u6a21\u7cca\u89c6\u9891\u4e2d\u6062\u590d\u53ef\u884c\u7684\u957f\u65f6\u7a0b\u8ba1\u5212\uff0c\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u6a21\u62df\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d", "conclusion": "GVP-WM\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u751f\u6210\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\uff0c\u6210\u529f\u5c06\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u89c6\u9891\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u89c4\u5212\u7684\u5b9e\u7528\u6027"}}
{"id": "2602.01962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01962", "abs": "https://arxiv.org/abs/2602.01962", "authors": ["Arip Asadulaev", "Maksim Bobrin", "Salem Lahlou", "Dmitry Dylov", "Fakhri Karray", "Martin Takac"], "title": "Zero-Shot Off-Policy Learning", "comment": null, "summary": "Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u89e3\u51b3\u79bb\u7b56\u7565\u5b66\u4e60\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u540e\u7ee7\u5ea6\u91cf\u4e0e\u5e73\u7a33\u5bc6\u5ea6\u6bd4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u79bb\u7b56\u7565\u5b66\u4e60\u76f4\u63a5\u4ece\u56fa\u5b9a\u6570\u636e\u96c6\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u4ef7\u503c\u51fd\u6570\u9ad8\u4f30\u504f\u5dee\u7684\u6311\u6218\uff0c\u5728\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fd9\u4e9b\u6311\u6218\u66f4\u52a0\u663e\u8457\u2014\u2014\u667a\u80fd\u4f53\u9700\u8981\u5728\u6d4b\u8bd5\u65f6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "method": "\u53d1\u73b0\u540e\u7ee7\u5ea6\u91cf\u4e0e\u5e73\u7a33\u5bc6\u5ea6\u6bd4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5229\u7528\u8fd9\u4e00\u6d1e\u5bdf\u8bbe\u8ba1\u7b97\u6cd5\u63a8\u65ad\u6700\u4f18\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\uff0c\u5b9e\u73b0\u5bf9\u4efb\u4f55\u4efb\u52a1\u7684\u5e73\u7a33\u5206\u5e03\u6821\u6b63\uff0c\u5e76\u96c6\u6210\u5230\u524d\u5411-\u540e\u5411\u8868\u793a\u6846\u67b6\u4e2d\u3002", "result": "\u5728SMPL Humanoid\u7684\u8fd0\u52a8\u8ddf\u8e2a\u4efb\u52a1\u3001ExoRL\u7684\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u548c\u957f\u65f6\u57dfOGBench\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u8bad\u7ec3\u81ea\u7531\u673a\u5236\u4e0b\u7684\u5feb\u901f\u4efb\u52a1\u9002\u5e94\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u79bb\u7b56\u7565\u5b66\u4e60\u4e0e\u96f6\u6837\u672c\u9002\u5e94\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u4e24\u4e2a\u7814\u7a76\u9886\u57df\u90fd\u5e26\u6765\u4e86\u76ca\u5904\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01966", "abs": "https://arxiv.org/abs/2602.01966", "authors": ["Hongzhuo Yu", "Fei Zhu", "Guo-Sen Xie", "Ling Shao"], "title": "Self-Consolidation for Self-Evolving Agents", "comment": null, "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.", "AI": {"tldr": "\u63d0\u51faLLM\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u603b\u7ed3\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u5de9\u56fa\u673a\u5236\u5c06\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u5b9e\u73b0\u957f\u671f\u8fdb\u5316\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u901a\u5e38\u662f\u9759\u6001\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u901a\u8fc7\u7ec8\u8eab\u4ea4\u4e92\u8fdb\u5316\u7684\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u68c0\u7d22\u6210\u529f\u8f68\u8ff9\u4f5c\u4e3a\u6f14\u793a\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09\u53ea\u5173\u6ce8\u6210\u529f\uff0c\u5ffd\u7565\u4e86\u5931\u8d25\u5c1d\u8bd5\u4e2d\u7684\u6559\u5b66\u4ef7\u503c\uff1b2\uff09\u6301\u7eed\u79ef\u7d2f\u6587\u672c\u7ecf\u9a8c\u4f1a\u589e\u52a0\u68c0\u7d22\u65f6\u95f4\u3001\u5f15\u5165\u566a\u58f0\u5e76\u8017\u5c3d\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u65b0\u8fdb\u5316\u673a\u5236\uff1a1\uff09\u5bf9\u6bd4\u53cd\u601d\u7b56\u7565\uff0c\u660e\u786e\u603b\u7ed3\u6613\u9519\u6a21\u5f0f\u5e76\u6355\u6349\u53ef\u91cd\u7528\u89c1\u89e3\uff1b2\uff09\u81ea\u6211\u5de9\u56fa\u673a\u5236\uff0c\u5c06\u975e\u53c2\u6570\u5316\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5c06\u5927\u91cf\u5386\u53f2\u7ecf\u9a8c\u5185\u5316\u5230\u5176\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u667a\u80fd\u4f53\u8fdb\u5316\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u6211\u8fdb\u5316\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u548c\u81ea\u6211\u5de9\u56fa\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u667a\u80fd\u4f53\u8fdb\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u957f\u671f\u8fdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.01975", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01975", "abs": "https://arxiv.org/abs/2602.01975", "authors": ["Meng Li", "Peisong Wang", "Yuantian Shao", "Qinghao Hu", "Hongjian Fang", "Yifan Zhang", "Zhihui Wei", "Jian Cheng"], "title": "IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.", "AI": {"tldr": "IntraSlice\uff1a\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5185PCA\u538b\u7f29\u526a\u679d\u7684LLM\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3cPCA\u65b9\u6cd5\u548c\u5168\u5c40\u526a\u679d\u6bd4\u4f8b\u4f30\u8ba1\u5668\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u5de8\u5927\u89c4\u6a21\u6311\u6218\uff0c\u7ed3\u6784\u5316\u526a\u679d\u867d\u80fd\u52a0\u901f\u4f46\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u73b0\u6709PCA\u526a\u679d\u65b9\u6cd5\u4ec5\u5728\u6a21\u5757\u95f4\u5e94\u7528\uff0c\u5f15\u5165\u989d\u5916\u53c2\u6570\u4e14\u6b8b\u5dee\u8fde\u63a5\u4f1a\u4e25\u91cd\u7834\u574f\u6fc0\u6d3b\u5206\u5e03\u3002", "method": "\u63d0\u51faIntraSlice\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5185\u5757\u7ea7PCA\u538b\u7f29\u526a\u679d\u3002\u5229\u7528Transformer\u6a21\u5757\u7ed3\u6784\u7279\u6027\u8bbe\u8ba1\u8fd1\u4f3cPCA\u65b9\u6cd5\uff0c\u5176\u53d8\u6362\u77e9\u9635\u53ef\u5b8c\u5168\u878d\u5408\u5230\u6a21\u578b\u4e2d\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002\u5f15\u5165\u57fa\u4e8ePCA\u7684\u5168\u5c40\u526a\u679d\u6bd4\u4f8b\u4f30\u8ba1\u5668\uff0c\u5728\u4f20\u7edf\u6a21\u5757\u91cd\u8981\u6027\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u8003\u8651\u538b\u7f29\u6fc0\u6d3b\u5206\u5e03\u3002", "result": "\u5728Llama2\u3001Llama3\u548cPhi\u7cfb\u5217\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5728\u5404\u79cd\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u6216\u63a8\u7406\u901f\u5ea6\u4e0b\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f18\u7684\u538b\u7f29\u6027\u80fd\u3002", "conclusion": "IntraSlice\u901a\u8fc7\u6a21\u5757\u5185PCA\u538b\u7f29\u526a\u679d\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5f15\u5165\u989d\u5916\u53c2\u6570\u548c\u7834\u574f\u6fc0\u6d3b\u5206\u5e03\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684LLM\u52a0\u901f\u3002"}}
{"id": "2602.01990", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01990", "abs": "https://arxiv.org/abs/2602.01990", "authors": ["Zhen-Hao Xie", "Jun-Tao Tang", "Yu-Cheng Shi", "Han-Jia Ye", "De-Chuan Zhan", "Da-Wei Zhou"], "title": "SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.", "AI": {"tldr": "SAME\u65b9\u6cd5\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u7a33\u5b9a\u4e13\u5bb6\u8def\u7531\uff0c\u5229\u7528\u5386\u53f2\u8f93\u5165\u534f\u65b9\u5dee\u8fdb\u884c\u66f2\u7387\u611f\u77e5\u7f29\u653e\u9632\u6b62\u4e13\u5bb6\u6f02\u79fb\uff0c\u5728\u65e0\u9700\u56de\u653e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6301\u7eed\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7a00\u758f\u4e13\u5bb6\u8def\u7531\u65b9\u6cd5\u5b58\u5728\u4e13\u5bb6\u8def\u7531\u6f02\u79fb\u95ee\u9898\uff1a\u968f\u7740\u6570\u636e\u5206\u5e03\u53d8\u5316\uff0c\u8def\u7531\u51b3\u7b56\u4e0d\u4e00\u81f4\uff08\u5982\u539f\u672c\u6fc0\u6d3b\u5b9a\u4f4d\u4e13\u5bb6\u7684\u63a5\u5730\u67e5\u8be2\u53ef\u80fd\u88ab\u8def\u7531\u5230\u65e0\u5173\u4e13\u5bb6\uff09\uff0c\u540c\u65f6\u5171\u4eab\u4e13\u5bb6\u4f1a\u88ab\u65b0\u4efb\u52a1\u8986\u76d6\u800c\u4e27\u5931\u539f\u6709\u529f\u80fd\u3002", "method": "\u63d0\u51faSAME\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u5c06\u8def\u7531\u52a8\u6001\u5206\u89e3\u4e3a\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5e76\u4ec5\u66f4\u65b0\u4efb\u52a1\u76f8\u5173\u65b9\u5411\u6765\u7a33\u5b9a\u4e13\u5bb6\u9009\u62e9\uff1b2) \u5229\u7528\u5386\u53f2\u8f93\u5165\u534f\u65b9\u5dee\u8fdb\u884c\u66f2\u7387\u611f\u77e5\u7f29\u653e\u6765\u8c03\u8282\u4e13\u5bb6\u66f4\u65b0\uff0c\u9632\u6b62\u4e13\u5bb6\u6f02\u79fb\uff1b3) \u5f15\u5165\u81ea\u9002\u5e94\u4e13\u5bb6\u6fc0\u6d3b\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u65f6\u51bb\u7ed3\u9009\u5b9a\u4e13\u5bb6\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u8de8\u4efb\u52a1\u5e72\u6270\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eSAME\u5728\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "SAME\u901a\u8fc7\u7a33\u5b9a\u4e13\u5bb6\u8def\u7531\u548c\u9632\u6b62\u4e13\u5bb6\u6f02\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u8def\u7531\u4e00\u81f4\u6027\u548c\u4e13\u5bb6\u529f\u80fd\u4fdd\u6301\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2602.01996", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.MS"], "pdf": "https://arxiv.org/pdf/2602.01996", "abs": "https://arxiv.org/abs/2602.01996", "authors": ["Theologos Anthimopoulos", "Milad Kokhazadeh", "Vasilios Kelefouras", "Benjamin Himpel", "Georgios Keramidas"], "title": "Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations", "comment": "36 pages, 16 figures, this is the author-accepted version of the article published in ACM Transactions on Embedded Computing Systems (TECS), Vol. 24, No. 6", "summary": "Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u4f4e\u79e9\u5206\u89e3\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\u548c\u4e13\u7528\u5de5\u5177\uff0c\u7528\u4e8e\u5728RISC-V\u5904\u7406\u5668\u4e0a\u4f18\u5316\u5168\u8fde\u63a5\u5c42\uff0c\u901a\u8fc7Tensor Train\u5206\u89e3\u548c\u7f16\u8bd1\u5668\u4f18\u5316\u5b9e\u73b03-8\u500d\u52a0\u901f\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982RISC-V\u5e73\u53f0\uff09\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u5168\u8fde\u63a5\u5c42\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\u3002\u4f4e\u79e9\u5206\u89e3\u867d\u80fd\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u5176\u8bbe\u8ba1\u7a7a\u95f4\u590d\u6742\uff0c\u9700\u8981\u5728FLOPs\u3001\u5185\u5b58\u5927\u5c0f\u3001\u63a8\u7406\u65f6\u95f4\u548c\u7cbe\u5ea6\u4e4b\u95f4\u6743\u8861\uff0c\u8fc7\u7a0b\u8017\u65f6\u4e14\u590d\u6742\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u4f4e\u79e9\u5206\u89e3\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528TensorFlow T3F\u5e93\u7684Tensor Train\u5206\u89e3\uff0c\u901a\u8fc7\u4e24\u6b65\u526a\u679d\u8bbe\u8ba1\u7a7a\u95f4\uff1a1)\u6392\u9664\u4f4e\u6548\u5206\u89e3\u5f62\u72b6\uff1b2)\u6392\u9664\u5728RISC-V\u67b6\u6784\u4e0a\u63a8\u7406\u6027\u80fd\u5dee\u7684\u65b9\u6848\u3002\u7136\u540e\u5e94\u7528\u7f16\u8bd1\u5668\u4f18\u5316\u63d0\u5347\u81ea\u5b9a\u4e49T3F\u5c42\u6027\u80fd\u3002", "result": "TT\u5206\u89e3\u5c42\u5e73\u5747\u6bd4IREE\u5feb3\u500d\uff0c\u6bd4Pluto\u5feb8\u500d\uff08\u76f8\u540c\u538b\u7f29\u6a21\u578b\u4e0b\uff09\u3002\u4e3aRISC-V\u67b6\u6784\u7684\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684DNN\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86DNN\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6311\u6218\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4e3aRISC-V\u5e73\u53f0\u7684\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.01997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01997", "abs": "https://arxiv.org/abs/2602.01997", "authors": ["Safal Shrestha", "Anubhav Shrestha", "Aadim Nepal", "Minwu Kim", "Keith Ross"], "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs", "comment": null, "summary": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.", "AI": {"tldr": "\u5c42\u526a\u679d\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u5f0f\u63a8\u7406\u4efb\u52a1\u4e0a\u4e25\u91cd\u9000\u5316\uff0c\u7279\u522b\u662f\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4e0e\u81ea\u751f\u6210\u54cd\u5e94\u53ef\u90e8\u5206\u6062\u590d\u6027\u80fd\uff0c\u4f46\u751f\u6210\u5f0f\u63a8\u7406\u7684\u6062\u590d\u4ecd\u6709\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u5c42\u526a\u679d\u6280\u672f\u5728\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u867d\u7136\u5728\u5206\u7c7b\u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u5f0f\u63a8\u7406\u4efb\u52a1\u4e0a\u4f1a\u51fa\u73b0\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6df1\u5ea6\u51cf\u5c11\u5bf9\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u6709\u9650\u7684\u8bad\u7ec3\u540e\u7ea6\u675f\u4e0b\u63a2\u7d22\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u8de8\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u5206\u6790\u6df1\u5ea6\u51cf\u5c11\u5bf9\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u5728\u8bad\u7ec3\u540e\u7ea6\u675f\u4e0b\uff08\u65e0\u9884\u8bad\u7ec3\u89c4\u6a21\u6570\u636e\u6216\u8ba1\u7b97\uff09\uff0c\u8bc4\u4f30\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u4e0e\u81ea\u751f\u6210\u54cd\u5e94\u7684\u7b80\u5355\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u4e0e\u5148\u524d\u7684\u526a\u679d\u540e\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u4fdd\u6301\u9ad8\u8fbe90%\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u5728\u751f\u6210\u57fa\u51c6\u4e0a\u76f8\u6bd4\u5148\u524d\u6280\u672f\u63d0\u534720-30\u4e2a\u767e\u5206\u70b9\u3002\u4f46\u751f\u6210\u5f0f\u63a8\u7406\u7684\u6062\u590d\u4ecd\u6709\u9650\u5236\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u8f83\u4f4e\u526a\u679d\u6bd4\u4f8b\u3002", "conclusion": "\u5c42\u526a\u679d\u5bf9\u751f\u6210\u5f0f\u63a8\u7406\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u4e3b\u8981\u9002\u7528\u4e8e\u8f83\u4f4e\u526a\u679d\u6bd4\u4f8b\u3002\u7814\u7a76\u4e3a\u5728\u53d7\u9650\u8bad\u7ec3\u540e\u673a\u5236\u4e0b\u6709\u6548\u5e94\u7528\u6df1\u5ea6\u51cf\u5c11\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.02001", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02001", "abs": "https://arxiv.org/abs/2602.02001", "authors": ["Yoonjun Cho", "Dongjae Jeon", "Soeun Kim", "Moongyu Jeon", "Albert No"], "title": "Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs", "comment": null, "summary": "Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\\mathbf{Q} + \\mathbf{L}\\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.", "AI": {"tldr": "SRR\u63d0\u51fa\u7ed3\u6784\u5316\u6b8b\u5dee\u91cd\u5efa\u6846\u67b6\uff0c\u5728PTQ\u4e2d\u901a\u8fc7\u4fdd\u7559\u6743\u91cd\u7684\u4e3b\u8981\u5947\u5f02\u5b50\u7a7a\u95f4\uff0c\u4ec5\u91cf\u5316\u6b8b\u5dee\u90e8\u5206\uff0c\u5e76\u7528\u5269\u4f59\u79e9\u9884\u7b97\u91cd\u5efa\u91cf\u5316\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709QER\u65b9\u6cd5\u5c06\u6240\u6709\u79e9\u9884\u7b97\u7528\u4e8e\u91cf\u5316\u8bef\u5dee\u91cd\u5efa\uff0c\u4f46\u5f53\u6743\u91cd\u672c\u8eab\u5177\u6709\u5185\u5728\u4f4e\u79e9\u7ed3\u6784\u4e14\u91cf\u5316\u7834\u574f\u4e86\u4e3b\u5bfc\u65b9\u5411\u65f6\uff0c\u8fd9\u79cd\u7b56\u7565\u662f\u6b21\u4f18\u7684\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u7559\u6743\u91cd\u91cd\u8981\u7ed3\u6784\u548c\u91cd\u5efa\u91cf\u5316\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u6b8b\u5dee\u91cd\u5efa(SRR)\uff1a1) \u4fdd\u7559\u6fc0\u6d3b\u7f29\u653e\u6743\u91cd\u7684\u524dk\u4e2a\u5947\u5f02\u5b50\u7a7a\u95f4\uff1b2) \u4ec5\u91cf\u5316\u6b8b\u5dee\u90e8\u5206\uff1b3) \u4f7f\u7528\u5269\u4f59\u79e9\u9884\u7b97(r-k)\u8fdb\u884c\u8bef\u5dee\u91cd\u5efa\u3002\u7406\u8bba\u6307\u5bfc\u9009\u62e9k\u503c\uff0c\u5e73\u8861\u91cf\u5316\u66b4\u9732\u80fd\u91cf\u548c\u79e9\u7ea6\u675f\u4e0b\u7684\u4e0d\u53ef\u6062\u590d\u8bef\u5dee\u3002\u8be5\u65b9\u6cd5\u81ea\u7136\u652f\u6301\u91cf\u5316\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(QPEFT)\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u7f29\u653e\u7a33\u5b9a\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u79cd\u6a21\u578b\u548c\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0cSRR\u5728PTQ\u4e2d\u6301\u7eed\u964d\u4f4e\u56f0\u60d1\u5ea6\u3002\u57282\u4f4d\u91cf\u5316QPEFT\u4e0b\uff0cGLUE\u4efb\u52a1\u5e73\u5747\u63d0\u53475.9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SRR\u901a\u8fc7\u7ed3\u6784\u5316\u79e9\u5206\u914d\u6846\u67b6\uff0c\u5728\u4fdd\u7559\u6743\u91cd\u91cd\u8981\u7ed3\u6784\u7684\u540c\u65f6\u6709\u6548\u91cd\u5efa\u91cf\u5316\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u91cf\u5316\u6027\u80fd\uff0c\u5e76\u4e3a\u91cf\u5316\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u7a33\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02009", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02009", "abs": "https://arxiv.org/abs/2602.02009", "authors": ["Ali Baheri"], "title": "Logic-Guided Vector Fields for Constrained Generative Modeling", "comment": null, "summary": "Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.", "AI": {"tldr": "LGVF\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u5f15\u5bfc\u7684\u5411\u91cf\u573a\u5c06\u7b26\u53f7\u7ea6\u675f\u6ce8\u5165\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u903b\u8f91\u635f\u5931\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u7ea6\u675f\u68af\u5ea6\u8c03\u6574\u91c7\u6837\uff0c\u663e\u8457\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\u3002", "motivation": "\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u65e8\u5728\u7ed3\u5408\u7b26\u53f7\u903b\u8f91\u7684\u8868\u8fbe\u7ed3\u6784\u548c\u795e\u7ecf\u5b66\u4e60\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u5728\u751f\u6210\u65f6\u5f3a\u5236\u6267\u884c\u58f0\u660e\u6027\u7ea6\u675f\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u903b\u8f91\u5f15\u5bfc\u5411\u91cf\u573a(LGVF)\u6846\u67b6\uff1a1) \u8bad\u7ec3\u65f6\u4f7f\u7528\u903b\u8f91\u635f\u5931\u60e9\u7f5a\u8fde\u7eed\u6d41\u8f68\u8ff9\u4e0a\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u6743\u91cd\u5f3a\u8c03\u76ee\u6807\u5206\u5e03\u9644\u8fd1\u7684\u6b63\u786e\u6027\uff1b2) \u63a8\u7406\u65f6\u4f7f\u7528\u7ea6\u675f\u68af\u5ea6\u8c03\u6574\u91c7\u6837\uff0c\u4f5c\u4e3a\u5b66\u4e60\u52a8\u529b\u5b66\u7684\u8f7b\u91cf\u7ea7\u903b\u8f91\u6821\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u7ea6\u675f\u751f\u6210\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cLGVF\u76f8\u6bd4\u6807\u51c6\u6d41\u5339\u914d\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd59-82%\uff0c\u5728\u7ebf\u6027\u548c\u73af\u5f62\u8bbe\u7f6e\u4e2d\u6539\u5584\u5206\u5e03\u4fdd\u771f\u5ea6(MMD)\uff0c\u5728\u591a\u969c\u788d\u8bbe\u7f6e\u4e2d\u89c2\u5bdf\u5230\u6ee1\u8db3\u5ea6\u4e0e\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u3002", "conclusion": "LGVF\u6210\u529f\u5c06\u7b26\u53f7\u77e5\u8bc6\u6ce8\u5165\u751f\u6210\u6a21\u578b\uff0c\u4ea7\u751f\u5177\u6709\u969c\u788d\u89c4\u907f\u884c\u4e3a\u7684\u7ea6\u675f\u611f\u77e5\u5411\u91cf\u573a\uff0c\u65e0\u9700\u663e\u5f0f\u8def\u5f84\u89c4\u5212\u5373\u53ef\u7ed5\u8fc7\u7981\u6b62\u533a\u57df\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u751f\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.02013", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02013", "abs": "https://arxiv.org/abs/2602.02013", "authors": ["Xiaoyi Jiang", "Andreas Nienk\u00f6tter"], "title": "SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation", "comment": null, "summary": "We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.", "AI": {"tldr": "SNAP\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u6d3d\u4e00\u81f4\u539f\u5219\u7684\u81ea\u76d1\u7763\u9c81\u68d2\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u4e00\u81f4\u6027\u5206\u914d\u6743\u91cd\uff0c\u5f3a\u8c03\u53ef\u4fe1\u9879\u76ee\u5e76\u964d\u4f4e\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u65e0\u9700\u76d1\u7763\u6216\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u4f20\u7edf\u9c81\u68d2\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u76d1\u7763\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e14\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002SNAP\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u3001\u80fd\u81ea\u52a8\u8bc6\u522b\u5e76\u6291\u5236\u5f02\u5e38\u503c\u7684\u7075\u6d3b\u9c81\u68d2\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u4e00\u81f4\u6027-\u53ef\u9760\u6027\u5047\u8bbe\uff0cSNAP\u901a\u8fc7\u91cf\u5316\u9879\u76ee\u95f4\u7684\u4e00\u81f4\u6027\u5206\u914d\u6743\u91cd\uff0c\u5f3a\u8c03\u53ef\u4fe1\u9879\u76ee\u5e76\u964d\u4f4e\u5f02\u5e38\u503c\u6743\u91cd\u3002\u5173\u952e\u521b\u65b0\u662f\u5f02\u5e38\u503c\u6743\u91cd\u7684\u6307\u6570\u6291\u5236\uff0c\u786e\u4fdd\u5f02\u5e38\u503c\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u5bf9\u8ba1\u7b97\u8d21\u732e\u53ef\u5ffd\u7565\u3002", "result": "SNAP\u5728\u5411\u91cf\u5e73\u5747\u548c\u5b50\u7a7a\u95f4\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u975e\u8fed\u4ee3\u7684SNAP\u4f18\u4e8e\u8fed\u4ee3\u7684Weiszfeld\u7b97\u6cd5\u548c\u4e24\u79cd\u591a\u5143\u4e2d\u4f4d\u6570\u5747\u503c\u53d8\u4f53\u3002\u5f02\u5e38\u503c\u6743\u91cd\u6307\u6570\u6291\u5236\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "conclusion": "SNAP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u6613\u7528\u3001\u5e7f\u6cdb\u9002\u7528\u7684\u9c81\u68d2\u8ba1\u7b97\u65b9\u6cd5\uff0c\u65e0\u9700\u76d1\u7763\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u81ea\u6d3d\u4e00\u81f4\u539f\u5219\u6709\u6548\u6291\u5236\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u5728\u591a\u79cd\u8ba1\u7b97\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2602.02015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02015", "abs": "https://arxiv.org/abs/2602.02015", "authors": ["Jewon Yeom", "Kyubyung Chae", "Hyunggyu Lim", "Yoonna Oh", "Dongyoon Yang", "Taesup Kim"], "title": "Robust Domain Generalization under Divergent Marginal and Conditional Distributions", "comment": null, "summary": "Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5904\u7406\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u590d\u5408\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u540c\u65f6\u89e3\u51b3\u8fb9\u9645\u6807\u7b7e\u5206\u5e03P(Y)\u548c\u6761\u4ef6\u5206\u5e03P(X|Y)\u7684\u53d8\u5316\uff0c\u901a\u8fc7\u98ce\u9669\u754c\u5206\u89e3\u548c\u5143\u5b66\u4e60\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6761\u4ef6\u5206\u5e03\u504f\u79fb\uff0c\u5047\u8bbeP(Y)\u7a33\u5b9a\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u51fa\u73b0P(Y)\u548cP(X|Y)\u540c\u65f6\u53d8\u5316\u7684\u590d\u5408\u5206\u5e03\u504f\u79fb\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u63a8\u5bfc\u65b0\u7684\u98ce\u9669\u754c\uff0c\u5c06\u8054\u5408\u5206\u5e03\u5206\u89e3\u4e3a\u8fb9\u9645\u548c\u6761\u4ef6\u5206\u91cf\uff0c\u91cf\u5316\u4e24\u79cd\u504f\u79fb\u6e90\u7684\u98ce\u9669\u5dee\u8ddd\uff1b2) \u8bbe\u8ba1\u5143\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5728\u53ef\u89c1\u57df\u4e0a\u6700\u5c0f\u5316\u548c\u9a8c\u8bc1\u8be5\u98ce\u9669\u754c\uff0c\u786e\u4fdd\u5bf9\u672a\u89c1\u57df\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u65b9\u6cd5\u5728\u4f20\u7edfDG\u57fa\u51c6\u6d4b\u8bd5\u548c\u5177\u6709\u663e\u8457\u8fb9\u9645\u4e0e\u6761\u4ef6\u504f\u79fb\u7684\u591a\u57df\u957f\u5c3e\u8bc6\u522b\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u590d\u5408\u5206\u5e03\u504f\u79fb\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u57df\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u9886\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02016", "abs": "https://arxiv.org/abs/2602.02016", "authors": ["Ionut-Vlad Modoranu", "Philip Zmushko", "Erik Schultheis", "Mher Safaryan", "Dan Alistarh"], "title": "DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers", "comment": null, "summary": "Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \\method (for \\textbf{D}istributed \\textbf{A}ccelerated \\textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.", "AI": {"tldr": "DASH \u662f\u4e00\u79cd\u66f4\u5feb\u7684\u5206\u5e03\u5f0f Shampoo \u4f18\u5316\u5668\u5b9e\u73b0\uff0c\u901a\u8fc7\u5f20\u91cf\u5806\u53e0\u548c\u65b0\u7684\u77e9\u9635\u6839\u9006\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.83\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u597d\u7684\u6536\u655b\u6027\u80fd\u3002", "motivation": "Shampoo \u4f5c\u4e3a\u9886\u5148\u7684\u8fd1\u4f3c\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u5728\u6a21\u578b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff08\u5982\u8d62\u5f97 MLCommons AlgoPerf \u7ade\u8d5b\u3001\u4ea7\u751f\u66f4\u6613\u538b\u7f29\u7684\u6a21\u578b\uff09\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u8ba1\u7b97\u901f\u5ea6\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86 DASH\uff08\u5206\u5e03\u5f0f\u52a0\u901f Shampoo\uff09\uff0c\u5305\u542b\u4e24\u9879\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u5c06\u9884\u5904\u7406\u5668\u5757\u5806\u53e0\u62103D\u5f20\u91cf\u4ee5\u63d0\u9ad8GPU\u5229\u7528\u7387\uff1b2\uff09\u5f15\u5165 Newton-DB \u8fed\u4ee3\u548c\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8fd1\u4f3c\u4f5c\u4e3a\u8ba1\u7b97\u77e9\u9635\u6839\u9006\u7684\u65b0\u65b9\u6cd5\u3002\u540c\u65f6\u5206\u6790\u4e86\u77e9\u9635\u7f29\u653e\u5bf9 Shampoo \u6536\u655b\u7684\u5173\u952e\u5f71\u54cd\u3002", "result": "GPU\u611f\u77e5\u7684\u5b9e\u73b0\u76f8\u6bd4\u4f18\u5316\u540e\u7684\u5206\u5e03\u5f0f Shampoo \u5b9e\u73b0\u4e86\u9ad8\u8fbe4.83\u500d\u7684\u4f18\u5316\u5668\u6b65\u9aa4\u52a0\u901f\u3002Newton-DB \u5728\u6240\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\u83b7\u5f97\u4e86\u6bcf\u8fed\u4ee3\u6700\u4f4e\u7684\u9a8c\u8bc1\u56f0\u60d1\u5ea6\u3002", "conclusion": "DASH \u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u548c\u9ad8\u6548\u5b9e\u73b0\u663e\u8457\u964d\u4f4e\u4e86 Shampoo \u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e8c\u9636\u4f18\u5316\u5668\u7684\u4f18\u52bf\u6027\u80fd\u3002"}}
{"id": "2602.02045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02045", "abs": "https://arxiv.org/abs/2602.02045", "authors": ["Yiming Yang", "Xiaoyuan Cheng", "Yi He", "Kaiyu Li", "Wenxuan Yuan", "Zhuo Sun"], "title": "On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems", "comment": null, "summary": "Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \\emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \\emph{robust diffusion posterior sampling}, which is provably \\emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u6269\u6563\u6a21\u578b\u5728\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9c81\u68d2\u7684\u6269\u6563\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u6765\u89e3\u51b3\u4f3c\u7136\u51fd\u6570\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u5f3a\u5927\u5148\u9a8c\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6c42\u89e3\u5668\u4f9d\u8d56\u4e8e\u5047\u5b9a\u7684\u89c2\u6d4b\u4f3c\u7136\u51fd\u6570\u3002\u5f53\u5047\u5b9a\u7684\u4f3c\u7136\u51fd\u6570\u4e0e\u771f\u5b9e\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e0d\u5339\u914d\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u4e00\u95ee\u9898\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u9c81\u68d2\u6269\u6563\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u8bc1\u660e\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u540e\u9a8c\u91c7\u6837\u5668\u517c\u5bb9\u3002", "result": "\u5728\u79d1\u5b66\u9006\u95ee\u9898\u548c\u81ea\u7136\u56fe\u50cf\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f3c\u7136\u51fd\u6570\u9519\u8bef\u8bbe\u5b9a\u4e0b\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u663e\u793a\u51fa\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6269\u6563\u6a21\u578b\u5728\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u4e2d\u7a33\u5b9a\u6027\u5206\u6790\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u9c81\u68d2\u7684\u6269\u6563\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f3c\u7136\u51fd\u6570\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002"}}
{"id": "2602.02055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02055", "abs": "https://arxiv.org/abs/2602.02055", "authors": ["Nan Qiao", "Sheng Yue"], "title": "FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification", "comment": "accetped by IEEE International Conference on Communications (ICC 2026)", "summary": "In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $\u03b4$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.", "AI": {"tldr": "FORLER\u662f\u4e00\u4e2a\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u7684Q-ensemble\u805a\u5408\u548c\u8bbe\u5907\u7aef\u7684actor\u4fee\u6b63\u6765\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u5bfc\u81f4\u7684\u7b56\u7565\u6c61\u67d3\u95ee\u9898\u3002", "motivation": "\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0c\u5728\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u98ce\u9669\u548c\u6210\u672c\u95ee\u9898\uff0c\u800c\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u5728\u4f4e\u8d28\u91cf\u3001\u5f02\u6784\u6570\u636e\u4e0b\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4e00\u4e2a\u8bbe\u5907\u7684\u6b21\u4f18\u7b56\u7565\u4f1a\u6c61\u67d3\u805a\u5408\u6a21\u578b\uff08\u7b56\u7565\u6c61\u67d3\uff09\u3002", "method": "1. \u670d\u52a1\u5668\u7aef\uff1aQ-ensemble\u805a\u5408\uff0c\u7a33\u5065\u5408\u5e76\u8bbe\u5907Q\u51fd\u6570\u4ee5\u6291\u5236\u7b56\u7565\u6c61\u67d3\uff0c\u5c06\u8ba1\u7b97\u8d1f\u62c5\u4ece\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u8f6c\u79fb\u5230\u670d\u52a1\u5668\uff1b2. \u8bbe\u5907\u7aef\uff1aactor\u4fee\u6b63\uff0c\u901a\u8fc7\u96f6\u9636\u641c\u7d22\u5bfb\u627e\u9ad8Q\u503c\u52a8\u4f5c\uff0c\u52a0\u4e0a\u5b9a\u5236\u6b63\u5219\u5316\u5668\u5c06\u7b56\u7565\u63a8\u5411\u8fd9\u4e9b\u52a8\u4f5c\uff1b3. \u03b4-\u5468\u671f\u6027\u7b56\u7565\u8fdb\u4e00\u6b65\u51cf\u5c11\u672c\u5730\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u7684\u6027\u80fd\u4fdd\u8bc1\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFORLER\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u5f02\u6784\u6027\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FORLER\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u6c61\u67d3\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u670d\u52a1\u5668\u805a\u5408\u548c\u8bbe\u5907\u7aef\u4f18\u5316\u673a\u5236\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.02060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02060", "abs": "https://arxiv.org/abs/2602.02060", "authors": ["Hyunsuk Chung", "Caren Han", "Yerin Choi", "Seungyeon Ji", "Jinwoo Kim", "Eun-Jung Holden", "Kyungreem Han"], "title": "FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance", "comment": null, "summary": "Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.", "AI": {"tldr": "FiLoRA\u662f\u4e00\u4e2a\u6307\u4ee4\u6761\u4ef6\u5316\u7684\u53c2\u6570\u9ad8\u6548\u9002\u914d\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u6a21\u5757\u5206\u89e3\u548c\u6307\u4ee4\u95e8\u63a7\uff0c\u5b9e\u73b0\u5bf9\u5185\u90e8\u7279\u5f81\u4f9d\u8d56\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u800c\u4e0d\u6539\u53d8\u9884\u6d4b\u76ee\u6807\u6216\u6807\u7b7e\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u4f9d\u8d56\u673a\u5236\u4e0d\u660e\u786e\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u4e8b\u540e\u5206\u6790\u6216\u7279\u5f81\u79fb\u9664\u65e0\u6cd5\u5728\u4e0d\u6539\u53d8\u4efb\u52a1\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\u8c03\u8282\u7279\u5f81\u4f9d\u8d56\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u663e\u5f0f\u63a7\u5236\u5185\u90e8\u7279\u5f81\u4f9d\u8d56\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faFiLoRA\u6846\u67b6\uff1a1) \u5c06\u9002\u914d\u5206\u89e3\u4e3a\u7279\u5f81\u7ec4\u5bf9\u9f50\u7684LoRA\u6a21\u5757\uff1b2) \u5e94\u7528\u6307\u4ee4\u6761\u4ef6\u5316\u95e8\u63a7\uff0c\u4f7f\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4f5c\u4e3a\u8ba1\u7b97\u7ea7\u63a7\u5236\u4fe1\u53f7\uff1b3) \u4fdd\u6301\u9884\u6d4b\u76ee\u6807\u548c\u6807\u7b7e\u7a7a\u95f4\u4e0d\u53d8\u3002", "result": "\u5728\u6587\u672c-\u56fe\u50cf\u548c\u97f3\u9891-\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6307\u4ee4\u6761\u4ef6\u5316\u95e8\u63a7\u80fd\u4e00\u81f4\u4e14\u56e0\u679c\u5730\u6539\u53d8\u5185\u90e8\u8ba1\u7b97\uff0c\u9009\u62e9\u6027\u5730\u653e\u5927\u6216\u6291\u5236\u6838\u5fc3\u548c\u865a\u5047\u7279\u5f81\u7ec4\uff0c\u5e76\u5728\u865a\u5047\u7279\u5f81\u5e72\u9884\u4e0b\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "FiLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d85\u8d8a\u76f8\u5173\u6027\u5b66\u4e60\u7684\u539f\u7406\u6027\u673a\u5236\uff0c\u80fd\u591f\u8c03\u8282\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u5185\u90e8\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u4e3a\u7406\u89e3\u548c\u63a7\u5236\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02061", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02061", "abs": "https://arxiv.org/abs/2602.02061", "authors": ["Seoungbin Bae", "Junyoung Son", "Dabeen Lee"], "title": "Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits", "comment": null, "summary": "Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit\" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit\" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\\widetilde{\\mathcal{O}}(\\sqrt{t})$ for routing and a queue length regret of $\\widetilde{\\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.", "AI": {"tldr": "\u63d0\u51faACQB\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u6392\u961f\u8d4c\u535a\u673a\u4e0e\u591a\u9879Logit\u53cd\u9988\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u7684\u9690\u5f0f\u53cd\u9988\u4f18\u5316LLM\u670d\u52a1\u7684\u8def\u7531\u4e0e\u8c03\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u961f\u5217\u7a33\u5b9a\u3002", "motivation": "LLM\u670d\u52a1\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7528\u6237\u4e0d\u6ee1\u610f\u4f1a\u91cd\u8bd5\u67e5\u8be2\u589e\u52a0\u670d\u52a1\u5668\u79ef\u538b\uff0c\u4ee5\u53ca\u663e\u5f0f\u53cd\u9988\u8bf7\u6c42\u964d\u4f4e\u7528\u6237\u4f53\u9a8c\u3002\u73b0\u6709\u5728\u7ebf\u7b97\u6cd5\u672a\u5145\u5206\u8003\u8651\u8fd9\u4e9b\u5bf9\u8bdd\u5f0fLLM\u670d\u52a1\u7684\u7279\u6027\u3002", "method": "\u63d0\u51faCQB-MNL\u6846\u67b6\uff0c\u5efa\u6a21\u67e5\u8be2\u91cd\u8bd5\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u7528\u6237\u504f\u597d\u5b66\u4e60\u3002\u5f00\u53d1ACQB\u7b97\u6cd5\uff0c\u7ed3\u5408Thompson\u91c7\u6837\u548c\u8870\u51cf\u7387\u7684\u5f3a\u5236\u63a2\u7d22\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u540c\u65f6\u4fdd\u6301\u961f\u5217\u7a33\u5b9a\u3002", "result": "ACQB\u7b97\u6cd5\u5728\u8def\u7531\u65b9\u9762\u5b9e\u73b0\u7d2f\u8ba1\u9057\u61be$\\widetilde{\\mathcal{O}}(\\sqrt{t})$\uff0c\u961f\u5217\u957f\u5ea6\u9057\u61be$\\widetilde{\\mathcal{O}}(t^{-1/4})$\u3002\u5728SPROUT\u3001EmbedLLM\u548cRouterBench\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u7b97\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u7684\u9690\u5f0f\u53cd\u9988\uff0cACQB\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3LLM\u670d\u52a1\u7684\u8def\u7531\u4e0e\u8c03\u5ea6\u95ee\u9898\uff0c\u5728\u5b66\u4e60\u548c\u961f\u5217\u7a33\u5b9a\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.02071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02071", "abs": "https://arxiv.org/abs/2602.02071", "authors": ["Zisheng Ye", "Xiaoyu He", "Maoyuan Song", "Guoliang Qiu", "Chao Liao", "Chen Wu", "Yonggang Sun", "Zhichun Li", "Xiaoru Xie", "Yuanyong Luo", "Hu Liu", "Pinyan Lu", "Heng Liao"], "title": "BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling", "comment": null, "summary": "As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.", "AI": {"tldr": "\u63d0\u51faHiF8\u4f4e\u7cbe\u5ea6\u8f6f\u6700\u5927\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc78\u4f4d\u6d6e\u70b9\u683c\u5f0f\u548c\u5757\u611f\u77e5\u7cbe\u5ea6\u91cd\u7f29\u653e\uff0c\u89e3\u51b3Transformer\u63a8\u7406\u4e2d\u8f6f\u6700\u5927\u64cd\u4f5c\u7684\u786c\u4ef6\u74f6\u9888\uff0c\u5b9e\u73b0\u5e26\u5bbd\u51cf\u534a\u548c\u6307\u6570\u5355\u5143\u9762\u79ef\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u968f\u7740\u91cf\u5316\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u6027\u80fd\u8fbe\u5230\u74f6\u9888\uff0c\u8f6f\u6700\u5927\u64cd\u4f5c\u6210\u4e3aTransformer\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\u3002\u8fd9\u6e90\u4e8e\u4e24\u4e2a\u786c\u4ef6\u9650\u5236\uff1a(1)\u77e9\u9635\u548c\u5411\u91cf\u8ba1\u7b97\u6838\u5fc3\u95f4\u7684\u6709\u9650\u6570\u636e\u5e26\u5bbd\uff0c(2)\u9ad8\u7cbe\u5ea6(FP32/16)\u6307\u6570\u5355\u5143(EXP2)\u7684\u663e\u8457\u9762\u79ef\u6210\u672c\u3002", "method": "\u5f15\u5165\u65b0\u9896\u7684\u4f4e\u7cbe\u5ea6\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u7279\u5b9a\u76848\u4f4d\u6d6e\u70b9\u683c\u5f0f(HiF8)\u548c\u5757\u611f\u77e5\u7cbe\u5ea6\u91cd\u7f29\u653e\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u4f7f\u77e9\u9635\u4e58\u6cd5\u8f93\u51fa\u7ea6\u675f\u57288\u4f4d\uff0c\u8ba1\u7b97\u6307\u6570\u65f6\u4f7f\u7528\u4f4e\u7cbe\u5ea6(8\u4f4d)\u3002", "result": "\u5728\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8bbe\u8ba1\u5c06\u6240\u9700\u6570\u636e\u79fb\u52a8\u5e26\u5bbd\u51cf\u534a\uff0c\u5e76\u5927\u5e45\u51cf\u5c11EXP2\u5355\u5143\u9762\u79ef\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u4f4e\u7cbe\u5ea6\u65b9\u6cd5\u5e26\u6765\u7684\u663e\u8457\u6a21\u578b\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u901a\u8fc7\u7f13\u89e3\u5411\u91cf\u8ba1\u7b97\u74f6\u9888\uff0c\u8be5\u5de5\u4f5c\u4e3a\u5728\u4e0d\u589e\u52a0\u82af\u7247\u9762\u79ef\u7684\u60c5\u51b5\u4e0b\u5c06\u7aef\u5230\u7aef\u63a8\u7406\u541e\u5410\u91cf\u7ffb\u500d\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5e76\u4e3a\u672a\u6765\u4f4e\u7cbe\u5ea6\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002"}}
{"id": "2602.02072", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02072", "abs": "https://arxiv.org/abs/2602.02072", "authors": ["Junyi Ji", "Derek Gloudemans", "Gergely Zach\u00e1r", "Matthew Nice", "William Barbour", "Daniel B. Work"], "title": "Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction", "comment": null, "summary": "The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePyTorch\u7684Python\u81ea\u9002\u5e94\u5e73\u6ed1\u65b9\u6cd5\u5b9e\u73b0\uff0c\u652f\u6301\u7aef\u5230\u7aef\u6821\u51c6\uff0c\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u4f18\u5316\u53c2\u6570\u5316\u6838\u51fd\u6570\uff0c\u4e3a\u4ea4\u901a\u72b6\u6001\u91cd\u5efa\u63d0\u4f9b\u57fa\u51c6", "motivation": "\u81ea\u9002\u5e94\u5e73\u6ed1\u65b9\u6cd5(ASM)\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u4ea4\u901a\u72b6\u6001\u91cd\u5efa\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u5b9e\u73b0\u548c\u7aef\u5230\u7aef\u6821\u51c6\u6846\u67b6", "method": "\u4f7f\u7528PyTorch\u5b9e\u73b0ASM\uff0c\u5c06\u6821\u51c6\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u53c2\u6570\u5316\u6838\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u7a00\u758f\u96f7\u8fbe\u4f20\u611f\u5668\u7f51\u7edc\u6570\u636e\u548c\u5168\u72b6\u6001\u89c2\u6d4b\u6d4b\u8bd5\u53f0\u8fdb\u884c\u7aef\u5230\u7aef\u6821\u51c6", "result": "\u901a\u8fc7\u901f\u5ea6\u5206\u5e03\u3001\u65f6\u7a7a\u8bef\u5dee\u5206\u5e03\u548c\u7a7a\u95f4\u8bef\u5dee\u8bc4\u4f30\u91cd\u5efa\u6548\u679c\uff0c\u5728\u591a\u6761\u9ad8\u901f\u516c\u8def\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u7528\u6027\uff0c\u4e3a\u4ea4\u901a\u91cd\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u57fa\u51c6\u6307\u6807", "conclusion": "\u8be5\u5b9e\u73b0\u5177\u6709\u53ef\u590d\u73b0\u6027\uff0c\u53ef\u4f5c\u4e3a\u5404\u79cd\u9ad8\u901f\u516c\u8def\u8fd0\u8425\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u4ea4\u901a\u6a21\u578b\u6821\u51c6\u7684\u53ef\u590d\u73b0\u6027\u6311\u6218\u548cASM\u7684\u5c40\u9650\u6027"}}
{"id": "2602.02079", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02079", "abs": "https://arxiv.org/abs/2602.02079", "authors": ["Daniil Orel", "Dilshod Azizov", "Indraneil Paul", "Yuxia Wang", "Iryna Gurevych", "Preslav Nakov"], "title": "AICD Bench: A Challenging Benchmark for AI-Generated Code Detection", "comment": null, "summary": "Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\\emph{2M examples}$, $\\emph{77 models}$ across $\\emph{11 families}$, and $\\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\\emph{i}$)~$\\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\\emph{ii}$)~$\\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\\emph{iii}$)~$\\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.", "AI": {"tldr": "AICD Bench\uff1a\u6700\u5168\u9762\u7684AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u57fa\u51c6\uff0c\u5305\u542b200\u4e07\u6837\u672c\u300177\u4e2a\u6a21\u578b\u30019\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5f15\u5165\u4e09\u79cd\u73b0\u5b9e\u68c0\u6d4b\u4efb\u52a1\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u548c\u6df7\u5408/\u5bf9\u6297\u4ee3\u7801\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u529f\u80fd\u4ee3\u7801\u80fd\u529b\u589e\u5f3a\uff0c\u68c0\u6d4bAI\u751f\u6210\u4ee3\u7801\u5bf9\u4f5c\u8005\u5f52\u5c5e\u3001\u8d23\u4efb\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u8fc7\u4e8e\u72ed\u7a84\uff0c\u901a\u5e38\u4ec5\u9650\u4e8e\u540c\u5206\u5e03\u4e0b\u7684\u4e8c\u5143\u4eba\u673a\u5206\u7c7b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efaAICD Bench\u57fa\u51c6\uff0c\u5305\u542b200\u4e07\u6837\u672c\u300177\u4e2a\u6a21\u578b\uff08\u6db5\u76d611\u4e2a\u6a21\u578b\u5bb6\u65cf\uff09\u30019\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5f15\u5165\u4e09\u79cd\u73b0\u5b9e\u68c0\u6d4b\u4efb\u52a1\uff1a\u9c81\u68d2\u4e8c\u5143\u5206\u7c7b\uff08\u5904\u7406\u8bed\u8a00\u548c\u9886\u57df\u5206\u5e03\u504f\u79fb\uff09\u3001\u6a21\u578b\u5bb6\u65cf\u5f52\u5c5e\uff08\u6309\u67b6\u6784\u8c31\u7cfb\u5206\u7ec4\u751f\u6210\u5668\uff09\u3001\u7ec6\u7c92\u5ea6\u4eba\u673a\u5206\u7c7b\uff08\u4eba\u7c7b\u3001\u673a\u5668\u3001\u6df7\u5408\u548c\u5bf9\u6297\u4ee3\u7801\uff09\u3002", "result": "\u5bf9\u795e\u7ecf\u548c\u7ecf\u5178\u68c0\u6d4b\u5668\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u6027\u80fd\u8fdc\u4f4e\u4e8e\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4ee5\u53ca\u5bf9\u6df7\u5408\u6216\u5bf9\u6297\u4ee3\u7801\u7684\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "AICD Bench\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u65e8\u5728\u63a8\u52a8\u4e0b\u4e00\u4ee3\u9c81\u68d2\u7684AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\u3002\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.02080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02080", "abs": "https://arxiv.org/abs/2602.02080", "authors": ["Aryan Alavi Razavi Ravari", "Farnam Mansouri", "Yuxin Chen", "Valentio Iverson", "Adish Singla", "Sandra Zilles"], "title": "Learning Half-Spaces from Perturbed Contrastive Examples", "comment": null, "summary": "We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.\n  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.", "AI": {"tldr": "\u7814\u7a76\u5728\u5e26\u566a\u58f0\u7684\u5bf9\u6bd4\u793a\u4f8boracle\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u5176\u4e2d\u5bf9\u6bd4\u793a\u4f8b\u7684\u6270\u52a8\u7a0b\u5ea6\u7531\u70b9\u5230\u51b3\u7b56\u8fb9\u754c\u7684\u8ddd\u79bb\u51b3\u5b9a\uff0c\u8ddd\u79bb\u8d8a\u8fd1\u6270\u52a8\u8d8a\u5c0f\u3002\u5206\u6790\u4e86\u4e00\u7ef4\u9608\u503c\u548c\u534a\u7a7a\u95f4\u5728\u56fa\u5b9a\u548c\u968f\u673a\u6270\u52a8\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "Mansouri\u7b49\u4eba\u63d0\u51fa\u4e86\u7406\u60f3\u7684\u5bf9\u6bd4\u793a\u4f8boracle\uff0c\u5176\u4e2d\u5bf9\u6bd4\u793a\u4f8b\u603b\u662f\u8ddd\u79bb\u67e5\u8be2\u70b9\u6700\u8fd1\u7684\u76f8\u53cd\u6807\u7b7e\u70b9\u3002\u7136\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u5bf9\u6bd4\u793a\u4f8b\u53ef\u80fd\u5e26\u6709\u566a\u58f0\u3002\u672c\u6587\u7814\u7a76\u5f53\u5bf9\u6bd4\u793a\u4f8b\u88ab\u6270\u52a8\u65f6\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u6270\u52a8\u7a0b\u5ea6\u7531\u70b9\u5230\u51b3\u7b56\u8fb9\u754c\u7684\u8ddd\u79bb\u63a7\u5236\uff0c\u8fd9\u66f4\u7b26\u5408\u5b9e\u9645\u60c5\u51b5\u3002", "method": "\u5f15\u5165\u53c2\u6570\u5316\u566a\u58f0\u51fd\u6570f\u7684\u673a\u5236\uff0c\u6270\u52a8\u7a0b\u5ea6\u4e3af(d)\uff0c\u5176\u4e2dd\u662f\u67e5\u8be2\u70b9\u5230\u51b3\u7b56\u8fb9\u754c\u7684\u8ddd\u79bb\u3002\u7814\u7a76\u4e24\u79cd\u8bbe\u7f6e\uff1a(1)\u56fa\u5b9a\u6700\u5927\u6270\u52a8\u5e45\u5ea6\uff0c(2)\u968f\u673a\u6270\u52a8\u3002\u5206\u6790\u4e00\u7ef4\u9608\u503c\u548c\u5747\u5300\u5206\u5e03\u6709\u754c\u57df\u4e0a\u534a\u7a7a\u95f4\u7684\u4e3b\u52a8\u548c\u88ab\u52a8\u5bf9\u6bd4\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u5728f\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5bf9\u6bd4\u793a\u4f8b\u7684\u5b58\u5728\u80fd\u591f\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u964d\u4f4e\u6e10\u8fd1\u67e5\u8be2\u590d\u6742\u5ea6\u548c\u671f\u671b\u67e5\u8be2\u590d\u6742\u5ea6\u3002\u5177\u4f53\u523b\u753b\u4e86\u6837\u672c\u590d\u6742\u5ea6\u5bf9\u51fd\u6570f\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u5e26\u566a\u58f0\u7684\u5bf9\u6bd4\u793a\u4f8boracle\u5728\u5b9e\u9645\u5b66\u4e60\u573a\u666f\u4e2d\u5177\u6709\u4ef7\u503c\uff0c\u5f53\u5bf9\u6bd4\u793a\u4f8b\u7684\u8d28\u91cf\u968f\u70b9\u5230\u51b3\u7b56\u8fb9\u754c\u8ddd\u79bb\u53d8\u5316\u65f6\uff0c\u4ecd\u80fd\u5e26\u6765\u5b66\u4e60\u52a0\u901f\u6548\u679c\u3002\u4e3a\u7406\u89e3\u5bf9\u6bd4\u5b66\u4e60\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u7406\u8bba\u6027\u8d28\u63d0\u4f9b\u4e86\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2602.02081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02081", "abs": "https://arxiv.org/abs/2602.02081", "authors": ["Farnam Mansouri", "Sandra Zilles", "Shai Ben-David"], "title": "Active learning from positive and unlabeled examples", "comment": null, "summary": "Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u4e3b\u52a8PU\u5b66\u4e60\u7684\u6807\u7b7e\u590d\u6742\u5ea6\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5176\u4e2d\u67e5\u8be2\u6807\u7b7e\u4ec5\u5728\u5b9e\u4f8b\u4e3a\u6b63\u4e14\u72ec\u7acb\u786c\u5e01\u6295\u63b7\u6210\u529f\u65f6\u624d\u4f1a\u88ab\u63ed\u793a", "motivation": "\u53d7\u5e7f\u544a\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u9a71\u52a8\uff0c\u7814\u7a76\u4e3b\u52a8PU\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5176\u4e2d\u5b66\u4e60\u8005\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u4ece\u65e0\u6807\u7b7e\u6c60\u4e2d\u67e5\u8be2\u5b9e\u4f8b\uff0c\u4f46\u67e5\u8be2\u6807\u7b7e\u4ec5\u5728\u5b9e\u4f8b\u4e3a\u6b63\u4e14\u72ec\u7acb\u786c\u5e01\u6295\u63b7\u6210\u529f\u65f6\u624d\u4f1a\u88ab\u63ed\u793a", "method": "\u7814\u7a76\u4e3b\u52a8PU\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5b66\u4e60\u8005\u53ef\u4ee5\u81ea\u9002\u5e94\u67e5\u8be2\u65e0\u6807\u7b7e\u6c60\u4e2d\u7684\u5b9e\u4f8b\uff0c\u4f46\u6807\u7b7e\u63ed\u793a\u673a\u5236\u53d7\u9650\uff1a\u4ec5\u5f53\u5b9e\u4f8b\u4e3a\u6b63\u4e14\u72ec\u7acb\u786c\u5e01\u6295\u63b7\u6210\u529f\u65f6\u624d\u4f1a\u663e\u793a\u6807\u7b7e\uff0c\u5426\u5219\u5b66\u4e60\u8005\u5f97\u4e0d\u5230\u4efb\u4f55\u4fe1\u606f", "result": "\u63d0\u4f9b\u4e86\u4e3b\u52a8PU\u5b66\u4e60\u6807\u7b7e\u590d\u6742\u5ea6\u7684\u9996\u6b21\u7406\u8bba\u5206\u6790", "conclusion": "\u8fd9\u662f\u5bf9\u4e3b\u52a8PU\u5b66\u4e60\u6807\u7b7e\u590d\u6742\u5ea6\u7684\u9996\u6b21\u7406\u8bba\u5206\u6790\uff0c\u4e3a\u53d7\u9650\u6807\u7b7e\u63ed\u793a\u673a\u5236\u4e0b\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.02087", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02087", "abs": "https://arxiv.org/abs/2602.02087", "authors": ["Andreas Kontogiannis", "Vasilis Pollatos", "Panayotis Mertikopoulos", "Ioannis Panageas"], "title": "Efficient Swap Regret Minimization in Combinatorial Bandits", "comment": "Accepted at AISTATS 2026", "summary": "This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ec4\u5408\u8d4c\u535a\u673a\u7684\u9ad8\u6548\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u9057\u61be\u4e0a\u5177\u6709\u5bf9\u52a8\u4f5c\u6570\u91cfN\u7684\u591a\u5bf9\u6570\u4f9d\u8d56\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5728\u7ec4\u5408\u8d4c\u535a\u673a\u4e2d\uff0c\u52a8\u4f5c\u6570\u91cfN\u76f8\u5bf9\u4e8e\u95ee\u9898\u7ef4\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002\u867d\u7136\u5916\u90e8\u9057\u61be\u6700\u5c0f\u5316\u95ee\u9898\u5df2\u6709\u8f83\u597d\u7814\u7a76\uff0c\u4f46\u5b9e\u73b0\u5177\u6709\u591a\u5bf9\u6570N\u4f9d\u8d56\u7684\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\u4e00\u76f4\u662f\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u4ea4\u6362\u9057\u61be\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u9057\u61be\u4e0a\u5177\u6709\u591a\u5bf9\u6570N\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u540c\u65f6\u5c55\u793a\u4e86\u5982\u4f55\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u9ad8\u6548\u5b9e\u73b0\u8be5\u7b97\u6cd5\uff0c\u4f7f\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u5448\u591a\u5bf9\u6570N\u589e\u957f\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7ec4\u5408\u8d4c\u535a\u673a\u7c7b\u522b\u4e2d\u5b9e\u73b0\u4e86\u7d27\u81f4\u7684\u9057\u61be\u754c\u9650\uff0c\u9057\u61be\u968fN\u5448\u591a\u5bf9\u6570\u589e\u957f\u3002\u7b97\u6cd5\u8fd8\u80fd\u9ad8\u6548\u5b9e\u73b0\uff0c\u6bcf\u6b21\u8fed\u4ee3\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e5f\u5448\u591a\u5bf9\u6570N\u589e\u957f\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u7ec4\u5408\u8d4c\u535a\u673a\u4e2d\u5b9e\u73b0\u9ad8\u6548\u65e0\u4ea4\u6362\u9057\u61be\u7b97\u6cd5\u7684\u957f\u671f\u6311\u6218\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u9057\u61be\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u90fd\u5177\u6709\u591a\u5bf9\u6570N\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.02098", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02098", "abs": "https://arxiv.org/abs/2602.02098", "authors": ["Yannik Schnitzer", "Mathias Jackermeier", "Alessandro Abate", "David Parker"], "title": "Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning", "comment": null, "summary": "Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u65b0\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4efb\u52a1\u7ea7\u6cdb\u5316\u548c\u6bcf\u4efb\u52a1\u7f6e\u4fe1\u4e0b\u754c", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u6027\u80fd\u4fdd\u8bc1\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u90e8\u7f72\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u4e3a\u8bad\u7ec3\u4e2d\u672a\u89c1\u4efb\u52a1\u63d0\u4f9b\u53ef\u9760\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u7ec4\u5408\u4e24\u90e8\u5206\uff1a(1) \u4ece\u6709\u9650\u6b21rollout\u83b7\u5f97\u7684\u6bcf\u4efb\u52a1\u7f6e\u4fe1\u4e0b\u754c\uff1b(2) \u4ece\u6709\u9650\u91c7\u6837\u4efb\u52a1\u83b7\u5f97\u7684\u4efb\u52a1\u7ea7\u6cdb\u5316\uff0c\u4e3a\u6765\u81ea\u76f8\u540c\u672a\u77e5\u5206\u5e03\u7684\u65b0\u4efb\u52a1\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1\u3002", "result": "\u5728\u591a\u79cd\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1RL\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u662f\u53ef\u9760\u7684\uff0c\u4e14\u5728\u73b0\u5b9e\u6837\u672c\u91cf\u4e0b\u80fd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.02117", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.02117", "abs": "https://arxiv.org/abs/2602.02117", "authors": ["Youqi Wu", "Farzan Farnia"], "title": "The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning", "comment": null, "summary": "Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Gr\u00fcnwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u6700\u5927\u71b5\u539f\u7406\u7684\u6781\u5c0f\u6781\u5927\u516c\u5f0f\u5230\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5\uff0c\u4e3a\u5bc6\u5ea6\u77e9\u9635\u548c\u8ff9\u5f52\u4e00\u5316\u6b63\u534a\u5b9a\u7b97\u5b50\u7684VNE\u6700\u5927\u5316\u63d0\u4f9b\u4e86\u535a\u5f08\u8bba\u89e3\u91ca\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u6838\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u6838\u77e9\u9635\u7684\u8c31\u591a\u6837\u6027\u5ea6\u91cf\u88ab\u91c7\u7528\uff0c\u4f46\u7f3a\u4e4f\u7ecf\u5178\u6700\u5927\u71b5\u6846\u67b6\u7684\u51b3\u7b56\u8bba\u548c\u535a\u5f08\u8bba\u89e3\u91ca\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u9a71\u52a8\u573a\u666f\u4e2d\u3002", "method": "\u5c06Gr\u00fcnwald\u548cDawid\u7684\u6700\u5927\u71b5\u539f\u7406\u6781\u5c0f\u6781\u5927\u516c\u5f0f\u6269\u5c55\u5230\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5\u8bbe\u7f6e\uff0c\u4e3a\u5bc6\u5ea6\u77e9\u9635\u548c\u8ff9\u5f52\u4e00\u5316\u6b63\u534a\u5b9a\u7b97\u5b50\u7684VNE\u6700\u5927\u5316\u63d0\u4f9b\u535a\u5f08\u8bba\u89e3\u91ca\u3002", "result": "\u63d0\u51fa\u4e86\u6700\u5927VNE\u539f\u7406\uff0c\u4e3a\u8c31\u57df\u4e2d\u7684\u6700\u5c0f\u627f\u8bfa\u63a8\u65ad\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u91ca\uff0c\u5e76\u5728\u4e24\u4e2a\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff1a\u4ece\u591a\u4e2a\u5f52\u4e00\u5316\u5d4c\u5165\u4e2d\u9009\u62e9\u6838\u8868\u793a\uff0c\u4ee5\u53ca\u4ece\u90e8\u5206\u89c2\u6d4b\u6761\u76ee\u5b8c\u6210\u6838\u77e9\u9635\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6838\u5b66\u4e60\u4e2d\u57fa\u4e8eVNE\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u57fa\u7840\uff0c\u5c06\u6700\u5927\u71b5\u539f\u7406\u7684\u51b3\u7b56\u8bba\u548c\u535a\u5f08\u8bba\u89e3\u91ca\u6269\u5c55\u5230\u91cf\u5b50\u4fe1\u606f\u8bba\u7684\u51af\u00b7\u8bfa\u4f9d\u66fc\u71b5\u3002"}}
{"id": "2602.02126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02126", "abs": "https://arxiv.org/abs/2602.02126", "authors": ["Junhan Kim", "Gukryeol Lee", "Seungwoo Son", "Jeewook Kim", "Yongkweon Jeon"], "title": "Two-Stage Grid Optimization for Group-wise Quantization of LLMs", "comment": "ICASSP 2026", "summary": "Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6539\u8fdbGPTQ\u5206\u7ec4\u91cf\u5316\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u63d0\u5347LLM\u4f4e\u6bd4\u7279\u91cf\u5316\u7cbe\u5ea6", "motivation": "GPTQ\u5206\u7ec4\u91cf\u5316\u65b9\u6cd5\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5ffd\u7565\u4e86\u8f93\u5165\u7edf\u8ba1\u4fe1\u606f\u548c\u7ec4\u95f4\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0e\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u7684\u76ee\u6807\u4e0d\u5339\u914d", "method": "\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728GPTQ\u524d\u521d\u59cb\u5316\u7ec4\u5c3a\u5ea6\u4ee5\u6700\u5c0f\u5316\u7ec4\u91cd\u5efa\u635f\u5931\uff1b\u7b2c\u4e8c\u9636\u6bb5\u51bb\u7ed3GPTQ\u5f97\u5230\u7684\u6574\u6570\u6743\u91cd\uff0c\u4f7f\u7528\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u548c\u95ed\u5f0f\u66f4\u65b0\u89c4\u5219\u4f18\u5316\u7ec4\u5c3a\u5ea6\u4ee5\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u5206\u7ec4\u91cf\u5316\u6027\u80fd\uff0c\u4ee5\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u83b7\u5f97\u66f4\u9ad8\u7684\u7cbe\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GPTQ\u5ffd\u7565\u8f93\u5165\u7edf\u8ba1\u548c\u7ec4\u95f4\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5c42\u91cd\u5efa\u635f\u5931\u663e\u8457\u63d0\u5347\u4e86\u5206\u7ec4\u91cf\u5316\u7684\u51c6\u786e\u6027"}}
{"id": "2602.02128", "categories": ["cs.LG", "cs.AI", "physics.bio-ph", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.02128", "abs": "https://arxiv.org/abs/2602.02128", "authors": ["Nima Shoghi", "Yuxuan Liu", "Yuning Shen", "Rob Brekelmans", "Pan Li", "Quanquan Gu"], "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics", "comment": "For associated project page, see https://bytedance-seed.github.io/ConfRover/starmd", "summary": "Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.", "AI": {"tldr": "STAR-MD\u662f\u4e00\u79cdSE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u5fae\u79d2\u7ea7\u86cb\u767d\u8d28\u8f68\u8ff9\uff0c\u5728ATLAS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u6a21\u62df\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u8fbe\u5230\u751f\u7269\u5b66\u76f8\u5173\u7684\u65f6\u95f4\u5c3a\u5ea6\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u751f\u6210\u4e2d\u5b58\u5728\u67b6\u6784\u9650\u5236\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faSTAR-MD\uff08\u65f6\u7a7a\u81ea\u56de\u5f52\u5c55\u5f00\u5206\u5b50\u52a8\u529b\u5b66\uff09\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684SE(3)-\u7b49\u53d8\u6269\u6563\u6a21\u578b\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u56e0\u679c\u6269\u6563\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u9ad8\u6548\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u907f\u514d\u5185\u5b58\u74f6\u9888\u3002", "result": "\u5728ATLAS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTAR-MD\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u6784\u8c61\u8986\u76d6\u3001\u7ed3\u6784\u6709\u6548\u6027\u548c\u52a8\u6001\u4fdd\u771f\u5ea6\u3002\u80fd\u591f\u7a33\u5b9a\u751f\u6210\u5fae\u79d2\u7ea7\u8f68\u8ff9\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5b8c\u5168\u5931\u8d25\u3002", "conclusion": "STAR-MD\u7684\u8054\u5408\u65f6\u7a7a\u5efa\u6a21\u80fd\u591f\u5728\u751f\u7269\u5b66\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u7a33\u5065\u7684\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u4e3a\u52a0\u901f\u63a2\u7d22\u86cb\u767d\u8d28\u529f\u80fd\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02137", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02137", "abs": "https://arxiv.org/abs/2602.02137", "authors": ["Minghao Li", "Ruihang Wang", "Rui Tan", "Yonggang Wen"], "title": "DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations", "comment": null, "summary": "Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.", "AI": {"tldr": "DCoPilot\uff1a\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d85\u7f51\u7edc\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u52a8\u6001\u6570\u636e\u4e2d\u5fc3\u751f\u6210\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u7b56\u7565\u751f\u6210\u548c\u9002\u5e94", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u5728\u9ad8\u529f\u7387\u5bc6\u5ea6\u548c\u5feb\u901f\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u9700\u8981\u5206\u949f\u7ea7\u9002\u5e94\u3002\u4f20\u7edf\u624b\u52a8\u8bbe\u8ba1\u7684DRL\u4ee3\u7406\u65e0\u6cd5\u8ddf\u4e0a\u9891\u7e41\u7684\u52a8\u6001\u53d8\u5316\u548cSLA\u53d8\u66f4\uff0c\u5bfc\u81f4\u7b56\u7565\u6ede\u540e\u53ef\u80fd\u5f15\u53d1\u670d\u52a1\u4e2d\u65ad\u3002", "method": "\u63d0\u51faDCoPilot\u6df7\u5408\u6846\u67b6\uff1a1\uff09LLM\u8fdb\u884c\u7b26\u53f7\u5316\u751f\u6210\u7ed3\u6784\u5316\u5956\u52b1\u5f62\u5f0f\uff1b2\uff09\u8d85\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u5316\u751f\u6210\u7b56\u7565\u6743\u91cd\u3002\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u6a21\u62df\u6269\u5c55\u3001\u5143\u7b56\u7565\u84b8\u998f\u3001\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u4e94\u4e2a\u63a7\u5236\u4efb\u52a1\u65cf\u4e0a\u8bc4\u4f30\uff0cDCoPilot\u5b9e\u73b0\u63a5\u8fd1\u96f6\u7ea6\u675f\u8fdd\u53cd\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86LLM\u7edf\u4e00\u5956\u52b1\u751f\u6210\u5bf9\u8d85\u7f51\u7edc\u7a33\u5b9a\u6536\u655b\u7684\u6709\u6548\u6027\u3002", "conclusion": "DCoPilot\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u80fd\u591f\u4e3a\u52a8\u6001\u6570\u636e\u4e2d\u5fc3\u64cd\u4f5c\u751f\u6210\u53ca\u65f6\u6709\u6548\u7684\u63a7\u5236\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u89c4\u8303\u5230\u7b56\u7565\u7684\u6ede\u540e\u95ee\u9898\uff0c\u786e\u4fdd\u6570\u636e\u4e2d\u5fc3\u7684\u5b89\u5168\u548c\u80fd\u6548\u8fd0\u884c\u3002"}}
{"id": "2602.02146", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02146", "abs": "https://arxiv.org/abs/2602.02146", "authors": ["Sunho Kim", "Susik Yoon"], "title": "Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting", "comment": "4 pages, Short paper accepted at The Web Conference (WWW) 2026", "summary": "Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.", "AI": {"tldr": "BTTF\u6846\u67b6\u901a\u8fc7\u524d\u77bb\u589e\u5f3a\u548c\u81ea\u6821\u6b63\u7cbe\u70bc\u63d0\u5347\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u5373\u53ef\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e76\u884c\u6548\u7387\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002\u76f4\u63a5\u591a\u6b65\u9884\u6d4b\u65b9\u6cd5\u901f\u5ea6\u5feb\u4f46\u5931\u53bb\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8fed\u4ee3\u591a\u6b65\u9884\u6d4b\u4fdd\u6301\u65f6\u95f4\u4f9d\u8d56\u6027\u4f46\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898", "method": "\u63d0\u51faBack to the Future (BTTF)\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u77bb\u589e\u5f3a\u548c\u81ea\u6821\u6b63\u7cbe\u70bc\u589e\u5f3a\u9884\u6d4b\u7a33\u5b9a\u6027\u3002\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u521d\u59cb\u9884\u6d4b\uff0c\u7136\u540e\u901a\u8fc7\u7b2c\u4e8c\u9636\u6bb5\u6a21\u578b\u5bf9\u589e\u5f3a\u540e\u7684\u9884\u6d4b\u8fdb\u884c\u96c6\u6210\u7cbe\u70bc", "result": "BTTF\u663e\u8457\u63d0\u5347\u957f\u671f\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe58%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5373\u4f7f\u5728\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6761\u4ef6\u4e0d\u7406\u60f3\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u7a33\u5b9a\u6539\u8fdb\uff0c\u6709\u6548\u7f13\u89e3\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u7684\u4e0d\u7a33\u5b9a\u6027", "conclusion": "\u5229\u7528\u6a21\u578b\u751f\u6210\u7684\u9884\u6d4b\u4f5c\u4e3a\u589e\u5f3a\u4fe1\u606f\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd"}}
{"id": "2602.02150", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02150", "abs": "https://arxiv.org/abs/2602.02150", "authors": ["Chu Zhao", "Enneng Yang", "Yuting Liu", "Jianzhe Zhao", "Guibing Guo"], "title": "ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning", "comment": "19 ppages", "summary": "Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faECHO\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u652f\u63a7\u5236\u548c\u7f6e\u4fe1\u5ea6\u526a\u679d\u89e3\u51b3\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u5206\u652f\u5d29\u6e83\u548c\u65e9\u671f\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5728\u6570\u5b66\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u591a\u6570\u6295\u7968\u6784\u5efa\u4f2a\u6807\u7b7e\u8fdb\u884c\u5728\u7ebf\u66f4\u65b0\uff0c\u73b0\u6709\u6811\u72b6\u7ed3\u6784\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u4f46\u4ecd\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1\uff09\u9ad8\u71b5\u5206\u652f\u5bfc\u81f4\u5206\u652f\u5d29\u6e83\uff0c\u5206\u652f\u9884\u7b97\u96c6\u4e2d\u5728\u5c11\u6570\u9ad8\u71b5\u8f68\u8ff9\u4e0a\uff1b2\uff09\u65e9\u671f\u4f2a\u6807\u7b7e\u566a\u58f0\u5927\u4e14\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u9510\u5316\u5e76\u6291\u5236\u63a2\u7d22\u3002", "method": "\u63d0\u51faEntropy Confidence Hybrid Group Relative Policy Optimization (ECHO)\u65b9\u6cd5\uff1a1\uff09\u5728rollout\u9636\u6bb5\uff0c\u8054\u5408\u5229\u7528\u5c40\u90e8\u71b5\u548c\u7ec4\u7ea7\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u63a7\u5236\u5206\u652f\u5bbd\u5ea6\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5728\u7ebf\u526a\u679d\u7ec8\u6b62\u6301\u7eed\u4f4e\u7f6e\u4fe1\u5ea6\u5206\u652f\uff1b2\uff09\u5728\u7b56\u7565\u66f4\u65b0\u9636\u6bb5\uff0c\u91c7\u7528\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u88c1\u526a\u548c\u71b5-\u7f6e\u4fe1\u5ea6\u6df7\u5408\u4f18\u52bf\u5851\u5f62\u65b9\u6cd5\u589e\u5f3a\u8bad\u7ec3\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECHO\u5728\u591a\u4e2a\u6570\u5b66\u548c\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5728\u6709\u9650rollout\u9884\u7b97\u4e0b\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ECHO\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u652f\u63a7\u5236\u548c\u7f6e\u4fe1\u5ea6\u526a\u679d\u6709\u6548\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u652f\u5d29\u6e83\u548c\u65e9\u671f\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.02157", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02157", "abs": "https://arxiv.org/abs/2602.02157", "authors": ["Egor Serov", "Ilya Kuleshov", "Alexey Zaytsev"], "title": "Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing", "comment": null, "summary": "Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6838\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5e73\u6ed1\u7684Neural CDE\u8def\u5f84\u6784\u5efa\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u6837\u6761\u63d2\u503c\uff0c\u901a\u8fc7\u591a\u89c6\u56feCDE\u6846\u67b6\u6062\u590d\u5e73\u6ed1\u635f\u5931\u7ec6\u8282\uff0c\u663e\u8457\u964d\u4f4e\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u548c\u63a8\u7406\u65f6\u95f4", "motivation": "\u4f20\u7edfNeural CDE\u4e2d\u9a71\u52a8\u63a7\u5236\u8def\u5f84\u7684\u7c97\u7cd9\u6027\u9650\u5236\u4e86\u6548\u7387\uff0c\u6807\u51c6\u6837\u6761\u5f15\u5165\u9ad8\u9891\u53d8\u5316\u8feb\u4f7f\u81ea\u9002\u5e94\u6c42\u89e3\u5668\u91c7\u7528\u8fc7\u5c0f\u6b65\u957f\uff0c\u5bfc\u81f4\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u8fc7\u9ad8", "method": "1) \u7528\u6838\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5e73\u6ed1\u66ff\u4ee3\u7cbe\u786e\u63d2\u503c\uff0c\u663e\u5f0f\u63a7\u5236\u8f68\u8ff9\u6b63\u5219\u6027\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u89c6\u56feCDE\u53ca\u5176\u5377\u79ef\u6269\u5c55\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u6307\u5bfc\u8def\u5f84\u91cd\u5efa\uff1b3) \u591a\u8f68\u8ff9\u6846\u67b6\u8ba9\u6a21\u578b\u5728\u4e0d\u540c\u8f68\u8ff9\u4e0a\u5206\u914d\u8868\u793a\u80fd\u529b\uff0c\u6355\u6349\u4e0d\u540c\u65f6\u95f4\u6a21\u5f0f", "result": "MVC-CDE with GP\u65b9\u6cd5\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6837\u6761\u7684\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u548c\u603b\u63a8\u7406\u65f6\u95f4", "conclusion": "\u901a\u8fc7\u6838/\u9ad8\u65af\u8fc7\u7a0b\u5e73\u6ed1\u4e0e\u591a\u89c6\u56feCDE\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684Neural CDE\u5efa\u6a21\uff0c\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861"}}
{"id": "2602.02161", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02161", "abs": "https://arxiv.org/abs/2602.02161", "authors": ["Aniq Ur Rahman", "Justin P. Coon"], "title": "Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction", "comment": null, "summary": "Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u5df2\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u56e0\u679c\u65f6\u5e8f\u4ea4\u4e92\u56fe\u6765\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u6355\u6349\u5230\u56e0\u679c\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u53ea\u57fa\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u79cd\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5224\u65ad\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u4e86\u63a7\u5236\u65f6\u5e8f\u4ea4\u4e92\u7684\u56e0\u679c\u673a\u5236\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9a8c\u8bc1\u6a21\u578b\u56e0\u679c\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u4e00\u4e2a\u652f\u6301\u5174\u594b\u548c\u6291\u5236\u6548\u5e94\u7684\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\uff1b2. \u5c06\u8be5\u673a\u5236\u6269\u5c55\u5230\u65f6\u5e8f\u4ea4\u4e92\u56fe\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u8de8\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u7684\u8ddd\u79bb\u5ea6\u91cf\u6765\u6bd4\u8f83\u56e0\u679c\u6a21\u578b\uff1b4. \u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5b9e\u4f8b\u5316\u53cd\u4e8b\u5b9e\u8bc4\u4f30\uff1a\u63a7\u5236\u751f\u6210\u6a21\u578b\u95f4\u7684\u56e0\u679c\u504f\u79fb\u548c\u65f6\u95f4\u6233\u6d17\u724c\u4f5c\u4e3a\u53ef\u6d4b\u91cf\u56e0\u679c\u8ddd\u79bb\u7684\u968f\u673a\u626d\u66f2\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u5047\u8bbe\uff1a\u5728\u4e00\u4e2a\u56e0\u679c\u6a21\u578b\u4e0a\u8bad\u7ec3\u7684\u9884\u6d4b\u5668\uff0c\u5728\u8db3\u591f\u8fdc\u7684\u6a21\u578b\u4e0a\u8bc4\u4f30\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u8be5\u6846\u67b6\u4e3a\u56e0\u679c\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cd\u4e8b\u5b9e\u9a8c\u8bc1\u6846\u67b6\u80fd\u591f\u8bc4\u4f30\u65f6\u5e8f\u94fe\u63a5\u9884\u6d4b\u6a21\u578b\u662f\u5426\u6355\u6349\u5230\u56e0\u679c\u673a\u5236\uff0c\u4e3a\u66f4\u5168\u9762\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u56e0\u679c\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.02162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02162", "abs": "https://arxiv.org/abs/2602.02162", "authors": ["Ratmir Miftachov", "Bruno Charron", "Simon Valentin"], "title": "Interpretable Tabular Foundation Models via In-Context Kernel Regression", "comment": null, "summary": "Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.", "AI": {"tldr": "KernelICL\u662f\u4e00\u4e2a\u4e3a\u8868\u683c\u57fa\u7840\u6a21\u578b\u589e\u52a0\u53ef\u91cf\u5316\u6837\u672c\u89e3\u91ca\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6700\u7ec8\u9884\u6d4b\u5c42\u66ff\u6362\u4e3a\u6838\u51fd\u6570\uff0c\u4f7f\u9884\u6d4b\u6210\u4e3a\u8bad\u7ec3\u6807\u7b7e\u7684\u900f\u660e\u52a0\u6743\u5e73\u5747\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u8868\u683c\u57fa\u7840\u6a21\u578b\uff08\u5982TabPFN\u548cTabICL\uff09\u867d\u7136\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5176\u67b6\u6784\u672c\u8d28\u4e0a\u662f\u4e0d\u900f\u660e\u7684\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u7684\u6837\u672c\u7ea7\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7c7b\u4f3c\u4e8e\u6838\u56de\u5f52\u7684\u6d1e\u5bdf\uff0c\u5c06\u6700\u7ec8\u9884\u6d4b\u5c42\u66ff\u6362\u4e3a\u6838\u51fd\u6570\uff08\u9ad8\u65af\u6838\u3001\u70b9\u79ef\u6838\u3001kNN\u6838\uff09\uff0c\u4f7f\u6bcf\u4e2a\u9884\u6d4b\u90fd\u6210\u4e3a\u8bad\u7ec3\u6807\u7b7e\u7684\u900f\u660e\u52a0\u6743\u5e73\u5747\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5c06\u6807\u51c6\u6838\u65b9\u6cd5\u3001\u73b0\u4ee3\u57fa\u4e8e\u90bb\u5c45\u7684\u65b9\u6cd5\u548c\u6ce8\u610f\u529b\u673a\u5236\u7edf\u4e00\u5728\u4e00\u4e2a\u6846\u67b6\u4e0b\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u6837\u672c\u6743\u91cd\u5206\u5e03\u7684\u56f0\u60d1\u5ea6\u6765\u91cf\u5316\u53ef\u68c0\u67e5\u6027\u3002", "result": "\u572855\u4e2aTALENT\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cKernelICL\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u8868\u683c\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8868\u660e\u5bf9\u6700\u7ec8\u5c42\u65bd\u52a0\u663e\u5f0f\u6838\u7ea6\u675f\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u68c0\u67e5\u7684\u9884\u6d4b\u3002", "conclusion": "KernelICL\u6846\u67b6\u6210\u529f\u5730\u4e3a\u8868\u683c\u57fa\u7840\u6a21\u578b\u589e\u52a0\u4e86\u53ef\u91cf\u5316\u7684\u6837\u672c\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6838\u673a\u5236\u4f7f\u9884\u6d4b\u8fc7\u7a0b\u900f\u660e\u5316\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8868\u683c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u3002"}}
{"id": "2602.02164", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02164", "abs": "https://arxiv.org/abs/2602.02164", "authors": ["Pengfei He", "Ash Fox", "Lesly Miculicich", "Stefan Friedli", "Daniel Fabian", "Burak Gokturk", "Jiliang Tang", "Chen-Yu Lee", "Tomas Pfister", "Long T. Le"], "title": "Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents", "comment": null, "summary": "Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.", "AI": {"tldr": "Co-RedTeam\u662f\u4e00\u4e2a\u5b89\u5168\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5b89\u5168\u9886\u57df\u77e5\u8bc6\u3001\u4ee3\u7801\u611f\u77e5\u5206\u6790\u3001\u6267\u884c\u57fa\u7840\u8fed\u4ee3\u63a8\u7406\u548c\u957f\u671f\u8bb0\u5fc6\uff0c\u6a21\u62df\u771f\u5b9e\u7ea2\u961f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u53d1\u73b0\u548c\u5229\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4ea4\u4e92\u6709\u9650\u3001\u6267\u884c\u57fa\u7840\u8584\u5f31\u3001\u7f3a\u4e4f\u7ecf\u9a8c\u590d\u7528\uff0c\u96be\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u6f0f\u6d1e\u53d1\u73b0\u548c\u5229\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u7ea2\u961f\u5de5\u4f5c\u6d41\u7a0b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faCo-RedTeam\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u6f0f\u6d1e\u5206\u6790\u5206\u89e3\u4e3a\u534f\u8c03\u7684\u53d1\u73b0\u548c\u5229\u7528\u9636\u6bb5\u3002\u667a\u80fd\u4f53\u57fa\u4e8e\u771f\u5b9e\u6267\u884c\u53cd\u9988\u8fdb\u884c\u89c4\u5212\u3001\u6267\u884c\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\uff0c\u540c\u65f6\u4ece\u5148\u524d\u8f68\u8ff9\u4e2d\u5b66\u4e60\u3002\u6846\u67b6\u96c6\u6210\u4e86\u5b89\u5168\u9886\u57df\u77e5\u8bc6\u3001\u4ee3\u7801\u611f\u77e5\u5206\u6790\u3001\u6267\u884c\u57fa\u7840\u8fed\u4ee3\u63a8\u7406\u548c\u957f\u671f\u8bb0\u5fc6\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCo-RedTeam\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u6f0f\u6d1e\u5229\u7528\u6210\u529f\u7387\u8d85\u8fc760%\uff0c\u6f0f\u6d1e\u68c0\u6d4b\u7edd\u5bf9\u6539\u8fdb\u8d85\u8fc710%\u3002\u6d88\u878d\u548c\u8fed\u4ee3\u7814\u7a76\u8bc1\u5b9e\u4e86\u6267\u884c\u53cd\u9988\u3001\u7ed3\u6784\u5316\u4ea4\u4e92\u548c\u8bb0\u5fc6\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "Co-RedTeam\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7ea2\u961f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u6267\u884c\u53cd\u9988\u3001\u7ed3\u6784\u5316\u4ea4\u4e92\u548c\u957f\u671f\u8bb0\u5fc6\uff0c\u6784\u5efa\u4e86\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u7f51\u7edc\u5b89\u5168\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u6f0f\u6d1e\u53d1\u73b0\u548c\u5229\u7528\u7684\u80fd\u529b\u3002"}}
{"id": "2602.02173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02173", "abs": "https://arxiv.org/abs/2602.02173", "authors": ["Jiancheng Tu", "Wenqi Fan", "Zhibin Wu"], "title": "Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach", "comment": null, "summary": "Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u7ebf\u6027\u6027\u80fd\u6307\u6807\uff08\u5982F1\u5206\u6570\uff09\u4e0b\u5b66\u4e60\u6700\u4f18\u5206\u7c7b\u6811\uff0c\u7279\u522b\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u52a0\u901f\u6280\u672f\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u51b3\u7b56\u6811\u7684\u5168\u5c40\u4f18\u5316\u662f\u7ec4\u5408\u4f18\u5316\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u4f46\u8fd9\u7c7b\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\u3002\u867d\u7136\u5df2\u6709\u6570\u5341\u5e74\u7814\u7a76\uff0c\u4f46\u76f4\u5230\u6700\u8fd1\u79bb\u6563\u4f18\u5316\u8fdb\u5c55\u624d\u4f7f\u5f97\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6c42\u89e3\u6700\u4f18\u5206\u7c7b\u6811\u95ee\u9898\u53d8\u5f97\u53ef\u884c\u3002\u6df7\u5408\u6574\u6570\u89c4\u5212\u63d0\u4f9b\u9ad8\u5ea6\u5efa\u6a21\u7075\u6d3b\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u5904\u7406\u975e\u7ebf\u6027\u6027\u80fd\u6307\u6807\uff08\u5982F1\u5206\u6570\uff09\u5e76\u660e\u786e\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u7ebf\u6027\u6027\u80fd\u6307\u6807\u4e0b\u5b66\u4e60\u6700\u4f18\u5206\u7c7b\u6811\u3002\u5f00\u53d1\u4e86\u95ee\u9898\u7279\u5b9a\u7684\u52a0\u901f\u6280\u672f\uff1a1\uff09\u5b9a\u5236\u7684\u5206\u652f\u5207\u5272\u7b97\u6cd5\uff1b2\uff09\u5b9e\u4f8b\u7f29\u51cf\u65b9\u6848\uff1b3\uff09\u70ed\u542f\u52a8\u7b56\u7565\u3002\u8fd9\u4e9b\u6280\u672f\u65e8\u5728\u63d0\u9ad8\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u572850\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u65b9\u6cd5\u3002\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u4f18\u5316\u975e\u7ebf\u6027\u6307\u6807\uff0c\u540c\u65f6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u66f4\u5f3a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u66f4\u77ed\u7684\u6c42\u89e3\u65f6\u95f4\u3002", "conclusion": "\u57fa\u4e8eMIP\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6700\u4f18\u5206\u7c7b\u6811\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u7ebf\u6027\u6027\u80fd\u6307\u6807\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u901a\u8fc7\u5f00\u53d1\u7684\u52a0\u901f\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u5de5\u5177\u3002"}}
{"id": "2602.02179", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02179", "abs": "https://arxiv.org/abs/2602.02179", "authors": ["Marina Mastroleo", "Alberto Archetti", "Federico Mastroleo", "Matteo Matteucci"], "title": "SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks", "comment": null, "summary": "Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.", "AI": {"tldr": "SurvKAN\uff1a\u57fa\u4e8eKAN\u67b6\u6784\u7684\u5b8c\u5168\u53c2\u6570\u5316\u3001\u65f6\u95f4\u8fde\u7eed\u751f\u5b58\u6a21\u578b\uff0c\u6d88\u9664\u6bd4\u4f8b\u98ce\u9669\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd", "motivation": "\u7ecf\u5178\u751f\u5b58\u6a21\u578b\uff08\u5982Cox\uff09\u4f9d\u8d56\u7ebf\u6027\u534f\u53d8\u91cf\u5173\u7cfb\u548c\u6bd4\u4f8b\u98ce\u9669\u5047\u8bbe\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u4e34\u5e8a\u52a8\u6001\uff1b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982DeepSurv\u3001DeepHit\uff09\u63d0\u9ad8\u4e86\u8868\u8fbe\u80fd\u529b\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\uff1b\u73b0\u6709\u6df7\u5408\u6a21\u578b\uff08\u5982CoxKAN\uff09\u4ecd\u53d7\u534a\u53c2\u6570Cox\u6846\u67b6\u7ea6\u675f", "method": "SurvKAN\uff1a\u57fa\u4e8eKolmogorov-Arnold Networks\uff08KAN\uff09\u67b6\u6784\u7684\u5b8c\u5168\u53c2\u6570\u5316\u3001\u65f6\u95f4\u8fde\u7eed\u751f\u5b58\u6a21\u578b\u3002\u5c06\u65f6\u95f4\u4f5c\u4e3aKAN\u7684\u663e\u5f0f\u8f93\u5165\uff0c\u76f4\u63a5\u9884\u6d4b\u5bf9\u6570\u98ce\u9669\u51fd\u6570\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5355\u53d8\u91cf\u51fd\u6570\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u7aef\u5230\u7aef\u8bad\u7ec3", "result": "\u5728\u6807\u51c6\u751f\u5b58\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSurvKAN\u5728\u4e00\u81f4\u6027\u548c\u6821\u51c6\u6307\u6807\u4e0a\u8fbe\u5230\u6216\u4f18\u4e8e\u7ecf\u5178\u548c\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u4e00\u81f4\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u6a21\u5f0f", "conclusion": "SurvKAN\u901a\u8fc7\u7ed3\u5408KAN\u67b6\u6784\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u7ade\u4e89\u6027\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u66f4\u7075\u6d3b\u7684\u751f\u5b58\u5206\u6790\u5de5\u5177"}}
{"id": "2602.02180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02180", "abs": "https://arxiv.org/abs/2602.02180", "authors": ["Weikang Meng", "Liangyu Huo", "Yadan Luo", "Jiawen Guan", "Jingyi Zhang", "Yingjian Li", "Zheng Zhang"], "title": "STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs", "comment": null, "summary": "Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.", "AI": {"tldr": "STILL\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c42\u5185\u6df7\u5408\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u663e\u8457\u6027\u8bc4\u5206\u548c\u4fdd\u8303\u7279\u5f81\u6620\u5c04\uff0c\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u8868\u793a\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7ebf\u6027\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u5206\u533a\u7684token\u8def\u7531\u65b9\u6cd5\u53ea\u80fd\u8fdb\u884c\u4f4d\u7f6e\u9009\u62e9\uff0c\u65e0\u6cd5\u6355\u6349token\u7279\u5b9a\u7684\u5168\u5c40\u91cd\u8981\u6027\uff1b2) \u53ef\u5b66\u4e60\u7684\u7279\u5f81\u6620\u5c04\u4f1a\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u626d\u66f2\u9884\u8bad\u7ec3\u7279\u5f81\u7684\u5927\u5c0f\u3002", "method": "STILL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5177\u6709\u5f3a\u5c40\u90e8-\u5168\u5c40\u4e00\u81f4\u6027\u7684\u81ea\u663e\u8457\u6027\u8bc4\u5206\uff0c\u7528\u4e8e\u51c6\u786etoken\u9009\u62e9\uff1b2) \u4fdd\u8303\u7279\u5f81\u6620\u5c04(NP-Map)\uff0c\u89e3\u8026\u7279\u5f81\u65b9\u5411\u548c\u5927\u5c0f\u5e76\u91cd\u65b0\u6ce8\u5165\u9884\u8bad\u7ec3\u8303\u6570\uff1b3) \u7edf\u4e00\u7684\u8bad\u7ec3-\u63a8\u7406\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5757\u5e76\u884c\u5316\u548c\u5ef6\u8fdf\u9009\u62e9\u4ee5\u63d0\u9ad8\u786c\u4ef6\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSTILL\u5728\u5e38\u8bc6\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u539f\u59cb\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u4e4b\u524d\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe86.2%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "STILL\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u663e\u8457\u6027\u8bc4\u5206\u548c\u4fdd\u8303\u7279\u5f81\u6620\u5c04\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\u3002"}}
{"id": "2602.02192", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02192", "abs": "https://arxiv.org/abs/2602.02192", "authors": ["Jie Xiao", "Meng Chen", "Qingnan Ren", "Song Jingwei", "Jiaqi Huang", "Yangshen Deng", "Chris Tong", "Wanyi Chen", "Suli Wang", "Ziqian Bi", "Shuo Lu", "Yiqun Duan", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning", "comment": "23 pages, 7 figures", "summary": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.", "AI": {"tldr": "ECHO-2\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u8fdc\u7a0b\u63a8\u7406\u5de5\u4f5c\u8282\u70b9\u548c\u91cd\u53e0\u7b56\u7565\u4f20\u64ad\u6765\u63d0\u5347\u6210\u672c\u6548\u7387", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u9700\u8981\u53cd\u590d\u8fdb\u884c\u7b56\u7565\u751f\u6210\u3001\u5956\u52b1\u8bc4\u4f30\u548c\u96c6\u4e2d\u5b66\u4e60\u3002\u5206\u5e03\u5f0f\u6267\u884c\u7b56\u7565\u751f\u6210\u53ef\u4ee5\u5229\u7528\u6210\u672c\u66f4\u4f4e\u7684\u63a8\u7406\u8d44\u6e90\uff0c\u4f46\u9762\u4e34\u5e7f\u57df\u534f\u8c03\u548c\u7b56\u7565\u4f20\u64ad\u5ef6\u8fdf\u7684\u6311\u6218", "method": "ECHO-2\u91c7\u7528\u96c6\u4e2d\u5b66\u4e60\u4e0e\u5206\u5e03\u5f0f\u7b56\u7565\u751f\u6210\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5c06\u7b56\u7565\u8fc7\u65f6\u6027\u4f5c\u4e3a\u7528\u6237\u53ef\u63a7\u53c2\u6570\uff0c\u5141\u8bb8\u7b56\u7565\u751f\u6210\u3001\u4f20\u64ad\u548c\u8bad\u7ec3\u91cd\u53e0\u6267\u884c\u3002\u5f15\u5165\u57fa\u4e8e\u91cd\u53e0\u7684\u5bb9\u91cf\u6a21\u578b\uff0c\u4f7f\u7528\u5bf9\u7b49\u8f85\u52a9\u6d41\u6c34\u7ebf\u5e7f\u64ad\u548c\u6210\u672c\u611f\u77e5\u7684\u5f02\u6784\u5de5\u4f5c\u8282\u70b9\u6fc0\u6d3b", "result": "\u5728\u771f\u5b9e\u5e7f\u57df\u5e26\u5bbd\u73af\u5883\u4e0b\u5bf94B\u548c8B\u6a21\u578b\u8fdb\u884cGRPO\u540e\u8bad\u7ec3\u7684\u5b9e\u9a8c\u8868\u660e\uff0cECHO-2\u5728\u4fdd\u6301\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u7387", "conclusion": "ECHO-2\u901a\u8fc7\u6709\u6548\u5904\u7406\u7b56\u7565\u4f20\u64ad\u5ef6\u8fdf\u548c\u5229\u7528\u5206\u5e03\u5f0f\u63a8\u7406\u8d44\u6e90\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6210\u672c\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6"}}
{"id": "2602.02195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02195", "abs": "https://arxiv.org/abs/2602.02195", "authors": ["Ao Sun", "Hongtao Zhang", "Heng Zhou", "Yixuan Ma", "Yiran Qin", "Tongrui Su", "Yan Liu", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He"], "title": "State Rank Dynamics in Linear Attention LLMs", "comment": null, "summary": "Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\\% reduction in KV-cache overhead while largely maintaining model accuracy.", "AI": {"tldr": "\u7ebf\u6027\u6ce8\u610f\u529bLLM\u7684\u72b6\u6001\u77e9\u9635\u5b58\u5728\"\u72b6\u6001\u79e9\u5206\u5c42\"\u73b0\u8c61\uff1a\u90e8\u5206\u6ce8\u610f\u529b\u5934\u4fdd\u6301\u4f4e\u79e9\uff0c\u90e8\u5206\u589e\u957f\u81f3\u9ad8\u79e9\u4e0a\u9650\uff0c\u8fd9\u662f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u7ed3\u6784\u7279\u6027\u800c\u975e\u8f93\u5165\u4f9d\u8d56\u7684\u6682\u6001\u3002\u4f4e\u79e9\u5934\u5bf9\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u79e9\u5934\u5197\u4f59\u5ea6\u9ad8\uff0c\u636e\u6b64\u63d0\u51fa\u7684\u8054\u5408\u79e9-\u8303\u6570\u526a\u679d\u7b56\u7565\u53ef\u51cf\u5c1138.9%\u7684KV\u7f13\u5b58\u5f00\u9500\u3002", "motivation": "\u7ebf\u6027\u6ce8\u610f\u529bLLM\u901a\u8fc7\u56fa\u5b9a\u5927\u5c0f\u7684\u72b6\u6001\u77e9\u9635\u538b\u7f29\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u6052\u5b9a\u65f6\u95f4\u63a8\u7406\uff0c\u4f46\u5176\u5185\u90e8\u538b\u7f29\u72b6\u6001\u7684\u52a8\u6001\u7279\u6027\u4ecd\u4e0d\u900f\u660e\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8fd0\u884c\u65f6\u72b6\u6001\u52a8\u6001\uff0c\u7279\u522b\u662f\u72b6\u6001\u77e9\u9635\u7684\u79e9\u6f14\u5316\u89c4\u5f8b\u3002", "method": "\u5bf9\u6700\u5148\u8fdb\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u5168\u9762\u7684\u8fd0\u884c\u65f6\u72b6\u6001\u52a8\u6001\u7814\u7a76\uff0c\u5206\u6790\u7ebf\u6027\u6ce8\u610f\u529b\u5934\u7684\u8c31\u7279\u6027\uff0c\u53d1\u73b0\u72b6\u6001\u79e9\u5206\u5c42\u73b0\u8c61\u3002\u901a\u8fc7\u8de8\u4e0d\u540c\u63a8\u7406\u4e0a\u4e0b\u6587\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u52a8\u6001\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u8bca\u65ad\u63a2\u9488\u5206\u6790\u529f\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u79e9-\u8303\u6570\u7684\u8054\u5408\u526a\u679d\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u7ebf\u6027\u6ce8\u610f\u529b\u5934\u5b58\u5728\u660e\u663e\u7684\u8c31\u5206\u53c9\uff1a\u4e00\u7ec4\u4fdd\u6301\u63a5\u8fd1\u96f6\u7684\u6709\u6548\u79e9\uff0c\u53e6\u4e00\u7ec4\u5feb\u901f\u589e\u957f\u5e76\u6536\u655b\u5230\u4e0a\u754c\u3002\u8fd9\u79cd\u52a8\u6001\u5728\u4e0d\u540c\u63a8\u7406\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u5934\u7684\u4f4e\u79e9/\u9ad8\u79e9\u7279\u6027\u662f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u5185\u5728\u7ed3\u6784\u5c5e\u6027\u3002\u8bca\u65ad\u663e\u793a\u4f4e\u79e9\u5934\u5bf9\u6a21\u578b\u63a8\u7406\u4e0d\u53ef\u6216\u7f3a\uff0c\u800c\u9ad8\u79e9\u5934\u5197\u4f59\u5ea6\u9ad8\u3002\u63d0\u51fa\u7684\u8054\u5408\u79e9-\u8303\u6570\u526a\u679d\u7b56\u7565\u5b9e\u73b0\u4e8638.9%\u7684KV\u7f13\u5b58\u5f00\u9500\u51cf\u5c11\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u7ebf\u6027\u6ce8\u610f\u529bLLM\u7684\u72b6\u6001\u77e9\u9635\u5b58\u5728\u56fa\u6709\u7684\u72b6\u6001\u79e9\u5206\u5c42\u73b0\u8c61\uff0c\u8fd9\u662f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u7ed3\u6784\u7279\u6027\u3002\u4f4e\u79e9\u5934\u5bf9\u63a8\u7406\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u9ad8\u79e9\u5934\u5177\u6709\u663e\u8457\u5197\u4f59\u6027\u3002\u57fa\u4e8e\u8fd9\u4e00\u6d1e\u5bdf\u7684\u526a\u679d\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.02197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02197", "abs": "https://arxiv.org/abs/2602.02197", "authors": ["Xindian Ma", "Yidi Lu", "Peng Zhang", "Jing Zhang"], "title": "Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models", "comment": "10 oages, 3 figures", "summary": "The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\\% with minimal accuracy loss (0.3\\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.", "AI": {"tldr": "HAE\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684KV\u7f13\u5b58\u9010\u51fa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u89c6\u89c9-\u6587\u672ctoken\u4ea4\u4e92\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u52a0\u901f\u63a8\u7406", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u9010\u51fa\u7b56\u7565\u672a\u80fd\u5904\u7406\u89c6\u89c9\u548c\u6587\u672ctoken\u4e4b\u95f4\u7684\u5f02\u8d28\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u6027\u80fd\u4e0b\u964d\u3002Transformer\u67b6\u6784\u7684\u4e8c\u6b21\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u662f\u591a\u6a21\u6001LLM\u7684\u74f6\u9888", "method": "\u63d0\u51fa\u5206\u5c42\u81ea\u9002\u5e94\u9010\u51fa(HAE)\u6846\u67b6\uff1a1)\u9884\u586b\u5145\u9636\u6bb5\u4f7f\u7528\u53cc\u91cd\u6ce8\u610f\u529b\u526a\u679d\uff08\u5229\u7528\u89c6\u89c9token\u7a00\u758f\u6027\u548c\u6ce8\u610f\u529b\u65b9\u5dee\uff09\uff1b2)\u89e3\u7801\u9636\u6bb5\u4f7f\u7528\u52a8\u6001\u89e3\u7801\u9010\u51fa\u7b56\u7565\uff08\u53d7\u64cd\u4f5c\u7cfb\u7edf\u56de\u6536\u7ad9\u542f\u53d1\uff09\u3002\u901a\u8fc7\u7d22\u5f15\u5e7f\u64ad\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u66f4\u4f4e\u8bef\u5dee\u8fb9\u754c", "result": "\u5728\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u4e2d\u51cf\u5c1141%\u7684KV\u7f13\u5b58\u5185\u5b58\uff0c\u4ec5\u635f\u59310.3%\u7684\u51c6\u786e\u7387\uff1b\u5728\u6545\u4e8b\u751f\u6210\u63a8\u7406\u4e2d\u52a0\u901f1.5\u500d\uff0c\u540c\u65f6\u4fdd\u6301Phi3.5-Vision-Instruct\u6a21\u578b\u7684\u8f93\u51fa\u8d28\u91cf", "conclusion": "HAE\u901a\u8fc7\u5206\u5c42\u81ea\u9002\u5e94\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001LLM\u4e2d\u89c6\u89c9-\u6587\u672ctoken\u4ea4\u4e92\u7684\u5f02\u8d28\u6ce8\u610f\u529b\u5206\u5e03\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6"}}
{"id": "2602.02201", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02201", "abs": "https://arxiv.org/abs/2602.02201", "authors": ["Abhijit Gupta"], "title": "Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction", "comment": null, "summary": "Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.", "AI": {"tldr": "CardinalGraphFormer\u662f\u4e00\u79cd\u56feTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408Graphormer\u7684\u7ed3\u6784\u504f\u7f6e\u548c\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5728\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6570\u636e\u6548\u7387\uff0c\u572811\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u9700\u8981\u9ad8\u6548\u7684\u5206\u5b50\u6027\u8d28\u9884\u6d4b\uff0c\u4f46\u5316\u5b66\u7a7a\u95f4\u5de8\u5927\uff08\u7ea610^60\u4e2a\u7c7b\u836f\u5206\u5b50\uff09\uff0c\u800c\u83b7\u6279\u836f\u7269\u4ec5\u6570\u5343\u79cd\u3002\u56e0\u6b64\uff0c\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u4e0b\uff0c\u5229\u7528\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u5206\u5b50\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5bf9\u4e8e\u6570\u636e\u9ad8\u6548\u7684\u5206\u5b50\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faCardinalGraphFormer\u56feTransformer\u6a21\u578b\uff0c\u7ed3\u5408Graphormer\u7684\u7ed3\u6784\u504f\u7f6e\uff08\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u3001\u4e2d\u5fc3\u6027\u3001\u76f4\u63a5\u952e\u8fb9\u504f\u7f6e\uff09\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08\u9650\u5236\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u22643\uff09\uff0c\u5e76\u589e\u52a0\u57fa\u6570\u4fdd\u6301\u7684\u975e\u5f52\u4e00\u5316\u805a\u5408\u901a\u9053\u3002\u9884\u8bad\u7ec3\u7ed3\u5408\u5bf9\u6bd4\u56fe\u7ea7\u5bf9\u9f50\u548c\u63a9\u7801\u5c5e\u6027\u91cd\u5efa\u3002", "result": "\u5728\u5b8c\u5168\u5339\u914d\u7684\u8bc4\u4f30\u534f\u8bae\u4e0b\uff0cCardinalGraphFormer\u572811\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5747\u63d0\u9ad8\u4e86\u5e73\u5747\u6027\u80fd\uff0c\u5728MoleculeNet\u3001OGB\u548cTDC ADMET\u4efb\u52a1\u768410\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u589e\u76ca\u3002", "conclusion": "CardinalGraphFormer\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u7a00\u758f\u6ce8\u610f\u529b\u548c\u57fa\u6570\u4fdd\u6301\u805a\u5408\uff0c\u5728\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02206", "abs": "https://arxiv.org/abs/2602.02206", "authors": ["Tong Yang", "Yemin Wang", "Chaoning Zhang", "Aming Wu"], "title": "Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning", "comment": null, "summary": "The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.", "AI": {"tldr": "Fat-Cat\u662f\u4e00\u4e2a\u6587\u6863\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7Markdown\u6587\u6863\u8868\u793a\u72b6\u6001\u3001\u6587\u672c\u7b56\u7565\u6f14\u5316\u548c\u95ed\u73af\u76d1\u63a7\uff0c\u63d0\u9ad8\u4e0a\u4e0b\u6587\u4fe1\u606f\u5229\u7528\u6548\u7387\uff0c\u5728\u63a8\u7406\u3001\u68c0\u7d22\u548c\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u521a\u6027\u3001\u8bed\u6cd5\u7e41\u91cd\u7684\u72b6\u6001\u8868\u793a\uff08\u5982\u5d4c\u5957JSON\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u5c06\u5927\u91cf\u6ce8\u610f\u529b\u6d88\u8017\u5728\u8bed\u6cd5\u5904\u7406\u800c\u975e\u8bed\u4e49\u63a8\u7406\u4e0a\uff0c\u9650\u5236\u4e86LLM\u667a\u80fd\u4f53\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faFat-Cat\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u8bed\u4e49\u6587\u4ef6\u7cfb\u7edf\uff0c\u5c06\u667a\u80fd\u4f53\u72b6\u6001\u8868\u793a\u4e3a\u4e0e\u9884\u8bad\u7ec3\u8bed\u6599\u5bf9\u9f50\u7684Markdown\u6587\u6863\uff1b2\uff09\u6587\u672c\u7b56\u7565\u6f14\u5316\u6a21\u5757\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u79ef\u7d2f\u4efb\u52a1\u89e3\u51b3\u77e5\u8bc6\uff1b3\uff09\u95ed\u73af\u76d1\u63a7\u5668\uff0c\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u63a8\u7406\u3001\u68c0\u7d22\u548c\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFat-Cat\u6301\u7eed\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u4f7fKimi-k2\u6a21\u578b\u5728HotPotQA\u4e0a\u8d85\u8d8a\u4e13\u6709\u7684GPT-4o\u57fa\u7ebf\u3002\u5c06\u6587\u6863\u72b6\u6001\u66ff\u6362\u4e3aJSON\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9a8c\u8bc1\u4e86\u6587\u6863\u9a71\u52a8\u72b6\u6001\u5efa\u6a21\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u6587\u6863\u9a71\u52a8\u7684\u72b6\u6001\u8868\u793a\u6bd4\u521a\u6027\u8bed\u6cd5\u8868\u793a\u66f4\u6709\u6548\uff0cFat-Cat\u901a\u8fc7\u63d0\u9ad8\u72b6\u6001\u7ba1\u7406\u7684\u4fe1\u566a\u6bd4\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u5bf9\u9f50\u6587\u6863\u5728\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.02213", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02213", "abs": "https://arxiv.org/abs/2602.02213", "authors": ["Gregory Barber", "Todd C. Henry", "Mulugeta A. Haile"], "title": "Generating Physically Sound Designs from Text and a Set of Physical Constraints", "comment": "NeurIPS 2025", "summary": "We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.", "AI": {"tldr": "TIDES\u662f\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ed3\u6784\u548c\u89c6\u89c9\u5c5e\u6027\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u548c\u7269\u7406\u7ea6\u675f\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u8bbe\u8ba1\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7269\u7406\u5de5\u7a0b\u8981\u6c42\u548c\u7528\u6237\u6307\u5b9a\u7684\u7f8e\u5b66\u6216\u529f\u80fd\u7279\u5f81\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u6587\u672c\u63cf\u8ff0\u7684\u8bbe\u8ba1\u610f\u56fe\u4e0e\u7269\u7406\u6027\u80fd\u7ea6\u675f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u8bc4\u4f30\u8bbe\u8ba1\u4e0e\u6587\u672c\u63d0\u793a\u7684\u89c6\u89c9\u5bf9\u9f50\u5ea6\uff0c\u540c\u65f6\u4f7f\u7528\u53ef\u5fae\u5206\u7269\u7406\u6a21\u62df\u5668\u8bc4\u4f30\u7269\u7406\u6027\u80fd\u3002\u8054\u5408\u4f18\u5316\u7ed3\u6784\uff08\u62d3\u6251\uff09\u548c\u89c6\u89c9\u5c5e\u6027\u3002", "result": "TIDES\u5728\u4e0d\u540c\u8f7d\u8377\u548c\u652f\u6491\u6761\u4ef6\u4e0b\u3001\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7ed3\u6784\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\u3002\u901a\u8fc7\u4e09\u70b9\u5f2f\u66f2\u5b9e\u9a8c\u9a8c\u8bc1\u4e862D\u6881\u8bbe\u8ba1\uff08\u6324\u51fa3D\u6253\u5370\uff09\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u5de5\u7a0b\u8bbe\u8ba1\u8981\u6c42\uff08\u67d4\u987a\u6027\u548c\u5bc6\u5ea6\uff09\u5e76\u5b9e\u73b0\u6587\u672c\u6307\u5b9a\u7684\u7279\u5f81\u3002", "conclusion": "TIDES\u80fd\u591f\u6210\u529f\u8054\u5408\u4f18\u5316\u7269\u7406\u6027\u80fd\u548c\u89c6\u89c9\u5bf9\u9f50\u4e24\u4e2a\u76ee\u6807\uff0c\u751f\u6210\u65e2\u6ee1\u8db3\u5de5\u7a0b\u8bbe\u8ba1\u8981\u6c42\u53c8\u5305\u542b\u6587\u672c\u6307\u5b9a\u7279\u5f81\u7684\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2602.02215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02215", "abs": "https://arxiv.org/abs/2602.02215", "authors": ["Sebastian M\u00fcller", "Vanessa Toborek", "Eike Stadtl\u00e4nder", "Tam\u00e1s Horv\u00e1th", "Brendan Balcerak Jackson", "Christian Bauckhage"], "title": "Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism", "comment": null, "summary": "Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.", "AI": {"tldr": "\u63d0\u51fa\"\u9ed1\u76d2\u79d1\u5b66\u7406\u8bba\"(SToBB)\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91caAI\u7b97\u6cd5\u6574\u5408\u4e3a\u4f34\u968f\u9ed1\u76d2\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u6301\u4e45\u3001\u53ef\u5ba1\u8ba1\u7684\u6587\u6863\uff0c\u901a\u8fc7\u67e5\u8be2\u63a5\u53e3\u63d0\u4f9b\u89e3\u91ca\u800c\u975e\u5b64\u7acb\u8f93\u51fa\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91caAI\u7b97\u6cd5\u4f17\u591a\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\u5c06\u9ed1\u76d2\u6a21\u578b\u7684\u89e3\u91ca\u4fe1\u606f\u6574\u5408\u4e3a\u4f34\u968f\u5176\u751f\u547d\u5468\u671f\u7684\u6301\u4e45\u3001\u53ef\u5ba1\u8ba1\u6587\u6863\uff0c\u65e0\u6cd5\u652f\u6301\u4e00\u81f4\u3001\u53ef\u590d\u7528\u7684\u5206\u6790\u548c\u7cfb\u7edf\u5916\u90e8\u5ba1\u67e5\u3002", "method": "\u57fa\u4e8e\u5efa\u6784\u7ecf\u9a8c\u4e3b\u4e49\u63d0\u51faSToBB\u6982\u5ff5\uff0c\u8981\u6c42\u6ee1\u8db3\u4e09\u4e2a\u4e49\u52a1\uff1a\u7ecf\u9a8c\u5145\u5206\u6027\u3001\u53ef\u9002\u5e94\u6027\u3001\u53ef\u5ba1\u8ba1\u6027\u3002\u5b9e\u73b0\u4e3a\u901a\u7528\u6846\u67b6\uff0c\u5305\u542b\u53ef\u6269\u5c55\u89c2\u5bdf\u5e93\u3001\u53ef\u8ffd\u6eaf\u5047\u8bbe\u7c7b\u3001\u6784\u9020\u4e0e\u4fee\u8ba2\u7b97\u6cd5\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u7b2c\u4e09\u65b9\u8bc4\u4f30\u6587\u6863\u3002\u5f00\u53d1\u4e86CoBoT\u7b97\u6cd5\u4f5c\u4e3a\u5b9e\u4f8b\u3002", "result": "\u63d0\u51fa\u4e86\u5b8c\u6574\u7684SToBB\u6846\u67b6\uff0c\u5e76\u5728\u8868\u683c\u4efb\u52a1\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u4e0a\u5b9e\u4f8b\u5316\u3002CoBoT\u7b97\u6cd5\u80fd\u591f\u5728\u7ebf\u6784\u5efa\u548c\u7ef4\u62a4\u7ecf\u9a8c\u5145\u5206\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u968f\u7740\u89c2\u5bdf\u79ef\u7d2f\u4e0d\u65ad\u66f4\u65b0\u3002", "conclusion": "SToBB\u4f5c\u4e3a\u751f\u547d\u5468\u671f\u89c4\u6a21\u7684\u53ef\u68c0\u67e5\u53c2\u8003\u70b9\uff0c\u652f\u6301\u4e00\u81f4\u3001\u53ef\u590d\u7528\u7684\u5206\u6790\u548c\u7cfb\u7edf\u5916\u90e8\u5ba1\u67e5\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4f34\u968f\u6574\u4e2a\u751f\u547d\u5468\u671f\u7684\u89e3\u91ca\u6587\u6863\u4f53\u7cfb\u3002"}}
{"id": "2602.02224", "categories": ["cs.LG", "cs.AI", "math.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02224", "abs": "https://arxiv.org/abs/2602.02224", "authors": ["Georgi Ivanov", "Narmeen Oozeer", "Shivam Raval", "Tasana Pejovic", "Shriyash Upadhyay", "Amir Abdullah"], "title": "Spectral Superposition: A Theory of Feature Geometry", "comment": null, "summary": "Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.", "AI": {"tldr": "\u63d0\u51fa\u7528\u8c31\u65b9\u6cd5\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u51e0\u4f55\u7ed3\u6784\uff0c\u5f15\u5165\u6846\u67b6\u7b97\u5b50\u7814\u7a76\u7279\u5f81\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\uff0c\u8bc1\u660e\u5bb9\u91cf\u9971\u548c\u5bfc\u81f4\u8c31\u5c40\u90e8\u5316\u7279\u5f81", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u7a00\u758f\u7ebf\u6027\u7279\u5f81\u4f46\u4e22\u5f03\u4e86\u51e0\u4f55\u7ed3\u6784\uff0c\u9700\u8981\u53d1\u5c55\u7406\u8bba\u6765\u7814\u7a76\u7279\u5f81\u7684\u51e0\u4f55\u7ed3\u6784", "method": "\u5f15\u5165\u6846\u67b6\u7b97\u5b50F=WW\u22a4\u4f5c\u4e3a\u8c31\u5ea6\u91cf\uff0c\u5206\u6790\u6743\u91cd\u77e9\u9635\u7684\u8c31\uff08\u7279\u5f81\u503c\u3001\u7279\u5f81\u7a7a\u95f4\u7b49\uff09\uff0c\u7814\u7a76\u7279\u5f81\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u8303\u6570\u5206\u914d", "result": "\u5728\u53e0\u52a0\u73a9\u5177\u6a21\u578b\u4e2d\u8bc1\u660e\u5bb9\u91cf\u9971\u548c\u8feb\u4f7f\u8c31\u5c40\u90e8\u5316\uff1a\u7279\u5f81\u574d\u7f29\u5230\u5355\u4e2a\u7279\u5f81\u7a7a\u95f4\uff0c\u7ec4\u7ec7\u6210\u7d27\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u8054\u65b9\u6848\u8fdb\u884c\u79bb\u6563\u5206\u7c7b", "conclusion": "\u8c31\u5ea6\u91cf\u5f62\u5f0f\u9002\u7528\u4e8e\u4efb\u610f\u6743\u91cd\u77e9\u9635\uff0c\u53ef\u8bca\u65ad\u7279\u5f81\u5c40\u90e8\u5316\uff0c\u6307\u5411\u5e94\u7528\u7b97\u5b50\u7406\u8bba\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u8ba1\u5212"}}
{"id": "2602.02229", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.02229", "abs": "https://arxiv.org/abs/2602.02229", "authors": ["Guangyi Zhang", "Yunlong Cai", "Guanding Yu", "Osvaldo Simeone"], "title": "Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts", "comment": null, "summary": "We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.", "AI": {"tldr": "\u63d0\u51fa\u9884\u6d4b\u9a71\u52a8\u98ce\u9669\u76d1\u63a7\uff08PPRM\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u9a71\u52a8\u63a8\u7406\u7684\u534a\u76d1\u7763\u98ce\u9669\u76d1\u63a7\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u6a21\u578b\u6027\u80fd\u76d1\u63a7\uff0c\u901a\u8fc7\u5408\u6210\u6807\u7b7e\u4e0e\u5c11\u91cf\u771f\u5b9e\u6807\u7b7e\u7ed3\u5408\u6784\u5efa\u8fd0\u884c\u98ce\u9669\u7684\u4e0b\u754c\uff0c\u5b9e\u73b0\u65e0\u9700\u5047\u8bbe\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u76d1\u63a7\u6a21\u578b\u6027\u80fd\u65f6\uff0c\u6807\u6ce8\u6570\u636e\u901a\u5e38\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u6709\u6548\u76d1\u63a7\u98ce\u9669\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u9884\u6d4b\u9a71\u52a8\u63a8\u7406\uff08PPI\uff09\uff0c\u7ed3\u5408\u5408\u6210\u6807\u7b7e\u4e0e\u5c11\u91cf\u771f\u5b9e\u6807\u7b7e\u6784\u5efa\u8fd0\u884c\u98ce\u9669\u7684\u968f\u65f6\u6709\u6548\u4e0b\u754c\uff0c\u901a\u8fc7\u9608\u503c\u6bd4\u8f83\u4e0e\u540d\u4e49\u98ce\u9669\u4e0a\u754c\u6765\u68c0\u6d4b\u6709\u5bb3\u504f\u79fb\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7535\u4fe1\u76d1\u63a7\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86PPRM\u7684\u6709\u6548\u6027\u3002", "conclusion": "PPRM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u76d1\u63a7\u6a21\u578b\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u65e0\u9700\u5047\u8bbe\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u80fd\u591f\u53ef\u9760\u68c0\u6d4b\u6709\u5bb3\u504f\u79fb\u3002"}}
{"id": "2602.02230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02230", "abs": "https://arxiv.org/abs/2602.02230", "authors": ["Ziyu Zhou", "Yuchen Fang", "Weilin Ruan", "Shiyu Wang", "James Kwok", "Yuxuan Liang"], "title": "SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting", "comment": "Under review", "summary": "Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.", "AI": {"tldr": "SEDformer\uff1a\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684IMTS\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u7a00\u758f-\u4e8b\u4ef6\u5bf9\u5076\u6027\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u9065\u6d4b\u6570\u636e\u9884\u6d4b", "motivation": "\u73b0\u6709\u56fe\u57fa\u548cTransformer\u57fa\u9884\u6d4b\u5668\u5ffd\u7565\u4e86\u4e0d\u89c4\u5219\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u7a00\u758f-\u4e8b\u4ef6\u5bf9\u5076\u6027\u7279\u6027\uff1a\u5747\u5300\u7f51\u683c\u9884\u5bf9\u9f50\u548c\u586b\u5145\u7834\u574f\u4e86\u7a00\u758f\u6027\uff0c\u5173\u7cfb\u91cd\u6784\u524a\u5f31\u4e86\u4e8b\u4ef6\u8bed\u4e49\u3002\u9700\u8981\u66f4\u7b26\u5408IMTS SED\u7279\u6027\u7684\u5efa\u6a21\u8303\u5f0f\u3002", "method": "\u63d0\u51faSEDformer\uff1a1) SED\u57fa\u8109\u51b2\u7f16\u7801\u5668\u4f7f\u7528\u4e8b\u4ef6\u5bf9\u9f50LIF\u795e\u7ecf\u5143\u5c06\u539f\u59cb\u89c2\u6d4b\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u540c\u6b65\u8109\u51b2\uff1b2) \u4e8b\u4ef6\u4fdd\u6301\u65f6\u95f4\u4e0b\u91c7\u6837\u6a21\u5757\u538b\u7f29\u957f\u95f4\u9699\u540c\u65f6\u4fdd\u7559\u663e\u8457\u8109\u51b2\uff1b3) SED\u57fa\u8109\u51b2Transformer\u5757\u5806\u53e0\uff0c\u4f7f\u7528\u57fa\u4e8e\u819c\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u8fdb\u884c\u5e8f\u5217\u5185\u4f9d\u8d56\u5efa\u6a21\u3002", "result": "\u5728\u516c\u5171\u9065\u6d4bIMTS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEDformer\u5728\u8fbe\u5230\u6700\u5148\u8fdb\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "SEDformer\u4e3a\u5efa\u6a21IMTS\u63d0\u4f9b\u4e86\u4e00\u6761\u81ea\u7136\u4e14\u9ad8\u6548\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u4e0eIMTS\u7684SED\u7279\u6027\u81ea\u7136\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u9884\u6d4b\u4e0e\u8d44\u6e90\u6548\u7387\u7684\u5e73\u8861\u3002"}}
{"id": "2602.02238", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02238", "abs": "https://arxiv.org/abs/2602.02238", "authors": ["Laura Yao", "Gengwei Zhang", "Moajjem Chowdhury", "Yunmei Liu", "Tianlong Chen"], "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution", "comment": null, "summary": "Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.", "AI": {"tldr": "TopoDiff\uff1a\u4e00\u79cd\u7528\u4e8eEEG\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u7684\u51e0\u4f55\u548c\u5173\u7cfb\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u56fe\u50cf\u5d4c\u5165\u548c\u52a8\u6001\u901a\u9053\u5173\u7cfb\u56fe\u6765\u63d0\u5347\u7a7a\u95f4\u751f\u6210\u6027\u80fd", "motivation": "\u73b0\u6709EEG\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u751f\u7406\u7a7a\u95f4\u7ed3\u6784\u7684\u8ba4\u77e5\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u751f\u6210\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3EEG\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u548c\u7535\u6781\u95f4\u5173\u7cfb\u7684\u6a21\u578b", "method": "\u63d0\u51faTopoDiff\u6a21\u578b\uff0c\u5305\u542b\uff1a1\uff09\u4eceEEG\u5730\u5f62\u56fe\u8868\u793a\u4e2d\u63d0\u53d6\u62d3\u6251\u611f\u77e5\u56fe\u50cf\u5d4c\u5165\uff0c\u63d0\u4f9b\u5168\u5c40\u51e0\u4f55\u4e0a\u4e0b\u6587\uff1b2\uff09\u52a8\u6001\u901a\u9053\u5173\u7cfb\u56fe\u7f16\u7801\u7535\u6781\u95f4\u5173\u7cfb\u5e76\u968f\u65f6\u95f4\u52a8\u6001\u6f14\u5316", "result": "\u5728\u591a\u4e2aEEG\u6570\u636e\u96c6\uff08SEED/SEED-IV\u60c5\u611f\u8bc6\u522b\u3001PhysioNet\u8fd0\u52a8\u60f3\u8c61\u3001TUSZ\u766b\u75eb\u68c0\u6d4b\uff09\u4e0a\uff0c\u65b9\u6cd5\u5728\u751f\u6210\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "TopoDiff\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u548c\u52a8\u6001\u5173\u7cfb\u5efa\u6a21\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7a7a\u95f4\u57fa\u7840\u624e\u5b9e\u7684EEG\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u6301\u7eed\u6539\u8fdb"}}
{"id": "2602.02239", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02239", "abs": "https://arxiv.org/abs/2602.02239", "authors": ["Giovanni De Felice", "Riccardo D'Elia", "Alberto Termine", "Pietro Barbiero", "Giuseppe Marra", "Silvia Santini"], "title": "Interpretability in Deep Time Series Models Demands Semantic Alignment", "comment": null, "summary": "Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5e94\u8ffd\u6c42\u8bed\u4e49\u5bf9\u9f50\uff0c\u5373\u9884\u6d4b\u9700\u7528\u5bf9\u7528\u6237\u6709\u610f\u4e49\u7684\u53d8\u91cf\u8868\u8fbe\uff0c\u5e76\u4fdd\u6301\u65f6\u95f4\u6f14\u5316\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u800c\u975e\u4ec5\u89e3\u91ca\u5185\u90e8\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u867d\u9884\u6d4b\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u56e0\u5176\u9ed1\u76d2\u6027\u8d28\u90e8\u7f72\u53d7\u9650\u3002\u5f53\u524d\u89e3\u91ca\u6027\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89e3\u91ca\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\uff0c\u672a\u8003\u8651\u662f\u5426\u4e0e\u4eba\u7c7b\u5bf9\u73b0\u8c61\u7684\u7406\u89e3\u65b9\u5f0f\u4e00\u81f4\u3002", "method": "\u5f62\u5f0f\u5316\u8bed\u4e49\u5bf9\u9f50\u8981\u6c42\uff1a\u9884\u6d4b\u9700\u7528\u5bf9\u7ec8\u7aef\u7528\u6237\u6709\u610f\u4e49\u7684\u53d8\u91cf\u8868\u8fbe\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u673a\u5236\u5b9e\u73b0\uff0c\u4e14\u8bed\u4e49\u5bf9\u9f50\u5fc5\u987b\u5728\u65f6\u95f4\u6f14\u5316\u4e2d\u4fdd\u6301\u3002\u63d0\u51fa\u8bed\u4e49\u5bf9\u9f50\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u84dd\u56fe\uff0c\u8bc6\u522b\u652f\u6301\u4fe1\u4efb\u7684\u7279\u6027\uff0c\u8ba8\u8bba\u6a21\u578b\u8bbe\u8ba1\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u8bed\u4e49\u5bf9\u9f50\u7684\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u65f6\u95f4\u5e8f\u5217\u89e3\u91ca\u6027\u7684\u72ec\u7279\u6311\u6218\uff08\u9700\u4fdd\u6301\u65f6\u95f4\u6f14\u5316\u4e00\u81f4\u6027\uff09\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u8bbe\u8ba1\u6307\u5bfc\u3002", "conclusion": "\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5e94\u8f6c\u5411\u8bed\u4e49\u5bf9\u9f50\uff0c\u786e\u4fdd\u9884\u6d4b\u8868\u8fbe\u4e0e\u4eba\u7c7b\u7406\u89e3\u4e00\u81f4\u4e14\u5728\u65f6\u95f4\u4e0a\u4fdd\u6301\u7a33\u5b9a\uff0c\u8fd9\u5bf9\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.02241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02241", "abs": "https://arxiv.org/abs/2602.02241", "authors": ["Roman Dyachenko", "Nikita Gushchin", "Kirill Sokolov", "Petr Mokrov", "Evgeny Burnaev", "Alexander Korotin"], "title": "Variational Entropic Optimal Transport", "comment": null, "summary": "Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\\log \\mathbb{E}[\\exp(\\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.", "AI": {"tldr": "\u63d0\u51faVarEOT\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u91cd\u6784log-partition\u9879\uff0c\u89e3\u51b3\u5f31\u5bf9\u5076EOT\u76ee\u6807\u4e2d\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\uff0c\u907f\u514dMCMC\u6a21\u62df\uff0c\u5b9e\u73b0\u53ef\u5fae\u5206\u5b66\u4e60\u76ee\u6807", "motivation": "\u8fde\u7eed\u7a7a\u95f4\u4e2d\u4e8c\u6b21\u6210\u672c\u7684\u71b5\u6700\u4f18\u4f20\u8f93\u662f\u89e3\u51b3\u57df\u8f6c\u6362\u95ee\u9898\u7684\u7ecf\u5178\u5de5\u5177\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5f31\u5bf9\u5076EOT\u76ee\u6807\u4f9d\u8d56\u4e8e\u5355\u4e2a\u52bf\u51fd\u6570\uff0c\u7531\u4e8elog-partition\u9879\u96be\u4ee5\u8ba1\u7b97\u800c\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9650\u5236\u4f20\u8f93\u65cf\uff08\u9ad8\u65af\u6df7\u5408\u53c2\u6570\u5316\uff09\uff0c\u8981\u4e48\u9700\u8981\u57fa\u4e8e\u6a21\u62df\u7684\u8bad\u7ec3\u8fc7\u7a0b", "method": "\u63d0\u51fa\u53d8\u5206\u71b5\u6700\u4f18\u4f20\u8f93(VarEOT)\uff0c\u5c06log-partition\u9879\u7cbe\u786e\u91cd\u6784\u4e3a\u8f85\u52a9\u6b63\u5f52\u4e00\u5316\u5668\u4e0a\u7684\u53ef\u5904\u7406\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u4ea7\u751f\u53ef\u5fae\u5206\u5b66\u4e60\u76ee\u6807\uff0c\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4f18\u5316\uff0c\u907f\u514d\u8bad\u7ec3\u671f\u95f4\u7684MCMC\u6a21\u62df", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u672a\u914d\u5bf9\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6216\u6539\u8fdb\u7684\u8f6c\u6362\u8d28\u91cf\uff0c\u4e0e\u4f7f\u7528\u76f8\u540c\u5f31\u5bf9\u5076EOT\u76ee\u6807\u7684\u6c42\u89e3\u5668\u6bd4\u8f83\u652f\u6301\u6240\u63d0\u4f18\u5316\u539f\u5219\u7684\u76ca\u5904", "conclusion": "VarEOT\u901a\u8fc7\u53d8\u5206\u91cd\u6784log-partition\u9879\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff08\u6709\u9650\u6837\u672c\u6cdb\u5316\u754c\u548c\u901a\u7528\u51fd\u6570\u903c\u8fd1\u4e0b\u7684\u8fd1\u4f3c\u7ed3\u679c\uff09\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u5fae\u5206\u7684EOT\u4f18\u5316"}}
{"id": "2602.02258", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02258", "abs": "https://arxiv.org/abs/2602.02258", "authors": ["Gaurav Bhatt", "Aditya Chinchure", "Jiawei Zhou", "Leonid Sigal"], "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization", "comment": null, "summary": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u9f50\u611f\u77e5\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6b63\u5219\u5316\u6574\u5408\u5916\u90e8\u5bf9\u9f50\u4fe1\u53f7\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\u5e73\u8861\u76d1\u7763\u548c\u5bf9\u9f50\u68af\u5ea6\uff0c\u5e76\u5b66\u4e60\u5bf9\u5b8c\u5168\u672a\u5bf9\u9f50\u8f93\u5165\u7684\u5f03\u6743\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u4efb\u52a1\u76ee\u6807\uff0c\u5ffd\u7565\u4e86\u5b89\u5168\u6027\u548c\u907f\u514d\u5e7b\u89c9\u7b49\u5173\u952e\u5bf9\u9f50\u76ee\u6807\uff0c\u5bfc\u81f4\u4e0b\u6e38\u5fae\u8c03\u53ef\u80fd\u7834\u574f\u6a21\u578b\u5bf9\u9f50\u6027\uff0c\u65e0\u6cd5\u7ea0\u6b63\u5df2\u6709\u7684\u672a\u5bf9\u9f50\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u5bf9\u9f50\u611f\u77e5\u5fae\u8c03\u6846\u67b6\uff1a1) \u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6b63\u5219\u5316\u6574\u5408\u5916\u90e8\u5bf9\u9f50\u4fe1\u53f7\uff1b2) \u5f15\u5165\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\uff0c\u57fa\u4e8e\u6bcf\u4e2a\u6837\u672c\u52a8\u6001\u5e73\u8861\u76d1\u7763\u548c\u5bf9\u9f50\u9a71\u52a8\u7684\u68af\u5ea6\uff1b3) \u5b66\u4e60\u5bf9\u5b8c\u5168\u672a\u5bf9\u9f50\u8f93\u5165\u7684\u5f03\u6743\u884c\u4e3a\uff0c\u5c06\u4fdd\u5b88\u54cd\u5e94\u76f4\u63a5\u7eb3\u5165\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u901a\u7528\u548c\u9886\u57df\u7279\u5b9a\u6307\u4ee4\u5fae\u8c03\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u51cf\u5c11\u4e86\u6709\u5bb3\u548c\u5e7b\u89c9\u8f93\u51fa\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u989d\u5916\u5206\u6790\u663e\u793a\u5bf9\u5bf9\u6297\u6027\u5fae\u8c03\u3001\u63d0\u793a\u653b\u51fb\u548c\u4e0d\u5b89\u5168\u521d\u59cb\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u95e8\u63a7\u5bf9\u9f50\u4f18\u5316\u662f\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u9f50\u4fdd\u6301\u548c\u5bf9\u9f50\u6062\u590d\u6a21\u578b\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.02260", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02260", "abs": "https://arxiv.org/abs/2602.02260", "authors": ["Zhengjia Zhuo", "Anupam Gupta", "Viswanath Nagarajan"], "title": "Learning Markov Decision Processes under Fully Bandit Feedback", "comment": null, "summary": "A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $\u0398(\\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \\emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\\widetilde{O}(\\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5b8c\u5168bandit\u53cd\u9988\u7684episodic MDP\u7684\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86$\\widetilde{O}(\\sqrt{T})$\u7684\u9057\u61be\u754c\uff0c\u5e76\u5728k-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u7b49\u5b9e\u9645\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u667a\u80fd\u4f53\u80fd\u89c2\u5bdf\u5230\u6bcf\u4e2a\u8bbf\u95ee\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u53ca\u5176\u5373\u65f6\u5956\u52b1\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u79cd\u8be6\u7ec6\u53cd\u9988\u5f80\u5f80\u4e0d\u73b0\u5b9e\u3002\u73b0\u6709\u7814\u7a76\u5df2\u63a2\u7d22\u4e86\u8f68\u8ff9\u53cd\u9988\u7b49\u53d7\u9650\u8bbe\u7f6e\uff0c\u4f46\u672c\u6587\u8003\u8651\u66f4\u4e25\u683c\u7684\"\u5b8c\u5168bandit\"\u53cd\u9988\u6a21\u578b\uff0c\u667a\u80fd\u4f53\u4ec5\u80fd\u83b7\u5f97\u805a\u5408\u5956\u52b1\uff0c\u65e0\u6cd5\u89c2\u5bdf\u5230\u8bbf\u95ee\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9episodic MDP\u7684\u5b8c\u5168bandit\u53cd\u9988\u5b66\u4e60\u7b97\u6cd5\u3002\u7b97\u6cd5\u8bbe\u8ba1\u8003\u8651\u4e86bandit\u53cd\u9988\u7684\u6311\u6218\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6280\u672f\u5904\u7406\u72b6\u6001-\u52a8\u4f5c\u5bf9\u4e0d\u53ef\u89c2\u6d4b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1$\\widetilde{O}(\\sqrt{T})$\u7684\u9057\u61be\u754c\u3002\u7b97\u6cd5\u5bf9horizon\u957f\u5ea6\u6709\u6307\u6570\u4f9d\u8d56\uff0c\u4f5c\u8005\u8bc1\u660e\u8fd9\u662f\u5fc5\u8981\u7684\u3002", "result": "1. \u5b9e\u73b0\u4e86$\\widetilde{O}(\\sqrt{T})$\u7684\u9057\u61be\u754c\uff0c\u5bf9horizon\u957f\u5ea6\u6709\u6307\u6570\u4f9d\u8d56\u4e14\u8bc1\u660e\u8fd9\u662f\u5fc5\u8981\u7684\uff1b2. \u5bf9\"\u6709\u5e8f\"MDP\u83b7\u5f97\u4e86\u6539\u8fdb\u7684\u8fd1\u4e4e\u7d27\u7684\u9057\u61be\u754c\uff1b3. \u5728k-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u95ee\u9898\u4e0a\uff0c\u5c3d\u7ba1\u53cd\u9988\u9ad8\u5ea6\u53d7\u9650\uff0c\u7b97\u6cd5\u6027\u80fd\u4e0e\u5177\u6709\u8be6\u7ec6\u72b6\u6001-\u52a8\u4f5c\u53cd\u9988\u7684\u6700\u5148\u8fdb\u5b66\u4e60\u7b97\u6cd5(UCB-VI)\u76f8\u5f53\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3a\u5b8c\u5168bandit\u53cd\u9988\u7684episodic MDP\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6781\u7aef\u53d7\u9650\u53cd\u9988\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u7a7a\u767d\u3002\u7b97\u6cd5\u4e0d\u4ec5\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u5728\u5b9e\u9645\u95ee\u9898\u5982k-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u4e2d\u4e5f\u8868\u73b0\u51fa\u4e0e\u8be6\u7ec6\u53cd\u9988\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u9ad8\u5ea6\u53d7\u9650\u53cd\u9988\u73af\u5883\u4e0b\u8fdb\u884c\u6709\u6548\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2602.02261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02261", "abs": "https://arxiv.org/abs/2602.02261", "authors": ["Daniil Shlenskii", "Alexander Varlamov", "Nazar Buzun", "Alexander Korotin"], "title": "Unlocking the Duality between Flow and Field Matching", "comment": null, "summary": "Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6761\u4ef6\u6d41\u5339\u914d\uff08CFM\uff09\u4e0e\u4ea4\u4e92\u573a\u5339\u914d\uff08IFM\uff09\u4e4b\u95f4\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff1a\u524d\u5411IFM\u4e0eCFM\u5b58\u5728\u53cc\u5c04\u5173\u7cfb\uff0c\u4f46\u4e00\u822cIFM\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u5305\u542bCFM\u65e0\u6cd5\u5b9e\u73b0\u7684\u7269\u7406\u573a\u6a21\u578b\u3002", "motivation": "CFM\u548cIFM\u662f\u4e24\u79cd\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff1aCFM\u57fa\u4e8e\u6570\u636e\u7a7a\u95f4\u7684\u6761\u4ef6\u6982\u7387\u8def\u5f84\uff0cIFM\u57fa\u4e8e\u589e\u5f3a\u6570\u636e\u7a7a\u95f4\u7684\u7269\u7406\u573a\u3002\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u8fd9\u4e24\u4e2a\u6846\u67b6\u662f\u672c\u8d28\u4e0d\u540c\u8fd8\u662f\u540c\u4e00\u52a8\u6001\u7684\u4e24\u79cd\u63cf\u8ff0\uff0c\u4ee5\u5efa\u7acb\u7406\u8bba\u8054\u7cfb\u5e76\u4fc3\u8fdb\u76f8\u4e92\u501f\u9274\u3002", "method": "1. \u8bc1\u660e\u524d\u5411IFM\u4e0eCFM\u4e4b\u95f4\u5b58\u5728\u53cc\u5c04\u5173\u7cfb\uff1b2. \u5c55\u793a\u4e00\u822cIFM\u6bd4CFM\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u5305\u542bEFM\u7b49CFM\u65e0\u6cd5\u5b9e\u73b0\u7684\u4ea4\u4e92\u573a\uff1b3. \u5229\u7528\u8fd9\u79cd\u5bf9\u5076\u6027\u4e3a\u524d\u5411IFM\u63d0\u4f9b\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u4e3aCFM\u5f00\u53d1\u65b0\u7684IFM\u9a71\u52a8\u6280\u672f\u3002", "result": "1. \u5efa\u7acb\u4e86CFM\u4e0e\u524d\u5411IFM\u4e4b\u95f4\u7684\u7b49\u4ef7\u6027\uff1b2. \u8bc1\u660e\u4e86\u4e00\u822cIFM\u6bd4CFM\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u9759\u7535\u5339\u914d\u7b49\u7269\u7406\u573a\u6a21\u578b\uff1b3. \u5c55\u793a\u4e86\u8fd9\u79cd\u5bf9\u5076\u6027\u5982\u4f55\u4f7f\u4e24\u4e2a\u6846\u67b6\u76f8\u4e92\u53d7\u76ca\u3002", "conclusion": "CFM\u548c\u524d\u5411IFM\u672c\u8d28\u4e0a\u662f\u540c\u4e00\u751f\u6210\u52a8\u6001\u7684\u4e24\u79cd\u63cf\u8ff0\uff0c\u800c\u4e00\u822cIFM\u662f\u66f4\u5e7f\u6cdb\u7684\u6846\u67b6\u3002\u8fd9\u79cd\u5bf9\u5076\u6027\u4e3a\u4e24\u4e2a\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u6280\u672f\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u751f\u6210\u6a21\u578b\u7406\u8bba\u7684\u7edf\u4e00\u3002"}}
{"id": "2602.02264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02264", "abs": "https://arxiv.org/abs/2602.02264", "authors": ["Paolo Marcandelli", "Natansh Mathur", "Stefano Markidis", "Martina Siena", "Stefano Mariani"], "title": "Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training", "comment": "51 pages, 15 figures, 6 tables", "summary": "Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u7b56\u7565\u548cPhIS-FNO\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8fb9\u754c\u6761\u4ef6\u7ea6\u675f\u548c\u6837\u6761\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u65e0\u76d1\u7763PDE\u6c42\u89e3\uff0c\u8fbe\u5230\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u9700\u8981\u76d1\u7763\u6570\u636e\uff0c\u800c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u6536\u655b\u4e0d\u7a33\u5b9a\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u7269\u7406\u7ea6\u675f\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u53c8\u80fd\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\u548c\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u7b56\u7565\uff1a1) \u6e10\u8fdb\u5f0f\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f3a\u5236\u8fb9\u754c\u6761\u4ef6\uff1b2) \u968f\u540e\u52a0\u5165\u5185\u90e8\u6b8b\u5dee\u9879\uff1b3) \u6bcf\u9636\u6bb5\u91cd\u65b0\u521d\u59cb\u5316\u4f18\u5316\u5668\u4f5c\u4e3a\u5ef6\u7eed\u673a\u5236\u3002\u540c\u65f6\u63d0\u51faPhIS-FNO\u6a21\u578b\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u5c42\u548cHermite\u6837\u6761\u6838\u8fdb\u884c\u5e73\u6ed1\u6b8b\u5dee\u8bc4\u4f30\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPhIS-FNO\u4ec5\u4f7f\u7528\u8fb9\u754c\u533a\u57df\u7684\u6807\u6ce8\u4fe1\u606f\u5c31\u8fbe\u5230\u4e86\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5206\u9636\u6bb5\u3001\u57fa\u4e8e\u6837\u6761\u7684\u4f18\u5316\u4e3a\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684PDE\u6c42\u89e3\u3002"}}
{"id": "2602.02268", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02268", "abs": "https://arxiv.org/abs/2602.02268", "authors": ["Sanggeon Yun", "Raheeb Hassan", "Ryozo Masukawa", "Sungheon Jeong", "Mohsen Imani"], "title": "HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control", "comment": null, "summary": "Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.", "AI": {"tldr": "HopFormer\uff1a\u4e00\u79cd\u4ec5\u901a\u8fc7\u5934\u7279\u5b9a\u7684n\u8df3\u63a9\u7801\u7a00\u758f\u6ce8\u610f\u529b\u6ce8\u5165\u56fe\u7ed3\u6784\u7684\u56feTransformer\uff0c\u65e0\u9700\u4f4d\u7f6e\u7f16\u7801\u6216\u67b6\u6784\u4fee\u6539\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u968f\u63a9\u7801\u7a00\u758f\u5ea6\u7ebf\u6027\u6269\u5c55\u7684\u540c\u65f6\u63d0\u4f9b\u663e\u5f0f\u4e14\u53ef\u89e3\u91ca\u7684\u63a5\u53d7\u57df\u63a7\u5236\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u56feTransformer\u8bbe\u8ba1\u4e2d\u7684\u666e\u904d\u5047\u8bbe\uff0c\u5373\u4f9d\u8d56\u663e\u5f0f\u4f4d\u7f6e/\u7ed3\u6784\u7f16\u7801\u548c\u5bc6\u96c6\u5168\u5c40\u6ce8\u610f\u529b\u6765\u878d\u5165\u56fe\u62d3\u6251\u3002\u7814\u7a76\u8868\u660e\u8fd9\u4e24\u8005\u90fd\u4e0d\u662f\u5fc5\u9700\u7684\u3002", "method": "\u5f15\u5165HopFormer\uff0c\u901a\u8fc7\u5934\u7279\u5b9a\u7684n\u8df3\u63a9\u7801\u7a00\u758f\u6ce8\u610f\u529b\u6ce8\u5165\u7ed3\u6784\uff0c\u4e0d\u4f7f\u7528\u4f4d\u7f6e\u7f16\u7801\u6216\u67b6\u6784\u4fee\u6539\u3002\u8fd9\u79cd\u8bbe\u8ba1\u63d0\u4f9b\u663e\u5f0f\u53ef\u89e3\u91ca\u7684\u63a5\u53d7\u57df\u63a7\u5236\uff0c\u540c\u65f6\u5b9e\u73b0\u771f\u6b63\u7a00\u758f\u7684\u6ce8\u610f\u529b\uff0c\u8ba1\u7b97\u6210\u672c\u968f\u63a9\u7801\u7a00\u758f\u5ea6\u7ebf\u6027\u6269\u5c55\u3002", "result": "\u5728\u8282\u70b9\u7ea7\u548c\u56fe\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u56fe\u7ed3\u6784\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\uff1a\u5728\u5177\u6709\u5f3a\u5c0f\u4e16\u754c\u5c5e\u6027\u7684\u56fe\u4e0a\uff0c\u5c40\u90e8\u6ce8\u610f\u529b\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u4e00\u81f4\u7684\u9ad8\u6027\u80fd\uff1b\u5728\u5177\u6709\u8f83\u5f31\u5c0f\u4e16\u754c\u6548\u5e94\u7684\u56fe\u4e0a\uff0c\u5168\u5c40\u6ce8\u610f\u529b\u5e26\u6765\u7684\u6536\u76ca\u9012\u51cf\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u56feTransformer\u8bbe\u8ba1\u4e2d\u7684\u666e\u904d\u5047\u8bbe\uff0c\u5e76\u5f3a\u8c03\u7a00\u758f\u63a7\u5236\u6ce8\u610f\u529b\u4f5c\u4e3a\u4e00\u79cd\u539f\u5219\u6027\u548c\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.02281", "categories": ["cs.LG", "cs.AI", "cs.NE", "physics.class-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.02281", "abs": "https://arxiv.org/abs/2602.02281", "authors": ["Antonino Emanuele Scurria"], "title": "Backpropagation as Physical Relaxation: Exact Gradients in Finite Time", "comment": "15 pages, 8 figures", "summary": "Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4e8c\u5143\u53cd\u5411\u4f20\u64ad\"\u6846\u67b6\uff0c\u5c06\u53cd\u5411\u4f20\u64ad\u89e3\u91ca\u4e3a\u7269\u7406\u52a8\u529b\u7cfb\u7edf\u7684\u6709\u9650\u65f6\u95f4\u677e\u5f1b\u8fc7\u7a0b\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u548c\u62c9\u683c\u6717\u65e5\u7406\u8bba\uff0c\u5728\u53cc\u500d\u72b6\u6001\u7a7a\u95f4\u4e0a\u63a8\u5bfc\u5168\u5c40\u80fd\u91cf\u51fd\u6570\uff0c\u5176\u978d\u70b9\u52a8\u529b\u5b66\u80fd\u57282L\u6b65\u5185\u7cbe\u786e\u6062\u590d\u6807\u51c6\u53cd\u5411\u4f20\u64ad\u3002", "motivation": "\u4f20\u7edf\u4e0a\u5c06\u53cd\u5411\u4f20\u64ad\u89c6\u4e3a\u7b26\u53f7\u8ba1\u7b97\u548c\u94fe\u5f0f\u6cd5\u5219\u7684\u9012\u5f52\u5e94\u7528\uff0c\u4f5c\u8005\u5e0c\u671b\u4ece\u7269\u7406\u52a8\u529b\u7cfb\u7edf\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u8fd9\u4e00\u57fa\u7840\u7b97\u6cd5\uff0c\u4e3a\u6a21\u62df\u548c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5e73\u53f0\u63d0\u4f9b\u7cbe\u786e\u68af\u5ea6\u8ba1\u7b97\u7684\u4e25\u683c\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u524d\u5411\u63a8\u7406\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u5e94\u7528\u975e\u4fdd\u5b88\u7cfb\u7edf\u7684\u62c9\u683c\u6717\u65e5\u7406\u8bba\u5904\u7406\u975e\u5bf9\u79f0\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u7f16\u7801\u6fc0\u6d3b\u548c\u654f\u611f\u5ea6\u7684\u53cc\u500d\u72b6\u6001\u7a7a\u95f4\u4e0a\u63a8\u5bfc\u5168\u5c40\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u8be5\u80fd\u91cf\u7684\u978d\u70b9\u52a8\u529b\u5b66\u540c\u65f6\u6267\u884c\u63a8\u7406\u548c\u4fe1\u7528\u5206\u914d\u3002", "result": "\u8bc1\u660e\u5355\u4f4d\u6b65\u957f\u6b27\u62c9\u79bb\u6563\u5316\uff08\u5c42\u8f6c\u6362\u7684\u81ea\u7136\u65f6\u95f4\u5c3a\u5ea6\uff09\u80fd\u5728\u6070\u597d2L\u6b65\u5185\u7cbe\u786e\u6062\u590dL\u5c42\u7f51\u7edc\u7684\u6807\u51c6\u53cd\u5411\u4f20\u64ad\uff0c\u65e0\u9700\u8fd1\u4f3c\u3002\u4e0e\u5148\u524d\u9700\u8981\u5bf9\u79f0\u6743\u91cd\u3001\u6e10\u8fd1\u6536\u655b\u6216\u5fae\u5c0f\u6270\u52a8\u7684\u80fd\u91cf\u65b9\u6cd5\u4e0d\u540c\uff0c\u672c\u6846\u67b6\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u5185\u83b7\u5f97\u7cbe\u786e\u68af\u5ea6\u3002", "conclusion": "\u53cd\u5411\u4f20\u64ad\u662f\u8fde\u7eed\u7269\u7406\u677e\u5f1b\u8fc7\u7a0b\u7684\u6570\u5b57\u5316\u4f18\u5316\u6295\u5f71\uff0c\u4e3a\u6a21\u62df\u548c\u795e\u7ecf\u5f62\u6001\u57fa\u677f\u4e2d\u7684\u7cbe\u786e\u68af\u5ea6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e25\u683c\u57fa\u7840\uff0c\u8fd9\u4e9b\u5e73\u53f0\u4e2d\u8fde\u7eed\u52a8\u6001\u662f\u539f\u751f\u7279\u6027\u3002"}}
{"id": "2602.02282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02282", "abs": "https://arxiv.org/abs/2602.02282", "authors": ["Susu Hu", "Stefanie Speidel"], "title": "MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology", "comment": null, "summary": "Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.", "AI": {"tldr": "MoLF\u662f\u4e00\u4e2a\u7528\u4e8e\u6cdb\u764c\u7ec4\u7ec7\u57fa\u56e0\u7ec4\u9884\u6d4b\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u4ef6\u6d41\u5339\u914d\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5728\u8de8\u7ec4\u7ec7\u6cdb\u764c\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63a8\u65ad\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u7ec4\u7ec7\u6a21\u578b\uff0c\u8fd9\u79cd\u788e\u7247\u5316\u65e0\u6cd5\u5229\u7528\u8de8\u764c\u75c7\u7c7b\u578b\u7684\u5171\u4eab\u751f\u7269\u5b66\u539f\u7406\uff0c\u4e14\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u5e94\u7528\u53d7\u9650\u3002\u867d\u7136\u6cdb\u764c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7531\u6b64\u4ea7\u751f\u7684\u5f02\u8d28\u6027\u6311\u6218\u4e86\u5355\u4e00\u67b6\u6784\u6a21\u578b\u3002", "method": "MoLF\uff08Mixture-of-Latent-Flow\uff09\u91c7\u7528\u6761\u4ef6\u6d41\u5339\u914d\u76ee\u6807\u5c06\u566a\u58f0\u6620\u5c04\u5230\u57fa\u56e0\u6f5c\u5728\u6d41\u5f62\uff0c\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u901f\u5ea6\u573a\u53c2\u6570\u5316\u3002\u8be5\u67b6\u6784\u901a\u8fc7\u52a8\u6001\u8def\u7531\u8f93\u5165\u5230\u4e13\u95e8\u5b50\u7f51\u7edc\uff0c\u6709\u6548\u89e3\u8026\u4e86\u4e0d\u540c\u7ec4\u7ec7\u6a21\u5f0f\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMoLF\u5728\u6cdb\u764c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e00\u81f4\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u3002\u6b64\u5916\uff0cMoLF\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u6570\u636e\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u5b83\u6355\u6349\u5230\u4e86\u57fa\u672c\u3001\u4fdd\u5b88\u7684\u7ec4\u7ec7-\u5206\u5b50\u673a\u5236\u3002", "conclusion": "MoLF\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u6d41\u5339\u914d\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86\u6cdb\u764c\u7ec4\u7ec7\u57fa\u56e0\u7ec4\u9884\u6d4b\u4e2d\u7684\u5f02\u8d28\u6027\u6311\u6218\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u8fd8\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u7ec4\u7ec7\u57fa\u56e0\u7ec4\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.02283", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02283", "abs": "https://arxiv.org/abs/2602.02283", "authors": ["Owen Shen", "Patrick Jaillet"], "title": "Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management", "comment": null, "summary": "We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \\emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\\varepsilon/(1-\u03b3))$ neighborhood of the optimal Q-function, where $\\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.", "AI": {"tldr": "\u63d0\u51fa\"\u9009\u62e9\u6a21\u578b\u8f85\u52a9\u5f3a\u5316\u5b66\u4e60\"\uff0c\u4f7f\u7528\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4f5c\u4e3a\u90e8\u5206\u4e16\u754c\u6a21\u578b\u6765\u4f30\u7b97\u5ef6\u8fdf\u53cd\u9988\uff0c\u5728\u56fa\u5b9a\u6a21\u578b\u90e8\u7f72\u4e0b\u8bc1\u660eQ\u5b66\u4e60\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u53c2\u6570\u504f\u79fb\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u6a21\u578b\u5047\u8bbe\u9519\u8bef\u65f6\u8868\u73b0\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u6536\u5165\u7ba1\u7406\u4e2d\u5ef6\u8fdf\u53cd\u9988\u95ee\u9898\uff0c\u5176\u4e2d\u5ba2\u6237\u53d6\u6d88\u548c\u4fee\u6539\u5728\u9884\u8ba2\u540e\u6570\u5929\u624d\u89c2\u5bdf\u5230\uff0c\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6a21\u578b\u8f85\u52a9\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a\u4f7f\u7528\u6821\u51c6\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4f5c\u4e3a\u56fa\u5b9a\u90e8\u5206\u4e16\u754c\u6a21\u578b\uff0c\u5728\u51b3\u7b56\u65f6\u4f30\u7b97\u5ef6\u8fdf\u7684\u5b66\u4e60\u76ee\u6807\u5206\u91cf\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8868\u683cQ\u5b66\u4e60\u6536\u655b\u5230\u6700\u4f18Q\u51fd\u6570\u7684\u90bb\u57df\uff1b\u5b9e\u9a8c\u663e\u793a\uff1a1) \u5728\u5e73\u7a33\u8bbe\u7f6e\u4e2d\u4e0e\u57fa\u7ebf\u65e0\u663e\u8457\u5dee\u5f02\uff1b2) \u5728\u53c2\u6570\u504f\u79fb\u65f6\u8868\u73b0\u66f4\u597d\uff0810\u4e2a\u573a\u666f\u4e2d5\u4e2a\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad812.4%\uff09\uff1b3) \u5728\u6a21\u578b\u5047\u8bbe\u9519\u8bef\u65f6\u8868\u73b0\u4e0b\u964d\uff08\u6536\u5165\u964d\u4f4e1.4-2.6%\uff09\u3002", "conclusion": "\u90e8\u5206\u884c\u4e3a\u6a21\u578b\u5728\u53c2\u6570\u504f\u79fb\u65f6\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u7ed3\u6784\u9519\u8bef\u8bbe\u5b9a\u65f6\u4f1a\u5f15\u5165\u6709\u5bb3\u504f\u5dee\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.02288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02288", "abs": "https://arxiv.org/abs/2602.02288", "authors": ["Zheng Li", "Jerry Cheng", "Huanying Gu"], "title": "An Optimization Method for Autoregressive Time Series Forecasting", "comment": "10 pages, 2 figures, 2 tables", "summary": "Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u81ea\u56de\u5f52\u9884\u6d4b\u8bef\u5dee\u968f\u9884\u6d4b\u8303\u56f4\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u5141\u8bb8\u6a21\u578b\u8fde\u63a5\u77ed\u671f\u9884\u6d4b\u5f62\u6210\u7075\u6d3b\u957f\u671f\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u6269\u5927\u6a21\u578b\u89c4\u6a21\u800c\u975e\u771f\u6b63\u7684\u81ea\u56de\u5f52\u5c55\u5f00\u6765\u5b9e\u73b0\u957f\u671f\u9884\u6d4b\uff0c\u4e14\u4f20\u7edf\u8bad\u7ec3\u8fc7\u7a0b\u5ffd\u7565\u4e86\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f3a\u5236\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff1a1) \u81ea\u56de\u5f52\u9884\u6d4b\u8bef\u5dee\u5e94\u968f\u9884\u6d4b\u8303\u56f4\u589e\u52a0\u800c\u589e\u52a0\uff0c\u8fdd\u53cd\u6b64\u539f\u5219\u88ab\u89c6\u4e3a\u968f\u673a\u731c\u6d4b\u5e76\u5728\u635f\u5931\u51fd\u6570\u4e2d\u660e\u786e\u60e9\u7f5a\uff1b2) \u4f7f\u6a21\u578b\u80fd\u591f\u8fde\u63a5\u77ed\u671f\u81ea\u56de\u5f52\u9884\u6d4b\u5f62\u6210\u7075\u6d3b\u7684\u957f\u671f\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684SOTA\uff0c\u76f8\u6bd4iTransformer\u548c\u5176\u4ed6\u8fd1\u671f\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u8d85\u8fc710%\u7684MSE\u964d\u4f4e\uff0c\u5e76\u4f7f\u77ed\u671f\u9884\u6d4b\u6a21\u578b\u80fd\u591f\u5728\u8d85\u8fc77.5\u500d\u957f\u7684\u8303\u56f4\u5185\u8fdb\u884c\u53ef\u9760\u7684\u957f\u671f\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5236\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u548c\u7075\u6d3b\u7684\u9884\u6d4b\u8fde\u63a5\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e0a\u3002"}}
{"id": "2602.02295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02295", "abs": "https://arxiv.org/abs/2602.02295", "authors": ["Shaima Ahmad Freja", "Ferhat Ozgur Catak", "Betul Yurdem", "Chunming Rong"], "title": "EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models", "comment": "15 pages (including appendix), 11 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.", "AI": {"tldr": "EvalQReason\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u901a\u8fc7\u6982\u7387\u5206\u5e03\u5206\u6790\u91cf\u5316LLM\u63a8\u7406\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u5305\u542bCSD\u548cSFC\u4e24\u79cd\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793aCSD\u7279\u5f81\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u52a8\u6001\u5177\u6709\u9886\u57df\u7279\u5f02\u6027\u3002", "motivation": "LLM\u5728\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u96be\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u80fd\u91cf\u5316\u63a8\u7406\u8d28\u91cf\u7684\u65b9\u6cd5\u6765\u6df1\u5165\u4e86\u89e3\u63a8\u7406\u6b65\u9aa4\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51faEvalQReason\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u6982\u7387\u5206\u5e03\u5206\u6790\u91cf\u5316\u63a8\u7406\u8d28\u91cf\u3002\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\uff1aCSD\uff08\u76f8\u90bb\u6b65\u9aa4\u5dee\u5f02\uff09\u6d4b\u91cf\u5c40\u90e8\u8fde\u8d2f\u6027\uff0cSFC\uff08\u6b65\u9aa4\u5230\u6700\u7ec8\u6536\u655b\uff09\u8bc4\u4f30\u5168\u5c40\u5bf9\u9f50\u3002\u6bcf\u79cd\u7b97\u6cd5\u4f7f\u7528\u4e94\u4e2a\u7edf\u8ba1\u6307\u6807\u6355\u6349\u63a8\u7406\u52a8\u6001\u3002", "result": "\u5728\u6570\u5b66\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1aCSD\u7279\u5f81\u5728\u6b63\u786e\u6027\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578bF1=0.78\u3001ROC-AUC=0.82\uff0c\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08F1=0.88\u3001ROC-AUC=0.97\uff09\u3002CSD\u59cb\u7ec8\u4f18\u4e8eSFC\uff0c\u5e8f\u5217\u67b6\u6784\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u3002\u63a8\u7406\u52a8\u6001\u5177\u6709\u9886\u57df\u7279\u5f02\u6027\uff1a\u6570\u5b66\u63a8\u7406\u663e\u793a\u6e05\u6670\u7684\u5206\u5316\u6a21\u5f0f\uff0c\u533b\u5b66\u63a8\u7406\u5219\u51e0\u4e4e\u6ca1\u6709\u533a\u5206\u4fe1\u53f7\u3002", "conclusion": "EvalQReason\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u8fc7\u7a0b\u611f\u77e5\u7684\u63a8\u7406\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u786e\u7acb\u4e86\u57fa\u4e8e\u6982\u7387\u7684\u5dee\u5f02\u5206\u6790\u4f5c\u4e3a\u53ef\u4fe1AI\u90e8\u7f72\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002\u63ed\u793a\u4e86LLM\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u63a8\u7406\u7684\u6839\u672c\u5dee\u5f02\u3002"}}
{"id": "2602.02296", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02296", "abs": "https://arxiv.org/abs/2602.02296", "authors": ["Xingli Fang", "Jung-Eun Kim"], "title": "Decoupling Generalizability and Membership Privacy Risks in Neural Networks", "comment": null, "summary": "A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPPTP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u4e2d\u6cdb\u5316\u80fd\u529b\u548c\u9690\u79c1\u98ce\u9669\u7684\u4e0d\u540c\u533a\u57df\uff0c\u5728\u6700\u5c0f\u5316\u6cdb\u5316\u635f\u5931\u7684\u540c\u65f6\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u83b7\u5f97\u67d0\u4e9b\u80fd\u529b\u6216\u7279\u6027\u65f6\u901a\u5e38\u9700\u8981\u727a\u7272\u5176\u4ed6\u6548\u7528\uff0c\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002\u4e0d\u540c\u9632\u5fa1\u65b9\u6cd5\u4e4b\u95f4\u7684\u635f\u5931\u5dee\u5f02\u8868\u660e\u5b58\u5728\u5c06\u6cdb\u5316\u80fd\u529b\u548c\u9690\u79c1\u98ce\u9669\u89e3\u8026\u4ee5\u6700\u5927\u5316\u9690\u79c1\u589e\u76ca\u7684\u6f5c\u529b", "method": "\u8bc6\u522b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u6cdb\u5316\u80fd\u529b\u548c\u9690\u79c1\u98ce\u9669\u5b58\u5728\u7684\u4e0d\u540c\u533a\u57df\uff0c\u57fa\u4e8e\u89c2\u5bdf\u63d0\u51fa\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u539f\u5219(PPTP)\uff0c\u4fdd\u62a4\u6a21\u578b\u7ec4\u4ef6\u514d\u53d7\u9690\u79c1\u98ce\u9669\u540c\u65f6\u6700\u5c0f\u5316\u6cdb\u5316\u80fd\u529b\u635f\u5931", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u663e\u8457\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "PPTP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u8026\u6cdb\u5316\u80fd\u529b\u548c\u9690\u79c1\u98ce\u9669\uff0c\u5728\u6700\u5c0f\u5316\u6cdb\u5316\u635f\u5931\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u9690\u79c1\u4fdd\u62a4\u6548\u679c"}}
{"id": "2602.02366", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02366", "abs": "https://arxiv.org/abs/2602.02366", "authors": ["Sharut Gupta", "Phillip Isola", "Stefanie Jegelka", "David Lopez-Paz", "Kartik Ahuja", "Mark Ibrahim", "Mohammad Pezeshki"], "title": "ReasonCACHE: Teaching LLMs To Reason Without Weight Updates", "comment": "26 pages, 17 Figures", "summary": "Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/", "AI": {"tldr": "ReasonCACHE\u901a\u8fc7\u524d\u7f00\u8c03\u4f18\u5c06\u6f14\u793a\u538b\u7f29\u5230\u56fa\u5b9a\u952e\u503c\u7f13\u5b58\u4e2d\uff0c\u4f7fLLM\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u5c31\u80fd\u5b66\u4e60\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u8d85\u8d8a\u6807\u51c6\u4e0a\u4e0b\u6587\u5b66\u4e60\u5e76\u5339\u914d\u6743\u91cd\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u867d\u7136\u6837\u672c\u9ad8\u6548\uff0c\u4f46\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u9700\u8981\u5927\u91cf\u6f14\u793a\uff0c\u800c\u7b80\u5355\u589e\u52a0\u6f14\u793a\u4f1a\u5bfc\u81f4\u6ce8\u610f\u529b\u6210\u672c\u4e8c\u6b21\u589e\u957f\u3001\u6027\u80fd\u9971\u548c\u6216\u4e0b\u964d\uff0c\u4e14\u4ecd\u662f\u6d45\u5c42\u5b66\u4e60\u3002\u6743\u91cd\u5b66\u4e60(IWL)\u867d\u7136\u6709\u6548\u4f46\u9700\u8981\u53c2\u6570\u66f4\u65b0\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6743\u91cd\u66f4\u65b0\u3001\u80fd\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7684\u63a8\u7406\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReasonCACHE\u65b9\u6cd5\uff0c\u57fa\u4e8e\u524d\u7f00\u8c03\u4f18\u5c06\u6f14\u793a\u84b8\u998f\u5230\u56fa\u5b9a\u7684\u952e\u503c\u7f13\u5b58\u4e2d\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\uff0c\u800c\u662f\u76f4\u63a5\u5c06\u952e\u503c\u5bf9\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u907f\u514d\u4e86\u4e0a\u4e0b\u6587\u7a97\u53e3\u8fc7\u8f7d\u95ee\u9898\u3002", "result": "\u5728GPQA-Diamond\u7b49\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasonCACHE\u4f18\u4e8e\u6807\u51c6ICL\uff0c\u5339\u914d\u6216\u8d85\u8d8aIWL\u65b9\u6cd5\u3002\u540c\u65f6\u5728\u6570\u636e\u6548\u7387\u3001\u63a8\u7406\u6210\u672c\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u66f4\u52a0\u9ad8\u6548\u3002", "conclusion": "ReasonCACHE\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6743\u91cd\u5b66\u4e60\u4e4b\u95f4\u627e\u5230\u4e86\u4e00\u6761\u4e2d\u95f4\u8def\u5f84\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u53c2\u6570\u5c31\u80fd\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u5b66\u4e60\u63a8\u7406\u6280\u80fd\u7684\u53ef\u6269\u5c55\u7b97\u6cd5\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u4f18\u4e8e\u4f4e\u79e9\u6743\u91cd\u66f4\u65b0\u3002"}}
{"id": "2602.02371", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02371", "abs": "https://arxiv.org/abs/2602.02371", "authors": ["Jing Wang", "Jie Shen", "Qiaomin Xie", "Jeremy C Weiss"], "title": "C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference", "comment": null, "summary": "Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \\emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.\n  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.\n  Evaluated on a real-world Long COVID cohort with 13,511 participants, \\emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.", "AI": {"tldr": "C-kNN-LSH\uff1a\u57fa\u4e8e\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7684\u6700\u8fd1\u90bb\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u7ef4\u6df7\u6dc6\u7684\u7eb5\u5411\u56e0\u679c\u63a8\u65ad\uff0c\u5728\u957f\u65b0\u51a0\u961f\u5217\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4ece\u7eb5\u5411\u8f68\u8ff9\u4e2d\u4f30\u8ba1\u56e0\u679c\u6548\u5e94\u5bf9\u4e8e\u7406\u89e3\u590d\u6742\u75be\u75c5\u8fdb\u5c55\u548c\u4f18\u5316\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5171\u75c5\u548c\u957f\u65b0\u51a0\u6062\u590d\u7b49\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5904\u7406\u9ad8\u7ef4\u3001\u6df7\u6dc6\u7684\u590d\u6742\u60c5\u51b5", "method": "\u63d0\u51faC-kNN-LSH\u6846\u67b6\uff0c\u5229\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u9ad8\u6548\u8bc6\u522b\u5177\u6709\u76f8\u4f3c\u534f\u53d8\u91cf\u5386\u53f2\u7684\"\u4e34\u5e8a\u53cc\u80de\u80ce\"\uff0c\u5b9e\u73b0\u8de8\u6f14\u5316\u75be\u75c5\u72b6\u6001\u7684\u5c40\u90e8\u6761\u4ef6\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u3002\u4e3a\u51cf\u8f7b\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u60a3\u8005\u6062\u590d\u7279\u5f81\u53d8\u5316\u5e26\u6765\u7684\u504f\u5dee\uff0c\u5c06\u90bb\u57df\u4f30\u8ba1\u5668\u4e0e\u53cc\u91cd\u7a33\u5065\u6821\u6b63\u76f8\u7ed3\u5408", "result": "\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u4f30\u8ba1\u5668\u7684\u4e00\u81f4\u6027\u548c\u5bf9\u5e72\u6270\u8bef\u5dee\u7684\u4e8c\u9636\u7a33\u5065\u6027\u3002\u5728\u5305\u542b13,511\u540d\u53c2\u4e0e\u8005\u7684\u771f\u5b9e\u4e16\u754c\u957f\u65b0\u51a0\u961f\u5217\u8bc4\u4f30\u4e2d\uff0cC-kNN-LSH\u5728\u6355\u6349\u6062\u590d\u5f02\u8d28\u6027\u548c\u4f30\u8ba1\u653f\u7b56\u4ef7\u503c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "C-kNN-LSH\u4e3a\u5904\u7406\u9ad8\u7ef4\u6df7\u6dc6\u7684\u7eb5\u5411\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u957f\u65b0\u51a0\u7b49\u590d\u6742\u4e34\u5e8a\u573a\u666f\uff0c\u80fd\u591f\u8bc6\u522b\u4e34\u5e8a\u53cc\u80de\u80ce\u5e76\u51c6\u786e\u4f30\u8ba1\u6cbb\u7597\u6548\u679c"}}
{"id": "2602.02381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02381", "abs": "https://arxiv.org/abs/2602.02381", "authors": ["Yipeng Zhang", "Hafez Ghaemi", "Jungyoon Lee", "Shahab Bakhtiari", "Eilif B. Muller", "Laurent Charlin"], "title": "Self-Supervised Learning from Structural Invariance", "comment": "ICLR 2026", "summary": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.", "AI": {"tldr": "AdaSSL\u901a\u8fc7\u5f15\u5165\u6f5c\u53d8\u91cf\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898\uff0c\u589e\u5f3a\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u7075\u6d3b\u6355\u6349\u6570\u636e\u5bf9\u4e2d\u7684\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5f53\u6570\u636e\u6765\u81ea\u81ea\u7136\u751f\u6210\u8fc7\u7a0b\uff08\u5982\u8fde\u7eed\u89c6\u9891\u5e27\uff09\u65f6\u5b58\u5728\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898", "method": "\u5f15\u5165\u6f5c\u53d8\u91cf\u5efa\u6a21\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\uff0c\u63a8\u5bfc\u914d\u5bf9\u5d4c\u5165\u95f4\u4e92\u4fe1\u606f\u7684\u53d8\u5206\u4e0b\u754c\uff0c\u5f97\u5230\u7b80\u5355\u6b63\u5219\u5316\u9879\uff0c\u53ef\u5e94\u7528\u4e8e\u5bf9\u6bd4\u5f0f\u548c\u84b8\u998f\u5f0f\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807", "result": "AdaSSL\u5728\u56e0\u679c\u8868\u793a\u5b66\u4e60\u3001\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u548c\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u7b49\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6f5c\u53d8\u91cf\u5efa\u6a21\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\u80fd\u6709\u6548\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b"}}
{"id": "2602.02383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02383", "abs": "https://arxiv.org/abs/2602.02383", "authors": ["Maksim Afanasyev", "Illarion Iov"], "title": "SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization", "comment": null, "summary": "Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.", "AI": {"tldr": "SLIME\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u53c2\u8003\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u504f\u597d\u5b66\u4e60\u4e0e\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4e2d\u7684\"\u9057\u5fd8\"\u548c\"\u683c\u5f0f\u5316\u5d29\u6e83\"\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u5b58\u5728\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u4f18\u5316\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\u7684\u76f8\u5bf9\u8fb9\u754c\u4e0d\u80fd\u4fdd\u8bc1\u4fdd\u7559\u9009\u62e9\u54cd\u5e94\u7684\u7edd\u5bf9\u6982\u7387\uff0c\u5bfc\u81f4\u6a21\u578b\u53ef\u80fd\"\u9057\u5fd8\"\u9ad8\u8d28\u91cf\u8f93\u51fa\u6216\u51fa\u73b0\"\u683c\u5f0f\u5316\u5d29\u6e83\"\u3002", "method": "SLIME\u91c7\u7528\u4e09\u90e8\u5206\u76ee\u6807\uff1a(1)\u951a\u5b9a\u9879\u6700\u5927\u5316\u504f\u597d\u54cd\u5e94\u7684\u4f3c\u7136\uff1b(2)\u7a33\u5b9a\u60e9\u7f5a\u9632\u6b62\u62d2\u7edd\u4ee4\u724c\u6982\u7387\u5d29\u6e83\u4e3a\u96f6\uff1b(3)\u7ed3\u5408\u786c\u7ea6\u675f\u548c\u8f6f\u7ea6\u675f\u7684\u53cc\u8fb9\u754c\u673a\u5236\u8fdb\u884c\u7cbe\u786e\u8fb9\u754c\u5851\u9020\u3002", "result": "SLIME\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u751f\u6210\u7a33\u5b9a\u6027\u3002", "conclusion": "SLIME\u901a\u8fc7\u89e3\u8026\u504f\u597d\u5b66\u4e60\u4e0e\u751f\u6210\u8d28\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u6709\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2602.02385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02385", "abs": "https://arxiv.org/abs/2602.02385", "authors": ["Adam Shai", "Loren Amdahl-Culleton", "Casper L. Christensen", "Henry R. Bigelow", "Fernando E. Rosas", "Alexander B. Boyd", "Eric A. Alt", "Kyle J. Ray", "Paul M. Riechers"], "title": "Transformers learn factored representations", "comment": null, "summary": "Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.", "AI": {"tldr": "Transformer\u901a\u8fc7\u56e0\u5b50\u5206\u89e3\u5c06\u4e16\u754c\u8868\u793a\u4e3a\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u5728\u6761\u4ef6\u72ec\u7acb\u65f6\u5b9e\u73b0\u65e0\u635f\u8868\u793a\uff0c\u5426\u5219\u5728\u7ef4\u5ea6\u6548\u7387\u548c\u51c6\u786e\u6027\u95f4\u6743\u8861\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u56e0\u5b50\u5206\u89e3\u7684\u5f52\u7eb3\u504f\u597d\u3002", "motivation": "\u7814\u7a76Transformer\u9884\u8bad\u7ec3\u4e2d\u5982\u4f55\u8868\u793a\u4e16\u754c\u7ed3\u6784\uff0c\u63a2\u7d22\u5176\u662f\u5c06\u56e0\u7d20\u8868\u793a\u4e3a\u9ad8\u7ef4\u4e58\u79ef\u7a7a\u95f4\u8fd8\u662f\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7684\u56e0\u5b50\u5206\u89e3\u8868\u793a\uff0c\u7406\u89e3\u6a21\u578b\u5728\u7ef4\u5ea6\u6548\u7387\u548c\u51c6\u786e\u6027\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8868\u793a\u5047\u8bbe\uff1a\u4e58\u79ef\u7a7a\u95f4\u8868\u793a\uff08\u7ef4\u5ea6\u6307\u6570\u589e\u957f\uff09\u548c\u56e0\u5b50\u5206\u89e3\u8868\u793a\uff08\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\uff09\u3002\u5728\u5177\u6709\u5df2\u77e5\u6f5c\u5728\u7ed3\u6784\u7684\u5408\u6210\u8fc7\u7a0b\u4e0a\u8bad\u7ec3Transformer\uff0c\u5206\u6790\u6fc0\u6d3b\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u5b50\u7a7a\u95f4\u6570\u91cf\u3001\u7ef4\u5ea6\u548c\u4e0a\u4e0b\u6587\u5d4c\u5165\u6392\u5217\u3002", "result": "\u5f53\u56e0\u7d20\u6761\u4ef6\u72ec\u7acb\u65f6\uff0c\u6a21\u578b\u5b66\u4e60\u56e0\u5b50\u5206\u89e3\u8868\u793a\uff1b\u5373\u4f7f\u5728\u566a\u58f0\u6216\u9690\u85cf\u4f9d\u8d56\u7834\u574f\u6761\u4ef6\u72ec\u7acb\u6027\u7684\u65e9\u671f\u8bad\u7ec3\u4e2d\uff0c\u6a21\u578b\u4ecd\u504f\u597d\u56e0\u5b50\u5206\u89e3\uff0c\u4ee5\u4fdd\u771f\u5ea6\u4e3a\u4ee3\u4ef7\u8868\u73b0\u51fa\u56e0\u5b50\u5206\u89e3\u7684\u5f52\u7eb3\u504f\u597d\u3002", "conclusion": "Transformer\u503e\u5411\u4e8e\u5c06\u4e16\u754c\u5206\u89e3\u4e3a\u90e8\u5206\uff0c\u8fd9\u79cd\u56e0\u5b50\u5206\u89e3\u7684\u5f52\u7eb3\u504f\u597d\u89e3\u91ca\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u4f4e\u7ef4\u7ed3\u6784\uff0c\u5373\u4f7f\u5728\u590d\u6742\u6570\u636e\u8bad\u7ec3\u4e2d\u4e5f\u53ef\u80fd\u6301\u7eed\u5b58\u5728\u3002"}}
{"id": "2602.02395", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02395", "abs": "https://arxiv.org/abs/2602.02395", "authors": ["Samuel Nellessen", "Tal Kachman"], "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning", "comment": "Under review. 8 main pages, 2 figures, 2 tables. Appendix included", "summary": "The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"Tag-Along Attacks\"\u5a01\u80c1\u6a21\u578b\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u5bf9\u8bdd\u5229\u7528\u5b89\u5168\u5bf9\u9f50\u64cd\u4f5c\u8005\u7684\u5de5\u5177\u6743\u9650\u8fdb\u884c\u7981\u6b62\u64cd\u4f5c\uff0c\u5e76\u5f00\u53d1Slingshot\u6846\u67b6\u81ea\u52a8\u53d1\u73b0\u653b\u51fb\u5411\u91cf\uff0c\u5728\u6781\u7aef\u56f0\u96be\u4efb\u52a1\u4e0a\u8fbe\u523067%\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u51fa\u73b0\u4e86\u5229\u7528\u5408\u6cd5\u5de5\u5177\u6743\u9650\u7684\u5bf9\u6297\u6027\u5931\u6548\u95ee\u9898\uff0c\u9700\u8981\u5c06\u5de5\u5177\u589e\u5f3a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8bc4\u4f30\u4ece\u4e3b\u89c2NLP\u4efb\u52a1\u8f6c\u53d8\u4e3a\u5ba2\u89c2\u63a7\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51faTag-Along Attacks\u5a01\u80c1\u6a21\u578b\uff0c\u5f00\u53d1Slingshot\u51b7\u542f\u52a8\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u81ea\u52a8\u53d1\u73b0\u653b\u51fb\u5411\u91cf\uff0c\u653b\u51fb\u6a21\u5f0f\u6536\u655b\u4e3a\u7b80\u77ed\u6307\u4ee4\u5f0f\u53e5\u6cd5\u800c\u975e\u591a\u8f6e\u8bf4\u670d\u3002", "result": "\u5728\u6781\u7aef\u56f0\u96be\u4efb\u52a1\u4e0a\uff0cSlingshot\u5bf9Qwen2.5-32B-Instruct-AWQ\u64cd\u4f5c\u8005\u8fbe\u523067.0%\u6210\u529f\u7387\uff08\u57fa\u7ebf1.7%\uff09\uff0c\u9996\u6b21\u6210\u529f\u5c1d\u8bd5\u6b21\u6570\u4ece52.3\u964d\u81f31.3\u3002\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u62ecGemini 2.5 Flash\uff0856.0%\uff09\u548cMeta-SecAlign-8B\uff0839.2%\uff09\u3002", "conclusion": "Tag-Along Attacks\u662f\u53ef\u9a8c\u8bc1\u7684\u4e00\u7ea7\u5a01\u80c1\u6a21\u578b\uff0c\u4ec5\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u5c31\u80fd\u4ece\u73b0\u6210\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u4e2d\u5f15\u53d1\u6709\u6548\u7684\u4ee3\u7406\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u5b89\u5168\u7684\u65b0\u6311\u6218\u3002"}}
{"id": "2602.02400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02400", "abs": "https://arxiv.org/abs/2602.02400", "authors": ["Qizhen Zhang", "Ankush Garg", "Jakob Foerster", "Niladri Chatterji", "Kshitiz Malik", "Mike Lewis"], "title": "An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence", "comment": null, "summary": "Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u7814\u7a76\u4e86\u566a\u58f0\u6570\u636e\u5982\u4f55\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u53d1\u6563\uff0c\u53d1\u73b0\u566a\u58f0\u7c7b\u578b\u3001\u566a\u58f0\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u662f\u5f71\u54cd\u53d1\u6563\u6982\u7387\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u533a\u5206\u566a\u58f0\u5f15\u53d1\u53d1\u6563\u4e0e\u9ad8\u5b66\u4e60\u7387\u5f15\u53d1\u53d1\u6563\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u5927\u91cf\u566a\u58f0\u6570\u636e\uff0c\u4e1a\u754c\u666e\u904d\u63a8\u6d4b\u8fd9\u4e9b\u566a\u58f0\u4f1a\u5bfc\u81f4LLM\u9884\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u751a\u81f3\u635f\u5931\u53d1\u6563\uff0c\u4f46\u8fd9\u4e00\u73b0\u8c61\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1\u566a\u58f0\u6570\u636e\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u5bfc\u81f4\u9884\u8bad\u7ec3\u53d1\u6563\u3002", "method": "\u5728\u539f\u672c\u5e72\u51c0\u7684\u6570\u636e\u96c6\u4e2d\u6ce8\u5165\u53d7\u63a7\u7684\u5408\u6210\u5747\u5300\u968f\u673a\u566a\u58f0\uff0c\u5206\u6790\u4ece480M\u52305.2B\u53c2\u6570\u89c4\u6a21\u7684\u4e0d\u540c\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53d8\u5316\uff0c\u7cfb\u7edf\u7814\u7a76\u566a\u58f0\u7c7b\u578b\u3001\u566a\u58f0\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u566a\u58f0\u6570\u636e\u786e\u5b9e\u4f1a\u5f15\u53d1\u8bad\u7ec3\u635f\u5931\u53d1\u6563\uff0c\u53d1\u6563\u6982\u7387\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u566a\u58f0\u7c7b\u578b\u3001\u566a\u58f0\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u3002\u566a\u58f0\u5f15\u53d1\u7684\u53d1\u6563\u8868\u73b0\u51fa\u4e0e\u9ad8\u5b66\u4e60\u7387\u5f15\u53d1\u53d1\u6563\u4e0d\u540c\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u533a\u5206\u8fd9\u4e24\u79cd\u6545\u969c\u6a21\u5f0f\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u901a\u8fc7\u5927\u89c4\u6a21\u53d7\u63a7\u5b9e\u9a8c\u7cfb\u7edf\u523b\u753b\u4e86\u566a\u58f0\u6570\u636e\u5bf9LLM\u9884\u8bad\u7ec3\u635f\u5931\u53d1\u6563\u7684\u5f71\u54cd\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u9884\u8bad\u7ec3\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2602.02405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02405", "abs": "https://arxiv.org/abs/2602.02405", "authors": ["Ethan Mendes", "Jungsoo Park", "Alan Ritter"], "title": "Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning", "comment": null, "summary": "Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.", "AI": {"tldr": "DAIL\u901a\u8fc7\u4e24\u6b65\u6cd5\u89e3\u51b3\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u4e0eLLM\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u5148\u5c06\u4e13\u5bb6\u65b9\u6848\u8f6c\u6362\u4e3a\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\uff0c\u518d\u7528\u5bf9\u6bd4\u5b66\u4e60\u805a\u7126\u4e13\u5bb6\u6d1e\u89c1\uff0c\u4ec5\u9700\u5c11\u91cf\u4e13\u5bb6\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u578b\u80fd\u91c7\u6837\u5230\u6b63\u786e\u89e3\u6216\u5b58\u5728\u66f4\u5f3a\u7684\u6a21\u578b\uff0c\u4f46\u8bb8\u591a\u96be\u9898\u5bf9\u524d\u6cbf\u6a21\u578b\u4ecd\u4e0d\u53ef\u89e3\u3002\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u867d\u7136\u8d28\u91cf\u9ad8\uff0c\u4f46\u76f4\u63a5\u6a21\u4eff\u4f1a\u5931\u8d25\uff0c\u56e0\u4e3a\u4e13\u5bb6\u65b9\u6848\u662f\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6559\u5b66\u6027\u5185\u5bb9\uff0c\u5b58\u5728\u63a8\u7406\u8df3\u8dc3\uff0c\u4e0e\u6a21\u578b\u5206\u5e03\u4e0d\u5339\u914d\u3002\u540c\u65f6\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\u6602\u8d35\uff0c\u9700\u8981\u6837\u672c\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5bf9\u9f50\u6a21\u4eff\u5b66\u4e60\uff08DAIL\uff09\uff0c\u5305\u542b\u4e24\u6b65\uff1a1\uff09\u5c06\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u8f6c\u6362\u4e3a\u8be6\u7ec6\u3001\u5206\u5e03\u5185\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5f25\u5408\u5206\u5e03\u5dee\u8ddd\uff1b2\uff09\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u805a\u7126\u5b66\u4e60\u4e13\u5bb6\u7684\u6d1e\u89c1\u548c\u65b9\u6cd5\u8bba\u3002", "result": "DAIL\u4ec5\u9700\u5c11\u4e8e1000\u4e2a\u9ad8\u8d28\u91cf\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\uff0c\u5c31\u80fd\u5728Qwen2.5-Instruct\u548cQwen3\u6a21\u578b\u4e0a\u5b9e\u73b010-25%\u7684pass@k\u63d0\u5347\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad82-4\u500d\uff0c\u5e76\u5177\u5907\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DAIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6837\u672c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f25\u5408\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u4e0eLLM\u4e4b\u95f4\u7684\u5206\u5e03\u5dee\u8ddd\uff0c\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.02415", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02415", "abs": "https://arxiv.org/abs/2602.02415", "authors": ["Vivienne Pelletier", "Daniel J. Rivera", "Obinna Nwokonkwo", "Steven A. Wilson", "Christopher L. Muhich"], "title": "Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models", "comment": null, "summary": "Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.", "AI": {"tldr": "ATBagging\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u79cd\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u88c5\u888b\u96c6\u6210\u6a21\u578b\u4f30\u8ba1\u5019\u9009\u6570\u636e\u70b9\u7684\u4fe1\u606f\u91cf\uff0c\u7ed3\u5408\u7279\u5f81\u7a7a\u95f4\u591a\u6837\u6027\u91c7\u6837\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e3b\u52a8\u5b66\u4e60\u65e9\u671f\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u901a\u5e38\u968f\u673a\u9009\u62e9\u521d\u59cb\u79cd\u5b50\u96c6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u65e9\u671f\u6027\u80fd\u4e0d\u4f73\u3002\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5b58\u5728\u76f8\u5173\u6216\u8fd1\u4f3c\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6784\u5efa\u66f4\u597d\u7684\u79cd\u5b50\u96c6\uff0c\u4ece\u800c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u5e76\u63d0\u5347\u4e3b\u52a8\u5b66\u4e60\u6548\u7387\u3002", "method": "ATBagging\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u8d1d\u53f6\u65af\u88c5\u888b\u96c6\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6bd4\u8f83\u888b\u5185\u548c\u888b\u5916\u9884\u6d4b\u5206\u5e03\u6765\u4f30\u8ba1\u5019\u9009\u6570\u636e\u70b9\u7684\u4fe1\u606f\u589e\u76ca\uff1b2) \u5f15\u5165\u7279\u5f81\u7a7a\u95f4\u591a\u6837\u6027\u7ea6\u675f\uff0c\u91c7\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u548c\u786e\u5b9a\u6027\u70b9\u8fc7\u7a0b(DPP)\u8fdb\u884c\u91c7\u6837\uff1b3) \u5c06\u4fe1\u606f\u91cf\u8bc4\u5206\u4e0e\u591a\u6837\u6027\u56e0\u5b50\u7ed3\u5408\uff0c\u907f\u514d\u5197\u4f59\u9009\u62e9\uff1b4) \u8be5\u65b9\u6cd5\u65e2\u7528\u4e8e\u521d\u59cb\u79cd\u5b50\u96c6\u9009\u62e9\uff0c\u4e5f\u7528\u4e8e\u4e3b\u52a8\u5b66\u4e60\u9636\u6bb5\u7684\u65b0\u6570\u636e\u70b9\u6536\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6(QM9\u3001ERA5\u3001Forbes 2000\u3001Beijing PM2.5)\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u76ee\u6807\u8fc1\u79fb\u548c\u7279\u5f81\u504f\u79fb\u573a\u666f\u3002\u5728\u79cd\u5b50\u96c6\u5927\u5c0fnseed=10-100\u8303\u56f4\u5185\uff0cATBagging\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u6216\u6301\u5e73\u5176\u4ed6\u79cd\u5b50\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u6700\u5f3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u66f2\u7ebf\u4e0b\u7684\u9762\u79ef\u3002", "conclusion": "ATBagging\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u56de\u62a5\u7684\u4e3b\u52a8\u5b66\u4e60\u6570\u636e\u6536\u96c6\u542f\u52a8\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u76f8\u5173\u6570\u636e\u96c6\u6784\u5efa\u66f4\u597d\u7684\u521d\u59cb\u79cd\u5b50\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e3b\u52a8\u5b66\u4e60\u65e9\u671f\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.02417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02417", "abs": "https://arxiv.org/abs/2602.02417", "authors": ["Zekun Wang", "Anant Gupta", "Christopher J. MacLellan"], "title": "Trust Region Continual Learning as an Implicit Meta-Learner", "comment": "19 pages, 23 tables", "summary": "Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \\emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \\emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4fe1\u4efb\u533a\u57df\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u751f\u6210\u56de\u653e\u4e0eFisher\u5ea6\u91cf\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\uff0c\u5728\u5c40\u90e8\u8fd1\u4f3c\u4e0b\u83b7\u5f97\u7c7b\u4f3cMAML\u7684\u5355\u6b65\u5185\u5c42\u66f4\u65b0\u7279\u6027\uff0c\u5b9e\u73b0\u5feb\u901f\u91cd\u6536\u655b\u5230\u5148\u524d\u4efb\u52a1\u6700\u4f18\u89e3\u3002", "motivation": "\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6838\u5fc3\u6743\u8861\uff1a\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\uff08\u5982EWC\uff09\u5728\u4efb\u52a1\u6700\u4f18\u89e3\u91cd\u53e0\u5ea6\u4f4e\u65f6\u53ef\u80fd\u8fc7\u5ea6\u7ea6\u675f\u66f4\u65b0\uff0c\u800c\u57fa\u4e8e\u56de\u653e\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u4fdd\u6301\u6027\u80fd\u4f46\u4f1a\u56e0\u4e0d\u5b8c\u7f8e\u56de\u653e\u800c\u6f02\u79fb\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4fe1\u4efb\u533a\u57df\u6301\u7eed\u5b66\u4e60\uff0c\u7ed3\u5408\u751f\u6210\u56de\u653e\u4e0eFisher\u5ea6\u91cf\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\u3002\u5728\u5c40\u90e8\u8fd1\u4f3c\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7c7b\u4f3cMAML\u7684\u5355\u6b65\u5185\u5c42\u66f4\u65b0\uff1a\u56de\u653e\u63d0\u4f9b\u65e7\u4efb\u52a1\u68af\u5ea6\u4fe1\u53f7\uff08\u7c7b\u4f3c\u67e5\u8be2\uff09\uff0cFisher\u52a0\u6743\u60e9\u7f5a\u63d0\u4f9b\u9ad8\u6548\u79bb\u7ebf\u66f2\u7387\u5851\u5f62\uff08\u7c7b\u4f3c\u652f\u6301\uff09\u3002", "result": "\u5728\u4efb\u52a1\u589e\u91cf\u6269\u6563\u56fe\u50cf\u751f\u6210\u548c\u6301\u7eed\u6269\u6563\u7b56\u7565\u63a7\u5236\u4efb\u52a1\u4e0a\uff0c\u4fe1\u4efb\u533a\u57df\u6301\u7eed\u5b66\u4e60\u83b7\u5f97\u6700\u4f73\u6700\u7ec8\u6027\u80fd\u548c\u4fdd\u7559\u7387\uff0c\u6bd4EWC\u3001\u56de\u653e\u548c\u6301\u7eed\u5143\u5b66\u4e60\u57fa\u7ebf\u66f4\u5feb\u6062\u590d\u65e9\u671f\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u6d8c\u73b0\u7684\u5143\u5b66\u4e60\u7279\u6027\uff1a\u6a21\u578b\u6210\u4e3a\u521d\u59cb\u5316\u70b9\uff0c\u5728\u6bcf\u6b21\u4efb\u52a1\u8f6c\u6362\u540e\u80fd\u5feb\u901f\u91cd\u65b0\u6536\u655b\u5230\u5148\u524d\u4efb\u52a1\u6700\u4f18\u89e3\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u4f18\u5316\u53cc\u5c42\u76ee\u6807\u3002\u8fd9\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6df7\u5408\u89c6\u89d2\u3002"}}
{"id": "2602.02422", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02422", "abs": "https://arxiv.org/abs/2602.02422", "authors": ["Sayak Chakrabarti", "Toniann Pitassi", "Josh Alman"], "title": "Poly-attention: a general scheme for higher-order self-attention", "comment": null, "summary": "The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.\n  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.\n  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\"\u591a\u6ce8\u610f\u529b\u673a\u5236\"\u7684\u5e7f\u4e49\u81ea\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u9ad8\u9636\u5f20\u91cf\u8ba1\u7b97\u548c\u4efb\u610ftoken\u5173\u7cfb\u7ed3\u6784\uff0c\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u63d0\u4f9b\u4e86\u65b0\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u5728\u4e8c\u6b21\u65f6\u95f4\u5185\u8ba1\u7b97\u4e14\u80fd\u6267\u884c\u4efb\u610f\u56fa\u5b9a\u6570\u91cf\u51fd\u6570\u7ec4\u5408\u7684\u65b0\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u867d\u7136\u80fd\u6709\u6548\u5efa\u6a21token\u95f4\u7684\u6210\u5bf9\u4ea4\u4e92\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u6d89\u53ca\u4e09\u4e2a\u6216\u66f4\u591atoken\u76f8\u5173\u6027\u7684\u57fa\u672c\u4efb\u52a1\uff0c\u4e5f\u65e0\u6cd5\u6267\u884c\u9700\u8981\u5f15\u7528\u591a\u4e2a\u8f93\u5165token\u7684\u7ec4\u5408\u4efb\u52a1\u3002\u73b0\u6709\u9ad8\u9636\u6ce8\u610f\u529b\u66ff\u4ee3\u65b9\u6848\u867d\u7136\u80fd\u5904\u7406\u8fd9\u4e9b\u591a\u5143\u7d20\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8d85\u4e8c\u6b21\u8fd0\u884c\u65f6\u95f4\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u7c7b\u5e7f\u4e49\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u2014\u2014\u591a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u5305\u542b\u4efb\u610f\u9ad8\u9636\u5f20\u91cf\u8ba1\u7b97\u548c\u4efb\u610ftoken\u5173\u7cfb\u7ed3\u6784\u3002\u7cfb\u7edf\u7814\u7a76\u4e86\u8fd9\u4e9b\u673a\u5236\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5305\u62ec\u7ed9\u51fa\u65b0\u7b97\u6cd5\u548c\u5339\u914d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0b\u754c\uff0c\u7cbe\u786e\u548c\u8fd1\u4f3c\u8ba1\u7b97\u6ce8\u610f\u529b\u77e9\u9635\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u786e\u5b9a\u5b83\u4eec\u80fd\u6267\u884c\u7684\u591a\u5143\u7d20\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5728\u4e8c\u6b21\u65f6\u95f4\u5185\u7cbe\u786e\u8ba1\u7b97\u7684\u65b0\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u6267\u884c\u4efb\u610f\u56fa\u5b9a\u6570\u91cf\u51fd\u6570\u7684\u7ec4\u5408\u3002\u5148\u524d\u673a\u5236\u5373\u4f7f\u53ea\u7ec4\u5408\u4e24\u4e2a\u51fd\u6570\u4e5f\u9700\u8981\u8d85\u4e8c\u6b21\u65f6\u95f4\uff0c\u800c\u65b0\u7684\u4e0b\u754c\u8868\u660e\u66f4\u5feb\u7684\u7b97\u6cd5\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8fd9\u4e9b\u673a\u5236\u5728\u4e0d\u540c\u9700\u6c42\u4e4b\u95f4\u7684\u6709\u8da3\u6743\u8861\uff0c\u5305\u62ec\u8868\u8fbe\u80fd\u529b\u4e0e\u6a21\u578b\u7cfb\u6570\u5927\u5c0f\u4e4b\u95f4\u7684\u7d27\u5bc6\u5173\u7cfb\u3002", "conclusion": "\u591a\u6ce8\u610f\u529b\u673a\u5236\u4e3aTransformer\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u65b0\u63d0\u51fa\u7684\u673a\u5236\u5728\u4fdd\u6301\u4e8c\u6b21\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5904\u7406\u5148\u524d\u9700\u8981\u8d85\u4e8c\u6b21\u65f6\u95f4\u7684\u51fd\u6570\u7ec4\u5408\u4efb\u52a1\uff0c\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.02425", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.02425", "abs": "https://arxiv.org/abs/2602.02425", "authors": ["Amaru Caceres Arroyo", "Lea Bogensperger", "Ahmed Allam", "Michael Krauthammer", "Konrad Schindler", "Dominik Narnhofer"], "title": "Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization", "comment": null, "summary": "Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.", "AI": {"tldr": "CHASE\uff1a\u901a\u8fc7\u538b\u7f29\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5229\u7528\u6761\u4ef6\u6d41\u5339\u914d\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u65e0\u9700\u9884\u6d4b\u5668\u6307\u5bfc\u5373\u53ef\u76f4\u63a5\u751f\u6210\u9ad8\u9002\u5e94\u6027\u86cb\u767d\u8d28\u53d8\u4f53", "motivation": "\u86cb\u767d\u8d28\u9002\u5e94\u6027\u4f18\u5316\u9762\u4e34\u5de8\u5927\u7ec4\u5408\u7a7a\u95f4\u6311\u6218\uff0c\u9ad8\u9002\u5e94\u6027\u53d8\u4f53\u6781\u5176\u7a00\u758f\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6027\u80fd\u4e0d\u8db3\uff0c\u8981\u4e48\u9700\u8981\u8ba1\u7b97\u6602\u8d35\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u91c7\u6837", "method": "1. \u91cd\u65b0\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u77e5\u8bc6\uff0c\u5c06\u5176\u5d4c\u5165\u538b\u7f29\u5230\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff1b2. \u8bad\u7ec3\u5e26\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7684\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff1b3. \u5728ODE\u91c7\u6837\u6b65\u9aa4\u4e2d\u65e0\u9700\u9884\u6d4b\u5668\u6307\u5bfc\u76f4\u63a5\u751f\u6210\u9ad8\u9002\u5e94\u6027\u53d8\u4f53", "result": "\u5728AAV\u548cGFP\u86cb\u767d\u8d28\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e2d\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u5f15\u5bfc\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd", "conclusion": "CHASE\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u77e5\u8bc6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u751f\u6210\u9ad8\u9002\u5e94\u6027\u86cb\u767d\u8d28\u53d8\u4f53\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.02427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02427", "abs": "https://arxiv.org/abs/2602.02427", "authors": ["Qihao Wen", "Jiahao Wang", "Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "title": "Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning", "comment": null, "summary": "Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6270\u52a8\u654f\u611f\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u4e2d\u95f4\u6b65\u9aa4\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u6709\u6548\u4e14\u9ad8\u6548\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u5404\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u4ecd\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u6216\u8bef\u5bfc\u6027\u8f93\u51fa\u3002\u5bf9\u4e8e\u63a8\u7406\u4efb\u52a1\uff0c\u4e0d\u4ec5\u9700\u8981\u91cf\u5316\u6700\u7ec8\u7b54\u6848\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u9700\u8981\u91cf\u5316\u4e2d\u95f4\u6b65\u9aa4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u4fbf\u8fdb\u884c\u66f4\u7cbe\u7ec6\u548c\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684token\u5bf9\u524d\u5e8ftoken\u5d4c\u5165\u7684\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4f7f\u7528\u6270\u52a8\u654f\u611f\u6027\u8bc4\u5206\u6765\u8bc6\u522b\u4e0d\u786e\u5b9a\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u901a\u8fc7\u6d4b\u91cftoken\u5bf9\u524d\u5e8f\u5d4c\u5165\u6270\u52a8\u7684\u654f\u611f\u5ea6\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6270\u52a8\u7684\u6307\u6807\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982token\u751f\u6210\u6982\u7387\u548ctoken\u71b5\uff09\u3002\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f9d\u8d56\u591a\u6b21\u91c7\u6837\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7b80\u6d01\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u6270\u52a8\u654f\u611f\u6027\u8bc4\u5206\u662f\u4e00\u79cd\u6709\u6548\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e0d\u786e\u5b9a\u4e2d\u95f4\u6b65\u9aa4\u7684\u65b9\u6cd5\uff0c\u4e3a\u66f4\u7cbe\u7ec6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u5e72\u9884\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2602.02432", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02432", "abs": "https://arxiv.org/abs/2602.02432", "authors": ["Jack M. Buckingham", "Ivo Couckuyt", "Juergen Branke"], "title": "Maximizing Reliability with Bayesian Optimization", "comment": "25 pages, 9 figures", "summary": "Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eThompson\u91c7\u6837\u548c\u77e5\u8bc6\u68af\u5ea6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6781\u5c0f\u5931\u6548\u6982\u7387\uff0810\u207b\u2076-10\u207b\u2078\uff09\u7684\u53ef\u9760\u6027\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u6280\u672f", "motivation": "\u5236\u9020\u4e1a\u4e2d\u9700\u8981\u4f18\u5316\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u5373\u6700\u5c0f\u5316\u968f\u673a\u6270\u52a8\u4e0b\u7684\u5931\u6548\u6982\u7387\uff0c\u4f46\u5931\u6548\u6982\u7387\u6781\u4f4e\uff0810\u207b\u2076-10\u207b\u2078\uff09\uff0c\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u6781\u7aef\u7a00\u6709\u4e8b\u4ef6", "method": "\u63d0\u51fa\u4e24\u79cd\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8eThompson\u91c7\u6837\u7684\u65b9\u6cd5\uff1b2\uff09\u57fa\u4e8e\u77e5\u8bc6\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u540e\u8005\u8fd1\u4f3c\u6700\u5c0f\u5316\u5931\u6548\u6982\u7387\u5bf9\u6570\u7684\u4e00\u6b65\u8d1d\u53f6\u65af\u6700\u4f18\u7b56\u7565\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u6765\u5904\u7406\u6781\u5c0f\u5931\u6548\u6982\u7387", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6781\u7aef\u548c\u975e\u6781\u7aef\u5931\u6548\u6982\u7387\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eThompson\u91c7\u6837\u548c\u77e5\u8bc6\u68af\u5ea6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6781\u5c0f\u5931\u6548\u6982\u7387\u7684\u53ef\u9760\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6781\u7aef\u548c\u975e\u6781\u7aef\u60c5\u51b5\u4e0b\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2602.02443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02443", "abs": "https://arxiv.org/abs/2602.02443", "authors": ["Yuanteng Chen", "Peisong Wang", "Nanxin Zeng", "Yuantian Shao", "Gang Li", "Jing Liu", "Jian Cheng"], "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE", "comment": "24 pages, 13 figures", "summary": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.", "AI": {"tldr": "Expert-Sample\u65b9\u6cd5\u5229\u7528\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u9009\u62e9\u7684\u540c\u65f6\u5411\u4f4e\u7f6e\u4fe1\u5ea6\u5c3e\u90e8\u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027\uff0c\u4ece\u800c\u5728\u4e0d\u7834\u574f\u8f93\u51fa\u7a33\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u591a\u6837\u6027\u751f\u6210\u6548\u679c\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u7f29\u653e\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5019\u9009\u89e3\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u57fa\u4e8etoken\u7684\u91c7\u6837\u9700\u8981\u6e29\u5ea6\u8c03\u4f18\u6765\u5e73\u8861\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\u3002\u7ec6\u7c92\u5ea6MoE\u5177\u6709\u4e30\u5bcc\u7684\u8def\u7531\u7a7a\u95f4\uff0c\u5176\u8def\u7531\u6a21\u5f0f\uff08\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u5934\u90e8+\u4f4e\u7f6e\u4fe1\u5ea6\u4e0d\u786e\u5b9a\u5c3e\u90e8\uff09\u4e3a\u591a\u6837\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51faExpert-Sample\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff1a\u5206\u6790\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u6a21\u5f0f\uff0c\u53d1\u73b0\u8def\u7531\u5668\u5206\u6570\u5448\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u5934\u90e8\u548c\u4f4e\u7f6e\u4fe1\u5ea6\u4e0d\u786e\u5b9a\u5c3e\u90e8\u7684\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u9009\u62e9\uff0c\u540c\u65f6\u5411\u4e0d\u786e\u5b9a\u5c3e\u90e8\u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u751f\u6210\u800c\u4e0d\u7834\u574f\u8f93\u51fa\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u6570\u5b66\u3001\u77e5\u8bc6\u63a8\u7406\u548c\u4ee3\u7801\u4efb\u52a1\uff0cExpert-Sample\u4e00\u81f4\u63d0\u5347pass@n\u548c\u57fa\u4e8e\u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u3002\u5728Qwen3-30B-A3B-Instruct\u4e0a\uff0cGPQA-Diamond\u4efb\u52a1\u7684pass@32\u4ece85.4%\u63d0\u5347\u523091.9%\uff0cBest-of-N\u9a8c\u8bc1\u51c6\u786e\u7387\u4ece59.1%\u63d0\u5347\u523062.6%\u3002", "conclusion": "\u7ec6\u7c92\u5ea6MoE\u7684\u8def\u7531\u6a21\u5f0f\u4e3a\u591a\u6837\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0cExpert-Sample\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8def\u7531\u7f6e\u4fe1\u5ea6\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\uff0c\u4e3aMoE\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.02445", "categories": ["cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.02445", "abs": "https://arxiv.org/abs/2602.02445", "authors": ["Seo Taek Kong", "R. Srikant"], "title": "Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation", "comment": null, "summary": "This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.\n  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $\u03b3_n^{1/6}$, where $\u03b3_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.", "AI": {"tldr": "\u672c\u6587\u63a8\u5bfc\u4e86\u975e\u7ebf\u6027\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u5728Wasserstein-p\u8ddd\u79bb\u4e0b\u7684\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u901a\u8fc7\u8026\u5408\u65b9\u6cd5\u5206\u6790\u6700\u540e\u8fed\u4ee3\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u5e76\u5904\u7406Polyak-Ruppert\u5e73\u5747\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u7684\u5206\u6790\u591a\u4e3a\u6e10\u8fd1\u7406\u8bba\u6216\u57fa\u4e8e\u77e9\u754c\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u6700\u540e\u8fed\u4ee3\u7684\u663e\u5f0f\u6709\u9650\u6837\u672c\u5206\u5e03\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u975e\u7ebf\u6027\u968f\u673a\u903c\u8fd1\u7b97\u6cd5\u63d0\u4f9b\u975e\u6e10\u8fd1\u7684\u5206\u5e03\u6536\u655b\u901f\u7387\u5206\u6790\u3002", "method": "\u91c7\u7528\u8026\u5408\u65b9\u6cd5\u5c06\u79bb\u6563\u65f6\u95f4\u8fc7\u7a0b\u4e0e\u6781\u9650Ornstein-Uhlenbeck\u8fc7\u7a0b\u6bd4\u8f83\uff0c\u83b7\u5f97\u6700\u540e\u8fed\u4ee3\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u3002\u540c\u65f6\u901a\u8fc7\u76f4\u63a5\u5206\u6790\u5904\u7406Polyak-Ruppert\u5e73\u5747\u7684\u6536\u655b\u901f\u7387\u3002\u5206\u6790\u9002\u7528\u4e8e\u4e00\u822c\u566a\u58f0\u6761\u4ef6\uff0c\u5305\u62ec\u9785\u5dee\u548c\u904d\u5386\u9a6c\u5c14\u53ef\u592b\u94fe\u51fd\u6570\u3002", "result": "\u5728\u9a71\u52a8\u566a\u58f0\u6ee1\u8db3\u975e\u6e10\u8fd1\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u7684\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u5f52\u4e00\u5316\u6700\u540e\u8fed\u4ee3\u4ee5\u03b3_n^{1/6}\u901f\u7387\u5728p-Wasserstein\u8ddd\u79bb\u4e0b\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\uff0cPolyak-Ruppert\u5e73\u5747\u4ee5n^{-1/6}\u901f\u7387\u6536\u655b\u3002\u8fd9\u4e9b\u5206\u5e03\u4fdd\u8bc1\u63a8\u5bfc\u51fa\u4f18\u4e8e\u77e9\u754c\u548c\u9a6c\u5c14\u53ef\u592b\u4e0d\u7b49\u5f0f\u7684\u9ad8\u6982\u7387\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u672c\u6587\u4e3a\u975e\u6e10\u8fd1\u968f\u673a\u903c\u8fd1\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7Wasserstein\u8ddd\u79bb\u5206\u6790\u8fde\u63a5\u4e86\u6709\u9650\u6837\u672c\u5206\u6790\u548c\u6e10\u8fd1\u7406\u8bba\u3002\u5728\u7ebf\u6027\u968f\u673a\u903c\u8fd1\u548c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4e24\u4e2a\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\uff0c\u91cf\u5316\u4e86\u4ece\u91cd\u5c3e\u5230\u9ad8\u65af\u884c\u4e3a\u7684\u8f6c\u53d8\u3002"}}
{"id": "2602.02451", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02451", "abs": "https://arxiv.org/abs/2602.02451", "authors": ["Patrick Cooper", "Alvaro Velasquez"], "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization", "comment": "9 pages, 5 figures", "summary": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.", "AI": {"tldr": "ACE\u4f7f\u7528\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u5e8f\u5217\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u7b56\u7565\uff0c\u901a\u8fc7\u5e72\u9884\u6bd4\u8f83\u800c\u975e\u5956\u52b1\u5e45\u5ea6\u6765\u7a33\u5b9a\u5b66\u4e60\uff0c\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\uff08\u968f\u673a\u91c7\u6837\u3001\u8d2a\u5a6a\u4fe1\u606f\u6700\u5927\u5316\u3001\u8f6e\u8be2\u8986\u76d6\uff09\u65e0\u6cd5\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5c06\u6bcf\u4e2a\u51b3\u7b56\u89c6\u4e3a\u5b64\u7acb\u95ee\u9898\uff0c\u4e0d\u80fd\u5229\u7528\u5386\u53f2\u4fe1\u606f\u4f18\u5316\u540e\u7eed\u5e72\u9884\u9009\u62e9\u3002", "method": "\u63d0\u51faActive Causal Experimentalist (ACE)\uff0c\u5c06\u5b9e\u9a8c\u8bbe\u8ba1\u5efa\u6a21\u4e3a\u5e8f\u5217\u7b56\u7565\u5b66\u4e60\u95ee\u9898\u3002\u5173\u952e\u6d1e\u5bdf\u662f\uff1a\u867d\u7136\u7edd\u5bf9\u4fe1\u606f\u589e\u76ca\u968f\u77e5\u8bc6\u79ef\u7d2f\u800c\u51cf\u5c11\uff08\u5bfc\u81f4\u57fa\u4e8e\u4ef7\u503c\u7684RL\u4e0d\u7a33\u5b9a\uff09\uff0c\u4f46\u5019\u9009\u5e72\u9884\u4e4b\u95f4\u7684\u76f8\u5bf9\u6bd4\u8f83\u59cb\u7ec8\u6709\u610f\u4e49\u3002ACE\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u4ece\u6210\u5bf9\u5e72\u9884\u6bd4\u8f83\u4e2d\u5b66\u4e60\uff0c\u800c\u975e\u4f9d\u8d56\u975e\u5e73\u7a33\u7684\u5956\u52b1\u5e45\u5ea6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u3001\u7269\u7406\u6a21\u62df\u548c\u7ecf\u6d4e\u6570\u636e\u4e0a\uff0cACE\u5728\u76f8\u540c\u5e72\u9884\u9884\u7b97\u4e0b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534770-71%\uff08p < 0.001\uff0cCohen's d ~ 2\uff09\u3002\u5b66\u4e60\u5230\u7684\u7b56\u7565\u81ea\u4e3b\u53d1\u73b0\u4e86\u78b0\u649e\u673a\u5236\u9700\u8981\u96c6\u4e2d\u5e72\u9884\u7236\u53d8\u91cf\u7684\u7406\u8bba\u7b56\u7565\u3002", "conclusion": "\u57fa\u4e8e\u504f\u597d\u7684\u5b66\u4e60\u80fd\u591f\u6062\u590d\u539f\u5219\u6027\u5b9e\u9a8c\u7b56\u7565\uff0c\u7528\u5b66\u4e60\u5230\u7684\u9886\u57df\u9002\u5e94\u8865\u5145\u7406\u8bba\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u81ea\u9002\u5e94\u5b9e\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.02458", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.02458", "abs": "https://arxiv.org/abs/2602.02458", "authors": ["Mingwei Hong", "Zheng Lin", "Zehang Lin", "Lin Li", "Miao Yang", "Xia Du", "Zihan Fang", "Zhaolu Kang", "Dianxin Luan", "Shunzhi Zhu"], "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning", "comment": "6 pages, 4 figures", "summary": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.", "AI": {"tldr": "\u63d0\u51faRL-CRP\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u4f18\u5316\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u4f20\u7edf\u5355\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u9ad8\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u800c\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u8986\u76d6\u91cd\u53e0\u548c\u9009\u62e9\u4e0d\u534f\u8c03\u4f1a\u5bfc\u81f4\u8d44\u6e90\u7ade\u4e89\u3001\u5e26\u5bbd\u51b2\u7a81\u548c\u8bad\u7ec3\u5931\u8d25", "method": "\u63d0\u51faRL-CRP\u6846\u67b6\uff1a1) \u4f7f\u7528\u5206\u7c7b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u57fa\u4e8e\u7a00\u758f\u5386\u53f2\u9009\u62e9\u5e8f\u5217\u9884\u6d4b\u5ba2\u6237\u7aef\u9009\u62e9\u51b2\u7a81\u98ce\u9669\uff1b2) \u7ed3\u5408\u516c\u5e73\u611f\u77e5\u5956\u52b1\u673a\u5236\u4fc3\u8fdb\u957f\u671f\u5ba2\u6237\u7aef\u53c2\u4e0e\uff1b3) \u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u5ef6\u8fdf\u548c\u8d44\u6e90\u7ade\u4e89", "result": "\u5b9e\u9a8c\u8868\u660eRL-CRP\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5305\u62ec\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6210\u672c", "conclusion": "RL-CRP\u6846\u67b6\u901a\u8fc7\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u548c\u516c\u5e73\u611f\u77e5\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6548\u7387\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.02482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02482", "abs": "https://arxiv.org/abs/2602.02482", "authors": ["Yuda Song", "Lili Chen", "Fahim Tajwar", "Remi Munos", "Deepak Pathak", "J. Andrew Bagnell", "Aarti Singh", "Andrea Zanette"], "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback", "comment": "43 pages, 6 figures", "summary": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLTF\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4ecb\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u5b8c\u6574\u6f14\u793a\u4e4b\u95f4\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff08\u81ea\u6211\u84b8\u998f\u548c\u53cd\u9988\u5efa\u6a21\uff09\u8ba9\u6a21\u578b\u5185\u5316\u53cd\u9988\u4ee5\u63d0\u5347\u5355\u8f6e\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRL\u7528\u4e8eLLM\u540e\u8bad\u7ec3\u5b58\u5728\u76d1\u7763\u4fe1\u53f7\u4e0d\u8db3\u7684\u95ee\u9898\uff1a\u57fa\u4e8e\u504f\u597d\u7684RL\u53ea\u63d0\u4f9b\u6bcf\u4e2arollout\u7684\u4e8c\u5143\u5956\u52b1\uff0c\u4fe1\u606f\u8fc7\u4e8e\u7a00\u758f\uff1b\u800c\u84b8\u998f\u9700\u8981\u5b8c\u6574\u6f14\u793a\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4e2d\u95f4\u4fe1\u53f7\uff0c\u6bd4\u6807\u91cf\u5956\u52b1\u66f4\u4e30\u5bcc\uff0c\u6bd4\u5b8c\u6574\u6f14\u793a\u66f4\u4fbf\u5b9c\uff0c\u662f\u81ea\u7136\u7684\u4eba\u7c7b\u4ea4\u4e92\u65b9\u5f0f\u4e14\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\u5df2\u5927\u91cf\u5b58\u5728\u3002", "method": "\u63d0\u51faRLTF\u6846\u67b6\uff1a\u591a\u8f6eRL\u8bbe\u7f6e\uff0c\u8bad\u7ec3\u65f6\u6709\u6587\u672c\u53cd\u9988\u4f46\u63a8\u7406\u65f6\u6ca1\u6709\u3002\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) RLTF-SD\uff08\u81ea\u6211\u84b8\u998f\uff09\uff1a\u8bad\u7ec3\u5355\u8f6e\u7b56\u7565\u4ee5\u5339\u914d\u5176\u81ea\u8eab\u53cd\u9988\u6761\u4ef6\u4e0b\u7684\u7b2c\u4e8c\u8f6e\u751f\u6210\uff1b2) RLTF-FM\uff08\u53cd\u9988\u5efa\u6a21\uff09\uff1a\u5c06\u9884\u6d4b\u53cd\u9988\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u65e8\u5728\u8ba9\u6a21\u578b\u5185\u5316\u6587\u672c\u53cd\u9988\u4ee5\u63d0\u5347\u6d4b\u8bd5\u65f6\u7684\u5355\u8f6e\u6027\u80fd\u3002", "result": "\u5728\u63a8\u7406\u8c1c\u9898\u3001\u7ade\u8d5b\u6570\u5b66\u548c\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\u8fdb\u884cRL\u7684\u6f5c\u529b\u3002", "conclusion": "\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4ecb\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u5b8c\u6574\u6f14\u793a\u4e4b\u95f4\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7RLTF\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u6027\u80fd\u3002\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\uff08\u81ea\u6211\u84b8\u998f\u548c\u53cd\u9988\u5efa\u6a21\uff09\u90fd\u80fd\u8ba9\u6a21\u578b\u5185\u5316\u53cd\u9988\u4fe1\u606f\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5229\u7528\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u5927\u89c4\u6a21RL\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.02494", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2602.02494", "abs": "https://arxiv.org/abs/2602.02494", "authors": ["Dulhan Jayalath", "Oiwi Parker Jones"], "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training", "comment": "19 pages, 8 figures, 5 tables", "summary": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .", "AI": {"tldr": "MEG-XL\u901a\u8fc72.5\u5206\u949f\u7684\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u8111\u78c1\u56fe\u6570\u636e\u7684\u5355\u8bcd\u89e3\u7801\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u6570\u636e\u5373\u53ef\u8fbe\u5230\u76d1\u7763\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8111\u673a\u63a5\u53e3\u9884\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u53ea\u4f7f\u7528\u51e0\u79d2\u7684\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6240\u9700\u7684\u957f\u65f6\u95f4\u795e\u7ecf\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u762b\u75ea\u60a3\u8005\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faMEG-XL\u6a21\u578b\uff0c\u4f7f\u75282.5\u5206\u949f\u7684\u957f\u4e0a\u4e0b\u6587\uff08\u76f8\u5f53\u4e8e191k tokens\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u957f5-300\u500d\uff0c\u7136\u540e\u5fae\u8c03\u7528\u4e8e\u8111\u6570\u636e\u7684\u5355\u8bcd\u89e3\u7801\u4efb\u52a1\u3002", "result": "MEG-XL\u4ec5\u9700\u5c11\u91cf\u6570\u636e\uff08\u59821\u5c0f\u65f6vs50\u5c0f\u65f6\uff09\u5373\u53ef\u5339\u914d\u76d1\u7763\u5b66\u4e60\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u8111\u57fa\u7840\u6a21\u578b\uff0c\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u5b66\u5230\u7684\u8868\u5f81\u5728\u5355\u8bcd\u89e3\u7801\u4efb\u52a1\u4e0a\u8fc1\u79fb\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u80fd\u591f\u6709\u6548\u5229\u7528\u5176\u4ed6\u65b9\u6cd5\u4e22\u5f03\u7684\u6269\u5c55\u795e\u7ecf\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e3a\u4e34\u5e8a\u8111\u673a\u63a5\u53e3\u7684\u6570\u636e\u9ad8\u6548\u6cdb\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u3002"}}
