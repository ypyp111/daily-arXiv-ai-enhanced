<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.CL](#cs.CL) [Total: 60]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 12]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.AI](#cs.AI) [Total: 48]
- [math.NA](#math.NA) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出PolypSeg-GradCAM框架，结合U-Net和Grad-CAM实现可解释的息肉分割，在Kvasir-SEG数据集上达到0.9257的IoU和0.96以上的Dice系数，增强临床可信度


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要癌症死因，胃肠道息肉是关键前兆。现有深度学习方法分割准确但缺乏可解释性，阻碍临床应用

Method: 集成U-Net架构和梯度加权类激活映射(Grad-CAM)，在Kvasir-SEG数据集(1000张内镜图像)上训练评估

Result: 测试集平均IoU达0.9257，训练和验证集Dice系数均>0.96，Grad-CAM可视化确认预测基于临床相关区域

Conclusion: PolypSeg-GradCAM将高分割精度与可解释性结合，向可靠、可信的AI辅助结肠镜检查迈进一步

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一个基于深度学习的远程眼科应用，使用视网膜图像自动检测糖尿病视网膜病变，准确率达到85.4%，适用于临床和远程医疗环境。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致成人视力丧失的主要原因，特别是在医疗资源不足地区。开发自动化筛查系统可以扩大筛查覆盖面，改善医疗服务可及性。

Method: 使用多种卷积神经网络（ResNet-18、EfficientNet-B0、SqueezeNet）开发和评估系统，寻找准确性和计算效率的最佳平衡。系统集成云端可扩展性、安全数据管理和多用户框架。

Result: 最终模型对疾病严重程度分类的准确率达到85.4%，支持实时筛查，能够促进早期诊断、改善医患互动并降低医疗成本。

Conclusion: 这项研究展示了AI驱动的远程医疗解决方案在扩大糖尿病视网膜病变筛查覆盖面方面的潜力，特别是在偏远和资源受限环境中。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了一种名为SIM的新型正则化框架，通过逆向映射机制减少前向传播中的信息损失，并开发了高效的ρSIM变体，在多个任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法通常依赖启发式规则，在不同设置下效果不稳定。需要一种更可靠、数据内在的正则化方法来增强表示学习。

Method: 提出Self Identity Mapping (SIM)框架，通过从变换后的输出重构输入来减少信息损失。开发ρSIM变体，采用补丁级特征采样和基于投影的方法重构潜在特征以降低计算复杂度。

Result: 在图像分类、少样本提示学习和领域泛化等任务中，ρSIM相比基线方法取得一致改进，且与现有正则化方法正交，能进一步提升其效果。在密集到密集任务和非视觉领域也表现良好。

Conclusion: SIM是一种模型无关、任务无关的正则化器，可作为即插即用模块，有效增强表示学习能力，在各种网络架构和任务中都具有广泛应用前景。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反演攻击方法，能够在单轮平均梯度SAG机制下实现高效的多图像重建，无需标签信息且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 在单轮平均梯度SAG机制下，每个样本的梯度线索被纠缠在批量平均梯度中，使得梯度反演变得极具挑战性。现有方法在大批量场景下重建效果不佳。

Method: MAGIA通过动量自适应校正，结合封闭形式组合重缩放和动量混合损失，探测随机数据子集中的潜在图像信号，实现标签无关的梯度反演。

Result: 实验表明MAGIA显著优于先进方法，在大批量场景下实现高保真多图像重建，且计算开销与标准求解器相当。

Conclusion: MAGIA为梯度反演攻击提供了有效的解决方案，在保护隐私的联邦学习等场景中具有重要安全意义。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专门为阿拉伯语文档OCR设计的视觉语言模型，通过在大规模合成和真实文档数据集上微调预训练的多模态大语言模型，显著提升了阿拉伯语OCR性能，在Misraj-DocOCR基准测试中达到了0.25的WER，创造了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文档OCR由于草书字体、多样化字体、变音符号和从右到左的书写方向而具有挑战性。现有的多模态大语言模型在高资源语言上表现良好，但在阿拉伯语上的性能有限。

Method: 采用解码器专用的微调策略，在结合合成和真实世界文档的大规模数据集上对预训练的MLLM进行微调，同时保留通用的视觉特征。还提出了Misraj-DocOCR基准测试用于严格评估阿拉伯语OCR系统。

Result: Baseer显著优于现有的开源和商业解决方案，在阿拉伯语文档OCR领域达到了0.25的WER，建立了新的最先进水平。

Conclusion: 研究结果表明，对通用MLLM进行领域特定适配具有显著优势，为像阿拉伯语这样形态丰富的语言的高精度OCR建立了强有力的基线。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习框架，将稀疏的InSAR时间序列数据转换为密集的时空张量，首次实现对地面变形的直接时空预测。


<details>
  <summary>Details</summary>
Motivation: 从稀疏的InSAR时间序列数据预测未来地面变形具有挑战性，现有方法难以有效处理时空复杂性。

Method: 设计了混合CNN-LSTM模型，同时学习空间模式和时间依赖性，将稀疏点测量转换为密集时空张量。

Result: 与LightGBM和LASSO回归相比，所提架构提供了更准确和空间一致的预测，建立了新的性能基准。

Conclusion: 研究证实了时空深度学习在高分辨率变形预测中的有效性和潜力，基线模型往往依赖简单的持续性模式，突显了集成时空方法的必要性。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成大规模数据集的新方法，用于探究AI模型学习到的概念，重点关注物体识别、位置关系和属性识别等基础概念。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在复杂任务中表现良好，但对其基础概念理解能力的系统性评估不足，需要一种能够生成多样化问题数据集的方法来验证模型对基本元素的理解。

Method: 通过生成包含大量关于单个概念问题和广泛语言变化的数据集，系统地测试模型在物体识别、绝对和相对位置、属性识别等方面的能力。

Result: 实验发现当代模型在物体识别和枚举方面表现良好，但在理解位置信息和处理有额外约束的问题时存在困难。MobileVLM-V2模型出现显著答案分歧，其他模型表现出肯定答案偏见，在几何形状和位置信息问题上表现不佳。

Conclusion: Scrapbook框架为生成多样化数据集提供了有价值的工具，可用于系统评估和提升AI模型性能，揭示了模型在基础概念理解方面需要改进的领域。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 该论文通过实证分析量化了视觉-语言-视觉管道中的信息损失，发现99.3%的样本存在显著感知退化，91.5%存在显著结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流程中的集成日益增加，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要，但目前通过文本中介传递视觉内容时的退化程度尚未得到充分量化。

Method: 生成了150对图像通过描述-生成管道，并应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存情况。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的局限性。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 开发AI驱动的工作流程，利用高分辨率卫星影像自动推断屋顶属性，以解决小岛屿发展中国家建筑结构数据缺失问题，支持灾害风险评估和城市韧性规划。


<details>
  <summary>Details</summary>
Motivation: 小岛屿发展中国家缺乏详细的建筑结构信息，难以进行灾害风险评估和城市韧性规划，特别是在气候脆弱地区如加勒比海区域。

Method: 比较地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效果，并评估引入邻国训练数据对模型性能的提升。

Result: 最佳模型在屋顶坡度和屋顶材料分类上分别达到0.88和0.83的F1分数。

Conclusion: 结合本地能力建设，该工作为小岛屿发展中国家提供了利用AI和地球观测数据实现更高效、基于证据的城市治理的新能力。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 本文提出VLA-LPAF轻量级模块，通过仅使用2D数据增强VLA模型的视角适应性，有效解决多视角视觉特征差异问题，在多个基准测试中显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理来自不同视角（全局和局部摄像头）的视觉观察时，由于视角异质性导致视觉特征差异显著，限制了模型的通用性。

Method: 提出VLA-LPAF模块，使用单视角图像进行微调，在潜在空间中融合多视角观察，有效桥接视角不一致带来的差距。基于RoboFlamingo模型实例化RoboFlamingo-LPAF框架。

Result: 实验显示RoboFlamingo-LPAF在CALVIN基准上平均提升约8%任务成功率，在LIBERO上提升15%，在定制仿真基准上提升30%，并在真实世界任务中展示了视角自适应特性。

Conclusion: VLA-LPAF能够有效解决VLA模型在多视角环境中的适应性问题，显著提升模型性能，具有实际应用价值。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件相机的立体深度估计方法URNet，通过局部-全局细化模块和KL散度不确定性建模，在DSEC数据集上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟的优势，但现有的基于事件相机的立体深度估计方法在细节捕捉和预测可靠性方面仍有改进空间。

Method: 提出了URNet不确定性感知细化网络，包含局部-全局细化模块来捕捉细粒度局部细节和长距离全局上下文，并引入基于KL散度的不确定性建模方法来增强预测可靠性。

Result: 在DSEC数据集上的大量实验表明，URNet在定性和定量评估中都一致优于现有的最先进方法。

Conclusion: URNet通过有效的局部-全局特征提取和不确定性建模，显著提升了基于事件相机的立体深度估计性能。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一个用于从多梯度DWI和形态MRI数据中识别周围神经系统的新型混合AI框架，通过模糊空间关系编码解剖知识，无需手动选择ROI，在子宫内膜异位症患者的腰骶神经丛识别中表现出显著改进。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经的成像仍然是一个挑战。

Method: Visionerves框架包含两个阶段：(A)使用深度学习模型自动分割解剖结构，(B)通过符号空间推理进行纤维束追踪和神经识别。该方法编码解剖知识通过模糊空间关系，无需手动选择ROI。

Result: 在10名子宫内膜异位症患者的腰骶神经丛应用中，Visionerves相比标准纤维束追踪方法有显著改进，Dice分数提升高达25%，空间误差减少到小于5mm。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，为非侵入性诊断子宫内膜异位症相关神经病变以及其他涉及神经的疾病铺平了道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶员行为数据集，结合智能手机惯性传感器、GPS数据和同步的道路视频，用于检测不安全驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要来自发达国家，缺乏新兴经济体驾驶行为的多样性表示，且驾驶员面部记录侵犯隐私。巴基斯坦等国家的异质道路条件、混合交通流和可变驾驶纪律使得可靠检测不安全驾驶行为成为改善道路安全的关键。

Method: 使用定制Android应用程序收集高频率加速度计、陀螺仪和GPS数据，并与连续视频同步，数据在多种道路类型上记录三种目标驾驶行为（正常、攻击性、危险）。数据集分为原始层、处理层和语义层。

Result: 创建了V-SenseDrive数据集，填补了全球驾驶员行为数据集的空白，为驾驶员行为分类、交通安全分析和ADAS开发提供了基础。

Conclusion: V-SenseDrive代表了巴基斯坦真实世界驾驶情况，为情境感知智能交通解决方案奠定了基础，解决了现有数据集在隐私保护和地域多样性方面的不足。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一个参数规模从3B到70B的多模态大语言模型系列，通过创新的领域增强技术实现了最先进的性能。该模型在通用基准测试中表现优异，在OCR、文档理解、数学推理等特定领域具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 开发能够同时保持强大通用性能并增强特定领域能力的多模态大模型，为企业部署提供有效的解决方案。

Method: 采用多阶段渐进式训练和高精度数据合成管道，结合领域增强策略，在百度昆仑P800芯片上进行大规模训练。

Result: 在CCBench、SEEDBench IMG、ScienceQA、MMStar等基准测试中达到最先进水平，OCRBench得分873，DocVQA准确率94.75%，MathVista得分78.6%。

Conclusion: 该工作确立了开发适合多样化企业部署场景的领域增强多模态模型的有效方法学，验证了大规模AI基础设施训练SOTA级多模态模型的能力。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于ODE的去雾框架，将大气散射模型重新表述为常微分方程，通过单步推理实现真实世界图像去雾，并利用马尔可夫链布朗运动生成非均匀雾霾数据来缓解训练数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法因缺乏配对的真实世界训练数据而难以泛化到真实场景，传统基于大气散射模型的物理方法在处理真实世界复杂雾霾模式时表现不佳。

Method: 提出HazeFlow框架，将大气散射模型转化为ODE，借鉴Rectified Flow思想学习从雾霾图像到清晰图像的最优ODE轨迹；引入基于马尔可夫链布朗运动的非均匀雾霾生成方法。

Result: 在多个真实世界去雾基准数据集上取得了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和创新的雾霾生成方法，有效解决了真实世界去雾的挑战，展示了优越的泛化能力。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版的EcoWeedNet模型，通过结构化通道剪枝、量化感知训练和TensorRT加速，在Jetson Orin Nano边缘设备上实现了高效的杂草检测。


<details>
  <summary>Details</summary>
Motivation: 农业领域部署深度学习模型面临边缘设备资源有限的挑战，需要开发轻量化但性能优越的模型。

Method: 采用结构化通道剪枝处理复杂架构（残差连接、注意力机制、拼接和CSP块），结合量化感知训练和TensorRT加速技术。

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达到184 FPS（FP16），比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率的EcoWeedNet优于YOLO11n和YOLO12n，达到83.7%精确率、77.5%召回率和85.9% mAP50。

Conclusion: 该方法证明了在保持高精度的同时显著提升效率，适用于精准农业应用。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出了一种新的多模态学习框架，通过增强模态dropout和对比学习来解决模态不平衡和缺失问题，在医疗诊断任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断越来越多地使用多模态数据，但现有模型难以有效融合异构信息并处理模态缺失问题，需要开发能够应对模态不平衡和缺失的鲁棒方法。

Method: 引入可学习的模态token来改进缺失感知的模态融合，将传统的单模态对比目标与融合的多模态表示相结合，使用增强的模态dropout策略。

Result: 在大规模临床数据集上的实验表明，该方法在疾病检测和预测任务中达到最先进性能，特别是在只有单一模态可用的实际场景中表现优异。

Conclusion: 该方法具有高效性、可扩展性和通用性，为多模态学习提供了低成本的解决方案，在真实临床应用中具有重要潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究系统评估了9种分割架构在肺栓塞（PE）分割任务上的表现，发现3D U-Net with ResNet encoder效果最佳，CNN模型优于ViT模型，分类预训练可能对分割性能产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 肺栓塞分割是临床诊断中的重要任务，但现有研究缺乏对不同分割架构在PE分割任务上的系统性比较和性能评估。

Method: 使用490个CTPA扫描构建密集标注数据集，在统一测试框架下评估9种CNN和ViT分割架构，比较预训练和随机初始化权重的影响。

Result: 最佳模型达到平均Dice分数0.7131，在60个测试扫描中检测到181个栓子，有49个假阳性和28个假阴性。3D模型在PE分割中表现优异，CNN模型普遍优于ViT模型。

Conclusion: 3D U-Net with ResNet encoder是PE分割的有效架构，中央和大栓子分割准确度满意，但远端栓子分割仍具挑战性，PE分类和分割可能依赖不同的判别特征。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出一种新颖的图神经网络，将时间动态与静态手形配置分离，用于手语手形识别，在37个手形类别上达到46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有基础音系作用，但计算方法很少显式建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学信息图结构和对比学习，解决手形识别中的关键挑战，包括细微的类间差异和时间变化。

Result: 在签名序列中建立了首个结构化手形识别基准，达到46%的准确率（基线方法为25%）。

Conclusion: 该方法显著提升了手形识别性能，为手语计算分析提供了新的技术途径。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文研究了分类任务选择对胎儿超声图像中分布外（OOD）检测性能的影响，发现OOD检测性能随任务变化显著，且最佳任务取决于ID-OOD标准（图像特征偏移或解剖特征偏移）。


<details>
  <summary>Details</summary>
Motivation: 可靠的分佈外检测对于在异质图像特征和临床环境中安全部署深度学习模型至关重要。现有研究主要关注不确定性量化方法，但本文研究分类任务本身的影响。

Method: 通过八个不确定性量化方法在四个分类任务上的实验，分析不同任务对OOD检测性能的影响。

Result: OOD检测性能随任务显著变化，最佳任务取决于ID-OOD标准类型（图像特征偏移或解剖特征偏移）。

Conclusion: 优越的OOD检测性能并不保证最优的弃权预测，强调在医学图像分析中需要根据具体下游应用调整任务选择和不确定性策略。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出OrthoLoC数据集，首个大规模无人机图像与正射地理数据配对数据集，用于解决无GPS情况下的视觉定位问题，并引入AdHoP优化技术提升匹配性能。


<details>
  <summary>Details</summary>
Motivation: 在无互联网或GPS支持的受限资源场景下，需要高精度视觉定位系统，但现有方法依赖大型图像数据库或重型3D模型不实用。正射地理数据轻量且易获取，但缺乏相关研究。

Method: 构建包含16,425张无人机图像的多模态数据集，解决无人机图像与地理数据间的域偏移问题。提出AdHoP优化技术，可与任何特征匹配器集成。

Result: 数据集支持公平基准测试，通过评估域偏移、数据分辨率和共视性对定位精度的影响。AdHoP技术将匹配性能提升高达95%，平移误差降低高达63%。

Conclusion: OrthoLoC填补了利用正射地理数据进行视觉定位的研究空白，为受限环境下的高精度定位提供了有效解决方案，数据集和代码已开源。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: SSDnet是一种零样本异常检测方法，无需训练数据，仅利用单张测试图像本身进行异常定位，通过自重建和感知损失实现高性能异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据的问题，开发仅依赖单张测试图像的无监督异常检测方法。

Method: 基于深度图像先验的补丁训练框架，通过掩码、补丁打乱和噪声注入避免恒等映射，使用基于内积相似度的感知损失。

Result: 在MVTec-AD数据集上达到0.99 AUROC和0.60 AUPRC，在fabric数据集上达到0.98 AUROC和0.67 AUPRC，优于现有方法。

Conclusion: SSDnet证明了无需外部训练数据的单图像异常检测的可行性，为实际应用提供了有效解决方案。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 提出了一种针对低资源孟加拉语的视觉-语言模型，通过三重损失函数（PAL+InfoNCE+OT）解决跨模态对齐问题，在Flickr30k和MSCOCO数据集上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如孟加拉语）在视觉-语言模型中面临的挑战，包括配对数据稀缺、翻译对齐断裂以及英语中心预训练忽略目标语言语义的问题

Method: 使用LaBSE验证的英-孟配对数据和11万双语提示合成图像训练计算感知的孟加拉语字幕生成管道，包含冻结的MaxViT视觉编码器、孟加拉语原生mBART-50解码器和轻量级跨模态桥接模块，核心创新是三重损失函数

Result: 在Flickr30k-1k上达到BLEU-4 12.29、METEOR 27.98、BERTScore-F1 71.20，在MSCOCO-1k上达到BLEU-4 12.00、METEOR 28.14、BERTScore-F1 75.40，显著超越基线方法，并将真实-合成数据质心差距缩小41%

Conclusion: 提出的三重损失函数协同机制有效改善了低资源语言的视觉-语言对齐，减少了虚假匹配，为低资源语言的多模态建模提供了有效解决方案

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个统一、仅使用相机的鸟瞰图框架，通过知识蒸馏将大型规划导向教师模型（UniAD）的全栈能力压缩到紧凑的实时学生模型中。


<details>
  <summary>Details</summary>
Motivation: 解决现有高效相机基线模型功能不全的问题，将全栈自动驾驶智能保留在资源受限的环境中，弥合大规模多模态感知规划模型与部署就绪实时自动驾驶之间的差距。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将高容量多模态知识有效转移到轻量级BEV表示中。

Result: 在nuScenes数据集上，TinyBEV实现了39.0 mAP的检测性能、1.08 minADE的运动预测和0.32的碰撞率，运行速度比UniAD快5倍（11 FPS），参数量减少78%。

Conclusion: 研究表明，在资源受限环境下可以保留全栈驾驶智能，为实时自动驾驶部署提供了可行的解决方案。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊球体标注策略，将球标注在模糊条纹的中心而非前缘，并显式标注模糊属性，同时发布了乒乓球检测数据集和BlurBall模型，实现了最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法将球标记在模糊条纹的前缘，这引入了不对称性并忽略了与速度相关的运动线索。运动模糊降低了快速移动物体的清晰度，给检测系统带来挑战。

Method: 引入新的标注策略，将球放置在模糊条纹中心并显式标注模糊属性；提出BlurBall模型，通过多帧输入的Squeeze-and-Excitation注意力机制联合估计球位置和运动模糊属性。

Result: 新标注方法在各种模型上一致提升了检测性能；BlurBall模型在球检测方面达到了最先进的结果；利用模糊信息不仅提高了检测精度，还实现了更可靠的轨迹预测。

Conclusion: 中心标注策略和模糊属性建模显著改善了运动模糊场景下的球体检测性能，为实时体育分析提供了更可靠的技术基础。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出MVP方法，利用压缩域运动向量在关键帧之间传播检测结果，减少开放词汇检测器调用次数，实现高效视频目标检测


<details>
  <summary>Details</summary>
Motivation: 解决在视频中逐帧运行大型开放词汇检测器计算成本高的问题，寻求无需训练、无需标签的高效检测方案

Method: 在固定间隔的关键帧上调用OWLv2检测器，使用压缩域运动向量（MV）通过3x3网格聚合进行检测传播，包含面积增长检查和可选单类切换

Result: 在ILSVRC2015-VID数据集上达到mAP@0.5=0.609，在宽松IoU阈值下接近逐帧检测性能，优于基于跟踪器的传播方法

Conclusion: 压缩域传播是减少检测器调用次数同时保持强大零样本覆盖率的实用方法，无需训练和标签

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文研究了HDR光照估计方法在颜色鲁棒性方面的表现，发现使用预训练的白平衡网络预处理输入图像可以显著提升现有模型的颜色准确性，且无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的HDR光照估计方法在颜色鲁棒性方面存在不足，这对增强现实应用中虚拟物体的真实渲染至关重要。大多数评估方法将颜色与其他光照属性混淆，需要专门研究颜色准确性。

Method: 使用包含多样化光照颜色的新型HDR数据集，系统评估多种适应策略，重点测试预训练白平衡网络预处理输入图像的效果。

Result: 白平衡网络预处理方法在所有测试场景中表现最佳，显著提升了颜色鲁棒性，且该方法适用于三种最先进的光照估计方法。

Conclusion: 简单的预处理技术可以有效提升现有光照估计模型的颜色准确性，这为增强现实应用提供了实用的改进方案。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一种无需训练的支票字段检测框架，利用视觉语言模型和多模态大语言模型实现零样本检测，解决传统方法对大量标注数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 支票作为金融基础工具面临欺诈风险，传统检测方法需要大量标注数据但数据稀缺，需要开发无需训练的高效检测方案。

Method: 结合视觉语言模型(VLM)和多模态大语言模型(MLLM)的零样本检测框架，无需训练即可识别支票关键字段如签名、MICR线、金额等。

Result: 在110张不同格式支票数据集上验证，表现出强大的性能和泛化能力，可作为高质量标注数据生成机制。

Conclusion: 该框架显著降低金融场景部署门槛，能为机构定制实时目标检测模型提供高质量训练数据基础。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 本文评估了视觉语言模型在图表理解中的表现，发现现有模型在面对噪声、遮挡等真实世界图表时性能显著下降，并出现幻觉问题。作者提出了CHART NOISe数据集来测试模型在退化条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准假设图表干净且查询基于事实，但真实世界图表常包含失真，需要超越简单匹配的推理能力。当前模型在噪声和遮挡条件下表现不佳，存在过度自信和幻觉问题。

Method: 评估ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro在图表理解任务中的表现；创建CHART NOISe数据集，结合图表损坏、遮挡和考试风格多选题；引入提示反向不一致性测试；提出质量过滤和遮挡检测等缓解策略。

Result: 模型在噪声或遮挡条件下性能急剧下降，幻觉问题（如数值捏造、趋势误解、实体混淆）更频繁；模型在退化设置下仍过度自信，生成看似合理但无支持的说明。

Conclusion: 研究揭示了图表推理中的系统性漏洞，建立了首个统一损坏、遮挡和反向不一致性的数据集，为提升图表理解的鲁棒性和可靠性提供了严格测试平台。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出一种基于神经表示的4D-MRI重建框架，用连续变形模型替代传统的离散相位分箱方法，通过空间解剖网络和时间运动网络的协同工作，实现高效准确的呼吸运动捕捉。


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法依赖相位分箱或模板扫描，存在无法捕捉时间变异性、工作流程复杂、计算负载重等问题，需要一种更高效准确的方法。

Method: 使用双网络架构：空间解剖网络编码连续3D解剖表示，时间运动网络基于Transformer提取的呼吸信号生成时间一致的变形场，将呼吸运动建模为平滑连续变形。

Result: 在19名志愿者的自由呼吸数据集上验证，该方法能准确捕捉规则和不规则呼吸模式，保持血管和支气管连续性，处理时间从5小时缩短至15分钟，单个体积推断时间小于1秒。

Conclusion: 该方法显著提升了4D-MRI重建效率和质量，在4D放射治疗规划和实时自适应治疗中具有强大应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 本文评估了五种基于卡尔曼滤波的跟踪方法在快速移动微小物体（如壁球）跟踪中的性能，发现DeepOCSORT在跟踪精度上表现最佳，但所有方法都存在显著的跟踪漂移问题。


<details>
  <summary>Details</summary>
Motivation: 快速移动微小物体的不可预测运动模式和小视觉标记使得精确跟踪成为计算机视觉中的挑战性问题，特别是在体育机器人应用中需要轻量级准确的跟踪系统。

Method: 使用包含10,000个标注壁球帧的自定义数据集，评估OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT五种卡尔曼滤波跟踪方法，重点分析推理速度和每图像更新频率对跟踪精度的影响。

Result: DeepOCSORT获得最低跟踪误差（平均ADE 31.15像素），ByteTrack处理速度最快（平均推理时间26.6ms），但所有跟踪器都存在3-11cm的空间误差，误差率比标准物体跟踪基准高3-4倍。

Conclusion: 当前基于卡尔曼滤波的跟踪方法在处理快速移动微小物体的不可预测运动模式时存在根本性限制，需要开发专门的方法论来改进此类应用的跟踪性能。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个基于运动向量的自适应裁剪模块，用于压缩域视频动作识别，无需训练即可提升精度或降低计算量


<details>
  <summary>Details</summary>
Motivation: 在压缩域视频动作识别中，如何高效利用运动信息来减少计算开销同时保持或提升识别精度

Method: 使用H.264视频中的运动向量定位运动密集区域，通过去噪合并、蒙特卡洛采样和自适应裁剪三个步骤生成裁剪区域

Result: 在UCF101上，MoCrop在相同FLOPs下提升Top-1精度3.5%，或在减少26.5%FLOPs的同时提升2.4%精度；在CoViAR上达到89.2%精度，同时将计算量从11.6降至8.5 GFLOPs

Conclusion: MoCrop具有强泛化性，适用于多种骨干网络，为压缩域实时部署提供了实用解决方案

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出CAFC-SE框架，通过码本自适应特征压缩和语义增强，在低比特率条件下实现更好的图像分析和压缩性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下性能较差，因为它们保留了过多冗余细节或学习过度集中的符号分布

Method: 使用向量量化将连续视觉特征映射到离散索引，通过码本选择性传输特征，保留更多信息性视觉模式

Result: 大量实验证明该方法在码率和准确性方面具有优越性

Conclusion: CAFC-SE框架对低比特率条件更具鲁棒性，能够有效提升边缘云系统的分析性能

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级的多核U形CNN架构，专为医学图像分割设计，在极低计算成本下实现优于现有方法的性能


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中现有方法计算复杂度高、参数量大，难以在资源受限环境中部署的问题

Method: 设计了多核深度卷积块(MKDC)处理多分辨率空间关系，并采用通道、空间和分组门控注意力机制来增强图像显著特征

Result: 仅需0.316M参数和0.314G FLOPs，在六个医学图像基准测试中DICE分数优于TransUNet、UNeXt等SOTA方法，同时参数减少333倍

Conclusion: MK-UNet在计算效率和分割精度上取得突破，为资源受限环境中的实时高保真医学诊断提供了无与伦比的解决方案

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，在手术视频和体积患者数据之间建立桥梁。该方法将3D高斯函数绑定到CT网格上，通过光度监督联合优化高斯参数和网格变形。


<details>
  <summary>Details</summary>
Motivation: 解决手术视频与术前CT数据之间的差距问题，实现更精确的手术导航和术中CT更新。

Method: 将3D高斯函数参数化相对于其父网格三角形，强制高斯函数与网格对齐，并通过光度监督联合优化高斯参数和网格变形，使变形能够传播回CT进行更新。

Result: 在猪内脏手术和人体肝脏合成数据上验证了有效性，展示了在单目RGB数据上对术前CT的合理变形。

Conclusion: BridgeSplat提供了一种有效的方法来桥接手术视频和体积患者数据，为可变形手术导航提供了新思路。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为DGLE的扩散引导标签增强框架，用于解决源自由域适应中伪标签噪声问题。该方法从少量高质量伪标签出发，利用扩散模型传播生成完整的高质量伪标签集。


<details>
  <summary>Details</summary>
Motivation: 当前源自由域适应方法在优化包含大量噪声的完整伪标签集时面临挑战，这限制了自训练方法的性能。需要一种能够避免直接优化噪声标签集的新方法。

Method: 首先通过置信度过滤和超分辨率增强获得少量高质量伪标签作为初始种子，然后利用扩散模型强大的去噪能力和复杂分布建模能力，将不完整种子伪标签传播生成完整的高质量伪标签。

Result: DGLE框架有效避免了直接优化完整伪标签集的困难，显著提高了伪标签质量，从而提升了模型在目标域的性能。

Conclusion: 提出的扩散引导标签增强方法为源自由域适应中的伪标签优化问题提供了有效解决方案，通过从高质量种子标签出发的传播策略，克服了传统方法的局限性。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 该论文提出了一种在双曲空间中嵌入特征提取器的方法，用于改进粗到细的少样本类增量学习任务，通过双曲对比损失和双曲全连接层优化模型性能，并使用最大熵分布生成增强特征来缓解少样本条件下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 双曲空间相比传统欧几里得空间在表示层次化数据方面具有优势，特别是在粗到细的少样本类增量学习任务中，能够更好地解释"粗到细"的学习范式。

Method: 使用庞加莱球模型将特征提取器嵌入双曲空间，引入双曲对比损失和双曲全连接层进行模型优化和分类，并采用双曲空间中的最大熵分布来估计细类特征向量的概率分布以生成增强特征。

Result: 在C2FSCIL基准测试上的实验表明，该方法有效提高了粗类和细类的分类准确率。

Conclusion: 双曲空间嵌入方法在粗到细的少样本类增量学习任务中表现出色，能够有效提升模型性能并缓解少样本条件下的过拟合问题。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种几何感知的两阶段框架，用于智能图像编辑中的对象移除，能够同时移除目标对象及其因果视觉伪影（如阴影和反射）。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像外观的方法要么严格遵循掩码对齐训练而无法移除未明确掩码的因果效果，要么采用松散掩码对齐策略缺乏可控性且可能过度擦除其他对象。这些限制源于忽略了对象几何存在与其视觉效果之间的因果关系。

Method: 提出两阶段框架：第一阶段从几何（如深度）中直接移除对象，使用严格掩码对齐监督实现结构感知编辑；第二阶段基于更新的几何条件渲染真实感RGB图像，因果视觉效果作为修改3D几何的结果被隐式考虑。引入基于正负样本对的偏好驱动目标来指导几何移除阶段的学习。

Result: 在两个流行基准测试上的广泛实验表明，该方法在移除对象及其相关伪影方面达到了最先进的性能。

Conclusion: 通过几何感知的方法有效解决了对象移除中的因果视觉伪影问题，实现了更完整和可控的对象移除效果。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了SEGA方法，通过高斯平滑和梯度集成来提高无参考图像质量评估模型在黑盒攻击中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的NR-IQA模型白盒攻击方法在更现实的黑盒场景中迁移性较差，因为攻击者无法访问目标模型。

Method: SEGA方法通过应用高斯平滑到源模型并集成其平滑梯度来近似目标模型的梯度，并使用专门设计的扰动过滤掩码来确保对抗性扰动的不可感知性。

Result: 在CLIVE数据集上的实验结果表明SEGA具有优越的迁移性，能够成功实现对NR-IQA模型的基于迁移的黑盒攻击。

Conclusion: SEGA是首个解决NR-IQA模型攻击中低迁移性挑战的方法，为揭示模型脆弱性和指导鲁棒系统设计提供了有效工具。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet是一种新颖的特征融合框架，通过参数自由的Hadamard乘法交互将基于transformer的表征与基于生理学的D-Markers直接集成，在微笑表情识别任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多任务学习框架在微笑表情识别中的计算效率低下问题，这些方法由于辅助任务监督和复杂的损失平衡需求而效率不高。

Method: 提出HadaSmileNet框架，系统评估了15种融合策略，发现Hadamard乘法融合能够实现直接特征交互同时保持计算效率。

Result: 在四个基准数据集上建立了深度学习方法的新的最先进结果：UvA-NEMO (88.7%, +0.8)、MMI (99.7%)、SPOS (98.5%, +0.7)和BBC (100%, +5.0)。计算分析显示参数减少26%，训练简化。

Conclusion: 该框架的效率和有效性使其特别适合需要实时情感计算能力的多媒体数据挖掘应用的实际部署。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的联合动态人体与静态场景重建框架，通过事件流引导解决快速运动下的模糊问题。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建动态人体和静态场景在快速运动下存在困难，RGB帧容易出现运动模糊。事件相机具有微秒级时间分辨率的优势，更适合动态人体重建。

Method: 使用统一的3D高斯集合，其中包含可学习的语义属性。只有被分类为人体的高斯会进行变形动画，场景高斯保持静态。提出事件引导损失函数，匹配连续渲染之间的模拟亮度变化与事件流。

Result: 在两个基准数据集ZJU-MoCap-Blur和MMHPSD-Blur上实现了最先进的人体-场景重建效果，在PSNR/SSIM指标上显著优于强基线，LPIPS降低，尤其对高速运动主体效果明显。

Conclusion: 该方法无需外部人体掩码，简化了单独高斯集合的管理，能够有效解决快速运动下的模糊问题，实现高质量的动态人体与静态场景联合重建。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个实时威胁监控框架，通过结构化语义元组、在线事件去重和基于LLM的推理机制，同时实现高性能的实时威胁检测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性能和决策可解释性的要求，需要一种统一框架来解决这一挑战。

Method: 1. 将视频帧解构为结构化的人-物-交互-地点语义元组；2. 提出高效的在线事件去重和更新机制；3. 使用思维链策略微调大语言模型进行透明推理。

Result: 在XD-Violence和UCF-Crime基准数据集上的实验表明，Live-E2T在威胁检测准确性、实时效率和可解释性方面显著优于现有最先进方法。

Conclusion: Live-E2T成功统一了实时性能和可解释性目标，为实时威胁监控提供了有效的解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文提出了PhotoCritique数据集、PhotoEye模型和PhotoBench基准，旨在提升多模态大语言模型在美学视觉理解方面的能力，特别是解决专业摄影场景中的美学分析需求。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在美学视觉理解方面存在局限，主要关注基础美学常识，缺乏专业摄影知识。本文旨在解决这一差距，使模型能够进行专业级的美学分析。

Method: 1) 构建大规模专业摄影数据集PhotoCritique；2) 提出PhotoEye模型，采用语言引导的多视角视觉融合机制；3) 建立专业美学评估基准PhotoBench。

Result: 在现有基准和PhotoBench上，PhotoEye模型相比现有模型展现出明显优势，证明了其在美学视觉理解方面的有效性。

Conclusion: 本文通过专业数据集、创新模型和评估基准，显著提升了MLLMs的美学视觉理解能力，为专业摄影场景的应用奠定了基础。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 提出基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛，通过记忆增强架构在长序列cine-MRI中分割肿瘤，满足临床实时需求


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，这对于提升癌症治疗的准确性和安全性至关重要

Method: 利用XMem模型（记忆增强架构）构建肿瘤分割框架，集成记忆机制来实时跟踪肿瘤运动，在有限标注数据下实现高效分割

Result: 由于详细实验记录丢失，无法报告精确定量结果，但初步开发印象显示XMem框架表现出合理的分割性能并满足临床实时要求

Conclusion: 该工作有助于改善MRI引导放疗中的肿瘤跟踪精度，为癌症治疗提供更准确和安全的技术支持

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率问题，通过动态空间扭曲、语义感知令牌聚合和空间频率融合模块，实现空间语义一致性和高频细节恢复。


<details>
  <summary>Details</summary>
Motivation: 传统方法在空间语义一致性建模不足，且未能充分利用频域信息，导致细粒度对齐差和高频细节恢复不足。

Method: SSCM模型包含三个核心模块：动态空间扭曲模块用于跨对比度空间对齐，语义感知令牌聚合块用于长程语义一致性，空间频率融合块用于精细结构恢复。

Result: 在公共和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，同时确保空间和语义一致的重建。

Conclusion: SSCM模型在多对比度MRI超分辨率任务中表现出色，有效解决了空间语义一致性和高频细节恢复的挑战。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出Oracle-educated GRPO (OraPO)和FactScore-based奖励(FactS)方法，在有限计算资源下实现高效的放射学报告生成，通过单阶段强化学习训练和基于临床事实的奖励机制，在CheXpert Plus数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成方法通常采用大规模多阶段训练和庞大骨干网络，导致数据需求和计算成本过高。本文旨在在有限预算下解决这一问题。

Method: 1) OraPO方法：将GRPO在罕见或困难病例上的失败探索通过轻量级oracle步骤转化为直接偏好监督，实现单阶段RL训练；2) FactS奖励：提取原子临床事实并与真实标签进行蕴含检查，提供密集、可解释的句子级奖励。

Result: 在CheXpert Plus数据集上达到0.341的F1分数，使用小规模基础视觉语言模型和少量训练数据（减少2-3个数量级），在普通硬件上实现SOTA性能。

Conclusion: OraPO和FactS共同构建了一个紧凑而强大的框架，显著提高了在临床挑战性病例上的学习效率，为资源受限环境下的放射学报告生成提供了有效解决方案。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个基于参考的训练自由框架，能够在扩散模型中实现多参考风格的可控融合，通过语义标记分解和相似性感知重加权模块解决现有方法只能处理单一风格和缺乏平衡机制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的方法存在两个主要限制：(a) 只能接受一张风格图像，无法实现混合美学和扩展到更多风格；(b) 缺乏平衡多种风格影响的原理性机制。

Method: AMSF通过语义标记分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的每个交叉注意力层中，然后通过相似性感知重加权模块在每个去噪步骤重新校准对每个风格组件的注意力分配。

Result: 定性和定量评估显示，AMSF生成的多风格融合结果始终优于最先进方法，其融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF是朝着扩散模型中表达性多风格生成的实用步骤，无需微调或外部适配器即可实现平衡且用户可控的混合效果。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 提出MLF-4DRCNet，一种用于4D雷达和相机图像多级融合的两阶段3D目标检测框架，通过点级、场景级和提议级融合解决雷达点云稀疏性问题，在VoD和TJ4DRadSet数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达成本低且鲁棒性强，但其点云稀疏且有噪声，限制了在3D目标检测中的独立应用。现有雷达-相机融合方法大多采用为LiDAR设计的BEV融合范式，忽略了雷达点云的稀疏性和不完整性，且仅进行粗粒度的场景级融合。

Method: 提出三阶段多级融合框架：1）ERPE模块通过2D图像实例增强雷达点云密度，使用三重注意力体素特征编码器编码；2）HSFP模块使用可变形注意力动态融合多尺度体素特征与2D图像特征；3）PLFE模块通过融合图像特征精炼区域提议。

Result: 在View-of-Delft和TJ4DRadSet数据集上的实验表明，MLF-4DRCNet达到了最先进的性能，在VoD数据集上的性能与基于LiDAR的模型相当。

Conclusion: MLF-4DRCNet通过多级融合策略有效解决了4D雷达点云稀疏性问题，实现了与LiDAR相媲美的3D目标检测性能，为低成本自动驾驶感知提供了可行方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Prompt-Guided Dual Latent Steering (PDLS)的训练免费框架，用于解决扩散模型在图像逆变换中的语义漂移问题。该方法通过双流控制机制，在保持结构完整性的同时实现语义引导的图像重建。


<details>
  <summary>Details</summary>
Motivation: 当前基于单潜在向量的图像逆变换方法在平衡结构保真度和语义准确性方面存在困难，容易导致语义漂移问题，如细节模糊或属性错误。

Method: PDLS将逆变换过程分解为两个互补流：结构路径保持源图像完整性，语义路径通过提示词引导。该方法将双引导制定为最优控制问题，并通过线性二次调节器(LQR)获得闭式解，动态控制生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K数据集上的广泛实验表明，PDLS在各种逆变换任务（包括高斯去模糊、运动去模糊、超分辨率和自由形式修复）中，相比单潜在基线方法，能够产生更忠实于原始图像且与语义信息更好对齐的重建结果。

Conclusion: PDLS框架通过双潜在引导机制有效解决了扩散模型图像逆变换中的语义漂移问题，无需昂贵的逐图像优化，在多个逆变换任务中表现出优越性能。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 开发了Prima，首个基于视觉语言模型的神经影像AI基础模型，用于临床MRI研究分析，在52种神经疾病诊断中达到92.0的平均AUC，优于现有AI模型。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求持续增长给医疗系统带来压力，特别是在资源匮乏地区。需要AI工具来缓解医生负担、缩短诊断时间并减少医疗系统偏见。

Method: 使用大型学术医疗系统作为数据引擎，训练了基于22万多个MRI研究的视觉语言模型Prima，采用分层视觉架构提供通用可迁移的MRI特征。

Result: 在包含3万例MRI研究的1年系统测试中，Prima在主要神经疾病诊断中表现优异，提供可解释的鉴别诊断、工作列表优先级和临床转诊建议。

Conclusion: Prima展示了健康系统规模视觉语言模型的变革潜力，能够推进AI驱动的医疗保健发展，特别是在减少医疗系统偏见方面。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Understanding-in-Generation (UiG)的新推理框架，通过将理解能力融入生成过程来增强统一模型的文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Chain-of-Thought方法将理解和生成过程分离，限制了其指导统一模型解决生成能力不足的能力。

Method: UiG框架引入"图像编辑"作为桥梁，在推理过程中利用统一模型的强大理解能力来指导生成。首先验证生成图像并将模型理解融入编辑指令，然后逐步增强图像生成质量。

Result: UiG在文本到图像生成方面显著优于现有方法，在TIIF基准测试的长提示设置上实现了3.92%的性能提升。

Conclusion: UiG框架通过理解与生成的深度融合，有效提升了统一模型的文本到图像生成能力。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了一个用于内窥镜图像的深度估计基准测试和新合成数据集EndoSynth，通过微调深度基础模型显著提升了在真实内窥镜图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像深度估计领域缺乏稳健的基准测试和高质量数据集，限制了该领域的发展和应用。

Method: 创建了包含真实内窥镜图像的基准测试，并开发了带有真实深度和分割掩码的合成数据集EndoSynth，用于微调深度基础模型。

Result: 使用合成数据集微调深度基础模型后，在大多数未见过的真实数据上准确率显著提升。

Conclusion: 通过提供基准测试和合成数据集，这项工作推动了内窥镜图像深度估计领域的发展，为未来研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba模型，通过局部强调状态空间模块和自适应融合模块，在RGB-D显著目标检测任务中实现高性能和高效率的平衡


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著目标检测方法在CNNs的局部感受野限制和Vision Transformers的二次复杂度之间难以平衡，需要一种能有效建模长距离依赖且计算高效的方法

Method: 提出LEAF-Mamba模型，包含局部强调状态空间模块(LE-SSM)捕获多尺度局部依赖，以及基于SSM的自适应融合模块(AFM)实现跨模态互补交互和可靠集成

Result: 在16个最先进的RGB-D SOD方法中表现最优，同时在RGB-T SOD任务上也表现出色，证明了强大的泛化能力

Conclusion: LEAF-Mamba模型在RGB-D显著目标检测任务中实现了性能与效率的良好平衡，并展现出优秀的跨任务泛化能力

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出一种轻量级食品图像分类算法，结合窗口多头注意力机制和空间注意力机制，在减少计算复杂度的同时保持高分类准确率


<details>
  <summary>Details</summary>
Motivation: 食品行业对生产质量和效率要求不断提高，但Vision Transformer模型参数多、计算复杂度高，难以在资源受限环境中部署

Method: 集成窗口多头注意力机制(WMHAM)和空间注意力机制(SAM)，WMHAM通过窗口划分降低计算成本，SAM自适应强调关键空间区域

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，显著减少了参数和FLOPs

Conclusion: 该方法在计算效率和分类性能之间取得了有效平衡，适合在资源受限环境中部署

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个用于遥感图像中开放集土地覆盖分析的三阶段框架，无需人工标注即可实现发现、分割和描述功能


<details>
  <summary>Details</summary>
Motivation: 遥感中的开放集土地覆盖分析需要同时实现精细空间定位和语义开放分类，既要检测和分割无类别监督的新对象，又要通过多模态推理为其分配可解释的语义标签

Method: 三阶段框架：1）使用可提示的微调分割模型（SAM）进行精确发现和掩码提取；2）通过两阶段微调的多模态大语言模型（MLLM）进行语义归因和上下文描述；3）使用LLM作为评判者和人工评分来评估MLLM

Result: OSDA结合了像素级精度和高层语义理解，解决了开放世界遥感解释中的关键挑战，支持跨不同卫星图像的鲁棒评估

Conclusion: 该工作为动态土地覆盖监测提供了可扩展和可解释的解决方案，在自动制图更新和大规模地球观测分析方面显示出强大潜力

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF 2021植物识别挑战赛，旨在评估如何利用植物标本馆数据改进生物多样性丰富但数据稀缺地区的植物自动识别，特别是针对圭亚那地盾地区。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别技术主要基于北美和西欧的数据，而生物多样性最丰富的热带地区数据稀缺。植物标本馆收藏了大量热带植物标本，可用于弥补这一数据缺口。

Method: 采用跨域分类任务，训练集包含数十万份植物标本馆标本和数千张野外照片，测试集仅包含野外照片。数据还包括5种形态和功能性状值。

Result: 挑战赛评估了不同研究团队的方法和系统，分析了主要结果，展示了利用标本馆数据改进热带地区植物识别的潜力。

Conclusion: 植物标本馆数据可以作为补充资源，显著改善数据稀缺热带地区的植物自动识别性能，为生物多样性保护提供技术支持。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出AGSwap方法解决多类别物体融合生成中的偏差和视觉混乱问题，并引入COF数据集进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法在跨类别物体融合时存在重叠伪影、集成效果差的问题，且缺乏全面的基准数据集。

Method: AGSwap方法包含组级嵌入交换和自适应组更新两个关键组件，通过特征操作融合不同概念的语义属性，并使用平衡评估分数指导动态优化。

Result: 实验表明AGSwap在简单和复杂提示下均优于现有最先进的组合式T2I方法，包括GPT-Image-1。

Conclusion: AGSwap是一种简单而高效的跨类别物体融合方法，配合新构建的COF数据集，显著提升了融合生成的质量和一致性。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2019植物识别挑战赛旨在评估在数据稀缺地区（如圭亚那地盾和亚马逊雨林）的植物自动识别性能，涉及1万种物种，并与热带植物专家进行性能对比。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别技术主要针对少数几万种物种，而全球有近36.9万种植物。该挑战赛旨在解决数据稀缺地区的植物识别问题。

Method: 基于圭亚那地盾和亚马逊雨林地区的1万种植物数据集，通过深度学习技术进行植物识别，并与专家识别结果进行对比评估。

Result: 挑战赛评估了不同研究团队的植物识别系统性能，并与热带植物专家的识别能力进行了比较分析。

Conclusion: 该挑战赛为数据稀缺地区的植物自动识别提供了重要基准，推动了植物识别技术在生物多样性丰富但数据不足区域的应用发展。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的基础模型实现开放世界场景下的目标定位


<details>
  <summary>Details</summary>
Motivation: 解决现有遥感视觉定位方法受限于封闭词汇集、依赖高质量数据集和耗时微调的问题，探索冻结通用基础模型在零样本开放词汇遥感视觉定位中的潜力

Method: 三阶段框架：1）概览阶段使用视觉语言模型获取文本查询与视觉区域的语义关联；2）聚焦阶段利用扩散模型的细粒度建模先验补充结构形状信息；3）进化阶段通过注意力进化模块抑制无关激活，生成纯净分割掩码

Result: 大量实验表明，该框架在无需任务特定训练的情况下，持续优于现有的弱监督和零样本方法

Conclusion: RSVG-ZeroOV提供了一个高效可扩展的解决方案，展示了冻结基础模型在零样本开放词汇遥感视觉定位中的巨大潜力

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种属性提示组合（APC）框架，利用文本语义联合增强目标重识别（ReID）的判别性和泛化性，通过属性提示生成器和快慢训练策略在传统和领域泛化ReID数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型局限于单域或跨域场景，单域模型容易过拟合域特定特征，而跨域模型依赖的归一化策略可能抑制身份特异性判别线索。需要同时提升判别性和泛化性。

Method: APC框架包含属性提示生成器（APG）和快慢训练策略（FSTS）。APG由语义属性字典（SAD）和提示组合模块（PCM）组成，SAD提供丰富语义描述，PCM自适应组合属性生成判别性特征。FSTS通过快速更新流获取ReID特异性知识，慢速更新流保留预训练视觉语言模型的泛化知识。

Result: 在传统和领域泛化ReID数据集上的广泛实验表明，该框架超越了最先进方法，在判别性和泛化性方面均表现出优越性能。

Conclusion: APC框架通过文本语义和视觉语言模型的结合，有效解决了ReID任务中判别性与泛化性的平衡问题，为实际应用提供了更实用的解决方案。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: OTCCLIP是一个基于最优传输的框架，用于重建图像-文本对，通过细粒度视觉和文本特征之间的最优传输距离来重新分配新的标题，以防御CLIP模型的数据中毒和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法仅依赖图像和标题的全局表示来纠正中毒的图像-文本对，忽略了细粒度特征，可能引入错误的配对并损害CLIP预训练。

Method: 提出基于最优传输的距离度量方法，计算细粒度视觉和文本特征集之间的距离，并基于此重新分配标题；同时使用最优传输目标函数促进模态间和模态内的细粒度对齐。

Result: OTCCLIP成功降低了中毒攻击的攻击成功率，并在中毒数据集上显著提高了CLIP的零样本和线性探测性能。

Conclusion: OTCCLIP通过细粒度特征的最优传输对齐，有效防御了CLIP模型的数据中毒攻击，并提升了模型性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出LFI框架，通过建模视觉理解作为交互过程，解决视觉基础模型从视觉语言模型知识迁移的局限性，实现更有效的跨模态知识传递。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型主要采用结果导向范式，忽视了视觉语言模型中编码的交互过程，导致知识迁移效率低且泛化能力受限。

Method: 提出交互查询机制保持跨网络层的持久关系结构，以及基于跨模态注意力机制的交互监督方法。

Result: 在多个基准测试中取得显著提升：TinyImageNet分类提升3.3mAP，COCO检测/分割提升1.6mAP/2.4AP，跨域设置下PACS和VLCS零样本性能分别提升2.4和9.3。

Conclusion: LFI框架通过建模交互过程实现了更忠实和高效的知识迁移，在参数开销最小的情况下获得更快收敛和更好泛化性能，人类评估显示其认知对齐性优于结果导向方法2.7倍。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出HyPSAM方法，利用SAM的零样本泛化能力解决RGB-热成像显著目标检测中的边界精度和完整性问题，通过动态融合网络和即插即用优化网络实现多模态特征融合和精确定位。


<details>
  <summary>Details</summary>
Motivation: RGB-热成像显著目标检测面临特征融合不足和数据稀缺的挑战，现有方法难以学习精确边界和完整对象。

Method: 提出HyPSAM框架：1）动态融合网络（DFNet）生成高质量初始显著图作为视觉提示；2）即插即用优化网络（P2RNet）使用混合提示（文本、掩码、框）指导SAM优化显著图。

Result: 在三个公开数据集上的实验表明，该方法达到最先进性能，且具有显著通用性，可与其他RGB-T SOD方法无缝集成实现性能提升。

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，为多模态显著目标检测提供了有效的解决方案。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一种多模态交叉注意力自编码器，通过整合文本先验、多视图图像的单目深度图和LiDAR点云，提高LiDAR感知在噪声和对抗性攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶和机器人感知中至关重要，但原始点云容易受到噪声、遮挡和对抗性破坏的影响。现有的自编码器在真实世界挑战性条件下性能下降。

Method: 提出TriFusion-AE模型，通过跨模态注意力机制对齐文本语义线索、图像几何特征和LiDAR空间结构，学习对随机噪声和对抗性扰动具有鲁棒性的表示。

Result: 在nuScenes-mini数据集上评估显示，在强对抗性攻击和重噪声条件下，模型显著优于CNN自编码器，后者在这些条件下会崩溃。

Conclusion: 该多模态融合框架是模型无关的，可与任何基于CNN的点云自编码器无缝集成进行联合表示学习，在恶劣条件下提供更鲁棒的感知能力。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 本文提出了COLT方法，旨在增强开源视频大语言模型的持续工具使用能力，使其能够在不断演化的工具流中自动学习工具使用，避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设工具库固定，难以适应现实世界中工具数据持续演化和流式输入的环境。

Method: COLT方法包含一个可学习的工具代码本作为工具特定记忆系统，根据用户指令与工具特征的相似度动态选择相关工具，并收集了视频中心工具使用指令调优数据集VideoToolBench。

Result: 在现有视频LLM基准和工具使用特定数据集VideoToolBench上的广泛实验表明，COLT达到了最先进的性能。

Conclusion: COLT方法成功解决了视频LLM在持续工具流环境中的工具使用问题，展现了优异的性能。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯溅射重建，通过更准确的跨视图一致性扩散先验和自适应渐进增强方案，有效去除伪影并填补缺失内容。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D重建由于视觉信息不足导致明显的伪影，现有基于生成先验的方法难以保证多视图一致性，产生模糊结构和不合理细节。

Method: 提出蒸馏方法提供更准确和跨视图一致的扩散先验，结合自适应渐进增强方案细化重建结果。

Result: 实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法。

Conclusion: FixingGS通过有效利用扩散模型能力，成功解决了稀疏视角3DGS重建中的伪影和多视图一致性问题。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM提出了一种基于高斯分位数的非均匀权重分离方法，通过显著性感知混合量化算法对视觉语言模型进行超低位宽量化（≤2位），显著提升硬件受限环境下的效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的计算需求和内存要求过高，限制了其在硬件受限环境中的应用，需要开发高效的超低位宽量化方法。

Method: 将模型权重基于高斯分位数非均匀分离为异常值（显著）和多个正常值（不显著）子集，提出显著性感知混合量化算法，根据显著性度量和压缩目标对缩放器和二进制矩阵施加不同约束。

Result: 在语言模型部分，Bi-VLM在视觉问答任务上比SOTA方法提升3%-47%；在整个VLM上提升4%-45%。量化模型存在90%-99%的图像令牌冗余，可进一步剪枝提升效率。

Conclusion: Bi-VLM通过非均匀权重分离和显著性感知量化，成功实现了视觉语言模型的超低位宽高效量化，为硬件受限环境下的应用提供了可行方案。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一个自监督学习框架，通过引入多尺度向量量化来创建离散表示瓶颈，抑制捷径学习，提高医学图像表示的鲁棒性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在医学图像中存在依赖复杂架构、解剖学先验和精心调优的数据增强等问题，容易产生捷径学习，特别是在解剖相似度高、病理特征细微的胸部X光等模态中。

Method: 将多尺度向量量化整合到自监督学习流程中，通过离散表示瓶颈约束模型学习可重复、结构感知的特征，同时抑制视图特定或低效模式。

Result: DiSSECT在分类和分割任务上表现出色，需要极少或无需微调，在低标签场景下具有高标签效率，在多个公共医学影像数据集上验证了其鲁棒性和泛化能力。

Conclusion: DiSSECT框架通过离散表示学习有效解决了医学图像自监督学习中的捷径学习问题，显著提升了表示的可迁移性和临床应用的实用性。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 本文提出结构化反思方法，将错误到修复的路径转化为明确、可控且可训练的动作，通过Reflect-Call-Final策略优化工具调用，在BFCL v3和Tool-Reflection-Bench基准上显著提升了多轮工具调用成功率和错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强的大型语言模型通常使用监督模仿或粗粒度强化学习进行训练，自我反思实践依赖启发式提示或单向推理，在多轮交互中脆弱，模型在失败后经常重复相同错误。

Method: 提出结构化反思方法，代理产生简短而精确的反思：使用前一步的证据诊断失败，然后提出正确、可执行的后续调用。结合DAPO和GSPO目标与针对工具使用的奖励方案，优化Reflect-Call-Final策略。

Result: 在BFCL v3和Tool-Reflection-Bench上的实验显示，多轮工具调用成功率和错误恢复能力大幅提升，冗余调用减少。

Conclusion: 使反思明确化并直接优化可以提高工具交互的可靠性，并为代理提供从失败中学习的可复现路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [70] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合热成像、深度学习和车联网通信的实时检测与驾驶员预警系统，用于减少鹿车碰撞事故。系统在12000张热成像鹿图像数据集上训练，达到98.84%的平均精度，并在实地测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿车碰撞事故，造成440人死亡、59000人受伤和100亿美元经济损失，同时导致鹿群数量下降。现有可见光摄像头在恶劣天气条件下检测效果不足60%，需要更可靠的解决方案。

Method: 系统采用热成像技术结合深度学习算法进行鹿的实时检测，当检测到高概率目标时，通过蜂窝车联网通信设备向周围车辆和路边单元广播传感器数据共享消息。

Result: 实验评估显示系统性能优异：平均精度98.84%，精确率95.44%，召回率95.96%。在实地测试中，热成像在恶劣天气条件下保持88-92%的检测准确率，系统端到端延迟始终低于100毫秒。

Conclusion: 该研究通过热成像和车联网技术为减少鹿车碰撞事故建立了可行的技术路径，系统在各种天气条件下都能提供可靠的检测和预警功能。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [71] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 本文提出了VIR-Bench基准测试，用于评估多模态大语言模型在长距离旅行视频理解方面的能力，发现现有模型在处理扩展时空尺度的视频时表现不佳，并通过案例研究验证了该基准对实际旅行规划应用的价值。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试主要关注室内场景或短距离户外活动，缺乏对长距离旅行挑战的评估。掌握扩展的地理时空轨迹对于下一代MLLMs至关重要，支持现实世界任务如具身AI规划和导航。

Method: 提出了VIR-Bench基准测试，包含200个旅行视频，将行程重建作为评估MLLMs地理时空智能的挑战性任务。通过实验评估最先进的MLLMs，并开发原型旅行规划代理进行案例研究。

Result: 实验结果表明，包括专有模型在内的最先进MLLMs难以获得高分，突显了处理扩展时空尺度视频的困难。案例研究中开发的旅行规划代理显示出显著改进的行程推荐效果。

Conclusion: VIR-Bench不仅有效评估模型性能，还能转化为面向用户应用的具体性能提升，为推进MLLMs的地理时空智能提供了重要基准。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [72] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过将扩散模型与下游任务偏好对齐，解决手术数据稀缺问题，提升分类和分割性能。


<details>
  <summary>Details</summary>
Motivation: 手术数据标注稀缺限制了深度学习系统发展，现有扩散模型存在数据记忆问题，生成样本不一致且多样性不足，可能损害下游任务性能。

Method: 构建偏好和非偏好合成图像对，通过轻量级微调使扩散模型生成过程与下游目标明确对齐。

Result: 在三个手术数据集上，分类任务提升7-9%，分割任务提升2-10%，对代表性不足类别改进显著。迭代细化进一步提升4-10%性能。

Conclusion: SAADi方法克服样本退化问题，确立任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [73] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval是一个新颖的基准测试，用于评估视觉语言模型在受色盲测试启发的视觉对抗场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在复杂视觉模式中识别数字信息的能力，特别是在对抗性颜色组合场景下的表现。

Method: 创建包含500张类似石原色盲测试图像的数据集，使用是/否和开放式提示评估9个VLMs，并与人类参与者进行对比。

Result: 实验显示VLMs在对抗性环境中识别数字的能力存在局限性，存在普遍的幻觉问题。

Conclusion: 该基准测试有助于提高VLMs在现实应用中的可靠性，特别是在需要高精度的场景下。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [74] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出一种基于模型的神经网络KMDS-Net，用于动态PET图像去噪，通过利用帧间空间相关性和帧内结构一致性，在模拟和真实数据上表现出优于基线方法的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 动态PET中短时间帧的图像质量受限于统计信息不足，而深度学习在医学图像去噪任务中显示出潜力，需要开发专门针对动态PET特性的去噪方法。

Method: 建立基于核空间的多维稀疏模型，利用动态PET的帧间空间相关性和帧内结构一致性，然后用神经网络替代参数估计过程，形成端到端的KMDS-Net神经网络。

Result: 在模拟和真实数据上的广泛实验表明，KMDS-Net在动态PET去噪方面表现出强大的性能，优于之前的基线方法。

Conclusion: 该方法可有效实现动态PET的高时间和空间分辨率，源代码已公开。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [75] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析和文本推理，在单一框架中实现病变定位、分割和诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型过于专业化，需要多个专用网络，泛化能力有限。临床应用需要精确的视觉定位、多模态整合和链式推理能力。

Method: 提出新颖的多模态训练方法，整合检测、分割和多模态链式推理，发布涵盖推理、检测、分割和文档理解任务的开放数据套件。

Result: Citrus-V在多个基准测试中优于现有开源医学模型和专家级影像系统，实现从视觉定位到临床推理的统一流程。

Conclusion: 该模型支持精确病变量化、自动报告生成和可靠第二意见，为临床诊断提供统一的多模态解决方案。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [76] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出了一种结合光流分割标签插值和多任务学习的新框架，用于解决机器人辅助手术中视觉数据理解面临的时空不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要精确理解手术过程中的视觉数据，但现有方法多为单任务学习，无法全面理解复杂的手术场景。同时，像素级分割数据标注成本高，导致长短期标注存在时空不平衡问题。

Method: 使用光流估计从标注的关键帧向相邻未标注帧传播标签，通过标签插值丰富稀疏的空间监督，结合多任务学习平衡时空信息进行训练。

Result: 该框架提高了手术场景理解的准确性和效率，增强了机器人辅助手术的实用性。

Conclusion: 提出的光流标签插值与多任务学习结合的方法有效解决了手术场景理解中的时空不平衡问题，为机器人辅助手术提供了更好的视觉数据理解能力。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [77] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一的多模态加速框架，通过推测解码和多阶段蒸馏技术，显著提升多模态理解和生成任务的效率，实现2倍以上的理解加速和16-22倍的生成加速。


<details>
  <summary>Details</summary>
Motivation: 随着多模态内容中交错标记数量的增加，扩散去噪和自回归解码的迭代过程带来了巨大的计算开销，需要高效的加速解决方案。

Method: 采用分治策略，使用推测解码进行下一个标记预测，结合多阶段蒸馏过程进行扩散去噪，并通过对抗蒸馏和人类反馈学习实现高效模型。

Result: 实现了2倍多模态理解加速，无损6-NFE模型在文本到图像生成中达到16.67倍加速，图像编辑22倍加速；1-NFE模型支持近实时交互编辑。

Conclusion: Hyper-Bagel框架通过先进的加速技术，在保持高质量输出的同时显著提升多模态任务的效率，使复杂的多模态交互变得无缝和即时。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [78] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 本研究评估了多模态大语言模型和视觉语言模型在基督教图像学单标签分类任务中的能力，发现GPT-4o和Gemini 2.5在大多数情况下优于ResNet50基线模型，支持将LLMs作为数字人文学科中的元数据管理工具。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态模型是否能够解释通常由监督分类器处理的基督教图像学，并评估其性能，探索LLMs在文化遗产领域的应用潜力。

Method: 使用ArtDL、ICONCLASS和Wikidata三个数据集，在三种条件下测试模型：仅使用类别标签、使用Iconclass描述、以及五样本的小样本学习，并与在相同数据集上微调的ResNet50基线进行比较。

Result: Gemini-2.5 Pro和GPT-4o在大多数情况下优于ResNet50基线，但在Wikidata数据集上准确率显著下降。使用类别描述通常能提高零样本性能，而小样本学习效果较差。

Conclusion: 通用多模态LLMs能够在视觉复杂的文化遗产领域进行分类，支持将其作为数字人文学科工作流中的元数据管理工具，未来需要研究提示优化和其他分类策略。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [79] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 本文提出了一种可学习的重参数化图构建方法（LRGC），用于视觉图神经网络（ViG），通过可学习的注意力机制和软阈值重参数化来构建图像图表示，无需超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 传统的ViG模型依赖非参数化的统计方法来构建图结构，无法为每个节点选择最佳邻域，且需要超参数搜索。现有方法（如k-NN、超图构建等）缺乏可学习的超参数无关的图构建能力。

Method: LRGC在每对节点间应用键-查询注意力机制，然后使用软阈值重参数化进行边选择，构建可微的数学训练模型。通过可学习参数选择邻域，避免了传统聚类或阈值方法的偏差。

Result: 在ImageNet-1k基准数据集上，ViG-LRGC方法在相似模型规模下优于最先进的ViG模型。

Conclusion: LRGC提供了一种可学习的、无需超参数搜索的图构建方法，能够自适应地调整每层的阈值，在图像表示学习任务中表现出优越性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [80] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 本文提出Point Prompt Defender框架，通过对抗性强化学习自动优化SAM的点提示，采用攻击-防御范式提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手动设计的提示，限制了可扩展性和泛化能力。提示质量对SAM性能至关重要但缺乏自动优化方法。

Method: 构建双空间图表示图像块，攻击者学习激活破坏性提示降低分割性能，防御者学习抑制这些提示恢复精度，使用深度Q网络训练。

Result: 实验表明该框架有效提升SAM的鲁棒性和泛化能力，建立灵活可解释的即插即用提示优化框架。

Conclusion: Point Prompt Defender为基于提示的分割提供了有效的自动优化解决方案，无需重新训练即可提升多种任务的分割性能。

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [81] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是首个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片/视频和生物声学记录，支持多模态AI研究用于环境监测和物种保护。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中多模态监测的需求，为保护计算机视觉研究提供开放数据集。

Method: 在220英亩牧场进行为期4天的同步监测，收集三种模态数据（无人机、相机陷阱、生物声学），并对传感器模态性能进行比较分析。

Result: 展示了不同传感器模态在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势，建立了可复制的多模态监测协议。

Conclusion: 该数据集为保护计算机视觉研究提供了重要资源，未来版本将包含GPS跟踪数据、公民科学数据和更广泛的时间覆盖范围。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [82] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 提出了一个名为RS3DBench的新型基准数据集，包含54,951对遥感图像与像素级对齐的深度图，用于推动通用大规模3D视觉模型在遥感领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集要么缺乏全面的深度信息，要么深度数据与遥感图像之间缺乏精确对齐，限制了3D视觉感知模型的发展。

Method: 构建了包含丰富地理背景的遥感图像-深度图配对数据集，并基于稳定扩散模型开发了遥感深度估计模型，利用其多模态融合能力。

Result: 在RS3DBench数据集上实现了最先进的性能，为遥感领域的3D视觉感知模型提供了有效的训练和评估工具。

Conclusion: 该工作对遥感领域3D视觉感知模型的发展和地理人工智能的进步做出了重要贡献，数据集、模型和代码已公开。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [83] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat：首个无需SfM的基于事件相机的去模糊3D高斯泼溅方法，通过DUSt3R直接获取初始点云，并利用事件流进行精细监督优化


<details>
  <summary>Details</summary>
Motivation: 解决运动模糊问题，避免传统SfM方法中相机位姿累积误差对点云位置的影响，利用事件相机对动态变化的高敏感性

Method: 1) 使用预训练的DUSt3R密集立体模块直接从模糊图像获取准确初始点云；2) 引入事件流解码潜在清晰图像，为场景重建优化提供精细监督信号

Result: 在多种场景下的实验表明，DeblurSplat不仅能生成高保真新视角，而且在渲染效率上显著优于现有去模糊3D-GS方法

Conclusion: 该方法成功实现了无需SfM的高效去模糊3D重建，结合事件相机的优势，在质量和效率上都取得了显著提升

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [84] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet是一个基于U-Net的卷积神经网络框架，通过协同整合频域和空间域特征来有效去除图像中的莫尔条纹伪影。


<details>
  <summary>Details</summary>
Motivation: 莫尔条纹是由显示器像素网格和相机传感器网格之间的频谱混叠产生的，表现为各向异性、多尺度的伪影，对数字图像去莫尔处理构成重大挑战。

Method: MoiréNet引入两个关键组件：方向频率空间编码器（DFSE）通过方向差分卷积识别莫尔条纹方向，以及频率空间自适应选择器（FSAS）实现精确的特征自适应抑制。

Result: 大量实验表明，MoiréNet在公共和实际使用数据集上实现了最先进的性能，同时具有很高的参数效率。仅需5.513M参数，比ESDNet-L减少48%。

Conclusion: MoiréNet将卓越的恢复质量与参数效率相结合，非常适合资源受限的应用，包括智能手机摄影、工业成像和增强现实。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [85] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了频率感知音频-视觉分割（FAVS）框架，通过频率域分解和重组来解决音频和视觉模态在频率域的矛盾，包含FDED和SCMC两个关键模块，在三个基准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVS方法忽视了音频和视觉模态在频率域的固有矛盾——音频高频信号普遍存在干扰噪声，而视觉高频信号包含丰富的结构细节，忽略这些差异会导致次优性能。

Method: 提出FAVS框架，包含频率域增强分解器（FDED）模块和协同跨模态一致性（SCMC）模块。FDED采用基于残差的迭代频率分解来区分模态特定语义和结构特征，SCMC利用专家混合架构通过动态专家路由增强语义一致性和模态特定特征保留。

Result: 在三个基准数据集上的广泛实验表明，FAVS框架实现了最先进的性能，丰富的定性可视化进一步验证了FDED和SCMC模块的有效性。

Conclusion: 通过将AVS任务重新表述为频率域分解和重组问题，提出的FAVS框架能够有效处理音频和视觉模态的频率域矛盾，取得了优异的性能表现。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [86] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了可解释人工智能（xAI）在视觉感知任务中的四种代表性方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，分析了它们的机制、优缺点和评估指标，为未来研究和应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分析中表现出色但缺乏可解释性，这限制了其在关键应用中的可靠性。xAI领域旨在解决这一挑战，提供理解AI模型决策过程的方法。

Method: 本文采用文献综述方法，系统性地分析和比较了四种主要的xAI方法：显著性图、概念瓶颈模型、原型方法和混合方法。

Result: 通过分析发现，每种方法都有其独特的机制和适用场景，但也都存在局限性，需要根据具体应用场景选择合适的方法。

Conclusion: xAI对于提高AI模型的可信度和可靠性至关重要，未来研究需要继续探索更有效的可解释性方法，并建立标准化的评估体系。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [87] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪扩散概率模型（DDPM）的方法，通过改进噪声调度和时间步嵌入技术来生成高质量合成LiDAR数据，以增强自动驾驶车辆的3D视觉系统性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR提供高分辨率深度数据，但真实LiDAR数据采集耗时且易受噪声和稀疏性影响。需要生成合成数据来增强模型性能。

Method: 使用改进的DDPM模型，包含新颖的噪声调度和时间步嵌入技术，能够生成更真实的点云数据。在IAMCV和KITTI-360数据集上进行评估。

Result: 该方法在多种配置下优于现有最先进方法，能有效减轻噪声和稀疏LiDAR数据的影响，生成具有丰富空间关系和结构细节的多样化点云。

Conclusion: 提出的方法在自动驾驶感知任务中表现出色，为LiDAR数据增强提供了有效解决方案。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [88] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了AGSSP（异常引导自监督预训练）新范式，通过异常先验指导表示学习，解决金属表面缺陷检测中预训练-微调范式的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 传统预训练方法面临两大挑战：ImageNet等自然图像数据集存在显著领域差距，而直接在工业数据上进行自监督预训练又难以区分细微缺陷模式与复杂背景噪声。

Method: 采用两阶段框架：1）通过从异常图中蒸馏知识预训练模型骨干，捕获缺陷显著特征；2）使用异常图生成的伪缺陷框预训练检测器，与定位任务对齐。开发了知识增强方法生成高质量异常图，并收集了12万张工业图像数据集。

Result: 实验表明AGSSP在各种设置下持续提升性能，相比基于ImageNet的模型，mAP@0.5提升高达10%，mAP@0.5:0.95提升11.4%。

Conclusion: AGSSP有效解决了金属表面缺陷检测中的预训练困境，为工业视觉检测提供了新的预训练范式。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [89] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出了首个音频驱动的通用逼真头像合成方法，结合了人物无关的语音模型和新型通用头部头像先验（UHAP），能够同时处理几何变形和外观变化，实现高质量的唇部同步和表情细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将音频特征映射到几何变形，而忽略了音频相关的外观变化。本文旨在开发一个能够同时处理几何和外观变化的通用音频驱动头像模型。

Method: 使用跨身份多视角视频训练UHAP，通过中性扫描数据进行监督以捕捉身份特定细节。构建通用语音模型将原始音频输入直接映射到UHAP潜在表达空间，该空间编码了几何和外观变化。采用单目编码器进行个性化，通过UHAP解码生成逼真头像。

Result: 该方法能够生成具有精确唇部同步和细腻表情细节（如眉毛运动、视线转移和真实口腔外观）的逼真头像。在唇部同步准确性、图像质量和感知真实性等指标上均优于现有几何方法。

Conclusion: 这是首个能够进行详细外观建模和渲染的通用音频驱动头像模型，在多个评估指标上表现出色，为逼真头像合成提供了新的解决方案。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [90] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 开发了一个基于机器学习的模块化流水线，用于自动检测、跟踪和提取树突棘在3D+时间显微镜数据中的特征，解决了大规模分析树突棘结构动力学的挑战。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑兴奋性突触的关键结构组成部分，其大小可作为突触效能的指标。然而，在3D+时间显微镜数据中大规模分析树突棘的结构动力学仍然具有挑战性且劳动密集。

Method: 采用模块化机器学习流水线，结合基于transformer的检测模块、集成空间特征的深度跟踪组件、利用空间一致性的时间跟踪模块，以及量化生物学相关脊柱特性的特征提取单元。

Result: 在开源标记的脊柱数据和两个新发布的注释数据集上验证了该方法，这些数据集包括检测和深度跟踪数据以及时间跟踪数据（据我们所知是此类数据的首次发布）。

Conclusion: 该方法为可扩展的端到端树突棘动力学分析建立了基准，并发布了数据、代码和预训练权重以促进未来研究。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [91] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据，仅使用类别名称即可实现动态适应


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在标注数据稀缺场景下的分类问题，利用视觉语言模型和迁移学习来处理零样本分类挑战

Method: 基于置信度的伪标签策略，在测试数据上直接训练轻量级分类器。VLM识别高置信度样本，预训练视觉模型增强其特征表示，通过迭代训练捕获语义和视觉线索

Result: 在十个不同数据集上的实验表明，该方法优于基线零样本方法

Conclusion: 该方法无需VLM微调或大型语言模型，仅依赖视觉模型减少对语义表示的依赖，实现了有效的零样本分类

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [92] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含镜面环境的3D重建和新视角合成问题，通过利用镜面反射作为补充视角来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在含镜面环境中性能下降，主要关注镜面表面的对称映射而忽视了镜面反射携带的丰富信息。镜面反射可以提供补充视角，填补缺失细节并显著提升重建质量。

Method: 提出了ReflectiveGS方法，这是3D高斯泼溅的扩展，将镜面反射作为补充视角而非简单的对称伪影，从而增强场景几何并恢复缺失细节。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS指标和训练速度上均优于现有方法。

Conclusion: ReflectiveGS为镜面丰富环境中的3D重建设定了新的基准，证明了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [93] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的胆道定位方法，通过Yolo检测算法和GAN生成合成训练数据，旨在提高腹腔镜胆囊切除术中胆管的可视化，减少胆管损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、美容效果好，但存在较高的胆管损伤风险，严重影响患者生活质量和生存率。为了减少这种风险，需要改进术中胆管的可视化。

Method: 构建并标注图像数据库用于训练Yolo检测算法，采用经典数据增强技术，并利用生成对抗网络（GAN）生成部分合成训练数据集。

Result: 实验结果表明该方法能够有效定位胆道结构，讨论了实验结果和伦理考量。

Conclusion: 深度学习技术可以有效提高腹腔镜手术中胆管的可视化，为减少胆管损伤提供了有前景的解决方案。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [94] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS框架，通过多任务学习和提示机制实现电子显微镜图像中细胞器的领域自适应分割，支持无监督、弱监督和交互式分割。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中细胞器实例分割的标注效率问题，利用SAM的提示机制但克服其需要为每个实例提供提示的限制。

Method: 提出可提示的多任务框架，结合辅助中心点检测任务和提示引导对比学习，支持不同提示配置（全点、稀疏点、无点）。

Result: 在多个挑战性基准测试中，该方法在UDA、WDA和基于SAM的方法上都表现出优越性能。

Conclusion: Prompt-DAS框架为电子显微镜图像分割提供了一种灵活高效的解决方案，显著提升了领域自适应分割的性能。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [95] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种用于视觉语言模型的逐步推理框架（Chain of Step Reasoning），通过细粒度的推理步骤评估和强化学习，显著提升了多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理方法在视觉语言任务中通常采用粗粒度的推理链，难以进行细粒度结构化推理，且难以评估中间推理步骤的质量和奖励。

Method: 提出了一个简单有效的透明框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练，实现了对推理步骤质量的准确评估。

Result: 在具有挑战性的视觉语言基准测试中取得了显著改进，建立了强基线。通过详细的实证分析和消融研究，揭示了各组件的影响和推理时缩放的有趣特性。

Conclusion: 该工作为视觉语言模型提供了基准，并为更复杂的多模态推理提供了重要见解，相关数据集、PRM和代码将开源。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [96] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于SAM和ViT的弱监督语义分割方法，用于食品图像分割，仅需图像级标注即可训练，在FoodSeg103数据集上达到0.54 mIoU


<details>
  <summary>Details</summary>
Motivation: 利用SAM的零样本能力和ViT的注意力机制，减少对像素级标注的依赖，加速食品图像标注过程

Method: 使用ViT（Swin Transformer）生成类激活图作为SAM的提示，结合图像预处理和单/多掩码生成策略

Result: 在FoodSeg103数据集上平均每张图像生成2.4个掩码（不含背景），多掩码场景下mIoU达到0.54

Conclusion: 该方法可作为食品图像标注的加速工具，或集成到食品营养追踪应用中

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [97] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的U-Net分割架构，通过构建回声动力学图和心脏相位动力学注意力机制，实现超声心动图分割的时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图容易变形和产生斑点噪声，导致帧间分割抖动，即使单帧分割精度高，时间不稳定性也会削弱功能估计并影响临床解释性。

Method: 提出DyL-UNet框架，构建回声动力学图提取视频动态信息，采用多个Swin-Transformer编码器-解码器分支处理单帧图像，在跳跃连接处引入心脏相位动力学注意力机制。

Result: 在CAMUS和EchoNet-Dynamic数据集上的实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案，能够实现时间稳定且精确的分割。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [98] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个高效稀疏视图3D高斯泼溅重建框架，通过将扩散过程转移到小波域，仅在低分辨率LL子带应用扩散，高频子带使用轻量网络优化，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在稀疏视图下性能急剧下降，现有方法使用扩散模型修复渲染结果但计算成本高昂。

Method: 提出小波域扩散方法，对低分辨率LL子带应用扩散，高频子带使用轻量网络优化；采用在线随机掩码策略替代低效的留一法。

Result: 在Mip-NeRF 360和OmniObject3D数据集上实验表明，WaveletGaussian在保持竞争性渲染质量的同时大幅减少训练时间。

Conclusion: WaveletGaussian为稀疏视图3D高斯重建提供了更高效的解决方案，通过小波域分解和优化策略实现了计算效率的显著提升。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [99] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是对Sa2VA模型的改进版本，通过修正训练和推理过程中的不一致性，在多个视频分割基准上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA模型在视频对象分割任务中未能充分发挥潜力，主要原因是训练和推理过程存在不一致性。

Method: 提出了Sa2VA-i模型，通过修正训练和推理过程中的不一致性问题来改进原Sa2VA模型。

Result: Sa2VA-i在多个视频基准上创造了新的最先进结果：MeViS提升+11.6 J&F，Ref-YT-VOS提升+1.4，Ref-DAVIS提升+3.3，ReVOS提升+4.1。Sa2VA-i-1B模型在MeViS基准上与原始Sa2VA-26B模型表现相当。

Conclusion: 这项工作强调了看似微不足道的实现细节的重要性，为视频分割领域提供了有价值的见解。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [100] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，将多光谱数据以零样本方式输入到仅训练过RGB数据的通用多模态模型中，使这些模型能够理解专业的多光谱信号。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像在遥感应用中很重要，但现有的机器学习模型需要专门训练且成本高，而强大的通用多模态模型无法处理多光谱输入。

Method: 利用多模态模型对视觉空间的理解，将多光谱数据适配到该空间，并将领域特定信息作为指令注入模型，以零样本方式工作。

Result: 在Gemini2.5模型上验证，在土地覆盖和土地利用分类的遥感基准测试中获得了显著的零样本性能提升。

Conclusion: 该方法使地理空间专业人员能够轻松利用强大的多模态模型来处理非标准专业输入，加速工作流程。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该研究评估了多模态大语言模型在零样本设置下检测和描述交通事故的能力，通过集成先进视觉分析技术提升模型性能，证明了MLLMs在自动化交通监控系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对公共安全至关重要，但缺乏多样化的基础设施事故数据集限制了传统方法的可扩展性。研究旨在利用MLLMs的零样本能力，减少对大量标注数据的依赖。

Method: 使用CARLA模拟的DeepAccident数据集，评估Gemini 1.5/2.0、Gemma 3和Pixtral模型在事故识别和描述方面的性能。集成YOLO、Deep SORT和SAM等视觉分析技术到增强提示中。

Result: Pixtral表现最佳（F1-score 0.71，召回率83%），Gemini模型通过增强提示提高了精确度（如Gemini 1.5达90%）但F1和召回率下降，Gemma 3性能最平衡。

Conclusion: MLLMs与先进视觉分析技术结合在交通事故检测中具有显著潜力，可增强现实世界自动化交通监控系统的适用性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一个基于Transformer的在线长期点跟踪模型，通过架构改进、内存优化和合成训练策略提升性能和效率，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决长期点跟踪问题，需要在视频帧中实现一致的点识别，应对显著的外观变化、运动和遮挡，特别针对在线实时和流式应用场景。

Method: 扩展Track-On模型为Track-On2，采用因果处理框架，通过内存机制保持时间一致性，在推理时进行粗粒度块级分类后细化，并系统研究合成训练设置对内存行为的影响。

Result: 在五个合成和真实世界基准测试中取得最先进结果，超越先前的在线跟踪器甚至利用双向上下文的强离线方法。

Conclusion: 基于因果、内存架构并纯合成数据训练的方法为真实世界点跟踪提供了可扩展的有效解决方案。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个用于多相机、多光谱同步和实时检测海豹与北极熊的综合系统，可将数据集处理时间减少80%


<details>
  <summary>Details</summary>
Motivation: 为阿拉斯加周边海域的冰相关海豹航空调查提供更高效的数据处理方案

Method: 采用严格的校准和硬件同步技术，利用多光谱进行目标检测，所有数据都带有元数据注释，并将图像和检测结果映射到世界平面

Result: 相比之前的方法，处理时间减少了80%，实现了准确的调查区域估计和快速结果评估

Conclusion: KAMERA系统有望在科学界激发其他测绘和检测工作，所有软件、模型和原理图都已完全开源

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一种神经形态协同推理架构，通过联合优化空间和时间冗余，显著降低数据传输和边缘能耗，同时减少端到端延迟，实现高效的脉冲神经网络边缘部署。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在边缘计算中具有能效优势，但固定高时间步长导致延迟和能耗问题。边缘-云协同推理系统面临高延迟和特征传输成本挑战。

Method: 引入学习型脉冲驱动压缩模块减少数据传输，采用动态提前退出机制基于输出置信度自适应终止推理。在ResNet-18和VGG-16骨干网络上进行原型实现。

Result: 数据传输减少高达2048倍，边缘能耗降低超过90%，端到端延迟减少3倍，精度损失小于2%。在静态图像和神经形态事件流数据集上验证有效性。

Conclusion: NeuCODEX使脉冲神经网络在资源受限环境中实现实用、高性能部署，解决了边缘计算中的延迟和能耗瓶颈问题。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 本文提出了一种鲁棒的自监督立体匹配方法，通过引入视觉基础模型的先验知识和场景对应先验，解决了恶劣天气条件下传统方法性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督立体匹配方法在恶劣天气条件下性能显著下降，主要原因是CNN特征提取器在退化区域表现不佳，以及光度一致性假设在恶劣天气下失效。

Method: 提出从视觉基础模型注入鲁棒先验到CNN特征提取器，并引入场景对应先验构建鲁棒监督信号。通过创建包含清晰和恶劣天气图像对的合成数据集，提出两阶段训练范式：鲁棒自监督场景对应学习和恶劣天气蒸馏。

Result: 大量实验证明该方法在恶劣天气条件下显著优于现有最先进的自监督方法，展现出有效性和通用性。

Conclusion: 通过结合视觉基础模型的先验知识和场景对应学习，本文提出的方法能够有效提升立体匹配在恶劣天气条件下的鲁棒性，为实际应用提供了可靠解决方案。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 提出了YOLO-LAN，一种基于YOLO的息肉检测管道，使用M2IoU损失、数据增强和负数据训练，在Kvasir-seg和BKAI-IGH NeoPolyp数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌始于结肠内壁的息肉生长，结肠镜检查是标准检测方法，但人工检测存在不一致性和漏检问题，需要更准确、实时的深度学习解决方案。

Method: 基于YOLO框架构建息肉检测管道，采用M2IoU损失函数、多样化数据增强技术和负数据训练，模拟真实临床场景。

Result: 在Kvasir-seg数据集上，YOLOv12达到mAP50为0.9619，mAP50:95为0.8599；YOLOv8达到mAP50为0.9540，mAP50:95为0.8487，显著提升了检测精度。

Conclusion: 该方法在息肉大小和精确定位检测方面表现出鲁棒性，具有临床相关性，可用于AI辅助结直肠癌筛查。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本文分析了MOSEv2赛道中的SeC框架，展示了长期记忆和概念感知记忆在视频对象分割中的优势，并在LSVOS挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 探索复杂半监督视频对象分割中的长期记忆和概念感知记忆机制，以解决遮挡和重现等核心挑战。

Method: 分析和改进SeC框架（增强版SAM-2），研究其长期记忆保持时间连续性和概念感知记忆提供语义先验的能力。

Result: 在MOSEv2测试集上获得39.89%的JF分数，在LSVOS挑战赛MOSEv2赛道中排名第一。

Conclusion: 长期记忆和概念感知记忆的结合能有效提升视频对象分割性能，特别是在处理遮挡和干扰物方面表现出色。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 该论文分析了视觉语言模型(VLMs)的视觉处理机制，基于人类视觉的双流假说将视觉处理分解为物体识别和空间感知两个部分进行研究，并提出了提升解码效率和空间推理能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs通过序列化图像处理视觉信息，这与人类视觉的并行处理方式存在差异，且模型内部机制不透明，阻碍了深入理解和架构创新。

Method: 将视觉处理分解为物体识别和空间感知：1）将图像转换为文本标记图研究物体识别过程；2）理论推导和实证验证VLMs位置表示的几何结构。基于发现提出了指令无关的标记压缩算法和RoPE缩放技术。

Result: 研究发现模型对图像内容的感知呈现从浅层到深层的两阶段过程，从属性识别到语义消歧。验证了VLMs位置表示的几何结构，提出的方法有效提升了解码效率和空间推理能力。

Conclusion: 该工作验证了对VLM内部机制的分析，为深入理解VLMs提供了新视角，并为设计更强大的未来架构提供了明确原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无需视觉编码器的文本到文本检索方法，通过VLLM生成结构化图像描述来替代传统文本到图像检索，显著减少模态差距并提升组合性。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习的视觉语言模型存在浅层语言理解、模态差距大、计算成本高和隐私问题等限制。

Method: 采用文本到文本检索范式，使用VLLM生成结构化图像描述，仅需少量GPU时间进行校准。

Result: 该方法在多个检索和组合性基准测试中达到最先进的零样本性能，甚至超越传统多模态模型。

Conclusion: 视觉无关的检索方法不仅性能优越，还提供了更隐私友好的替代方案，证明了在检索任务中视觉编码器并非必要。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本文研究了对比视觉语言模型中组合性与长标题理解之间的关系，发现两者存在双向促进关系，但需要高质量数据和适当模型设计的支持。


<details>
  <summary>Details</summary>
Motivation: 当前对比视觉语言模型在绑定视觉和文本信息方面取得进展，但理解长而密集的标题仍是一个挑战。作者假设组合性（推理对象-属性绑定和对象间关系的能力）是理解长标题的关键。

Method: 训练和评估了一系列针对组合性和长标题理解能力的模型，研究了不同训练策略和数据质量对模型性能的影响。

Result: 发现组合性训练能提高长标题检索性能，长标题训练也能促进组合性，但这些收益对数据质量和模型设计敏感。高质量长标题数据训练可获得两种任务的强性能。

Conclusion: 组合性理解和长标题理解是相互交织的能力，可以通过在密集、基于基础的描述上进行训练来共同学习，为改进VLM泛化提供了实用指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出了一种利用合成RGB图像、少量真实标注和GAN跨模态对齐来增强热图像语义分割的框架，在复杂田间环境中显著提升了植物分割性能。


<details>
  <summary>Details</summary>
Motivation: 热图像中的植物分割在户外高通量表型分析中面临挑战，主要是植物与杂草之间的低对比度和频繁遮挡问题。

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，采用CycleGAN-turbo进行RGB到热图像的跨模态对齐。

Result: 与全真实数据基线相比，杂草类别相对改进22%，植物类别相对改进17%。

Conclusion: 结合合成数据、有限手动标注和生成模型的跨域翻译可以显著提升复杂田间环境中多模态图像的语义分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: 提出了HyKid数据集，这是一个包含48名儿童脑积水患者的开源数据集，包含3D MRI图像和手动校正的脑组织分割，特别是脉络丛分割，为脑积水评估提供了高质量基准。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估具有挑战性，现有研究缺乏公开的专家标注数据集，特别是包含脉络丛分割的数据集。

Method: 收集48名儿科脑积水患者的3D MRI图像，使用切片到体积算法从常规低分辨率图像重建1mm各向同性分辨率图像，由经验丰富的神经学家手动校正脑组织分割，并使用检索增强生成框架从临床放射学报告中提取结构化数据。

Result: 脉络丛体积与总脑脊液体积之间存在强相关性，为脑积水评估提供了潜在生物标志物，预测模型表现出色（AUC = 0.87）。

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，并揭示了脉络丛相关特征在脑积水评估中的重要性。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出了一种多尺度特征交互网络（MsFIN）用于从行车记录仪视频中进行早期事故预测，通过多尺度特征聚合、时序特征处理和后融合来解决交通参与者特征交互和复杂多时序行为建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着行车记录仪的普及和计算机视觉技术的发展，从行车记录仪视角开发事故预测模型对于主动安全干预变得至关重要。但存在两个关键挑战：建模交通参与者之间的特征级交互（在行车记录仪视图中经常被遮挡）和捕捉事故前复杂、异步的多时序行为线索。

Method: MsFIN包含三个层次：多尺度特征聚合层（设计多尺度模块提取短、中、长期时序尺度的场景表示，利用Transformer架构促进全面特征交互）、时序特征处理层（在因果约束下捕捉场景和对象特征的顺序演化）和多尺度特征后融合层（融合多个时序尺度的场景和对象特征生成全面的风险表示）。

Result: 在DAD和DADA数据集上的实验表明，MsFIN在预测准确性和及时性方面显著优于使用单尺度特征提取的最先进模型。消融研究验证了MsFIN中每个模块的有效性。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越的性能，为行车记录仪视角的早期事故预测提供了有效解决方案。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种基于持续学习的数字人脸伪造检测方法，采用发展性混合专家架构，通过Real-LoRA学习真实人脸特征，多个Fake-LoRA捕捉不同伪造类型的增量信息，有效应对不断演变的伪造技术。


<details>
  <summary>Details</summary>
Motivation: 数字人脸生成和操纵技术的快速发展带来了严重的社会风险，现有检测模型难以跟上伪造技术的快速演变。需要使模型能够快速适应新领域，同时避免遗忘已学习的伪造类型。

Method: 采用发展性混合专家架构，使用LoRA模型作为专家。分为Real-LoRA组学习真实人脸知识，多个Fake-LoRA组捕捉不同伪造类型的增量信息。通过正交梯度和正交损失防止梯度干扰和灾难性遗忘。

Result: 在数据集和操纵类型增量协议下的实验结果表明该方法具有有效性。

Conclusion: 将人脸伪造检测构建为持续学习问题，提出的发展性MoE架构能够有效应对不断演变的伪造技术，在保持计算和数据效率的同时防止知识遗忘。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解和生成任务，具备物体定位、图像编辑和高分辨率图像合成等新能力，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型仅支持简单的图像级理解任务和低分辨率图像生成，缺乏高级功能如物体定位和高质量图像编辑。Lavida-O旨在填补这一空白，通过统一模型实现更强大的多模态能力。

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过规划和迭代自反思机制，利用理解能力提升图像生成和编辑效果。

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等基准测试中达到最先进性能，超越Qwen2.5-VL和FluxKontext-dev等模型，并在推理时提供显著加速。

Conclusion: Lavida-O作为首个统一的掩码扩散模型，成功展示了理解能力如何通过规划和自反思机制提升生成任务性能，为多模态AI系统的发展提供了新方向。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 提出了基于概念的视频相似度估计任务ConViS，通过预定义的关键语义概念计算可解释的相似度分数，并构建了ConViS-Bench基准数据集来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频相似度评估通常依赖全局相似度分数，无法像人类那样从多个不同维度（如动作、地点等）进行细粒度比较。大型多模态模型的出现为利用自然语言进行视频比较任务提供了新机会。

Method: 引入ConViS任务框架，通过预定义的关键语义概念对视频对进行可解释的相似度评分。构建ConViS-Bench基准数据集，包含精心标注的视频对，每个视频对都有概念级相似度分数以及差异和相似性的文本描述。

Result: 在ConViS基准上对多个最先进模型进行基准测试，结果显示不同模型在概念相似度估计上存在显著性能差异，某些概念对模型更具挑战性。

Conclusion: ConViS-Bench将成为语言驱动视频理解研究的重要资源，有助于推动视频相似度评估向更细粒度、更符合人类认知的方向发展。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化压缩时空热图，有效解决人体运动重建中的运动模糊和时间不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动理解在计算机视觉中具有高维度和内在冗余性挑战，需要高效的压缩和表示方法来分析复杂运动动态。

Method: 结合密集运动标记化和对抗性精炼的VQ-GAN框架，通过密集标记化压缩时空热图，利用对抗训练消除重建伪影。

Result: 在CMU Panoptic数据集上，方法比dVAE基线SSIM提升9.31%，时间不稳定性降低37.1%。发现2D运动最优表示为128个标记词汇，3D运动需要1024个标记代码本。

Conclusion: 该方法为多样化运动分析应用提供了实际部署可行性，证明了密集标记化策略在运动复杂度分析中的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 本文提出了一种新的图放射组学学习（GrRAiL）描述符，用于在临床MRI扫描中表征病灶内异质性，通过图论方法量化病灶内空间关系，在区分肿瘤复发与放射性效应方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决常规影像学中可靠区分恶性肿瘤与混淆病理的挑战，传统放射组学方法往往忽略了病灶内复杂空间关系，需要更有效的方法来表征病灶内异质性。

Method: GrRAiL方法首先基于逐体素放射组学测量识别亚区域簇，然后计算图论指标来量化簇间的空间关联，生成的加权图编码了病灶内的高阶空间关系。

Result: 在947名受试者的多中心评估中，GrRAiL在三个应用场景中均显著优于现有方法：胶质母细胞瘤中测试准确率78%（提升>10%），脑转移瘤中74%（提升>13%），胰腺IPMN风险分层中75%（提升>10%）。

Conclusion: GrRAiL通过图论方法有效捕捉病灶内异质性，在区分恶性肿瘤与混淆病理方面表现出优越性能，具有临床可行性。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是第一个仅使用自我中心视觉感知环境并导航的人类化身，通过将低级运动技能学习与高级视觉控制分离来生成人类化运动。


<details>
  <summary>Details</summary>
Motivation: 当前人类运动生成方法忽视了感知与运动之间的相互依赖关系，使用与人类感知截然不同的任务特定感知。作者认为生成人类化化身行为需要人类化感知。

Method: 首先在大型运动捕捉数据集上训练运动先验模型，然后使用Q学习训练策略，将自我中心视觉输入映射到运动先验的高级控制命令。

Result: 实验证明自我中心视觉能够产生人类化运动特征，例如化身能够根据视觉场中的障碍物调整行走路径。

Conclusion: 为化身配备人类化传感器（特别是自我中心视觉）有望训练出行为像人类的化身。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 该论文针对布局到图像生成中边界框重叠问题，提出了OverLayScore量化指标和OverLayBench基准，并开发了CreatiLayout-AM模型来改善重叠区域的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理边界框显著重叠时表现不佳，特别是面对大重叠区域和语义区分度小的重叠实例时。现有基准存在偏向简单情况的偏差，无法有效评估模型在挑战性条件下的性能。

Method: 1) 提出OverLayScore指标量化边界框重叠复杂度；2) 构建OverLayBench基准，包含高质量标注和平衡的OverLayScore分布；3) 开发CreatiLayout-AM模型，在精选的amodal掩码数据集上进行微调。

Result: 分析表明现有基准偏向低OverLayScore的简单情况。新基准提供了更全面的评估框架，初步模型改进显示在处理复杂重叠场景上的潜力。

Conclusion: 该研究为在现实和挑战性场景下实现更鲁棒的布局到图像生成奠定了基础，通过系统性评估指标和基准推动了该领域的发展。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式的3D高斯溅射表示中，无需多视图训练数据即可生成3D场景


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界的多视图数据，但这些数据并不总是容易获得。视频扩散模型具有强大的想象力，但其2D特性限制了在需要与环境交互的仿真应用中的使用

Method: 通过在RGB解码器基础上增加3D高斯溅射解码器，该解码器由RGB解码器的输出进行监督训练，完全使用视频扩散模型生成的合成数据进行训练

Result: 实验结果表明，该框架在静态和动态3D场景生成方面达到了最先进的性能

Conclusion: 该框架能够从文本提示或单张图像实时合成3D场景，并可扩展到从单目输入视频生成动态3D场景

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视图前馈3D高斯溅射方法，用体素对齐的高斯预测替代传统的像素对齐方法，解决了像素对齐的局限性，实现了更鲁棒的多视图一致性和更好的新视角渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的像素对齐高斯预测方法存在多个固有局限性：重建的3D模型严重依赖输入视图数量、产生视角偏置的密度分布、在遮挡或低纹理区域引入对齐误差。

Method: VolSplat采用体素对齐的高斯预测范式，直接从预测的3D体素网格预测高斯分布，避免了对误差敏感的2D特征匹配，实现了自适应的高斯密度控制。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat实现了最先进的性能，产生了更合理和视角一致的高斯重建结果，渲染质量显著提升。

Conclusion: 该方法为前馈3D重建建立了一个更可扩展的框架，提供了更密集和鲁棒的表示，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种条件感知重参数化方法，通过调整源分布和目标分布来缩短概率路径，提升条件生成模型的训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散和流匹配方法需要模型同时学习质量传输和条件注入，这增加了模型的学习负担。作者希望通过调整源分布和目标分布来简化模型的学习任务。

Method: 提出CAR-Flow方法，通过学习一个轻量级的偏移量来条件化源分布、目标分布或两者，从而缩短概率路径。

Result: 在低维合成数据上可视化和量化了CAR的效果；在ImageNet-256数据集上，将SiT-XL/2模型配备CAR-Flow后，FID从2.07降至1.68，仅增加不到0.6%的参数。

Conclusion: CAR-Flow通过条件感知重参数化有效简化了条件生成模型的学习任务，在保持参数效率的同时显著提升了生成质量。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 本研究提出了一种统一的多任务学习框架，通过动态提示调度机制解决大语言模型在多任务和跨域设置下的泛化限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法如SPoT依赖固定提示模板，在多任务和跨域场景下存在泛化能力不足的问题，需要更灵活的提示调度机制。

Method: 采用动态提示调度机制，包括提示池和任务感知调度策略，通过任务嵌入和门控机制实现提示信号的精细控制，并结合联合多任务学习优化目标。

Result: 实验表明该方法在语言理解和知识推理任务上显著提升性能，有效保持模型稳定性并增强可迁移性。

Conclusion: 提出的动态提示调度机制在多任务建模和跨域适应方面具有显著优势和应用价值。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [125] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS是一个评估LLM数学能力的基准测试，涵盖12个核心技能维度，分为三个领域：知识与理解、问题解决与沟通、元技能与创造力。通过分类问题和设计任务来隔离特定能力，GAUSS构建了全面、细粒度且可解释的模型数学能力画像。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估LLM数学基础技能的多维度基准测试，需要一种能够揭示模型数学智能底层结构的评估方法。

Method: 将数学问题按认知技能分类，设计能够隔离特定能力的任务，构建包含12个技能维度的评估框架，分为三个主要领域。

Result: 通过GAUSS基准测试，成功构建了GPT-5-thinking的技能画像，揭示了其优势、劣势以及与o4-mini-high的差异。

Conclusion: GAUSS基准测试提供了多维度的、基于技能的评估方法，能够更准确地反映LLM的数学智能水平，为模型评估提供了新的视角。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [126] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本文提出了一种基于Rubin因果模型的事件因果关系识别方法，通过将第一个事件视为治疗、第二个事件视为结果，利用合成控制方法生成虚拟双胞胎来估计因果效应，在COPES-hard基准测试中表现优于包括GPT-4在内的现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的事件因果关系识别方法主要依赖语言模式和多跳关系推理，容易因因果关系的非正式使用和虚假的图推理而产生错误识别。需要更稳健的方法来区分因果关系和相关关系。

Method: 采用Rubin因果模型框架，将时序上先发生的事件视为治疗，后发生的事件视为结果。为了解决文本领域无法实际实施干预的问题，使用合成控制方法从相关历史数据中生成虚拟双胞胎，利用文本嵌入合成和反演技术来估计因果效应。

Result: 该方法在因果关系基准测试COPES-hard上表现优于包括GPT-4在内的现有方法，能够更稳健地识别事件间的因果关系。

Conclusion: 基于Rubin因果模型的合成控制方法为事件因果关系识别提供了更可靠的框架，通过概念性干预和虚拟双胞胎生成技术，有效解决了文本领域因果推断的实践难题。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [127] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA是一种自动提示优化框架，通过联合优化系统提示和用户提示，使用结构化评分标准和快速迭代，在少量样本下实现高效提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法通常只关注用户提示，依赖非结构化反馈，需要大量样本和长迭代周期，导致成本高且脆弱。

Method: ZERA框架使用八个可泛化标准对提示进行结构化评分，基于这些结构化批评修订提示，实现快速收敛到高质量提示。

Result: 在五个LLM和九个多样化数据集上的实验结果显示，ZERA相比强基线方法有持续改进。消融研究验证了各组件对有效提示构建的贡献。

Conclusion: ZERA通过联合优化系统提示和用户提示，使用结构化评分和快速迭代，实现了高效、低成本的自动提示优化。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [128] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文研究了外部信息对具有逐步思考能力的大语言模型推理过程的因果影响，发现模型的思考模式是一把双刃剑：有帮助的信息能提高准确性，但误导性信息会导致性能灾难性下降。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，LLMs经常被增强以包含可能有帮助、无关甚至误导的外部信息，需要系统评估这些辅助信息对模型推理过程的影响。

Method: 引入SciAux数据集（基于ScienceQA），系统测试模型对三种类型信息（有帮助、无关、误导）的鲁棒性。

Result: 研究发现模型的思考过程会放大误导信息的负面影响，思考不仅没有提供鲁棒性，反而强化了错误程度。

Conclusion: 关键挑战不仅在于让模型"思考"，更要赋予它们评估推理所依据信息的批判性能力。

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [129] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: 提出了一种过程监督的多智能体框架来优化检索增强生成（RAG）系统中检索器和生成器之间的协调问题，通过决策制定器和知识选择器两个轻量级智能体，结合过程级奖励和树状结构探索策略，实现了更准确、稳定和可解释的RAG系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统中检索器和生成器独立开发导致交互不理想：检索器可能返回不相关或冗余文档，生成器未能充分利用检索到的证据。需要解决两者之间的协调问题。

Method: 1. 引入两个轻量级智能体：决策制定器（决定何时继续检索或停止生成答案）和知识选择器（过滤检索文档保留最有用的证据）
2. 使用LLM-as-a-Judge提供过程级监督，评估每个中间动作
3. 采用树状结构展开策略探索多样化推理路径
4. 使用PPO算法端到端训练两个智能体

Result: 在单跳和多跳问答基准测试中，该方法相比标准RAG基线实现了更高的准确率、更稳定的收敛性，并产生了更可解释的推理轨迹。

Conclusion: 该框架是模块化和即插即用的，无需修改检索器或生成器，适用于实际RAG应用，有效解决了检索器与生成器之间的协调问题。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [130] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: 提出了一种新颖的对话情感识别和预测架构ERFC，用于预测未来话语的情感，在呼叫中心等场景中具有重要商业价值


<details>
  <summary>Details</summary>
Motivation: 在呼叫中心等对话场景中，了解未来话语的情感可以帮助客服人员及时调整策略，提升客户体验，将不满客户转化为满意客户

Method: ERFC架构考虑多模态、情感的不同属性、上下文以及对话中说话者话语之间的相互依赖关系

Result: 在IEMOCAP数据集上的密集实验证明了ERFC的可行性

Conclusion: 该方法在呼叫中心等客户满意度至关重要的应用中具有巨大的商业价值

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [131] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 本文评估了八个开源LLM检测反犹内容的能力，提出了Guided-CoT提示技术，显著提升了模型性能，并分析了LLM在效用、可解释性和可靠性方面的差异。


<details>
  <summary>Details</summary>
Motivation: 检测仇恨内容是重要且具有挑战性的任务，需要自动化工具持续适应社交媒体环境的变化。

Method: 利用上下文定义作为政策指南，探索多种提示技术，设计了新的类似思维链的提示方法Guided-CoT，并量化模型生成理由的语义差异。

Result: Guided-CoT显著提升了所有评估模型的性能，Llama 3.1 70B超越了微调的GPT-3.5，同时揭示了LLM在语义分歧和矛盾行为方面的显著差异。

Conclusion: 实验凸显了不同LLM在效用、可解释性和可靠性方面的差异，为仇恨内容检测提供了新的技术路径和分析框架。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [132] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: TEMPO是一种无critic的强化学习算法，通过前缀树结构进行非参数化信用分配，在数学和医学QA任务中优于PPO和GRPO


<details>
  <summary>Details</summary>
Motivation: 解决长序列推理任务中稀疏延迟奖励的信用分配问题，特别是在可验证奖励设置下，只有少数决策token对结果有显著影响

Method: 提出Prefix-to-Tree(P2T)方法将响应组转换为前缀树，计算非参数化前缀值，并基于此开发TEMPO算法，在分支token处加入时间差分修正

Result: 在Qwen3-1.7B/4B模型上，TEMPO在分布内(MATH、MedQA)和分布外(GSM-HARD、AMC23等)基准测试中均优于PPO和GRPO

Conclusion: TEMPO提供精确的token级信用分配，无需学习价值网络或额外评判器，在相同训练时间内达到更高验证准确率

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [133] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 该论文探索了将LLMs作为知识图谱推理路径的奖励模型，用于医疗诊断推理，通过验证推理路径而非生成路径来提升诊断可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过检索增强生成或微调将知识图谱内容插入提示中，但缺乏结构化推理能力。受验证解决方案比生成更容易的计算理论启发，以及医生诊断评估过程的启发，研究将LLM作为奖励模型来评判知识路径是否支持正确诊断。

Method: 首先系统评估了五种知识路径评判任务形式和八种训练范式，然后测试路径评判能力是否泛化到下游诊断任务（诊断总结和医疗问答）。使用三个开源指令调优LLM进行实验。

Result: 实验显示特定奖励优化和蒸馏能带来强大的路径评判性能，但向下游任务的迁移性较弱，表现出既有希望又脆弱的特性。

Conclusion: 这是对临床知识图谱进行'奖励模型风格'推理的首次系统评估，为结构化、基于奖励的监督如何影响医疗GenAI系统的诊断推理提供了见解。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [134] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec是一种无需训练、无损的加速参数卸载方法，通过生成低比特量化替代层来构建高度对齐的草稿模型，在内存受限的GPU上实现大语言模型的高效推理加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在内存有限的消费级GPU上部署面临挑战，现有压缩方法会降低质量，而参数卸载方法虽然保持质量但推理速度慢。需要一种既能保持质量又能显著加速推理的方法。

Method: 提出SubSpec方法：1）从卸载的目标LLM部分生成低比特量化替代层构建高度对齐的草稿模型；2）共享剩余的GPU驻留层和KV-Cache以减少内存开销并增强对齐；3）使用推测解码技术，通过草稿模型提出多个令牌并由目标LLM并行验证。

Result: 在8GB VRAM限制下，Qwen2.5 7B在MT-Bench上实现9.1倍加速；在24GB VRAM限制下，Qwen2.5 32B在流行生成基准测试中平均实现12.5倍加速，达到了较高的平均接受长度。

Conclusion: SubSpec是一种即插即用的参数卸载加速方法，无需额外训练即可实现无损加速，通过构建高度对齐的量化草稿模型和共享策略，显著提升了内存受限环境下的LLM推理效率。

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [135] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign是一种并行语音文档对齐方法，通过单调对齐语音段嵌入，不依赖文本转录，相比基线方法产生更长的语音对齐且噪声更少。


<details>
  <summary>Details</summary>
Motivation: 开发不依赖文本转录的语音文档对齐方法，提高语音到语音翻译的数据质量，减少对大量原始语音文档的依赖。

Method: 使用单调对齐语音段嵌入的方法，与Global Mining和Local Mining两种语音挖掘变体进行比较。

Result: 在3000小时未标记的英德平行语音文档上应用，获得约1000小时高质量对齐。En-to-De和De-to-En性能分别比Global Mining提高0.37和0.18 ASR-BLEU。

Conclusion: Speech Vecalign在减少8倍原始语音文档的情况下，匹配或超越了SpeechMatrix模型的性能，证明了其有效性和效率。

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [136] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出一个LLM辅助的说话人日志校正系统，通过实时用户反馈来修正说话人归属错误，显著降低说话人日志错误率


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音处理系统在"开环"模式下运行，缺乏用户反馈，而人在回路的工作流程可以显著提高准确性

Method: 系统执行流式ASR和说话人日志，使用LLM提供简洁摘要给用户，接受简短语音反馈并立即整合。开发了SWM技术检测和分割多说话人段，以及基于用户校正的在线说话人注册

Result: 在AMI测试集上的LLM驱动模拟显示，系统显著降低DER 9.92%和说话人混淆错误44.23%

Conclusion: 该系统通过实时用户反馈和在线说话人注册，有效提高了说话人日志的准确性，为人在回路的语音处理系统提供了可行方案

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [137] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: NormGenesis是一个多文化对话生成框架，通过Violation-to-Resolution对话类型建模社会规范违反到修复的过程，提升对话系统在跨文化语境中的社会适应性。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统缺乏对社会规范的深入理解，特别是在多文化语境下。需要构建能够识别和修复规范违反的对话模型，以提高对话的社会可接受性和文化适应性。

Method: 提出Violation-to-Resolution对话类型，采用基于范例的迭代精炼方法，在对话合成早期引入语言、情感和社会文化对齐。构建包含10,800个多轮对话的数据集，标注规范遵守、说话者意图和情感响应。

Result: 人类和LLM评估显示，NormGenesis在精炼质量、对话自然度和泛化性能上显著优于现有数据集。使用V2R增强数据训练的模型在伦理敏感情境中表现出更好的语用能力。

Conclusion: 该工作为文化自适应对话建模建立了新基准，提供了跨语言和文化多样化语言的规范感知生成的可扩展方法。

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [138] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本文评估了大语言模型生成富含文化相关表达的波斯文学文本的能力，通过构建包含20个主题的数据集，并采用托兰斯创造力测试的四个维度进行自动化评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语文学创作，缺乏对非英语文学传统的标准化创造力评估方法，特别是波斯文学。

Method: 构建波斯文学数据集，采用LLM作为自动评分工具，通过类内相关系数验证其与人工评分的一致性，并分析模型对四种文学手法的理解能力。

Result: LLM评分与人工评分具有强一致性，模型在波斯文学文本生成方面展现出优势但也存在局限性。

Conclusion: LLMs在波斯文学创作中表现出潜力，但仍需进一步改进，特别是在文化相关表达和文学手法运用方面。

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [139] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究开发了一种自动化方法来测量医患共同决策(SDM)，通过语言建模和对话对齐(CA)评分，利用深度学习模型和微调的BERT模型来分析医患对话，发现CA评分与SDM结果显著相关。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏自动测量医患共同决策(SDM)的方法，而SDM对于实现以患者为中心的护理至关重要。本研究旨在开发一种可扩展的自动化方法来测量SDM。

Method: 使用157个视频记录的医患对话转录成42,559个句子，采用上下文-响应对和负采样训练深度学习模型和微调的BERT模型，通过下一句预测任务计算四种CA评分，并分析CA评分与SDM结果(决策冲突量表和OPTION12评分)的关联。

Result: 深度学习模型和微调的BERTbase模型在对话对齐任务上表现良好，其中BERTbase达到最高recall@1为0.640。CA评分与OPTION12和决策冲突量表显著相关，模型大小不影响CA评分与SDM的关联。

Conclusion: 本研究引入了一种自动化、可扩展的方法来通过可解释的CA评分测量医患共同决策，具有大规模评估SDM策略的潜力。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [140] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: CogniLoad是一个基于认知负荷理论(CLT)的合成基准测试，用于精确分析LLM的长上下文推理能力，通过独立调节内在难度、干扰因子比例和任务长度三个维度来系统评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文推理基准测试往往模糊了关键因素（如任务内在复杂性、干扰因子影响和任务长度），无法进行精确的失败分析。

Method: 基于认知负荷理论(CLT)，设计自然语言逻辑谜题生成系统，可独立调节三个核心维度：内在难度(d)控制内在负荷，干扰因子比例(ρ)调节外部负荷，任务长度(N)作为相关负荷的代理指标。

Result: 评估22个最先进的推理LLM，发现任务长度是主要约束因素，模型对内在复杂性有不同容忍度，对干扰因子比例呈现U型响应。

Conclusion: CogniLoad通过系统控制认知负荷维度，为剖析LLM推理局限性和指导未来模型开发提供了可重现、可扩展且诊断性强的工具。

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [141] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: LAWCAT是一种线性注意力框架，通过将预训练transformer转换为线性复杂度架构，显著降低计算成本并扩展上下文窗口，仅需少量训练数据即可实现高性能长上下文处理。


<details>
  <summary>Details</summary>
Motivation: 传统transformer的二次计算复杂度成为长上下文应用的瓶颈，而从头训练线性复杂度模型又需要大量资源。LAWCAT旨在高效迁移预训练transformer能力到线性架构。

Method: 集成因果Conv1D层增强局部依赖建模，使用归一化门控线性注意力提升不同上下文长度的泛化能力，通过知识蒸馏从预训练模型转移能力。

Result: 仅用1K长度序列蒸馏Mistral-7B，在22K tokens内实现90%+的passkey检索准确率；Llama3.2-1B变体在多个长上下文基准上表现优异，预训练token需求减少99.9%以上；在8K+序列上预填充速度超过FlashAttention-2。

Conclusion: LAWCAT为边缘部署提供了高效的高性能长上下文线性模型路径，减少了对大量长序列训练数据和计算资源的依赖。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [142] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文通过大规模系统评估，分析了LLM在图数据上的能力，发现代码生成方法在图形推理任务中表现最佳，特别是在长文本或高度数图上，且所有交互策略在异质图上都有效。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM与图数据交互能力的系统性理解，特别是在文本丰富的图机器学习任务中，需要评估不同交互模式在不同图结构下的表现。

Method: 通过控制多个关键变量轴进行大规模评估，包括LLM-图交互模式（提示、工具使用、代码生成）、数据集域、结构机制、特征特性和模型配置，并系统性地截断特征、删除边和移除标签来量化输入依赖。

Result: 1）代码生成方法整体表现最强，在长文本或高度数图上优势明显；2）所有交互策略在异质图上都有效；3）代码生成能灵活调整对结构、特征或标签的依赖以利用最信息丰富的输入类型。

Conclusion: 研究为当前LLM-图交互模式提供了全面的优劣势分析，并为未来方法设计提供了关键原则，特别是代码生成方法的灵活性和适应性优势。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [143] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种使用ByT5模型在阿拉伯诗歌中插入短语以符合特定韵律的方法，通过基于规则的字符到节拍转换和条件去噪目标训练，实现了高韵律对齐和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自动生成符合古典阿拉伯诗歌韵律的文本插入方法，以支持诗歌创作过程中的协同创作应用。

Method: 采用基于规则的字符到节拍转换提取韵律，使用条件去噪目标微调ByT5模型，采用课程学习策略（先在通用阿拉伯语数据集预训练，再在诗歌数据集微调），并探索从英语到阿拉伯语的跨语言迁移。

Result: 实验结果表明，所提出的模型在保持语义连贯性的同时实现了高韵律对齐。

Conclusion: 该模型在古典阿拉伯诗歌创作过程中具有协同创作应用的潜力。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [144] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: 提出了一种轻量级框架，通过分析文本内部结构来检测原始和经过提示修改的AI生成文本，解决了现有词级检测器对改写敏感、存在偏见等问题。


<details>
  <summary>Details</summary>
Motivation: ChatGPT的广泛使用引发了对AI生成文本滥用的担忧，现有词级检测器容易受到改写攻击，存在ChatGPT词级模式和训练数据偏见，且需要大型模型或在线LLM交互。

Method: 使用预训练语言模型编码句子嵌入，通过注意力机制建模句子间关系，采用对比学习减轻自回归生成的嵌入偏见，并结合因果图与反事实方法从主题相关偏见中分离结构特征。

Result: 在两个精心策划的数据集（包括摘要比较和修订的生活FAQ）上的实验验证了该方法的有效性。

Conclusion: 提出的基于文本内部结构的轻量级框架能够有效检测AI生成文本，对词级修改具有鲁棒性，解决了现有检测器的局限性。

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [145] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: CCQA是一种针对小型语言模型的新型推理方法，通过循环一致性原理生成问题并评估相似度来选择最佳答案


<details>
  <summary>Details</summary>
Motivation: 现有的推理策略在大模型上有效，但在小模型上效果不佳，需要开发专门适用于小模型的高效推理方法

Method: 基于循环一致性原理，从推理路径和答案生成问题，评估与原问题的相似度，选择相似度最高的候选方案作为最终答案；使用专门的Flan-T5模型进行问题生成

Result: 在数学和常识推理基准测试中，CCQA在八个模型上一致优于现有最先进方法

Conclusion: 该方法为小语言模型的高效推理建立了新的实用基准

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [146] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出了一种基于先验的数据过滤方法，使用语料库级词频统计来估计标记先验，作为困惑度（PPL）过滤的快速替代方案，无需模型推理即可实现高效数据筛选。


<details>
  <summary>Details</summary>
Motivation: 传统基于困惑度的数据过滤方法存在时间成本高、对噪声或分布外样本不可靠的问题，需要一种更高效可靠的替代方案。

Method: 利用语料库级词频统计估计标记先验，通过标记先验的均值和标准差来过滤文档，该方法受语言学中词角色和词汇密度见解的启发。

Result: 在20个下游基准测试中取得最高平均性能，同时将时间成本相比PPL过滤降低了1000倍以上，且适用于代码、数学等符号语言以及多语言语料库。

Conclusion: 基于先验的过滤方法是一种简单而强大的数据选择技术，能够在大规模语言模型预训练中实现高效可靠的数据筛选。

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [147] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: TsqLoRA是一种新颖的参数高效微调方法，结合数据质量选择和敏感性感知的低秩适应，通过质量感知采样和动态秩分配来提高微调效率


<details>
  <summary>Details</summary>
Motivation: 完全微调大模型参数计算成本高且内存密集，现有参数高效微调方法忽视了不同模型层的敏感性差异和训练数据的重要性

Method: 包含两个主要组件：质量感知采样机制选择最有信息量的训练数据，动态秩分配模块根据每层对参数更新的敏感性调整其秩

Result: 实验结果表明TsqLoRA在各种NLP任务上提高了微调效率，同时保持甚至改善了性能

Conclusion: TsqLoRA通过数据质量驱动选择和敏感性感知低秩适应的集成，为资源受限环境下的模型微调提供了有效解决方案

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [148] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: UniECG是首个能够同时执行基于证据的心电图解释和文本条件心电图生成任务的统一模型，通过解耦的两阶段训练方法解决现有统一模型无法正确理解心电图信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型（如GPT-5）在视觉语言任务上取得了进展，但无法正确理解心电图信号并提供准确的医疗诊断，也不能正确生成心电图信号。

Method: 采用解耦的两阶段训练方法：首先学习基于证据的解释技能（ECG-to-Text），然后通过潜在空间对齐注入心电图生成能力（Text-to-ECG）。

Result: UniECG能够根据用户输入自主选择解释或生成心电图，显著扩展了当前心电图模型的能力边界。

Conclusion: UniECG是首个能够同时处理心电图解释和生成任务的统一模型，代码和检查点将在接受后公开。

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [149] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 论文通过Planorama实验发现，用户偏好、模型偏好和智能体成功率并不能准确预测哪些计划真正对用户有帮助，表明当前基于偏好的对齐方法可能与实际帮助性存在偏差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证LLM生成计划的帮助性与用户偏好之间的一致性，挑战当前基于偏好的对齐方法（如RLHF和ChatbotArena）的假设。

Method: 使用Planorama接口，让126名用户回答300个多步骤问题，收集4388个计划执行和5584个比较数据，测量计划帮助性（QA成功率）和用户偏好。

Result: 发现用户偏好、模型偏好和智能体成功率与计划实际帮助性不匹配；用户偏好与帮助性无关；表面特征（如简洁性）影响偏好但无法预测帮助性。

Conclusion: 强调需要基于真实用户交互的反馈来对齐有帮助的LLM，而不仅仅是基于看起来有帮助的偏好，并讨论了解决这一问题的研究方向。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [150] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: CAPE-KG是一种基于知识图谱的一致性感知参数保持知识编辑框架，用于多跳问答任务，解决了现有方法在一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识图谱的参数保持知识编辑方法在多跳问答中存在一致性问题，导致知识污染、更新不稳定和检索行为与编辑意图不符，影响了多跳推理的可靠性。

Method: 提出CAPE-KG框架，确保知识图谱的构建、更新和检索始终与多跳问答任务要求对齐，在未编辑和已编辑知识上保持连贯推理。

Result: 在MQuAKE基准测试上的广泛实验显示，该方法在多跳问答的参数保持知识编辑性能上取得了准确率提升。

Conclusion: CAPE-KG通过解决一致性问题，有效提升了参数保持知识编辑在多跳问答中的性能，证明了保持一致性对PPKE的重要性。

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [151] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: 该论文提出了首个通过保形预测分析LLM作为评判者评估不确定性的框架，为LLM评分提供预测区间，并设计了针对离散评分任务的序数边界调整方法。


<details>
  <summary>Details</summary>
Motivation: LLM作为评判者的评估范式在自然语言生成评估中应用广泛，但其评估的不确定性尚未充分探索，这种可靠性不足可能限制其在实际应用中的部署。

Method: 使用保形预测方法构建连续预测区间，设计序数边界调整处理离散评分任务，并提出基于区间中点的评分作为原始模型评分和加权平均的低偏差替代方案。

Result: 实验表明保形预测能够提供具有覆盖保证的有效预测区间，区间中点和重新提示评判者方法能够改善评估质量。

Conclusion: 该框架为LLM评估提供了可靠的不确定性量化方法，有助于提升LLM作为评判者在实际应用中的可信度。

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [152] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: MemOrb是一个轻量级、即插即用的语言强化记忆层，通过将多轮交互提炼为紧凑的策略反思来提升LLM智能体在客服场景中的长期可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM智能体在客服场景中存在跨会话遗忘、重复错误和缺乏持续自我改进机制的问题，导致在动态环境中不可靠。

Method: 提出MemOrb方法，将多轮交互提炼为策略反思存储在共享记忆库中，通过检索这些反思来指导决策，无需微调。

Result: 实验显示MemOrb显著提高了任务成功率和稳定性，多轮成功率提升高达63个百分点，在重复试验中表现更一致。

Conclusion: 结构化反思是增强冻结LLM智能体在客服场景中长期可靠性的有效机制。

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [153] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS是一个公开的泰语会议语料库，包含114小时自发对话，用于推进远场对话语音识别研究。该数据集通过多种麦克风在不同距离录制，并提供了基准系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别模型在远场泰语对话场景下表现不佳，需要专门的数据集来改进远场语音识别的鲁棒性。

Method: 收集114小时自发泰语对话，使用9个独立单通道设备在0.12-10米距离录制，提供标准数据集划分和可复现的基准系统，评估Whisper模型的零样本和微调性能。

Result: 零样本模型随距离增加性能显著下降，微调后整体WER从64.3降至38.3，远场WER从81.6降至49.5，最远麦克风改进最大。

Conclusion: 距离多样化的训练数据对鲁棒语音识别至关重要，LOTUSDIS数据集为远场泰语ASR研究提供了重要资源。

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [154] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: DyGRASP是一种针对动态文本属性图的新方法，结合LLM和时序GNN，有效捕捉近期和全局时间语义，在节点检索任务上提升34%


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态文本属性图，难以处理动态图中的近期-全局时间语义，且LLM在处理大量动态文本时存在效率问题

Method: 设计节点中心隐式推理和滑动窗口机制捕捉近期语义；利用显式推理和RNN链结构捕获全局语义；通过更新和融合层整合时间语义与图结构信息

Result: 在DyTAG基准测试中，目标节点检索任务的Hit@10指标提升达34%，且在不同时序GNN和LLM上表现出强泛化能力

Conclusion: DyGRASP有效解决了动态文本属性图的时间语义建模问题，为动态图分析提供了高效解决方案

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [155] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: 该研究探讨了多语言分词器中词汇重叠对跨语言迁移的影响，通过控制实验发现词汇重叠有助于跨语言语义关系的捕捉，并能提升跨语言理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对于多语言分词器中词汇重叠是否促进跨语言迁移还是引入语言间干扰存在争议，部分原因是实验设置和混杂因素（如词频、分词粒度）的差异。

Method: 设计了控制实验，在多种语言对上训练双语自回归模型，系统性地改变词汇重叠设置，并引入新的维度——跨语言共享词汇的语义相似性。

Result: 分析模型隐藏表示发现，任何类型的重叠都能创建捕捉跨语言语义关系的嵌入空间，而在词汇不相交的模型中这种效应较弱。在XNLI和XQuAD任务上，有重叠的模型表现优于词汇不相交的模型，且迁移性能随重叠增加而提升。

Conclusion: 词汇重叠在多语言模型中具有优势，保持大量共享词汇仍然是多语言分词器的有益设计选择。

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [156] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 本文研究发现，与长上下文预训练导致短上下文任务性能下降的普遍现象相反，长上下文监督微调（SFT）反而能提升大语言模型在短上下文任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着实际应用对长上下文窗口需求的增加，长上下文数据的持续预训练和SFT成为常见方法。虽然数据长度在持续预训练中的影响已被广泛研究，但其在SFT中的影响尚不明确。

Method: 系统研究SFT数据长度如何影响LLM在短上下文任务上的行为，解耦分析多头注意力（MHA）和前馈网络（FFN）两个关键组件，并研究它们的相互作用。

Result: 发现长上下文SFT独立地使MHA和FFN都受益，但揭示了知识偏好偏差：长上下文SFT促进上下文知识，而短上下文SFT偏好参数知识，仅依赖长上下文SFT并非最优。

Conclusion: 混合训练可以缓解这种偏差，为微调LLM提供了可解释的指导。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [157] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 提出一种从10-K文件中提取企业间风险关系的系统性方法，利用自然语言处理技术捕捉隐含风险连接，优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家判断和手动分析企业间风险关系的方法存在主观性强、劳动密集、难以扩展的问题，需要自动化解决方案

Method: 基于10-K文件的时序和词汇模式进行无监督微调，开发特定领域金融编码器，引入定量风险关系评分

Result: 广泛的实验表明该方法在多个评估设置下优于强基线

Conclusion: 该方法为投资组合管理和投资策略等应用提供了可扩展、透明的企业间风险关系分析工具

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [158] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 该论文建立了AECBench基准测试，用于评估大语言模型在建筑、工程和施工领域的性能表现，涵盖5个认知层次和23个任务，包含4800个问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在AEC领域的应用日益增多，需要评估这些模型在安全关键领域的鲁棒性和可靠性。

Method: 创建包含23个代表性任务的五级认知评估框架（知识记忆、理解、推理、计算、应用），构建4800个问题的数据集，并采用LLM-as-a-Judge方法进行可扩展评估。

Result: 评估9个LLM发现，模型在基础任务上表现良好，但在解释建筑规范表格、复杂推理计算和生成专业文档方面存在显著性能缺陷。

Conclusion: 该研究为未来将LLM可靠集成到安全关键工程实践奠定了基础，揭示了当前模型在专业领域的局限性。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [159] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: 该研究使用模型差异分析（model diffing）方法比较Gemma-2-9b-it模型与其SimPO增强变体，发现SimPO主要增强了安全性、多语言能力和指令跟随能力，同时减少了模型自引用和幻觉管理。


<details>
  <summary>Details</summary>
Motivation: 随着微调成为改进大语言模型的主要方法，传统基准测试往往无法解释模型性能差异的原因，需要更深入的分析方法来理解模型在微调过程中的具体变化。

Method: 采用模型差异分析（一种机制可解释性方法）和crosscoders技术，识别和分类两个模型之间的潜在表示差异。

Result: SimPO获得的潜在概念主要增强了安全性机制（+32.8%）、多语言能力（+43.8%）和指令跟随能力（+151.7%），同时减少了模型自引用（-44.1%）和幻觉管理（-68.5%）的强调。

Conclusion: 模型差异分析能够提供超越排行榜指标的细粒度洞察，将性能差距归因于具体的机制能力，为比较LLMs提供了透明和有针对性的框架。

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [160] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX是一个用于关键词提取的多智能体协作框架，通过动态适应文档长度的双路径策略，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督提示方法通常采用单阶段推理流程和统一提示策略，无法充分利用LLM的推理和生成能力，特别是在处理不同长度文档时效果受限。

Method: MAPEX引入多智能体协作，包含专家招募、候选提取、主题指导、知识增强和后处理模块，采用双路径策略：短文本使用知识驱动提取，长文本使用主题引导提取。

Result: 在6个基准数据集和3种不同LLM上的实验表明，MAPEX在F1@5指标上平均优于最先进的无监督方法2.44%，优于标准LLM基线4.01%。

Conclusion: MAPEX框架通过多智能体协作和动态适应策略，显著提升了关键词提取的性能，展示了强大的泛化能力和普适性。

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [161] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 本文比较了开源大语言模型与闭源模型在生物医学问答任务中的表现，发现开源模型在某些情况下甚至优于闭源模型，特别是在使用集成策略时。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型的快速发展，研究者希望验证小型开源模型是否能够有效替代大型闭源模型，特别是在生物医学问答这一专业领域。

Method: 通过参与BioASQ挑战赛Task 13B Phase B，使用嵌入距离检索相关片段、上下文学习和结构化输出等技术，并对精确答案问题采用集成方法整合不同模型的输出。

Result: 开源大语言模型与闭源模型表现相当，在某些情况下，特别是应用集成策略时，开源模型甚至超越了闭源模型。

Conclusion: 开源大语言模型在生物医学问答领域具有与闭源模型相当的竞争力，证明了开源模型的实用价值。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [162] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: 多特征集成方法对AI文本检测的性能提升有限（仅0.4-0.5%），但计算成本显著增加（4.2倍开销），表明现代神经网络模型可能已有效捕获大部分检测信号。


<details>
  <summary>Details</summary>
Motivation: 研究多特征方法是否能显著提升AI文本检测性能，验证语义、句法和统计特征组合是否提供互补信号的理论假设。

Method: 实现MHFD（多层次特征检测）方法，通过自适应融合集成基于DeBERTa的语义分析、句法解析和统计概率特征。

Result: 在多个基准数据集上，MHFD方法在域内检测达到89.7%准确率，跨域检测保持84.2%稳定性能，相比现有方法有0.4-2.6%的适度提升。

Conclusion: 多特征集成带来的性能收益有限，计算开销较大，现代神经网络语言模型可能已高效捕获相关检测信号。

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [163] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: DivEye是一个基于意外性特征的AI生成文本检测框架，通过捕捉文本中不可预测性的波动来区分人类和AI生成内容


<details>
  <summary>Details</summary>
Motivation: 现有检测器依赖词元级似然度或不透明的黑盒分类器，难以应对高质量生成文本且缺乏可解释性，需要一种能捕捉人类文本中更丰富不可预测性变化的检测方法

Method: 利用基于意外性的统计特征来捕捉文本中词汇和结构不可预测性的波动，这些特征具有可解释性

Result: 在多个基准测试中，DivEye比现有零样本检测器性能提升高达33.2%，与微调基线模型性能相当，对改写和对抗攻击具有鲁棒性，跨领域和模型泛化能力强

Conclusion: DivEye不仅提供有效的检测性能，还能提供可解释的检测依据，表明节奏不可预测性是LLM检测中一个强大但未被充分探索的信号

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [164] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: JEDI是一个仅使用编码器的架构，联合执行提取式原子事实分解和可解释推理，无需在推理时使用生成模型，在NLI任务中实现了竞争性准确性并显著提高了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖资源密集型的生成式大语言模型进行原子事实分解，这影响了效率和可解释性。作者希望开发更高效的替代方案。

Method: 提出JEDI编码器架构，联合执行提取式原子事实分解和可解释推理；使用合成理性语料库进行训练，涵盖多个NLI基准。

Result: JEDI在分布内达到竞争性准确率，在分布外和对抗性设置中显著提高了鲁棒性，优于仅基于提取式理性监督的模型。

Conclusion: 研究表明，使用仅编码器架构和合成理性语料可以在NLI中实现可解释性和鲁棒泛化。

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [165] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 该论文提出了一种基于动态时间规整（DTW）的方法来对齐语音和文本嵌入，以解决端到端语音翻译中的模态差异问题，相比现有方法在准确性和效率方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译中语音和文本模态之间存在表示差异，现有方法需要依赖对齐工具且不适用于所有语言，而基于最近邻相似性搜索的方法对齐效果不佳。

Method: 在训练过程中使用动态时间规整（DTW）算法来对齐语音和文本嵌入，不需要额外的对齐工具。

Result: 该方法在5/6的语言方向上优于先前工作，特别是在低资源设置下表现更好，同时训练速度显著更快。

Conclusion: DTW方法能够有效缩小模态差异，提供更准确的对齐效果，在端到端语音翻译任务中表现出色且效率更高。

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [166] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: 本文首次系统研究了测试时缩放（TTS）在机器翻译中的应用，通过最佳N选择框架在WMT24基准上验证了TTS对高资源语言翻译质量的提升效果，并分析了不同模型大小和计算预算下的效率权衡。


<details>
  <summary>Details</summary>
Motivation: 传统通过增加模型参数规模来提升NLP系统性能的方法计算成本高昂，测试时缩放（TTS）通过在推理时生成多个候选并选择最佳结果来提供替代方案，但在机器翻译领域尚未得到系统研究。

Method: 采用简单实用的最佳N选择框架，在WMT24基准上对6个高资源和1个低资源语言对进行实验，涵盖5种模型规模（3B-72B）和多种TTS计算预算（N最大1024）。

Result: 对于高资源语言，TTS能显著提升翻译质量；通过大N增强小模型可以匹配或超越大模型在N=1时的性能；在固定计算预算下，大模型通常更高效，而TTS在低资源情况下可能因度量盲点而降低质量。

Conclusion: TTS是机器翻译中有效的性能提升策略，特别是在高资源语言场景下，但在实际应用中需要考虑计算成本与性能的权衡，以及低资源情况下的适用性限制。

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [167] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: 本文分析了意大利计算语言学和自然语言处理领域过去十年的研究趋势，通过研究CLiC-it会议（2014-2024）的论文，构建了CLiC-it语料库，并对元数据和论文内容进行了综合分析。


<details>
  <summary>Details</summary>
Motivation: 跟踪意大利CL和NLP社区的研究趋势，了解从词汇语义资源到语言建模和多模态的转变，为研究社区提供有价值的见解。

Method: 收集CLiC-it会议前10届（2014-2024）的论文集，构建CLiC-it语料库，分析元数据（作者来源、性别、隶属关系等）和论文内容。

Result: 提供了对意大利CL和NLP领域发展趋势的全面分析，揭示了研究重点的转变。

Conclusion: 该研究为意大利和国际研究社区提供了对领域发展趋势的深入理解，支持该领域的明智决策和未来发展方向。

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [168] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: 提出了一种名为Pathways of Thoughts (PoT)的推理阶段方法，用于个性化问答系统，通过动态选择认知操作来探索多种推理轨迹，并根据用户偏好聚合候选响应。


<details>
  <summary>Details</summary>
Motivation: 个性化问答系统对提高准确性和用户满意度至关重要，但目前面临从长、嘈杂和隐含上下文中推断偏好，以及生成同时正确、上下文适当且符合用户期望和背景知识的响应的挑战。

Method: PoT是一种推理阶段方法，适用于任何大型语言模型，无需任务特定微调。它将LLM的推理建模为迭代决策过程，动态选择推理、修订、个性化和澄清等认知操作，探索多种推理轨迹，生成多样化的候选响应。

Result: 在LaMP-QA个性化问答基准测试中，PoT始终优于竞争基线，相对改进高达13.1%。人工评估显示，66%的情况下注释者更喜欢PoT的输出，仅有15%的情况为平局。

Conclusion: PoT通过探索多样化的推理路径并根据用户偏好进行聚合，有效提升了个性化问答系统的性能，证明了其在生成正确、上下文适当且符合用户期望的响应方面的优势。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [169] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: 该论文通过实证研究检验了语言学中关于大多数语言表达都是独特的说法，使用NLTK库分析不同体裁语料库中的句子重复率。


<details>
  <summary>Details</summary>
Motivation: 检验语言学中长期存在的观点，即大多数语言表达都是独特的、首次出现的组合，利用现代大型语料库进行实证验证。

Method: 使用Python的NLTK库解析不同体裁的语料库，统计每个语料库中完全相同的字符串匹配数量。

Result: 研究发现，虽然完全独特的句子在多数语料库中占主导地位，但这高度受体裁限制，且重复句子在任何单个语料库中都不是微不足道的部分。

Conclusion: 语言学中关于语言表达独特性的传统观点需要根据体裁进行修正，重复句子在不同类型的语言使用中具有显著存在。

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [170] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: KyrgyzNER是首个吉尔吉斯语命名实体识别数据集，包含1,499篇新闻文章、10,900个句子和39,075个实体标注，涵盖27个实体类别。研究评估了多种模型，发现多语言RoBERTa模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为资源有限的吉尔吉斯语创建首个手动标注的命名实体识别数据集，填补该语言在NLP领域的空白。

Method: 构建包含27个实体类别的标注数据集，评估了条件随机场等传统序列标注方法和基于多语言transformer的预训练模型。

Result: 所有模型在罕见实体类别上都表现困难，多语言RoBERTa模型在精确率和召回率之间取得了最佳平衡。

Conclusion: 多语言预训练模型在处理资源有限语言方面具有潜力，未来需要更细粒度的标注方案来提升吉尔吉斯语处理管道的评估效果。

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [171] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: 提出了一种新颖的上下文感知层次化分类生成框架，结合LLM引导的多方面编码和动态聚类，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 科学文献快速增长需要高效组织方法，现有基于无监督聚类或直接提示LLM的方法缺乏连贯性和细粒度

Method: 利用LLM识别论文关键方面（如方法、数据集、评估），生成方面特定摘要，然后进行编码和聚类形成层次结构

Result: 在包含11.6k论文的156个专家构建分类基准上实验，方法在连贯性、细粒度和可解释性方面达到最先进性能

Conclusion: 该方法为科学文献组织提供了有效的层次化分类生成解决方案

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [172] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: 提出'轶事伪造'方法，通过多语言知识图谱增强攻击LLM，提高对抗性提示生成的成功率，解决生成式AI在多语言文化环境中的虚假信息风险。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的滥用风险中，虚假信息是首要威胁。当前红队评估主要基于英语和美国文化，缺乏多语言和跨文化鲁棒性。

Method: 从三个语言（英语、西班牙语、印地语）和两个地区（美国、印度）的事实核查网站收集虚假信息声明，聚类成叙事主题，构建知识图谱增强攻击LLM。

Result: 相比少样本提示，该方法在攻击成功率上有显著提升，并提供更好的可解释性。

Conclusion: 研究强调了需要基于真实世界对抗性滥用的、可全球扩展的虚假信息缓解措施。

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [173] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: 本文提出了AI "slop"（低质量AI生成文本）的明确定义和评估框架，通过专家访谈建立了分类体系，并发现二元判断具有一定主观性但能与连贯性等维度相关。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对AI生成低质量文本（"slop"）的统一定义和评估方法，需要建立系统的分类和测量框架。

Method: 通过NLP、写作和哲学领域专家的访谈建立"slop"分类体系，采用span级标注方法评估文本质量。

Result: 发现二元"slop"判断具有主观性，但与连贯性、相关性等潜在维度相关；提出的框架可用于AI文本的质量评估。

Conclusion: 该框架为AI生成文本的质量评估提供了新视角，有助于理解影响质量判断的语言和风格因素。

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [174] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习学习连续思维链的可扩展方法，使用软标记和输入嵌入噪声进行探索，在数学推理基准测试中表现优于离散标记方法。


<details>
  <summary>Details</summary>
Motivation: 连续标记比离散标记具有更强的表达能力，但实际应用受到训练困难的限制，现有方法要么只在推理时使用连续标记，要么需要从离散思维链蒸馏且计算成本高。

Method: 使用强化学习学习连续思维链，采用软标记（标记混合）和输入嵌入噪声进行探索，计算开销最小化，可学习包含数百个标记的连续思维链。

Result: 在Llama和Qwen模型上的数学推理基准测试中，连续思维链训练在pass@1上匹配离散标记方法，在pass@32上超越，显示出更大的思维链多样性。最佳方案是训练时使用连续标记，推理时使用离散标记。

Conclusion: 连续思维链强化学习训练能更好地保留基础模型在域外任务上的预测能力，为基座模型提供更温和的调整。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [175] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: OPRL是一种用于智能体强化学习的通用信用分配策略，通过交替优化隐式过程奖励模型和策略，将轨迹偏好转化为隐式步骤奖励，解决了稀疏和不可验证奖励下的时间信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自主智能体在交互环境中进行长期推理和行动时，稀疏且有时不可验证的奖励使得时间信用分配极具挑战性。现有方法存在标注偏差、奖励攻击、高方差等问题。

Method: OPRL通过轨迹DPO目标将轨迹偏好转化为隐式步骤奖励，然后结合结果奖励计算步骤级优势，与回合级优势结合进行策略更新，形成自增强循环。理论保证学习到的步骤奖励与轨迹偏好一致且作为基于势能的塑造奖励。

Result: 在WebShop、VisualSokoban和SOTOPIA三个不同智能体基准测试中，OPRL表现出优于前沿LLM和强RL基线的性能，实现了最先进的结果，具有更高的样本效率和更低的训练方差。

Conclusion: OPRL通过高效探索使用更少的动作，展示了其在现实世界智能体学习中的潜力，为稀疏奖励环境下的信用分配提供了有效的解决方案。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [176] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: SafeCoDe是一个轻量级、模型无关的解码框架，通过对比解码和全局感知的token调节策略，动态调整多模态大语言模型的安全决策，平衡过度敏感和敏感不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在安全决策方面存在平衡问题，要么过度敏感（拒绝良性查询），要么敏感不足（错过视觉风险），需要一种能根据多模态上下文动态调整安全决策的方法。

Method: SafeCoDe采用两阶段方法：1）对比解码机制，通过对比真实图像和高斯噪声图像来突出对视觉上下文敏感的token；2）全局感知的token调节策略，将场景级推理与token级调整相结合，根据预测的安全判断自适应调整拒绝行为。

Result: 在多种MLLM架构和安全基准测试中，SafeCoDe在保持模型帮助性的同时，显著改善了上下文敏感的拒绝行为，在过度敏感、敏感不足和一般安全评估方面表现一致优异。

Conclusion: SafeCoDe是一个有效的解码框架，能够显著提升多模态大语言模型的安全决策能力，同时保持模型的实用性，为解决MLLM安全对齐问题提供了有前景的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [177] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 该研究比较了多种预训练注意力模型在电子健康记录信息提取任务上的性能，发现临床数据预训练的模型在检测药物和药物事件方面更有效，而通用领域预训练的Bert Base在药物相关事件上下文分类方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的模型已成为临床笔记自然语言处理的主要方法，本研究旨在比较不同预训练注意力模型在电子健康记录信息提取任务上的表现，为开发有效的临床信息提取解决方案提供指导。

Method: 研究比较了Bert Base、BioBert、Bio+Clinical Bert的两种变体、RoBerta和Clinical Long-former等预训练模型，使用哈佛医学院2022年n2c2挑战赛Track 1的任务和CMED数据集，对每个模型进行微调以执行药物提取、医疗事件检测和多维药物事件上下文分类。

Result: 结果显示，在临床数据上预训练的模型在检测药物和药物事件方面更有效，但通用领域预训练的Bert Base在分类药物相关事件的上下文方面表现最佳。

Conclusion: 预训练数据的选择对模型性能有显著影响，临床数据预训练的模型在检测任务上表现更好，而通用领域预训练的模型在分类任务上可能更具优势，这为临床NLP应用中的模型选择提供了重要参考。

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [178] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: CompLLM是一种针对大语言模型长上下文处理的软压缩技术，通过分段独立压缩实现线性复杂度、可扩展性和可重用性，在保持性能的同时显著提升处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有软上下文压缩方法将整个上下文作为单一单元压缩，导致二次压缩复杂度和无法在重叠上下文的查询间重用计算，限制了实际应用。

Method: 将上下文划分为多个段，对每个段独立进行压缩，支持压缩段的缓存和跨查询重用，实现线性压缩复杂度和良好的可扩展性。

Result: 在2倍压缩率下，CompLLM在高上下文长度时可将首词生成时间加速4倍，KV缓存减少50%，且在长序列上性能甚至优于未压缩上下文。

Conclusion: CompLLM通过分段压缩设计实现了高效、可扩展和可重用的长上下文处理，为大语言模型的实际部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [179] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: RLPT是一种新的训练时扩展范式，通过强化学习让LLM从预训练数据中自主学习，无需人工标注奖励信号，在多个基准测试中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 计算资源指数级增长与高质量文本数据有限增长之间的差距限制了传统LLM扩展方法，需要新的训练范式来优化模型能力。

Method: 采用基于下一段推理目标的强化学习方法，直接从预训练数据中获取奖励信号，让策略自主探索有意义的轨迹来学习。

Result: 在Qwen3-4B-Base模型上，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25等基准测试中分别获得3.0、5.1、8.1、6.0、6.6和5.3的绝对提升。

Conclusion: RLPT展现了良好的扩展行为，为扩展LLM推理边界和提升RLVR性能提供了坚实基础，具有持续增益的潜力。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [180] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: 本文提出了一种从大型语言模型中提取概念空间的方法，通过使用原型描述来编码特征，并通过对齐原型嵌入与概念空间维度来改进策略。


<details>
  <summary>Details</summary>
Motivation: 概念空间在认知科学中被广泛使用，并有望成为可解释AI的基石，但目前缺乏从LLMs中提取概念空间的实用方法。

Method: 使用原型描述（如"非常甜的食物"）来编码特征，并通过微调LLM来对齐原型嵌入与概念空间维度。

Result: 实证分析表明该方法非常有效。

Conclusion: 该方法为从LLMs中提取概念空间提供了一种可行的解决方案。

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [181] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: 该论文针对斯洛伐克语等低资源语言的自动语音识别（ASR）数据稀缺问题，构建了SloPalSpeech大规模数据集（2,806小时议会语音），并基于此微调Whisper模型，显著降低了词错误率（WER）。


<details>
  <summary>Details</summary>
Motivation: 低资源语言如斯洛伐克语的ASR系统因训练数据稀缺而性能受限，需要构建大规模高质量数据集来提升识别效果。

Method: 开发了稳健的处理流程，将长格式议会录音对齐分割为30秒音频-文本对，并使用该数据集微调多个OpenAI Whisper模型（small、medium、large-v3和large-v3-turbo）。

Result: 在Common Voice和FLEURS等标准斯洛伐克语基准测试中，微调后的模型WER显著降低，其中Whisper-small模型的WER下降高达70%，接近更大模型Whisper-large-v3的基线性能。

Conclusion: SloPalSpeech数据集和微调模型的公开发布将促进低资源语音识别研究的进一步发展。

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [182] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: 该论文发布了沃洛夫语意图分类数据集WolBanking77，包含9,791条银行领域文本句子和超过4小时的语音数据，旨在解决低资源语言意图分类的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有意图分类研究主要关注高资源语言，而像沃洛夫语这样的低资源语言（塞内加尔90%人口使用，但文盲率达42%）缺乏相关数据集，限制了相关技术的发展。

Method: 构建了沃洛夫语银行领域数据集WolBanking77，包含文本和语音数据，并在多种基线模型（包括文本和语音的最先进模型）上进行了实验评估。

Result: 在该数据集上的实验结果非常理想，报告了NLP模型的F1分数和ASR模型的词错误率等基线指标，并进行了模型间比较。

Conclusion: 该工作填补了低资源语言意图分类数据集的空白，计划共享数据集并进行维护更新，同时发布开源代码，促进相关研究发展。

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [183] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: DRISHTIKON是一个专注于印度文化的多模态多语言基准数据集，包含15种语言、超过64,000个对齐的文本-图像对，用于评估生成式AI系统的文化理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集多为通用或全球范围，缺乏对特定文化（尤其是印度文化）的深度覆盖。DRISHTIKON旨在填补这一空白，提供对印度多元文化的细粒度评估。

Method: 构建包含印度所有邦和联邦属地的多模态数据集，涵盖节日、服饰、美食、艺术形式和历史遗产等文化主题。评估多种视觉语言模型（包括开源模型、专有系统、推理专用模型和印度语模型）在零样本和思维链设置下的表现。

Result: 当前模型在处理文化基础的多模态输入方面存在明显局限，特别是在低资源语言和较少记录的传统文化方面表现不佳。

Conclusion: DRISHTIKON为包容性AI研究提供了重要工具，为开发具有文化意识和多模态能力的语言技术提供了强大的测试平台。

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [184] [Wake dynamics of a square cylinder while moving upward in quiescent water](https://arxiv.org/abs/2509.18305)
*Intesaaf Ashraf,Stephane Dorbolo,Neetu Tiwari*

Main category: physics.flu-dyn

TL;DR: 研究方形圆柱在静水中上升时的尾流动力学，发现尾流由一对持续的反向旋转涡旋主导而非周期性脱落，涡旋强度随弗劳德数增加而增强，揭示了涡旋-自由表面相互作用的新物理机制。


<details>
  <summary>Details</summary>
Motivation: 研究涡旋-自由表面相互作用机制，为多相流和海军、海上结构的水动力设计提供理论基础。

Method: 使用时间分辨粒子图像测速技术获取速度和涡量场，通过压力重建和涡旋特征分析，基于涡旋强度、Okubo-Weiss参数和剪切-涡旋相互作用度量进行诊断。

Result: 发现环流对弗劳德数呈现两阶段依赖关系，涡旋面积几乎恒定但涡旋强度随弗劳德数增加而增强，表明卷吸增强源于旋转强化而非涡旋足迹扩大。

Conclusion: 研究结果提供了涡旋-自由表面相互作用的新物理见解，丰富了非定常尾流中卷吸机制的理解，对多相流和海洋工程具有重要应用价值。

Abstract: We experimentally investigate the wake dynamics of a square cylinder rising
through quiescent water over a range of Froude numbers ($\mathrm{Fr}$).
Time-resolved Particle Image Velocimetry provides velocity and vorticity fields
that enable pressure reconstruction and vortex characterization. Diagnostics
based on swirl strength ($\lambda_{ci}$), the Okubo-Weiss parameter ($W$), and
a shear-vortex interaction measure ($\Lambda$) reveal that the wake is governed
by a persistent pair of counter-rotating vortices rather than by periodic
shedding. Circulation exhibits a two-regime dependence on $\mathrm{Fr}$, with a
sharp increase below $\mathrm{Fr}\approx 1$ and saturation above this
threshold, mirroring entrainment force scaling reported previously. While
vortex area remains nearly constant, swirl strength and negative-$W$ regions
expand with $\mathrm{Fr}$, indicating that entrainment enhancement arises from
intensified rotation rather than an enlarged vortex footprint. These findings
provide new physical insight into vortex-free-surface interactions and enrich
the understanding of entrainment mechanisms in unsteady wakes, with
implications for multiphase flows and the hydrodynamic design of naval and
offshore structures.

</details>


### [185] [Waves drive the rise and fall of 2D flows in rotating turbulence](https://arxiv.org/abs/2509.18323)
*Sébastien Gomé,Anna Frishman*

Main category: physics.flu-dyn

TL;DR: 本文研究了旋转湍流中三维惯性波与二维大尺度流动之间的相互作用，揭示了近共振相互作用导致能量从3D波向2D流动转移的机制，并发现了随着旋转增强，能量转移逐渐消失的相变现象。


<details>
  <summary>Details</summary>
Motivation: 研究旋转湍流中为什么即使只激发三维波，二维流动仍能持续存在的基本问题，探索波-平均流相互作用如何驱动大尺度自组织。

Method: 采用旋转三维Navier-Stokes方程的大规模数值模拟和准线性波动力学理论相结合的方法，分析近共振相互作用机制。

Result: 发现惯性波必须分别保持其螺旋度符号的守恒性，这约束了波将能量转移给大尺度二维运动；随着旋转增强，共振条件变得更严格，3D到2D的能量转移逐渐消失。

Conclusion: 建立了旋转湍流中二维化的机制，更广泛地说明了波-平均流相互作用如何驱动大尺度自组织过程。

Abstract: Turbulence follows a few well-known organizational principles, rooted in
conservation laws. One such principle states that a system conserving two
sign-definite invariants self-organizes into large-scale structures. Ordinary
three-dimensional turbulence does not fall within this paradigm. However, when
subject to rotation, 3D turbulence is profoundly altered: rotation produces 3D
inertial waves, while also sustaining emergent two-dimensional structures and
favoring domain-scale flows called condensates. This interplay raises a
fundamental question: why and when are 2D flows sustained even when only 3D
waves are excited? Using extensive numerical simulations of the rotating 3D
Navier-Stokes equations together with a quasi-linear wave-kinetic theory, we
show that near-resonant interactions between 3D waves and a large-scale 2D flow
impose an additional conservation law: inertial waves must conserve their
helicity separately for each helicity sign. This emergent sign-definite
invariant constrains the waves to transfer their energy to large-scale 2D
motions. However, as rotation increases, resonance conditions become more
restrictive and the energy transfer from 3D to 2D progressively vanishes,
leading to a transition from condensate-dominated turbulence to pure
inertial-wave turbulence for the 3D modes. We derive analytical expressions for
this 3D-2D energy transfer as a function of rotation, Reynolds number and
domain geometry, and verify them numerically. Together, these results establish
a mechanism underlying two-dimensionalization in rotating turbulence, and, more
broadly, illustrate how wave-mean flow interactions can drive large-scale
self-organization.

</details>


### [186] [Artificial Bottleneck Effect in Large Eddy Simulations](https://arxiv.org/abs/2509.18512)
*Mostafa Kamal,Perry L. Johnson*

Main category: physics.flu-dyn

TL;DR: 本文研究了Navier-Stokes湍流中的瓶颈效应，特别是大涡模拟中由涡粘性模型误差引起的人工瓶颈效应，并提出基于Stokes流正则化的动态混合模型来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 在湍流大涡模拟中，使用涡粘性模型会在截止尺度附近产生人工瓶颈效应，导致动能过度预测，即使使用动态过程也难以完全消除这一现象。

Method: 采用Stokes流正则化方法，结合动能考虑生成一系列LES模型，通过局部确定动态模型系数（无需在均匀方向平均），并在各向同性湍流中进行后验测试。

Result: 研究表明，在残余应力闭合中添加非线性梯度分量形成动态混合模型，能有效缓解瓶颈效应，改善残余应力结构和能量级联效率的表征。

Conclusion: 基于SFR的动态混合模型方法能够有效减轻大涡模拟中的人工瓶颈效应，提高模拟精度。

Abstract: In Navier-Stokes turbulence, a bottleneck effect in the energy cascade near
the viscous cutoff causes an overshoot in the energy spectrum, or spectral
bump, relative to Kolmogorov's -5/3 scaling. A similar overshoot occurs in
large-eddy simulations (LES) when an eddy viscosity model is used. It is not a
viscous phenomenon but is caused by error in the residual stress model. This
artificial bottleneck effect in LES leads to an over-prediction of kinetic
energy even when a dynamic procedure is used to capture the spectral decay at
the cutoff scale. Recently, Johnson [2022, J. Fluid Mech., 934, A30] introduced
a generalization of spatial filtering that provides a dynamic procedure without
a test filter. In this paper, this method of Stokes flow regularization (SFR)
is used with kinetic energy considerations to generate a range of LES models
and explore the bottleneck effect in more detail. The coefficients for each
dynamic model are determined locally, without averaging over homogeneous
directions. A posteriori tests of the models in isotropic turbulence are
reported, demonstrating the robustness of the SFR-based procedure for different
model forms and enabling fair comparisons in terms of their impact on the
bottleneck effect. An effective way to mitigate the bottleneck is to add a
nonlinear gradient component in the residual stress closure, forming a dynamic
mixed model. This approach improves representation of residual stress structure
and energy cascade efficiencies.

</details>


### [187] [Quantifying Sand Transport Sensitivity to Dune Shape: Field-Validated CFD with AirSketcher](https://arxiv.org/abs/2509.18513)
*Wichai Pattanapol,Kanisorn Thanutwutthigorn,Tipaporn Homdee*

Main category: physics.flu-dyn

TL;DR: 本文提出了AirSketcher工作流，通过侧剖面图像自动检测沙丘轮廓并计算近地表流场，为沙丘管理提供快速可解释的运输评估指标。


<details>
  <summary>Details</summary>
Motivation: 海岸沙丘管理经常改变沙丘形态，但量化小形状变化对沙粒运输的影响通常需要复杂模型，因此需要一种简化的评估方法。

Method: 开发了基于CFD的AirSketcher工作流，使用中性大气边界层入口条件和单方程涡粘模型，通过近地表速度的多项式积分形成几何感知的运输代理指标。

Result: 模型验证显示与实测数据高度相关。对三种沙丘形状（基准、顶部切割、背部切割）的评估表明，运输量分别减少17%和40%。

Conclusion: 该方法为沙丘修改提供了快速、可解释的筛选工具，可在进行完全耦合的形态动力学建模之前进行初步评估。

Abstract: Coastal dune management often alters crest and lee geometry, yet quantifying
the transport impact of small shape changes is difficult without heavy models.
This paper presents a streamlined, field-anchored CFD workflow (AirSketcher)
that ingests a side-profile image, auto-detects the dune outline, and computes
near-surface flow. A neutral ABL power-law inlet is applied consistently across
scenarios; turbulence is closed with a one-equation eddy-viscosity model. Model
skill is established against multi-height mast measurements at Tomahawk Beach
(Dunedin, NZ): height-matched profile correlations are strong and the solver
reproduces stoss speed-up, crest amplification, and lee-side recovery. For
design comparison, a geometry-aware transport proxy is formed as a cubic
line-integral of speed along a near-surface polyline (Bagnold-type scaling).
Three shapes are evaluated under the validated wind: baseline, top-cut, and
back-cut. Integrated proxies indicate sand-transport reductions of 17%
(top-cut) and 40% (back-cut) relative to the baseline. Flow patterns explain
the reductions, crest-peak attenuation, muted lee jets, earlier reattachment,
concentrating deposition nearer the crest, especially for the back-cut. The
approach offers a rapid, interpretable metric for screening dune modifications
before committing to fully coupled morpho dynamic modelling.

</details>


### [188] [Comparison of turbulence statistics in isothermal and non-isothermal large eddy simulations of supercritical carbon dioxide jets](https://arxiv.org/abs/2509.18528)
*Julia Ream,Marc T. Henry de Frahan,Shashank Yellapantula,Michael J. Martin,Mark Sussman,Ray Grout*

Main category: physics.flu-dyn

TL;DR: 本文研究了超临界二氧化碳湍流射流的物理特性，特别关注伪沸腾对流动动力学的影响，发现跨越伪沸点会导致显著不同的射流行为，包括增强的Kelvin-Helmholtz不稳定性和更快的射流衰减。


<details>
  <summary>Details</summary>
Motivation: 超临界二氧化碳在碳捕集、利用和封存以及先进发电循环等工程问题中具有重要应用。非理想物理性质的变化会影响这些系统的物理特性，理解热力学性质剧烈变化如何影响流动物理对于优化未来碳捕集和封存技术设计至关重要。

Method: 使用PeleC反应流求解器进行大涡模拟，采用二阶有限体积离散化方法和自适应网格细化技术，使用Soave-Redlich-Kwong状态方程来更准确地考虑非理想气体行为。

Result: 等温超临界射流与理想气体湍流射流具有相似的流动特性，但射流衰减和扩展速率存在微小差异。非等温射流在跨越伪沸点时表现出明显不同的行为，显示出增强的Kelvin-Helmholtz不稳定性和更快的射流衰减。

Conclusion: 伪沸点对超临界二氧化碳射流动力学有显著影响，这种影响会改变射流过渡区的混合程度，表明可能具有更大的传热潜力和更快的燃烧动力学。

Abstract: Supercritical carbon dioxide is of interest in a wide range of engineering
problems, including carbon capture, utilization, and storage as well as
advanced cycles for power generation. Non-ideal variations in physical
properties of supercritical carbon dioxide impact the physics of these systems.
It is important to understand how drastic changes in thermodynamic properties
influence these flow physics to aid and optimize the design of future
technologies related to carbon capture and sequestration. In this study, we
simulate turbulent supercritical carbon dioxide jets to gain a better
understanding of these physics. Of particular interest is the impact of
pseudo-boiling on supercritical flow dynamics. We use a second-order finite
volume discretization method with adaptive mesh refinement as implemented in
the reacting flow solver, PeleC, to perform a large eddy simulation of three
turbulent jets of supercritical carbon dioxide. We use the Soave-Redlich-Kwong
equation of state to close the system and more accurately incorporate the
departure from ideal gas behavior into the turbulent flow physics. We find that
the isothermal supercritical jet exhibits many similar flow characteristics
compared to ideal gas round turbulent jets, with minor differences seen in the
decay and spreading rate of the jet and in a noticeable anisotropy between
resolved turbulent kinetic energy components. The non-isothermal jet excluding
the pseudo-boiling point exhibits only small difference compared to the
isothermal case. Crossing the pseudo-boiling point results in markedly
different behavior, with evidence indicative of increased Kelvin-Helmholtz-like
instabilities and much faster jet decay and disintegration. These factors
impact the degree of mixing in the transition region of the jet, indicating a
potential for larger heat transfer and more rapid combustion dynamics.

</details>


### [189] [Reconstruction of three-dimensional turbulent flows from sparse and noisy planar measurements: A weight-sharing neural network approach](https://arxiv.org/abs/2509.18687)
*Yaxin Mo,Luca Magri*

Main category: physics.flu-dyn

TL;DR: 提出了一种无需地面真实数据即可从稀疏测量中重建三维湍流的方法，使用权重共享网络从三个平面的速度测量和一个平面的边界压力推断完整流场。


<details>
  <summary>Details</summary>
Motivation: 受实验配置启发，开发一种能够从有限测量中准确重建三维湍流场的有效方法，避免训练过程中对地面真实数据的依赖。

Method: 采用权重共享网络，沿均匀方向共享相同参数，实现高效数据利用和降低计算内存需求。比较了权重共享网络与PC-DualConvNet在无噪声和有噪声测量下的重建性能。

Result: 权重共享网络能够准确恢复时间平均三维流场和正确的能量谱，能够推断远离测量平面的流动结构，在噪声条件下表现出更好的泛化能力。

Conclusion: 权重共享网络具有良好的泛化能力、参数效率和超参数鲁棒性，为从实验数据中进行三维流动重建提供了可能性。

Abstract: This paper proposes a method for reconstructing three-dimensional turbulent
flows from sparse measurements without the need for ground truth data during
training. A weight-sharing network is developed to infer the full flow fields
from measurements of velocity sampled at three planes and boundary pressure at
one additional plane, inspired by experimental configurations. The
weight-sharing network shares identical parameters along homogeneous
directions, which results in efficient data utilization and reduced
computational memory requirements. First, we compare the weight-sharing network
to the PC-DualConvNet, adapted from prior work, by reconstructing a 3D
Kolmogorov flow from noise-free measurements with a snapshot-enforced loss.
Both networks accurately recover time-averaged 3D flow fields and the correct
energy spectrum up to wavenumber 10. The weight-sharing network has the ability
to infer flow structures distant from measurement planes. Second, we carry out
reconstruction from measurements corrupted with white noise (SNR 15) using a
mean-enforced loss. We show that, for the weight-sharing network, validation
sensor loss on unseen data decreases with training sensor loss -- unlike
PC-DualConvNet. This shows improved generalization and that training sensor
loss estimates generalization error. The weight-sharing network offers good
generalization, parameter efficiency, and hyperparameter robustness. The
proposed method opens the possibility of three-dimensional flow reconstruction
from experiments.

</details>


### [190] [Validation of a Reynolds-averaged numerical simulation environment to simulate high-pressure, auto-igniting hydrogen diffusion flames](https://arxiv.org/abs/2509.18841)
*N. Diepstraten,L. M. T. Somers,J. A. van Oijen*

Main category: physics.flu-dyn

TL;DR: 本文验证了RANS CFD模型在模拟高压自燃氢射流中的有效性，比较了三种湍流模型（两种涡粘模型和雷诺应力模型）在预测氢分布、点火延迟、压力上升和放热率方面的表现。


<details>
  <summary>Details</summary>
Motivation: 开发氢燃料直喷压缩点火氩气动力循环技术需要可靠的CFD模型，但现有RANS燃烧模型在模拟高压自燃氢射流复杂现象时的有效性尚未得到验证。

Method: 使用包含详细化学机理的RANS CFD环境，比较三种湍流模型（两种涡粘模型和RSM）在非反应射流和自燃射流中的预测能力，并与文献实验数据进行对比验证。

Result: 所有模型在扩散燃烧阶段均表现出良好的一致性，尽管燃烧模型缺乏湍流-化学相互作用。RSM模型在预测环境温度和氧气浓度变化趋势方面表现最佳。

Conclusion: RANS CFD模型能够有效模拟高压自燃氢射流，其中RSM模型在预测环境参数变化对燃烧特性的影响方面表现最优。

Abstract: The hydrogen (H2) fueled direct-injection (DI) compression-ignition (CI)
argon power cycle (APC) is an attractive technology to counteract the mismatch
between energy demand and supply from renewable energy sources. The development
of the APC, as well as air-breathing DI CI H2 engines, can be advanced by
computational fluid dynamics (CFD) models. Reynolds-averaged Navier-Stokes
(RANS) combustion models are an effective approach to predict global
in-cylinder variables such pressure and heat release rate. However, validity of
this method to simulate the complex phenomena associated with high-pressure,
auto-igniting hydrogen jets is not ensured due to the underlying model
assumptions. In this study, a RANS CFD environment using two commonly used
eddy-viscosity models and the Reynolds stress model (RSM) is extensively
validated. Combustion is modeled with a detailed chemistry model. First,
hydrogen distributions in non-reacting jets are compared against literature
data to assess the accuracy of the models on turbulent mixing at high pressure.
Subsequently, the capability of the model to simulate auto-igniting jets is
assessed by comparison with reported measurement data of closed-vessel
experiments at high pressure. Ignition delay times, pressure rise profiles, and
heat release rate profiles are compared for different ambient temperature and
oxygen concentration. Adequate agreement is found for the diffusive combustion
phase for all models, despite the lack of turbulence-chemistry interaction in
the combustion model. Trends with ambient temperature and oxygen concentration
were well predicted and the best agreement is found for the RSM.

</details>


### [191] [Pore-Scale Dynamics of Multiphase Reactive Transport in Water-Wet Carbonates under CO2-Acidified Brine Injection: Dissolution Patterns and Reaction Rates](https://arxiv.org/abs/2509.18907)
*Qianqian Ma,Rukuan Chai,Sajjad Foroughi,Yanghua Wang,Martin J. Blunt,Branko Bijeljic*

Main category: physics.flu-dyn

TL;DR: 本研究揭示了残余油如何影响碳酸盐岩储层中CO2酸化盐水注入时的溶解模式和有效反应速率，发现孔隙结构、残余油饱和度和油驱替之间的耦合控制着流动异质性、反应表面可及性、溶解模式和反应速率。


<details>
  <summary>Details</summary>
Motivation: 枯竭碳酸盐岩油气藏是地质CO2封存的有前景场所，但残余烃类的存在引入了复杂的孔隙尺度相互作用，影响固体溶解动力学。需要准确理解烃类相反应的影响以提高CO2封存效率预测。

Method: 结合时间分辨X射线显微断层扫描(micro-CT)、岩心驱替实验和直接数值模拟，评估孔隙空间异质性、油分布和注入速率的影响。

Result: 在低注入速率下，油驱替增强了通道加宽溶解；在高注入速率下，发生更均匀的溶解。两相流中的有效反应速率低于等效单相情况，比批次反应速率低两个数量级。

Conclusion: 这些发现为碳酸盐岩中多相反应输运提供了机理见解，强调了准确理解烃类相对反应影响的重要性，以改进CO2封存效率预测。

Abstract: Depleted carbonate hydrocarbon reservoirs are promising sites for geological
CO2 storage, yet the presence of residual hydrocarbons introduces complex
pore-scale interactions that influence the dynamics of solid dissolution. This
study reveals how residual oil affects dissolution patterns and effective
reaction rates during CO2-acidified brine injection into Ketton limestone under
reservoir conditions. We combine time-resolved X-ray microtomography
(micro-CT), core-flooding experiments, and direct numerical simulations to
assess the impact of pore space heterogeneity, oil distribution and injection
rate. We find that the coupling between pore structure, residual oil saturation
and oil displacement control flow heterogeneity, reactive surface
accessibility, dissolution patterns and the reaction rates. At low injection
rate, dissolution by channel widening is enhanced by oil displacement. This
mechanism is especially important when dissolution is suppressed by
heterogeneity in the pore space and the residual oil. At high injection rates,
a more uniform dissolution occurs and can be enhanced by re-mobilisation of oil
blocking brine flow. Effective reaction rates in two-phase flow are found to be
lower than in the the equivalent single-phase case and up to two orders of
magnitude lower than the batch rates due to persistent transport limitations.
These findings offer mechanistic insights into multiphase reactive transport in
carbonates and highlight the need for accurate understanding of the impact of
the hydrocarbon phase on reaction to improve predictions of CO2 storage
efficiency.

</details>


### [192] [Data-driven transient growth analysis](https://arxiv.org/abs/2509.18974)
*Yin Wang,Xuerui Mao*

Main category: physics.flu-dyn

TL;DR: 本文提出了一种数据驱动的瞬态增长分析方法，无需构建伴随方程，通过扰动瞬态快照提取最优初始扰动及其能量增长。


<details>
  <summary>Details</summary>
Motivation: 传统全局瞬态增长分析需要构建伴随方程，过程复杂且计算成本高。本文旨在开发一种更简便的方法来降低瞬态动力学研究的门槛。

Method: 使用数据驱动算法，从线性化方程的扰动瞬态快照中提取最优初始扰动和能量增长信息，并利用POD模式合成最优模式。

Result: 方法在Ginzburg-Landau方程、后向台阶流和Batchelor涡旋等案例中得到验证，相比传统方法显著减少了计算需求。

Conclusion: 该方法为瞬态增长分析提供了更高效的框架，有望显著促进瞬态动力学研究的发展。

Abstract: Transient growth analysis has been extensively studied in asymptotically
stable flows to identify their short-term amplification of perturbations.
Generally, in global transient growth analyses, matrix-free methods are
adopted, requiring the construction of adjoint equations, either in the
discrete or continuous form. This paper introduces a data-driven algorithm that
circumvents the adjoint equations by extracting the optimal initial
perturbation and its energy growth over a specified time horizon from transient
snapshots of perturbations. This method is validated using data from the
linearised complex Ginzburg-Landau equation, backward-facing step flow, and the
Batchelor vortex. Unlike model-based methods, which require $S$ sets of
integrations of the linearised governing equation and its adjoint for $S$ time
horizons, the proposed approach collects the snapshots of $S$ time horizons in
one integration of the linearised equation. Furthermore, this study provides a
robust framework for utilising proper orthogonal decomposition (POD) modes to
synthesise optimal modes. The developed capacity to conduct transient growth
analyses without solving the adjoint equations is expected to significantly
reduce the barriers to transient dynamics research.

</details>


### [193] [Numerical study of friction reduction and underlying mechanism of air film via parallel and tandem injections on a hypersonic flat plate at high altitude](https://arxiv.org/abs/2509.18975)
*Xiuzheng Cheng,Mengyu Wang,Qin Li,Lingyun Wang,Yihui Weng,Pan Yan*

Main category: physics.flu-dyn

TL;DR: 本文通过数值模拟研究了高超声速高海拔条件下平板表面空气薄膜的减阻效果，分析了质量流量、喷射间距和喷射孔纵横比对减阻性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究高超声速飞行器在高海拔条件下的减阻技术，通过空气喷射形成薄膜来降低壁面摩擦阻力，提高飞行效率。

Method: 使用数值模拟方法，在Ma=15、H=60km的自由流条件下，对平行和串联喷射配置的平板进行仿真，分析不同质量流量、喷射间距和喷射孔纵横比的影响。

Result: 研究发现空气喷射能显著降低近壁速度梯度实现减阻，但后续流动再附着可能导致局部阻力增加。增加质量流量会扩大薄膜覆盖范围但限制减阻增益，增大喷射间距会降低减阻效率，而增大纵横比能显著提升减阻效果。

Conclusion: 喷射孔纵横比是影响减阻效果的关键参数，增大纵横比能改善流动均匀性并显著增强减阻性能，这为高超声速飞行器的减阻设计提供了重要指导。

Abstract: This study uses numerical simulations to analyze a flat-plate with air films
generated via parallel and tandem injections under freestream conditions of
Ma=15 at an altitude of H=60km. Simulations are carried out using various mass
flow rates and jet spacing, followed by an examination of the effect of the
streamwise-to-spanwise aspect ratio of the injection hole. The following key
findings are acquired: (1) Under hypersonic and high-altitude conditions, the
air injection lifts the incoming flow, producing a pronounced low-speed region
near the injection site and over the downstream wall. A plateau appears in the
wall-normal distribution of the streamwise velocity, with a value significantly
lower than that of its freestream counterpart. This reduces the near-wall
velocity gradient, thereby achieving drag reduction. However, the subsequent
flow reattachment may lead to localized drag increases; (2) As the mass flow
rate of injection increases, the wall-normal extent of the near-wall low-speed
layer expands and the film coverage broadens; however, the increased friction
limits gains in drag reduction, and greater flow losses are also incurred; (3)
Increasing the hole spacing in the parallel injection reduces the penetration
height of the film, the spanwise coverage, and the drag reduction efficiency,
as well as decreasing the flow losses. In contrast, for the studied tandem
injections, no significant differences are observed with analogous spacing
variations; (4) Increasing the injection aspect ratio enlarges the spanwise
coverage of the low-speed layer, improves the spanwise uniformity of the
streamwise velocity distribution, diminishes the friction-increasing zones, and
significantly enhances the drag reduction. Notably, the consequences are the
inverse of those of the low-speed situation.

</details>


### [194] [A flux bounce-back scheme for the filtered Spectral Element Lattice Boltzmann Method](https://arxiv.org/abs/2509.19171)
*Chunheng Zhao,Saumil Patel,Hai Lu Lin,Taehun Lee*

Main category: physics.flu-dyn

TL;DR: 提出了一种结合谱元法和通量反弹格式的格子玻尔兹曼方法（SELBM），用于在非结构化网格上精确模拟单相流体动力学。


<details>
  <summary>Details</summary>
Motivation: 传统LBM在非结构化网格上存在精度限制，需要开发高精度方法处理复杂边界和湍流问题。

Method: 采用欧拉描述代替传统LBM的完美偏移，使用谱元法空间离散对流项，SSPRK方法进行时间积分，并引入通量反弹格式处理复杂边界。

Result: 通过多个基准测试验证了方法的准确性，包括Couette流、Poiseuille流、Taylor-Green涡旋问题和圆柱管湍流，正确预测了湍流边界层剖面。

Conclusion: 所提出的SELBM方法能够有效处理复杂边界条件下的流体动力学问题，具有高精度和良好的收敛性。

Abstract: We develop a spectral element lattice Boltzmann method (SELBM) with the flux
bounce-back (FBB) scheme, to enable accurate simulations of single-phase fluid
dynamics in unstructured mesh. We adopt an Eulerian description of the
streaming process in place of the perfect shift in the regular LBM. The
spectral element method is used to spatially discretize the convective term,
while the strong stability-preserving Runge-Kutta (SSPRK) method is used for
time integration. To increase stability, we investigate the use of an explicit
filter, particularly in the context of the sensitive double shear layer
problem. The results indicate that by using the high-order polynomial, we can
effectively eliminate the small vortices around the neck region. We introduce
the flux bounce-back scheme to enable the current scheme to handle complex
boundaries. The proposed scheme and flux boundary method are validated through
benchmark simulations, including the unsteady Couette flow and the planar
Poiseuille flow. Further validation is provided through the Taylor-Green vortex
problem, demonstrating the accuracy and convergence of the scheme for isotropic
turbulence. Finally, we consider a fully developed turbulent flow within a
cylindrical pipe and correctly predict the turbulent boundary layer profile.

</details>


### [195] [A novel generalized inner product-based wave scattering from an underwater source in a compressible ocean](https://arxiv.org/abs/2509.19196)
*R. Pethiyagoda S. Das,B. Wilks,M. H. Meylan*

Main category: physics.flu-dyn

TL;DR: 本文研究了海洋中初始压力扰动的演化过程，特别关注水下爆炸和火山喷发应用，通过引入特殊内积和自伴算子理论构建希尔伯特空间，分析声重力波的传播特性。


<details>
  <summary>Details</summary>
Motivation: 研究水下爆炸和火山喷发等应用中海洋压力扰动的演化过程，考虑水的动态和静态压缩效应以及自由表面的影响。

Method: 引入特殊内积应用自伴算子理论求解线性可压缩海洋运动方程，构建希尔伯特空间使声重力模式正交，计算初始扰动引起的自由表面和次表面压力场时域演化。

Result: 模拟显示压力脉冲初始径向传播，随后从水面和刚性海底反射，最终导致水平传播；静态压缩效应虽小但不可忽略。

Conclusion: 成功建立了分析海洋压力扰动演化的理论框架，验证了静态压缩对压力传播的微小但显著影响。

Abstract: Motivated by applications to underwater explosions and volcanic eruptions,
this paper considers the evolution of an initial pressure disturbance in the
ocean, including effects due to the dynamic and static compression of water and
the free surface. In order to solve the equations of motion of a linear
compressible ocean, a special inner product is introduced, which allows us to
apply self-adjoint operator theory. What results is a Hilbert space in which
the acoustic-gravity modes are orthogonal in the generalised sense. This allows
the time-domain evolution of the free surface and subsurface pressure field
resulting from an initial disturbance to be calculated. Our simulations show
initial radial propagation of the pressure pulse and subsequent reflection from
the water surface and the rigid ocean floor, eventually leading to horizontal
propagation away from the source point. The solutions with and without the
inclusion of the static compression are compared, and the effect of static
compression is shown to be small but not negligible.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [196] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 使用图像机器学习模型分析Ulam螺旋中不同区域的质数分布规律性，发现在较大整数区域（约5亿附近）比小整数区域（低于2500万）存在更易学习的模式，表明质数分布在较大数值时呈现更强的规律性。


<details>
  <summary>Details</summary>
Motivation: 质数分布具有确定性定义但表现出类似随机过程的统计行为，研究旨在探索机器学习能否作为数论研究的新实验工具，特别是分析质数在不同数量级区域的分布模式差异。

Method: 采用基于图像的机器学习模型，从Ulam螺旋的不同区域提取数据块进行训练和比较，具体对比了低整数区域（<25m）和高整数区域（约500m附近）的模式识别效果。

Result: 模型在500m区域训练的准确率优于25m区域，表明较大数值区域存在更易学习的质数分布规律；精确率和召回率分析显示模型在不同区域采用不同分类策略：低数值区域侧重识别质数模式，高数值区域侧重排除合数。

Conclusion: 机器学习可作为数论研究的新实验工具，验证了数论猜想中关于质数分布在大数值时噪声减少、平均规律占主导的观点，该方法在密码学中强弱质数模式研究方面具有应用潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [197] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一个基于Wasserstein距离的联邦学习框架，用于解决数据市场中数据估值和选择的挑战，通过预测模型性能和数据兼容性来提升联邦学习模型交易的可信度。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，数据市场需要可信的模型交易解决方案。联邦学习虽然能保护数据隐私，但在异构数据源的有效估值和选择方面仍面临挑战。

Method: 提出基于Wasserstein距离的估计器框架，包括分布式近似Wasserstein距离的隐私保护方法，以及利用神经缩放定律进行模型性能外推的数据选择策略。

Result: 在标签偏斜、错误标注和未标注等多种场景下的实验表明，该方法能一致性地识别高性能数据组合。

Conclusion: 该框架为构建更可靠的基于联邦学习的模型市场铺平了道路，解决了数据异构性和算法兼容性的关键问题。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [198] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该论文研究连续时间库存动态学习，比较完全学习的神经ODE与物理信息通用微分方程在预测牛鞭效应时的表现，发现在结构化需求机制下UDE表现更好，而在重尾需求下NODE更优。


<details>
  <summary>Details</summary>
Motivation: 研究结构偏差在不同需求机制下对牛鞭效应预测的帮助或阻碍作用，为混合建模提供具体指导。

Method: 使用单级测试平台，比较完全学习的NODE和保留守恒结构的UDE在三种需求机制（AR(1)、高斯i.i.d.、重尾对数正态）下的表现，通过不同训练数据比例评估多步预测能力。

Result: 在结构化需求机制下，UDE泛化能力更好（库存RMSE从4.92降至0.26）；在重尾需求下，NODE的灵活性更优。随着训练数据减少，NODE出现相位漂移，而UDE保持稳定但对稀有峰值反应不足。

Conclusion: 当噪声为轻尾或时间相关时应强制执行结构约束，当极端事件主导时应放松结构约束，这为科学和工程系统中的混合建模提供了指导原则。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [199] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 本研究提出了一种基于模型的迁移学习方法，利用神经网络代理模型，使在一个桥梁上训练的模型能够适应具有相似特征的另一个桥梁，以解决大型桥梁网络中结构评估的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着永久监测系统的广泛使用，数据可用性增加，为结构评估提供了新机会，但在大型桥梁网络中面临可扩展性挑战。管理多个结构需要高效跟踪和比较长期行为，因此类似结构之间的知识转移变得至关重要。

Method: 采用基于模型的迁移学习方法，使用神经网络代理模型。将在一个桥梁上训练的模型迁移到具有相似特征的另一个桥梁上，这些模型捕捉共享的损伤机制。将迁移后的模型集成到贝叶斯推理框架中，基于监测数据的模态特征进行连续损伤评估。

Result: 使用两个桥梁的真实数据进行验证，结果显示该方法对损伤位置、严重程度和范围具有高敏感性。

Conclusion: 该方法增强了实时监测能力，实现了跨结构知识转移，促进了智能监测策略，提高了网络层面的韧性。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [200] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出AdaMixT架构，通过自适应多尺度专家变换器解决多元时间序列预测中多尺度特征融合不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖预定义的单尺度补丁或缺乏有效的多尺度特征融合机制，无法充分捕捉时间序列中的复杂模式，导致性能受限和泛化能力不足

Method: AdaMixT引入多种补丁，利用通用预训练模型和领域特定模型进行多尺度特征提取，并通过门控网络动态分配不同专家的权重，实现自适应多尺度融合

Result: 在8个广泛使用的基准数据集上的综合实验一致证明了AdaMixT在真实场景中的有效性

Conclusion: AdaMixT通过自适应多尺度专家融合机制，显著提升了多元时间序列预测的性能和泛化能力

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [201] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化的框架，利用大语言模型进行迭代式算法解决方案生成，集成了生成、测试、分析和评估功能


<details>
  <summary>Details</summary>
Motivation: 简化算法设计过程，为用户提供对错误处理、分析和质量评估的完全控制，同时抽象化提示设计和模型管理的复杂性

Method: 通过可复现的反馈循环集成多个LLM在互补角色中（如生成器、分析师和评估器）进行协作

Result: 提供了一个透明且可扩展的平台，使研究人员和从业者能够在不同领域共同设计算法和其他生成式解决方案

Conclusion: EASE框架通过模块化架构和LLM协同工作，有效降低了算法设计的复杂性，为跨领域生成式解决方案开发提供了实用工具

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [202] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 基于AIS轨迹的轻量级特征机器学习管道，在波罗的海博恩霍尔姆海峡实现了92.15%的船舶类型分类准确率，随机森林模型表现最佳


<details>
  <summary>Details</summary>
Motivation: 准确识别船舶类型对安全监管和打击非法、不报告和不管制(IUU)活动至关重要，需要开发仅使用AIS数据的分类方法

Method: 使用8天历史AIS数据，经过数据预处理后提取31个轨迹级特征，采用基于树的机器学习模型进行五类船舶分类，使用分组训练/测试分割避免数据泄露

Result: 随机森林模型在测试集上达到92.15%准确率，宏精确率94.11%，宏召回率92.51%，宏F1分数93.27%，ROC-AUC最高达0.9897

Conclusion: 基于AIS轨迹的轻量级特征能够实现海峡区域的实时船舶类型分类，桥位比和最大航速是最具区分性的特征

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [203] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于patch的PCA-Net框架，通过将解场分解为小块进行PCA降维，显著降低计算复杂度，比全局PCA快3.7-4倍


<details>
  <summary>Details</summary>
Motivation: 传统PCA处理高维PDE解场计算开销大，需要更高效的方法

Method: 两种patch-based方法：局部到全局PCA和局部到局部PCA，并探索重叠patch平滑滤波和CNN精炼两种优化

Result: 基于patch的PCA显著降低计算复杂度，同时保持高精度

Conclusion: 该方法为PDE系统的高效算子学习提供了有前景的技术

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [204] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 本文提出了一种基于上下文优化(CoOp)的新框架，将子空间表示学习与提示调优相结合，以改进大规模视觉语言模型中的分布外(OOD)检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽略了视觉语言模型在数百万样本上学习到的丰富特征嵌入的判别潜力。

Method: 通过将分布内(ID)特征投影到提示向量张成的子空间中，同时将ID无关特征投影到正交零空间中，从而提高ID-OOD可分性。设计了易于处理的端到端学习准则。

Result: 在真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 所提出的框架成功整合了子空间表示学习和提示调优，显著提升了OOD检测性能，同时保持了较高的ID分类准确率。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [205] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 该研究首次全面比较了用于自动产前CTG分析的最先进AI方法，发现微调的LLMs在性能上优于基础模型和领域特定方法。


<details>
  <summary>Details</summary>
Motivation: 电子胎儿监护(CTG)分析对评估胎儿健康至关重要，但目前主要依赖主观临床解读，导致诊断准确性存在差异。基础模型和LLMs在医疗领域表现出色，但在CTG分析方面的潜力尚未充分探索。

Method: 系统比较时间序列基础模型、LLMs与已建立的CTG特定架构，评估超过500个不同时长的CTG记录，提供跨不同建模范式的稳健性能基准。

Result: 微调的LLMs在性能上优于基础模型和领域特定方法，为临床CTG解读提供了有前景的替代途径。

Conclusion: 研究结果为胎儿监护应用中不同AI方法的相对优势提供了关键见解，并为产前护理中未来临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [206] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 本文提出了一种利用BlueField-3数据处理器（DPU）来实时检测和缓解多节点张量并行推理中负载不均衡问题的框架，旨在提高大型语言模型自回归推理的运行时效率。


<details>
  <summary>Details</summary>
Motivation: 大型基于Transformer的语言模型在自回归推理阶段面临运行时效率挑战，特别是在解码阶段，GPU分片间的负载不均衡会导致吞吐量下降和延迟峰值。

Method: 通过DPU辅助框架，将监控任务卸载到DPU，分析GPU遥测数据和节点间通信模式，为推理控制器和调度器提供可操作的反馈。

Result: 该研究识别了多GPU执行LLM张量计算（训练和推理）中出现的偏斜/不均衡/病理条件，评估了它们对计算性能的影响。

Conclusion: 研究评估了这些条件是否可以通过DPU网络进行跟踪以进行潜在缓解，为优化LLM推理性能提供了新的技术路径。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [207] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间平衡注意力块用于时空预测，通过划分空间图为子图并分别使用子图内注意力和子图间注意力来平衡局部空间邻近性和全局相关性。


<details>
  <summary>Details</summary>
Motivation: 为了解决时空预测中如何在遵循空间邻近性和捕获全局相关性之间取得平衡的问题，传统方法往往难以同时兼顾这两个方面。

Method: 将空间图划分为一组子图，使用子图内注意力学习局部空间相关性，通过聚合节点生成子图表示，使用子图间注意力实现子图间的消息传递，构建多尺度时空预测模型。

Result: 在现实世界的中大型时空数据集上的实验表明，该方法在低运行成本下比基线方法性能提升高达7.7%。

Conclusion: 该方法既具有可扩展性，又能产生结构化的空间相关性，且易于实现，在时空预测任务中表现出色。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [208] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Amortized Latent Steering (ALS) 是一种新的测试时优化方法，通过离线计算单个向量来替代昂贵的迭代优化，在推理时以恒定成本应用，实现2-5倍加速，同时保持或超越贪婪CoT和自一致性基线的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时优化方法（如迭代优化和多步验证）计算成本过高，难以大规模应用。LatentSeek等方法虽然通过操纵隐藏表示来优化，但仍需要昂贵的逐查询优化循环。

Method: ALS计算成功生成与失败生成的隐藏状态之间的平均差异向量，在推理时使用这个方向来校准模型的隐藏表示。当解码偏离成功流形时，ALS将激活值推回正确方向。

Result: 在GSM8K和MATH-500基准测试中，ALS相比迭代方法实现2-5倍加速，同时匹配或超越贪婪CoT和自一致性基线，效率-准确率权衡提升高达101%。

Conclusion: 研究表明，潜在优化的主要收益可以通过离线方式获取，使得复杂的推理技术能够实际应用于生产环境。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [209] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 本文提出了一个AI驱动的框架，用于为给定线性系统推荐MCMC参数，以加速Krylov子空间求解器的收敛。该框架通过图神经网络预测预处理速度，并使用贝叶斯采集函数选择最优参数集，在减少50%搜索预算的情况下实现了约10%的收敛迭代次数减少。


<details>
  <summary>Details</summary>
Motivation: 大型稀疏线性系统在科学和工程中普遍存在，Krylov子空间求解器是解决这些系统的成熟方法。但对于病态矩阵，收敛可能很慢，因此实际部署通常需要预处理器。基于MCMC的矩阵求逆可以生成此类预处理器并加速Krylov迭代，但其有效性取决于参数，而最优参数因矩阵而异，手动或网格搜索成本高昂。

Method: 开发了一个AI驱动的框架：1）使用图神经网络代理模型从矩阵A和MCMC参数预测预处理速度；2）采用贝叶斯采集函数选择最有可能最小化迭代次数的参数集。

Result: 在先前未见过的病态系统上，该框架仅使用传统方法50%的搜索预算就实现了更好的预处理效果，收敛迭代次数减少了约10%。

Conclusion: 这些结果表明了将基于MCMC的预处理器纳入大规模系统的可行途径，为自动化参数优化提供了有效解决方案。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [210] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 基于贝叶斯统计的机器学习方法，用于设计能够动态适应不同用户和使用策略的数字界面


<details>
  <summary>Details</summary>
Motivation: 开发能够根据用户个人浏览习惯（而非群体偏好）动态适应的数字界面，提升用户体验

Method: 使用贝叶斯统计建模用户浏览行为，采用在线增量学习算法，生成任务模型并提供导航的图形化表示

Result: 模拟实验表明该方法在静态和非静态环境中都有效，能够可靠预测用户行为，即使数据量少或环境变化

Conclusion: 该研究为自适应系统开辟了新途径，通过帮助用户更好地导航和操作界面来改善用户体验

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [211] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 将轻量级随机梯度下降（L-SGD）算法扩展到RISC-V架构的微控制器单元（MCU），通过8位量化版本在RISC-V平台上实现内存使用减少4倍和训练速度提升2.2倍，同时保持可接受的精度损失。


<details>
  <summary>Details</summary>
Motivation: 现代物联网设备缺乏GPU或专用加速器，使得本地训练不可行，通常需要依赖云服务，这引发隐私问题和连接依赖性。联邦学习（FL）虽然能解决这些问题，但需要高效的优化算法。

Method: 扩展L-SGD算法到RISC-V MCU平台，评估32位浮点运算性能，并引入8位量化版本的L-SGD来缓解RISC-V MCU缺乏浮点单元（FPU）的性能限制。

Result: 在RISC-V平台上，8位量化L-SGD实现了内存使用减少近4倍，训练速度提升2.2倍，精度损失可忽略不计。

Conclusion: 该工作证明了在资源受限的RISC-V MCU上实现高效神经网络训练的可行性，为物联网设备的本地机器学习应用提供了重要支持。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [212] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线强化学习框架，通过难度自适应GRPO算法（ADAGRPO）提升移动GUI代理的性能，在AndroidWorld和AndroidLab上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 开发有效的移动GUI代理面临任务难度分布不均和大规模环境采样效率低下的挑战。

Method: 提出ADAGRPO算法，包含难度自适应正向回放、失败课程过滤和最短路径奖励调整策略，应用于Qwen2.5-VL-7B-Instruct和GLM-4.1V-9B-Base模型。

Result: MOBILERL-9B模型在AndroidWorld上达到75.8%的成功率，在AndroidLab上达到46.8%的成功率，均为最先进水平。

Conclusion: MOBILERL框架能稳定强化学习训练，提高样本效率，在多样化移动应用和任务中表现出色，已应用于AutoGLM产品并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [213] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen是一个基于生成式AI和势博弈理论的协作学习框架，用于解决跨组织联邦学习中的统计异质性和经济竞争问题，通过数据生成策略最大化社会福利。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注统计异质性，但忽略了组织间经济竞争对协作学习参与意愿的影响，以及统计异质性和竞争性交互作用的系统性影响。

Method: 提出CoCoGen框架，通过学习和效用公式建模竞争与统计异质性，将每轮训练建模为加权势博弈，并推导基于生成式AI的数据生成策略。

Result: 在Fashion-MNIST数据集上的实验表明，CoCoGen在不同异质性和竞争水平下均优于基线方法，有效提升了社会福利。

Conclusion: CoCoGen成功解决了联邦学习中统计异质性和经济竞争的复合挑战，为竞争环境下的协作学习提供了有效解决方案。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [214] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 该研究应用监督机器学习算法，结合文本和数值特征预测咖啡评分，发现集成方法和多层感知器在性能上优于简单分类器。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习在咖啡感官评价中的应用，为传统专业品鉴提供数据驱动的补充方法。

Method: 通过文本清洗、TF-IDF特征提取和SelectKBest特征选择，训练六种机器学习模型（决策树、K近邻、多层感知器、随机森林、极端随机树和XGBoost），并使用优化超参数。

Result: 集成方法（极端随机树、随机森林、XGBoost）和多层感知器在F1分数、G-mean和AUC等指标上表现优于简单分类器。

Conclusion: 严格的特征选择和超参数调优对于构建稳健的感官产品评价预测系统至关重要，为传统咖啡品鉴提供了数据驱动的补充方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [215] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL是一个基于强化学习的护士-患者分配框架，通过结构化状态编码、约束动作掩码和注意力机制，解决传统方法难以处理的多约束动态调度问题。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临有限护理资源的高效分配压力，需要考虑技能异质性、患者病情严重程度、员工疲劳和护理连续性等多重因素，传统优化和启发式方法难以有效处理这些动态多约束环境。

Method: 使用近端策略优化（PPO）算法，结合可行性掩码确保分配符合现实约束，通过结构化状态编码和基于注意力的技能、疲劳和地理上下文表示，动态适应患者到达和护士可用性变化。

Result: 在真实护士和患者数据的模拟中，NurseSchedRL相比基线启发式和无约束强化学习方法，实现了更高的调度效率、更好的技能与患者需求匹配度以及更低的疲劳水平。

Conclusion: 研究结果表明强化学习在复杂高风险医疗人力管理决策支持方面具有巨大潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [216] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 本文评估了联邦学习在电动汽车充电站异常检测中的性能，特别是在系统异构性和非IID数据条件下的表现。研究发现FedAvgM在异构环境下优于FedAvg，能够在不显著损失性能的情况下实现隐私保护的EVCS安全。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的快速扩张，保护基于物联网的充电站免受网络威胁变得至关重要。集中式入侵检测系统因涉及敏感网络和用户数据而存在隐私问题，联邦学习成为一种有前景的替代方案。

Method: 使用FedAvg和FedAvgM两种广泛研究的优化方法，在系统异构性和非IID数据条件下进行实验，评估它们在电动汽车充电站异常检测中的有效性。

Result: 在IID设置下，FedAvg使用相同神经网络实现了优于集中式模型的性能。但在非IID数据和系统异构性条件下性能下降。FedAvgM在异构设置中始终优于FedAvg，显示出更好的收敛性和更高的异常检测准确率。

Conclusion: 联邦学习能够处理基于物联网的EVCS中的异构性而不会显著损失性能，FedAvgM是构建稳健、隐私保护的EVCS安全的有前景解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [217] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL是一个用于解释大型语言模型中稀疏自编码器（SAE）特征的框架，旨在提升安全领域的机制理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前大多数安全研究主要关注评估LLM输出或特定安全任务，难以应对更广泛的未定义风险。现有的SAE应用未能用细粒度的安全相关概念来解释特征，无法充分处理安全关键行为。

Method: 系统识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略来扩展解释过程。

Result: 将发布包含SAE检查点和人类可读神经元解释的全面工具包，支持对安全风险进行实证分析。

Conclusion: 该框架有助于促进LLM安全研究，通过机制性理解来应对安全风险。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [218] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种基于Gauss-Hermite求积的方法，用于解耦机器学习代理模型中的嵌套不确定性，提高可靠性分析的准确性


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在物理可靠性分析中广泛应用，但其模型近似误差引入的认识不确定性会与模型输入的随机不确定性耦合，影响可靠性预测的准确性

Method: 使用Gauss-Hermite求积方法解耦嵌套不确定性，在随机不确定性条件下使用一阶和二阶可靠性方法评估条件失效概率，然后在整个认识不确定性实现上积分这些概率

Result: 三个示例表明，该方法在保持计算效率的同时，比忽略模型不确定性的传统方法产生更可信的预测结果

Conclusion: 所提出的方法能够有效处理机器学习代理模型中的不确定性耦合问题，为可靠性分析提供更准确的预测

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [219] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合STL时间序列分解和GRU神经网络的模型，用于预测地铁换乘客流，相比现有方法显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 在地铁智能交通系统中，准确的换乘客流预测是优化运营计划和提升运输效率的关键环节，需要改进现有预测理论以支持智能运营决策。

Method: 首先使用Keras构建GRU模型，预处理地铁刷卡数据并使用深度优先搜索算法识别乘客出行路径，构建换乘客流时间序列；然后采用STL算法将时间序列分解为趋势、周期和残差分量，使用3σ原则处理异常值；最后完成客流预测。

Result: 实验结果表明，与LSTM、GRU和STL-LSTM模型相比，STL-GRU模型在工作日（除周五外）、周五和休息日的预测精度显著提高，MAPE分别降低了至少2.3、1.36和6.42个百分点。

Conclusion: STL-GRU组合预测模型能够有效提高地铁换乘客流的预测准确性，为地铁智能运营决策提供了可靠支持。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [220] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 论文发现基于Transformer的机器学习应用在解决物理问题时，其权重矩阵呈现随机特征，与物理问题的数学结构没有直接可识别联系，这表明机器学习与科学方法可能是两种不同但互补的知识获取路径。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习模型（特别是Transformer）在解决物理问题时的内在工作机制，以及其权重矩阵是否能够反映物理问题的数学结构。

Method: 通过分析Transformer模型在解决两个代表性物理应用中的权重矩阵特征，观察其与物理问题数学结构的关系。

Result: 研究发现权重矩阵呈现随机特征，与物理问题的数学结构没有直接可识别联系，但可能与广义路径积分技术有相似之处。

Conclusion: 机器学习与科学方法可能是互补的知识获取路径，但严格的解释性（网络参数与物理结构的直接对应）可能难以实现，需要警惕缺乏洞察力的知识获取风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [221] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一个参数高效的对抗性混合专家框架，用于工业规模的自进化持续指令调优，通过双专家设计和GAN鉴别器来平衡知识保留和跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 在现实工业环境中，大语言模型需要持续学习以适应动态数据分布，但现有持续学习方法存在灾难性遗忘问题，即训练新任务会降低对早期任务的性能。

Method: 采用双LoRA专家设计：专用专家保留任务特定知识，共享专家实现跨任务迁移；集成任务感知鉴别器通过对抗学习确保共享专家只传递任务相关信息。

Result: 在MTL5基准和腾讯3基准上的实验验证了有效性，在腾讯视频平台的内容合规审查A/B测试中，手动审核成本降低了15.3%。

Conclusion: MoE-CL在大规模工业部署中具有实用性，能够实现持续适应和稳定迁移。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [222] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文提出了一种加权梯度跟踪的分布式隐私保护算法，解决了梯度跟踪技术中的隐私泄露风险，并在时变异构步长下证明了算法的精确收敛性。


<details>
  <summary>Details</summary>
Motivation: 梯度跟踪技术虽然能提高分布式优化的收敛速度，但存在固有的隐私泄露风险，需要保护智能体在优化过程中的私有信息不被攻击者获取。

Method: 提出加权梯度跟踪分布式隐私保护算法，通过衰减权重因子消除梯度跟踪中的隐私泄露风险，并在时变异构步长条件下分析算法收敛性。

Result: 在温和假设下证明了算法能精确收敛到最优解，并通过分布式估计问题和卷积神经网络分布式训练的数值模拟验证了算法有效性。

Conclusion: 所提出的算法成功解决了梯度跟踪的隐私泄露问题，同时保持了良好的收敛性能，适用于实际的分布式优化场景。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [223] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出了一种新颖的静态-动态图融合网络（SDGF），通过双路径图结构学习方法捕捉多尺度序列间相关性，解决现有方法在建模多尺度依赖关系方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中序列间相关性至关重要，但这些关系在不同时间尺度上表现出复杂的动态特性。现有方法在建模这些多尺度依赖关系方面存在局限，难以捕捉其复杂且演化的本质。

Method: 使用基于先验知识的静态图锚定长期稳定依赖关系，同时采用多级小波分解提取多尺度特征构建自适应学习的动态图。设计了注意力门控模块智能融合这两种互补信息源，并使用多核扩张卷积网络加深对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上的综合实验证明了所提出模型的有效性。

Conclusion: SDGF模型能够有效捕捉多尺度序列间相关性，在多元时间序列预测任务中表现出色。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [224] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文提出了一个大规模数据集，系统分析了大型语言模型（LLMs）结构配置与性能之间的关系，通过数据挖掘和机制可解释性技术验证结构选择对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在规模和能力上快速增长，但关于结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。本文旨在填补这一空白。

Method: 构建包含多样化开源LLM结构及其在多基准测试中性能的大规模数据集，采用系统性数据挖掘分析方法，结合机制可解释性技术验证发现。

Result: 研究量化了结构配置与性能之间的关系，分析了不同结构选择在各类基准测试中的影响，并通过可解释性技术进一步证实了这些发现。

Conclusion: 这项工作为LLM优化提供了数据驱动的见解，旨在指导未来模型的针对性开发和应用，相关数据集将公开发布。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [225] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了一个名为LoRALib的统一基准，用于标准化LoRA-MoE方法的评估，通过大规模实验发现LoRAMoE性能最佳，且优先选择与目标任务相关的LoRA模块能进一步提升MoE性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA-MoE方法在模型、数据集、超参数和评估方法上缺乏统一标准，难以进行公平比较。

Method: 将40个下游任务的数据集标准化为统一格式，使用相同超参数微调获得680个LoRA模块，基于此LoRA库在3种代表性LoRA-MoE方法和不同LoRA选择机制上进行大规模实验。

Result: 广泛实验表明LoRAMoE表现最佳，优先选择与目标任务相关的LoRA模块能进一步提升MoE性能。

Conclusion: 这些发现将为未来工作提供启发，数据集和LoRA库已开源。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [226] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM算法通过利用秩基准与分布基准的结构等价性，直接在秩诱导Plackett-Luce参数化中更新，确保算法在每一轮都保持在秩诱导分布类中。


<details>
  <summary>Details</summary>
Motivation: 现有方法在专家身份上操作，无法保持秩基准与分布基准的等价性。RIPLM旨在开发一个既保持秩忠实性又具有方差自适应的算法。

Method: 使用秩诱导Plackett-Luce镜像下降方法，直接在秩诱导PL参数化中更新，确保分布始终属于秩诱导分布类。

Result: RIPLM是第一个在休眠专家设置中同时具有秩忠实性和方差自适应性的算法。

Conclusion: RIPLM通过结构等价性的利用，成功实现了在保持秩基准等价性的同时具备方差自适应能力。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [227] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本文比较了规则分类器FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。研究发现FOLD-SE在保持可解释性的同时，性能损失很小，证明了规则方法可以在可解释性和性能之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着对兼具准确性、效率和可解释性的机器学习模型需求增长，传统模型存在准确性与可解释性之间的权衡。本研究旨在评估新型规则分类器FOLD-SE在平衡这些指标方面的表现。

Method: 使用分类数据集，以准确率、F1分数和处理时间作为主要性能指标，比较FOLD-SE与FOLD-R++在二分类任务中的表现，以及FOLD-SE与XGBoost在多分类任务中的性能。

Result: FOLD-SE在二分类中比FOLD-R++提供更少的规则，仅损失少量准确性和处理效率；在多分类中比XGBoost更精确高效，同时生成可理解的规则集。

Conclusion: FOLD-SE是二分类和多分类任务的更好选择，规则方法如FOLD-SE能够弥合可解释性与性能之间的差距，成为黑盒模型的有力替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [228] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究开发了一个结合预测建模和基因不可知通路映射的机器学习框架，用于识别2型糖尿病高风险个体并发现潜在治疗靶点。


<details>
  <summary>Details</summary>
Motivation: 代谢性疾病特别是2型糖尿病对遗传易感人群（如皮马印第安人）造成重大健康负担，需要开发早期检测和精准干预的新方法。

Method: 使用皮马印第安人数据集，应用逻辑回归和t检验识别T2DM关键预测因子，结合通路映射策略将预测因子与胰岛素信号、AMPK和PPAR等关键信号网络联系起来。

Result: 模型整体准确率达到78.43%，通过通路富集分析验证了GLP-1/GIP受体激动剂、AMPK激活剂、SIRT1调节剂和植物化学物等治疗策略。

Conclusion: 该框架通过提供可解释和可扩展的解决方案，推进了代谢性疾病早期检测和靶向干预的精准医学发展。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [229] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个完全自动化的AI驱动管道，用于从Kaplan-Meier图中高精度重建个体患者数据，解决了传统手动数字化方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动数字化，容易出错且缺乏可扩展性，需要自动化解决方案来提高临床研究中的证据合成效率。

Method: KM-GPT整合了先进的图像预处理、GPT-5驱动的多模态推理和迭代重建算法，采用混合推理架构自动将非结构化信息转换为结构化数据流。

Result: 在合成和真实数据集上的严格评估显示，KM-GPT始终表现出卓越的准确性，成功应用于胃癌免疫治疗试验的荟萃分析。

Conclusion: KM-GPT通过自动化传统手动流程，为临床研究提供可扩展的基于网络的解决方案，支持基于证据的决策制定。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [230] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI是一个基于条件扩散模型的时空数据插补方法，通过双向S4模型进行预插补，并设计了噪声感知的时空网络来捕捉不同扩散步骤中的依赖关系变化，在三个真实数据集上实现了高达46.4%的误差降低。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因存在缺失值，扩散模型在时空插补任务中表现出色，但现有方法在提取时空依赖关系时存在误差累积问题，且忽略了不同扩散步骤中噪声数据的依赖关系变化。

Method: 提出AdaSTI方法，包含基于双向S4模型的BiS4PI网络进行预插补，通过时空条件化器(STC)提取条件信息，以及带有门控注意力机制的噪声感知时空(NAST)网络来捕捉不同扩散步骤的变异性依赖关系。

Result: 在三个真实世界数据集上的广泛实验表明，AdaSTI在所有设置下都优于现有方法，插补误差最高降低了46.4%。

Conclusion: AdaSTI通过自适应地处理不同扩散步骤中的时空依赖关系变化，显著提升了时空数据插补的性能，证明了所提方法的有效性。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [231] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本研究提出了一种多标签分类框架，用于预测ICU患者的护理升级触发因素（CETs），包括呼吸衰竭、血流动力学不稳定、肾功能损害和神经功能恶化。使用MIMIC-IV数据库，基于前24小时数据训练XGBoost模型，在四个CET类别上取得了优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的早期预警系统（如SOFA或MEWS）局限于单一结果预测，无法捕捉临床恶化的多维度特征。ICU患者常表现出复杂、重叠的生理恶化迹象，需要及时升级护理。

Method: 使用MIMIC-IV数据库中85,242例ICU住院数据，基于前24小时的生命体征汇总、实验室值和静态人口统计学特征，训练多标签分类模型预测24-72小时内发生的四种CETs。评估了多种分类模型，包括XGBoost。

Result: XGBoost模型表现最佳，在四个CET类别上的F1分数分别为：呼吸0.66、血流动力学0.72、肾功能0.76、神经功能0.62。特征分析显示呼吸频率、血压和肌酐等临床相关参数是最具影响力的预测因子。

Conclusion: 该框架展示了在不依赖复杂时间序列建模或自然语言处理的情况下，实现早期、可解释临床警报的实际潜力，为ICU患者的多维度临床恶化提供了有效的预测工具。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [232] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一个基于概念的CNN可解释性框架，通过追踪概念在层间的传播来模拟模型的内部"思考路径"，包含概念注意力和概念路径两个关键组件。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了单个过滤器的语义角色和概念在层间的动态传播，需要更深入地理解CNN的内部推理逻辑。

Method: 提出ConceptFlow框架：1）概念注意力将每个过滤器与相关高级概念关联；2）概念路径通过概念转移矩阵量化概念在过滤器间的传播和转换。

Result: 实验结果表明ConceptFlow能够产生语义上有意义的模型推理洞察，验证了概念注意力和概念路径在解释决策行为方面的有效性。

Conclusion: 通过建模层次化的概念路径，ConceptFlow为CNN内部逻辑提供了更深入的见解，支持生成更忠实和人类对齐的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [233] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏表示的训练效率框架STS，通过视觉令牌压缩器和层动态跳过器来提升多模态大语言模型的训练效率


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型训练效率低下，主要由于多模态数据引入的长输入序列和层间计算利用率低的问题

Method: STS框架包含两个关键组件：视觉令牌压缩器（减少视觉令牌信息负载）和层动态跳过器（在前后向传播中动态跳过语言模型中不必要的层）

Result: 该方法在多个基准测试中进行了广泛评估，证明了其有效性和效率

Conclusion: 该稀疏训练方案适用于多种MLLM架构，能够显著提升训练效率

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [234] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS是一种新颖的神经架构搜索预测器，通过全局编码方案和共享超网络增强架构表示学习，在少样本场景下显著提升性能预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索性能评估耗时严重，现有神经预测器由于无法充分捕捉架构间复杂关系而泛化能力差。

Method: 提出HyperNAS框架，包含全局编码方案捕捉宏观结构信息，共享超网络作为辅助任务增强架构间模式探索，并采用动态自适应多任务损失确保训练稳定性。

Result: 在五个代表性搜索空间（包括ViT）上的实验显示，HyperNAS在少样本场景下优势明显，在CIFAR-10上达到97.60% top-1准确率，ImageNet上达到82.4% top-1准确率，且样本使用量减少至少5倍。

Conclusion: HyperNAS通过增强架构表示学习，显著提升了神经架构搜索预测器的性能，特别是在数据稀缺情况下表现出色，为NAS领域提供了有效的解决方案。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [235] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一个用于测井解释的基础模型，通过三阶段训练（标记化、自监督预训练、多任务适应）在1200口井的多曲线测井数据上预训练，在孔隙度估计和岩性分类任务上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 测井解释面临工具响应异质性、噪声信号和有限标签的挑战，需要开发能够处理这些问题的可扩展AI方法。

Method: 三阶段方法：1）将测井数据块标记化为地质标记；2）使用掩码标记建模和地层感知对比学习进行自监督预训练；3）通过少样本微调进行多任务适应。

Result: WLFM在孔隙度估计上达到0.0041 MSE，岩性分类准确率74.13%；微调后分别提升至0.0038 MSE和78.10%准确率。模型表现出层感知能力，可学习可重用的地质词汇表。

Conclusion: WLFM为地质AI提供了一个可扩展、可解释和可迁移的骨干网络，为测井、地震和文本数据的多模态集成奠定了基础。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [236] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion是一个深度学习框架，通过结合64亿参数蛋白质语言模型和强化学习，从头设计抗生素肽，实现了100%的命中率和广谱抗菌活性。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性预计到205年每年导致1000万人死亡，迫切需要开发新型抗生素。

Method: 使用64亿参数蛋白质语言模型，通过强化学习优化，结合最小抑菌浓度分类器和可微分物理化学目标进行多目标优化。

Result: 体外评估100种设计肽显示低MIC值（部分达到纳摩尔范围），100%命中率，99/100化合物对至少两种临床相关细菌具有广谱抗菌活性。

Conclusion: 该方法通过统一生成、评分和多目标优化，在单一流程中快速产生多样化、高效的候选分子，为肽类抗生素提供了可扩展的途径。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [237] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的多模态大语言模型，通过架构、数据和训练方法的创新，在保持高效率的同时实现了超越更大模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型训练和推理效率低下的问题，使其更易获取和扩展。

Method: 采用统一的3D-Resampler模型架构实现图像和视频的紧凑编码；使用统一学习范式处理文档知识和文本识别；采用混合强化学习策略提升长短推理能力。

Result: 在OpenCompass评估中超越了GPT-4o-latest和Qwen2.5-VL 72B等模型，在VideoMME基准测试中达到30B以下模型的最优性能，仅需Qwen2.5-VL 7B的46.7%GPU内存和8.7%推理时间。

Conclusion: MiniCPM-V 4.5证明了通过精心设计的架构和训练策略，小规模模型也能实现超越大规模模型的性能，同时显著提升效率。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [238] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本文提出并比较了9种训练方法，用于探索具有参数化线性B样条激活函数的神经网络中的双重优化动态，相比传统ReLU模型实现了显著更低的错误率。


<details>
  <summary>Details</summary>
Motivation: 传统的激活函数（如ReLU、tanh、sigmoid）是静态选择的，通过优化激活函数的形状，可以训练出更参数高效和准确的模型。

Method: 使用参数化线性B样条激活函数，探索9种不同的训练方法来实现神经网络中激活函数和权重的双重优化。

Result: 实验结果显示，在FNN中实现了高达94%的更低端模型错误率，在CNN中实现了51%的更低错误率，相比传统基于ReLU的模型。

Conclusion: 虽然这种方法带来了显著的准确率提升，但代价是增加了开发和训练复杂性以及端模型延迟。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [239] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 本文研究卡车与无人机协同的最后一公里配送问题，提出了一种混合强化学习方法，在电池管理约束下优化配送时间。该方法在50个节点的测试实例上比传统ALNS方法提升2.73%，接近神经网络方法的性能。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送中卡车与无人机的协同配送具有效率优势，但需要解决复杂的电池管理约束（飞行速度、续航预算、充电时间等），传统方法难以有效处理这些约束。

Method: 混合强化学习求解器：ALNS生成卡车路线，指针/注意力策略调度无人机任务。使用精确时间线模拟器确保可行性，通过掩码贪婪/波束解码计算真实完工时间。

Result: 在N=50、E=0.7、R=0.1的欧几里得实例上，平均完工时间为5.203±0.093，比ALNS的5.349±0.038提升2.73%，与神经网络的5.208±0.124相差仅0.10%。学习到的调度器在平衡卡车等待时间方面表现优异。

Conclusion: 所提出的混合强化学习方法能有效处理卡车-无人机协同配送的复杂约束，在最小化总完工时间方面优于传统启发式方法，为实际应用提供了可行的解决方案。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [240] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: 本文提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数，指导扩散大语言模型理解数学和逻辑模式，在数学和逻辑问题上分别实现5-10%和约2%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在数值敏感的数学任务和顺序敏感的逻辑任务学习上面临挑战，现有训练方法缺乏对数学和逻辑模式的全面理解。

Method: 提出DSFT策略，调整掩码策略和损失函数，可与预训练、强化学习等训练方法灵活结合。

Result: 在LLaDA和Dream系列模型上验证，小规模数据上的DSFT能在数学问题上提升5-10%，逻辑问题上提升约2%。

Conclusion: 这种掩码方法为未来学习特定模式提供了思路，可轻松高效地与其他训练方法结合并应用于各种dLLMs。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [241] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个用于移动数据预测的基础模型，能够统一预测基站流量、用户应用使用和信道质量三种数据类型，通过软提示学习和时间掩码机制实现多任务预测。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法需要为不同数据类型定制专门模型，增加了大规模异构网络中的复杂性和部署成本，需要一种统一的基础模型来简化预测流程。

Method: 提出MobiGPT基础模型，采用软提示学习方法帮助模型理解不同数据类型的特征，并引入时间掩码机制支持短期预测、长期预测和分布生成三种预测任务。

Result: 在包含10万多个样本的真实数据集上评估，MobiGPT相比现有模型将预测准确率提高了27.37%、20.08%和7.27%，并在零样本/少样本场景下表现优异，改进超过21.51%。

Conclusion: MobiGPT作为一个基础模型，展示了强大的泛化能力和迁移能力，能够有效支持移动网络中的多类型数据预测需求。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [242] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种新型训练和推理架构，通过物理隔离的专家混合模型将计算能力内生集成到神经网络中，解决大语言模型无法进行高精度数值计算的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型无法内生集成高精度数值计算能力，而主流多智能体方法存在通信开销大、多模态能力效率低和可扩展性有限的问题。

Method: 提出PiMoE架构，分别训练专家、文本到计算模块和路由器，在推理时路由器在token级别指导计算和推理，实现单链思维中的迭代交替。

Result: 在推理-计算任务上的评估显示，PiMoE不仅比直接微调LLM精度更高，而且在响应延迟、token使用和GPU能耗方面相比主流多智能体方法有显著改进。

Conclusion: PiMoE为下一代科学或工业智能系统提供了高效、可解释和可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [243] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA框架通过投影优先策略解决联邦图学习中域偏移导致的梯度噪声问题，使用重要性感知的两阶段管道来去噪客户端更新，实现更稳定收敛和更高准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习在域偏移场景下（如Twitch Gamers和多语言Wikipedia网络）会导致客户端模型表示不兼容，传统聚合方法不稳定且无效。研究发现主要问题不是权重方案而是噪声梯度信号。

Method: 提出FedIA框架，采用投影优先策略：1）服务器端top-ρ掩码保留约5%最信息丰富的坐标；2）轻量级影响正则化动量权重抑制异常客户端。该方法不增加上行流量且服务器内存开销可忽略。

Result: 在同构（Twitch Gamers）和异构（Wikipedia）图上，FedIA相比九个强基线方法实现了更平滑稳定的收敛和更高的最终准确率。收敛分析显示动态投影保持了最优收敛速率。

Conclusion: FedIA通过重要性感知的投影优先策略有效解决了联邦图学习中的域偏移问题，提供了一种高效可部署的解决方案。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [244] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR是一种新颖的大语言模型量化方法，通过高斯分布感知的码本表示和定制CUDA内核，在4位量化下实现最先进的性能，同时获得2.21-3.04倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ量化方法存在两个主要问题：RTN方法无法适应LLM权重的高斯分布特性，码本方法虽然考虑了分布但存在内存访问效率低下的问题。

Method: 提出SBVR方法，将权重值映射到非均匀表示点，其分布遵循LLM权重的实际高斯分布；设计定制CUDA内核，支持SBVR格式的直接矩阵向量乘法而无需解压缩。

Result: 在各种模型上的评估显示，SBVR在4位量化下实现了最先进的困惑度和准确度基准性能，相比原生FP16模型获得2.21-3.04倍的端到端token生成加速。

Conclusion: SBVR通过硬件友好的高斯分布码本表示方法，成功克服了现有量化方法的局限性，在保持高精度的同时显著提升了推理速度。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [245] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 该论文提出了一个大规模基准测试，用于评估大语言模型的地理空间路线认知能力，发现LLMs在路线反转任务中存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过自然语言理解地理空间信息，但大语言模型的地理空间认知能力尚未得到充分探索。先前研究受限于不可量化的指标、有限的评估数据集和模糊的研究层次。

Method: 创建了包含全球12个大都市36000条路线的大规模评估数据集；开发了PathBuilder工具，用于自然语言指令与导航路线之间的双向转换；提出了新的评估框架和指标来评估11个SOTA LLMs的路线反转能力。

Result: 基准测试显示LLMs在路线反转任务中存在明显限制：大多数反转路线既无法返回起点，也不接近最优路线。LLMs还面临路线生成鲁棒性低和对错误答案置信度高的问题。

Conclusion: LLMs在地理空间路线认知方面存在显著局限性，需要进一步改进模型的地理空间理解能力。研究提供了代码和数据集供后续研究使用。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [246] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 提出多模态思维链框架MCoT，解决对话导航中的自我中心到世界中心的空间方向推理问题，在中文对话导航基准上达到高精度表现


<details>
  <summary>Details</summary>
Motivation: 解决室内或复杂设施中GPS信号弱、详细地图不可用时，对话代理需要将自我中心表达转换为世界中心方向的关键挑战

Method: 提出多模态思维链框架，整合ASR转录语音和地标坐标，通过三步推理过程：提取空间关系、坐标映射到绝对方向、推断用户朝向，并在Taiwan-LLM-13B-v2.0-Chat模型上采用课程学习策略

Result: MCoT在干净转录本上达到100%方向准确率，ASR转录本上达到98.1%，显著优于单模态和非结构化基线，在噪声对话条件下表现出鲁棒性

Conclusion: 结构化MCoT空间推理为实现可解释和资源高效的具身导航提供了有前景的路径

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [247] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出了变分任务向量组合方法，通过贝叶斯推理框架将组合系数作为潜变量进行估计，实现样本特定的任务向量组合，相比传统任务级别方法有更好性能。


<details>
  <summary>Details</summary>
Motivation: 传统任务向量组合方法在任务级别操作，无法实现样本特定的灵活组合。观察到任务向量存在结构冗余，需要稀疏化处理以保留最有信息量的组件。

Method: 引入Spike-and-Slab先验促进稀疏性，开发门控采样机制基于不确定性和重要性过滤组合系数，构建可控后验分布，减少高维稀疏空间中的方差和采样低效问题。

Result: 实验结果表明，该方法在所有数据集上均优于现有方法，通过选择性利用任务向量中最可靠和最有信息量的组件实现更好性能。

Conclusion: 该方法为高效有效的任务向量组合建立了新标准，具有实际应用价值，提高了变分框架的稳定性、可解释性和泛化能力。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [248] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE是一个包含2.22亿个化合物的大规模、多样化、严格筛选的数据集，旨在解决现有分子数据集规模小、质量差的问题，为分子表示学习提供类似ImageNet的标准资源。


<details>
  <summary>Details</summary>
Motivation: 现有分子数据集的规模、多样性和质量限制了基础模型的泛化能力，阻碍了分子表示学习在化学信息学中的发展。

Method: 从6个大规模数据库中通过自动化筛选流程构建MolPILE数据集，并对现有预训练数据集进行全面分析，然后在MolPILE上重新训练现有模型。

Result: 在MolPILE上重新训练现有模型能够提高模型的泛化性能。

Conclusion: MolPILE为分子化学提供了一个标准化的模型训练资源，解决了该领域对类似ImageNet数据集的需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [249] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP是一种通过将多令牌预测训练与其推理模式对齐来改进多步草稿质量的简单有效方法，显著提升推测解码性能，实现2.03倍平均加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型变得越来越强大，自回归生成的顺序性质造成了基本吞吐量瓶颈，限制了实际部署。虽然多令牌预测在模型训练效率和性能方面表现出显著优势，但其固有的推理加速潜力尚未得到充分探索。

Method: 通过自蒸馏数据对具有位置共享权重的单个MTP头进行微调，使其能够捕获连续未来令牌之间的依赖关系，并在多个递归草稿步骤中保持高接受率。同时集成语言感知动态词汇压缩到MTP头中，进一步减少起草过程中的计算开销。

Result: 在七个不同基准测试中，FastMTP相比标准下一个令牌预测实现了2.03倍平均加速，且输出质量无损，比普通MTP性能提升82%。

Conclusion: FastMTP仅需轻量级训练，可与现有推理框架无缝集成，为加速LLM推理提供了实用且快速部署的解决方案。

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [250] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出了一种新的多工作者选择算法M-DSL，用于解决分布式群体学习中的非独立同分布数据挑战，通过引入非i.i.d.度度量来指导有效的工作者选择，提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 分布式群体学习在边缘物联网中面临非独立同分布数据的挑战，这会降低学习性能并导致训练行为发散，目前缺乏关于数据异构性如何影响模型训练准确性的理论指导。

Method: 提出M-DSL算法，引入新的非i.i.d.度度量来量化本地数据集间的统计差异，建立数据异构性度量与DSL性能评估之间的联系，指导选择对全局模型更新有显著贡献的多工作者。

Result: 在不同异构数据集和非i.i.d.数据设置下进行广泛实验，数值结果验证了M-DSL在性能提升和网络智能增强方面优于基准方法。

Conclusion: M-DSL算法有效解决了分布式异构数据环境下的学习挑战，提供了理论收敛性分析，并通过实验证明了其优越性能。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [251] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar是一种新颖的图神经网络全局解释方法，通过识别嵌入空间中的代表性节点（范例）并使用自然语言规则解释预测，解决了现有方法在大规模现实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在节点分类中应用广泛，但其决策过程不透明限制了信任和采用。现有的全局解释方法依赖小图中的模式发现，在大规模现实场景中效果不佳，因为子图重复罕见、节点属性高维且预测来自复杂的结构-属性交互。

Method: GnnXemplar受认知科学中的范例理论启发，将范例选择建模为反向k近邻的覆盖最大化问题，并提供高效的贪心近似算法。使用大语言模型的自优化提示策略生成可解释的自然语言规则。

Result: 在多样化基准测试中，GnnXemplar在保真度、可扩展性和人类可解释性方面显著优于现有方法，通过60名参与者的用户研究验证了其有效性。

Conclusion: GnnXemplar为图神经网络提供了一种可扩展且人类可理解的全局解释方法，解决了现有方法在大规模现实场景中的关键限制。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [252] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 提出了GETAD框架，通过整合道路网络拓扑、路段语义和历史出行模式来检测轨迹异常，使用图注意力网络和Transformer解码器，在道路约束环境中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅考虑轨迹的有限方面，在欧几里得空间中分析轨迹，忽略了底层移动网络（如道路网络）的约束和连通性信息。

Method: GETAD框架使用图注意力网络学习道路感知嵌入，结合基于图的位置编码，采用Transformer解码器建模序列移动，并使用结合自回归预测和监督链接预测的多目标损失函数。

Result: 在真实世界和合成数据集上的实验表明，GETAD相比现有方法取得了持续改进，特别是在检测道路约束环境中的细微异常方面表现优异。

Conclusion: 将图结构和上下文语义整合到轨迹建模中，能够实现更精确和上下文感知的异常检测。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [253] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文探讨了为什么强化学习预训练算法能够产生支持上下文强化学习的网络参数，并通过理论证明支持这一假设。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习智能体通过更新神经网络参数来适应任务，但研究发现某些预训练的智能体能够在参数固定的情况下，仅通过上下文信息解决新任务。本文旨在解释为什么标准RL预训练算法能够产生支持这种上下文强化学习能力的参数。

Method: 通过案例研究，证明当Transformer网络被预训练用于策略评估时，预训练损失的全局最小值之一能够支持上下文时序差分学习。

Result: 理论分析表明，预训练损失的全局最小值确实能够使网络具备上下文强化学习的能力。

Conclusion: 研究为"支持上下文强化学习的参数是预训练损失的最小值"这一假设提供了初步支持，揭示了预训练过程与上下文学习能力之间的理论联系。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [254] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 这篇论文对深度学习优化器进行了系统性综述，涵盖了从随机梯度下降到最新优化器（如Momentum、AdamW、Sophia、Muon等）的发展历程，详细分析了各种优化器的更新规则、技术特点、超参数设置及其对优化过程的贡献。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的快速发展，各种不同方法的优化器被开发出来，学习效果高度依赖于训练过程中使用的优化器。本研究旨在对文献中提出并受到关注的各类优化器进行综述，为理解当前优化器状态和识别未来潜在发展领域提供全面资源。

Method: 采用文献综述方法，按时间顺序（从随机梯度下降到最新的Momentum、AdamW、Sophia、Muon等）逐一考察各种优化器，详细呈现每种优化器的更新规则，解释相关概念和变量，并讨论这些优化器应用的技术、对优化过程的贡献以及默认超参数设置。

Result: 研究提供了深度学习优化器的全面概述，突出了各种优化器的独特特征，包括它们的技术方法、性能特点和适用场景。

Conclusion: 该研究不仅为理解当前优化器状态提供了有价值的资源，还指出了深度学习模型优化中面临的开放挑战，为未来发展方向提供了见解。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [255] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种新颖的信息保留混沌游戏表示方法（R-CGR），解决了传统CGR方法在几何映射过程中丢失序列信息的根本限制，实现了完整的序列恢复。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在生物序列分析中存在序列信息丢失的问题，这限制了其在需要精确序列恢复的应用中的使用。

Method: 通过显式路径编码结合有理数精度控制，实现了完美的序列重建。与传统纯几何方法不同，R-CGR通过全面的路径存储来保持每个步骤的位置和字符信息。

Result: 在生物序列分类任务中表现出与传统序列方法相竞争的性能，同时提供可解释的几何可视化。

Conclusion: 该方法为生物信息学分析开辟了新途径，在需要准确性和序列恢复的应用中具有重要价值。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [256] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 本文提出KANDI方法，结合Kolmogorov-Arnold网络和扩散策略，用于解决离线强化学习在医疗健康应用中奖励函数定义困难和策略对齐的挑战，特别是在老年人跌倒风险干预场景。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗健康应用中面临两大挑战：直接奖励函数难以定义，以及逆强化学习在复杂环境中难以准确推断专家行为的奖励函数。特别是在老年人跌倒风险干预的物理活动促进场景中，这些问题尤为突出。

Method: KANDI方法利用Kolmogorov-Arnold网络的灵活函数逼近能力从低跌倒风险老年人（专家）行为中学习奖励函数，同时在Actor-Critic框架中使用基于扩散的策略进行动作优化，提高离线强化学习的效率。

Result: KANDI在D4RL基准测试中优于现有最先进方法，并在PEER研究的临床试验中验证了其在跌倒风险干预项目中促进老年人身体活动的实际应用效果。

Conclusion: KANDI为解决医疗健康应用中离线强化学习的关键挑战提供了有效解决方案，在活动促进干预策略方面展现出巨大潜力。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [257] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet结合图神经网络和神经常微分方程，解决了传统数值求解器计算成本高和GNN自回归预测误差累积问题，在结构力学问题上实现了更好的长期预测精度和计算加速。


<details>
  <summary>Details</summary>
Motivation: 传统网格物理系统数值求解器计算成本高，而标准GNN自回归方法在长期预测中存在误差累积和不稳定性问题，需要开发更有效的替代模型。

Method: 提出MeshODENet框架，将GNN的空间推理能力与神经ODE的连续时间建模相结合，用于处理非线性大变形结构力学问题。

Result: 在一维和二维弹性体大变形问题上，该方法在长期预测精度和稳定性上显著优于基线模型，同时相比传统求解器实现了大幅计算加速。

Conclusion: MeshODENet为开发数据驱动替代模型提供了一种强大且通用的方法，可加速复杂结构系统的分析和建模。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [258] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一个基于Transformer的多模态框架，用于持续和长期血糖预测，通过交叉注意力和多尺度注意力机制整合生理和行为信号，并采用知识保留技术防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决血糖预测中多源信号采样率不同、长期时间依赖性以及持续学习中的灾难性遗忘问题。

Method: 设计并行交叉注意力和多尺度注意力机制整合血糖数据与生理行为信号；引入知识保留模块增强模型持续学习能力。

Result: 在AIREADI数据集上，GluMind相比现有最优模型在RMSE和MAE上分别提升约15%和9%，表现出稳定的性能和学习适应性。

Conclusion: GluMind通过创新的注意力机制和知识保留技术，有效解决了多模态血糖预测中的关键挑战，为糖尿病管理提供了可靠的预测工具。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [259] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA是一种新的降维算法，将PPCA推广到非线性流形，通过几何坐标系更好地描述分布在非线性流形周围的数据分布


<details>
  <summary>Details</summary>
Motivation: 传统PPCA及其扩展主要基于线性模型，只能描述欧几里得坐标系中的数据，但许多神经科学应用中的数据可能分布在非线性流形周围而非欧几里得空间中

Method: 开发了PGPCA算法，显式地整合给定的非线性流形知识，推导出几何坐标系来捕捉数据与流形的偏差和噪声，并推导了数据驱动的EM算法来学习模型参数

Result: 在模拟和脑数据分析中，PGPCA能有效建模各种给定流形周围的数据分布，且在此类数据上优于PPCA。PGPCA还提供了测试几何坐标系是否比欧几里得坐标系更好地描述数据的能力

Conclusion: PGPCA通过结合非线性流形几何学，增强了高维数据分析中降维的有效性，特别是对于表现出噪声且分布在非线性流形周围的数据

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [260] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探讨了离散时间扩散过程及其变体，包括加性高斯噪声、乘性高斯噪声、模糊噪声及其混合，实验表明离散时间过程在语音质量上与连续扩散模型相当，但训练和推理更高效一致。


<details>
  <summary>Details</summary>
Motivation: 连续时间扩散模型在训练和推理之间存在不匹配问题，且通常局限于加性高斯噪声。离散时间过程可以避免这些限制，减少推理步骤，并保持训练/推理条件的一致性。

Method: 提出多种离散时间扩散过程变体：加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊和高斯噪声的混合。这些方法在离散时间框架下进行建模。

Result: 实验结果显示，离散时间过程在主观和客观语音质量评估上与连续扩散模型相当，同时提供了更高效和一致的训练与推理方案。

Conclusion: 离散时间扩散过程是连续扩散模型的有效替代方案，具有相似的性能但更高的效率和一致性，为语音生成提供了新的研究方向。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [261] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ是一种新的向量压缩技术，通过非均匀向量量化在高保真度下实现计算和空间效率，相比现有技术提高了精度且计算成本最小。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量的大尺寸对现代向量搜索技术造成问题：从内存/存储检索大向量成本高且占用空间大。

Method: 使用新颖的简约且计算高效的非线性构建非均匀向量量化器，关键是为每个索引向量单独学习量化器。

Result: 实验结果表明NVQ相比现有技术表现出改进的精度，计算成本最小。

Conclusion: NVQ是一种在高保真度下计算和空间效率高的向量压缩技术，通过单独学习每个向量的量化器实现更好的性能。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [262] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是一个基于流匹配的蛋白质折叠模型，首次仅使用通用Transformer块构建，挑战了传统依赖复杂领域特定架构的设计理念。


<details>
  <summary>Details</summary>
Motivation: 质疑蛋白质折叠模型中复杂的领域特定架构是否必要，探索是否可以使用通用架构达到同等性能。

Method: 使用标准Transformer块和自适应层，通过生成流匹配目标和结构项进行训练，模型参数达30亿，在约900万个蒸馏蛋白质结构和实验PDB数据上训练。

Result: 在标准折叠基准测试中，SimpleFold-3B达到与最先进基线竞争的性能，在集成预测方面表现强劲，且在消费级硬件上部署和推理效率高。

Conclusion: SimpleFold挑战了蛋白质折叠中对复杂领域特定架构设计的依赖，为未来进展开辟了替代设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [263] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 本文提出了一种基于Kolmogorov Arnold Networks（KANs）的新方法，用于量子动力学预测，通过物理信息损失函数增强，显著减少了训练数据需求并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间中的演化使得传统数值方法计算成本高昂，现有神经网络架构需要大量训练数据且存在虚假振荡问题，影响物理可解释性。

Method: 使用Kolmogorov Arnold Networks（KANs）结合物理信息损失函数（强制执行Ehrenfest定理），并引入Chain of KANs架构直接嵌入时间因果关系。

Result: 该方法仅需200个样本（占Temporal Convolution Networks所需样本的5.4%），即可实现更高的准确性，显著减少了数据需求。

Conclusion: 物理信息KANs相比传统黑盒模型具有明显优势，在保持数学严谨性和物理一致性的同时，大幅降低了数据需求。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [264] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 本文提出使用混合数据集来增强合成数据集在反洗钱（AML）模型训练中的效用，通过结合公开可用的真实世界特征，既保护隐私又提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构面临的全球性关键问题，但开发自动化AML模型时缺乏训练数据，因为隐私和保密性问题限制了真实数据的访问。合成数据虽能解决隐私问题，但纯合成数据集训练AML模型存在挑战。

Method: 提出使用混合数据集方法，将合成数据与公开可用、易于获取的真实世界特征相结合，以增强合成数据集的实用性。

Result: 研究表明，混合数据集不仅能保护隐私，还能提高模型效用，为金融机构增强AML系统提供了实用途径。

Conclusion: 混合数据集方法为解决AML模型训练中的数据隐私和效用矛盾提供了有效解决方案，具有实际应用价值。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [265] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL是一种新的强化学习训练方法，通过主动部分rollout策略解决长尾响应分布导致的GPU空闲问题，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RL训练中，rollout生成占90%以上运行时间，且长尾响应分布导致GPU空闲，限制了模型的可扩展性。

Method: APRIL在rollout阶段过度配置请求，达到目标响应数后终止，未完成的响应在后续步骤中继续使用，避免丢弃任何rollout同时减少GPU空闲时间。

Result: 实验显示APRIL将rollout吞吐量最多提高44%，加速收敛，在多个任务上获得最多8%的最终准确率提升。

Conclusion: APRIL统一了系统级和算法级考虑，提高了RL训练效率，可部署在不同硬件上，为RL系统优化提供了新思路。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [266] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 提出了一种名为RCCR的简单且模型无关的微调目标，通过惩罚模型对DNA序列及其反向互补序列预测之间的差异，来提升DNA语言模型的反向互补一致性。


<details>
  <summary>Details</summary>
Motivation: DNA序列的反向互补序列通常具有相同的生物学意义，但现有的DNA语言模型经常无法捕捉这种对称性，导致对序列及其反向互补序列的预测不一致，降低了模型的可靠性。

Method: 引入反向互补一致性正则化（RCCR），在微调过程中直接惩罚模型对序列及其反向互补序列预测之间的分歧。在三个不同骨干网络（Nucleotide Transformer、HyenaDNA、DNABERT-2）上评估，涵盖序列分类、标量回归和轮廓预测等多种基因组任务。

Result: RCCR显著提高了反向互补鲁棒性，大幅减少了预测翻转和错误，同时相比基线方法（如反向互补数据增强和测试时平均）保持或提高了任务准确性。

Conclusion: 通过将关键的生物学先验直接整合到学习过程中，RCCR为多样化的生物学任务提供了一种单一、内在鲁棒且计算高效的模型微调方案。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [267] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: 本文提出Symphony-MoE框架，通过整合来自不同预训练模型的专家来构建更强大的混合专家模型，解决了传统方法专家多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型通过复制单个预训练密集模型的FFN层来构建专家，但这种方法限制了专家多样性。本文旨在利用多个架构相同但来源不同的预训练模型来构建更具多样性的MoE模型。

Method: 提出两阶段框架：1）训练前阶段通过层感知融合策略构建共享骨干网络，并使用基于激活的功能对齐缓解参数错位；2）轻量级路由器训练阶段协调整个架构。

Result: 实验表明该方法成功整合了异构来源的专家，在多领域任务和分布外泛化方面显著超越基线方法。

Conclusion: Symphony-MoE框架能够有效整合来自不同预训练模型的专家，构建出性能优越的MoE模型，为构建更强大的专家混合模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [268] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 本文从理论上解释了在Sigmoid损失函数下同步可训练逆温度和偏置的优势，分析了SigLIP和SigLIP2模型中的(m, b_rel)-Constellations配置，并提出了带有显式相对偏置的Sigmoid损失重参数化方法。


<details>
  <summary>Details</summary>
Motivation: 对比预训练中的表示获取和对齐任务日益重要，需要从理论上理解SigLIP和SigLIP2模型中温度和偏置参数的作用机制。

Method: 通过理论分析(m, b_rel)-Constellations这一新型组合对象，研究球形编码相关的配置类别，并提出了带有显式相对偏置的Sigmoid损失重参数化方法。

Result: 理论证明了SigLIP在检索任务上的成功，解释了SigLIP中存在的模态差距，并确定了产生高质量表示所需的必要维度。

Conclusion: 提出的重参数化方法在合成数据实验中改善了训练动态，为对比学习提供了理论支撑和实践指导。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [269] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文是关于可解释图神经网络在痴呆症研究中的首次全面综述，涵盖了阿尔茨海默病、帕金森病等多种痴呆亚型的诊断应用，并提出了专门针对痴呆相关任务的可解释性方法分类体系。


<details>
  <summary>Details</summary>
Motivation: 痴呆症具有临床和生物学异质性，使得诊断和亚型区分极具挑战性。传统图神经网络在脑连接建模中表现出潜力，但其鲁棒性有限、数据稀缺和缺乏可解释性限制了临床采用。可解释图神经网络通过结合图学习和可解释性来解决这些障碍。

Method: 本文对痴呆症研究中的可解释图神经网络进行了系统性综述，建立了专门针对痴呆相关任务的可解释性方法分类体系，比较了现有模型在临床场景中的表现，并分析了当前面临的挑战。

Result: 综述发现XGNNs能够识别疾病相关生物标志物、分析脑网络中断，并为临床医生提供透明见解。同时指出了泛化能力有限、未充分探索领域以及大语言模型整合等挑战。

Conclusion: 本综述旨在指导未来工作朝着可信赖、具有临床意义且可扩展的XGNNs在痴呆症研究中的应用方向发展，为解决痴呆症诊断挑战提供了重要框架。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [270] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出了Interaction Topological Transformer (ITT)框架，通过多尺度拓扑交互建模解决多孔材料性能预测的挑战，在吸附、传输和稳定性预测方面达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 多孔材料结构多样但预测建模困难，主要挑战在于结构-性能关系的多尺度特性以及标记数据稀疏且分布不均的问题

Method: ITT框架利用新型交互拓扑在多尺度（结构、元素、原子、成对元素组织）捕获材料信息，通过Transformer架构整合尺度感知特征，采用两阶段训练策略（60万无标签结构自监督预训练+监督微调）

Result: ITT在吸附、传输和稳定性属性预测方面实现了最先进的、准确且可迁移的预测性能

Conclusion: 该框架为结构多样和化学多样的多孔材料中的学习引导发现提供了一条有原则且可扩展的路径

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [271] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 提出了DS-Diffusion模型，通过风格引导核的扩散框架避免特定条件重训练，基于时间信息的分层去噪机制减少生成数据与真实数据的分布偏差，使推理过程更加可解释。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成的扩散模型需要为特定条件重训练整个框架，存在生成数据与真实数据的分布偏差，且推理过程不可解释。

Method: 开发基于风格引导核的扩散框架和基于时间信息的分层去噪机制(THD)，生成样本能清晰指示数据来源风格。

Result: 相比ImagenTime等SOTA模型，预测分数和判别分数分别降低5.56%和61.55%，分布偏差进一步减少，推理过程更可解释。

Conclusion: DS-Diffusion通过避免重训练增强了模型对特定条件的灵活性和适应性，同时提高了生成质量和可解释性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [272] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT方法通过在行动前增加反思步骤，显著提升LLM在交互决策任务中的成功率，在三个测试环境中分别实现最高24%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM决策方法存在错误累积问题且缺乏有效的自我纠正机制，需要一种能够即时纠错并适应环境反馈的方法

Method: 提出"Reflect before Act"(REBACT)方法，在采取下一个行动之前引入关键的反思步骤，实现即时错误纠正

Result: 在ALFWorld、WebShop和TextCraft三个环境中，REBACT显著超越基线方法，成功率分别提升6.72%(达98.51%)、24%(达61%)和0.5%(达99.5%)

Conclusion: REBACT通过少量修改步骤即可实现性能提升，证明了其计算效率，为LLM交互决策提供了有效的自我纠正机制

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [273] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: 提出了Flow Marching算法，将神经算子学习与流匹配相结合，构建生成式PDE基础模型，通过联合采样噪声水平和物理时间步长，实现长期稳定预测和不确定性感知的集成生成。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型主要基于确定性Transformer架构，缺乏生成灵活性，无法满足科学和工程应用中对不确定性和生成能力的需求。

Method: 1. Flow Marching算法：联合采样噪声水平和物理时间步长，学习统一速度场
2. P2VAE：将物理状态嵌入紧凑潜在空间
3. FMT：结合扩散强制方案和潜在时间金字塔，实现高效计算

Result: 在12个PDE家族的250万条轨迹上训练，在未见过的Kolmogorov湍流上评估，展示长期稳定性优于确定性模型，并呈现不确定性分层的集成结果。

Conclusion: 生成式PDE基础模型对于现实世界应用具有重要意义，Flow Marching方法在计算效率和生成能力方面具有显著优势。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [274] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，通过行列缩放矩阵实现高秩更新，仅需n+m个可训练参数即可适配n×m矩阵，在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 基础模型在多样化任务上表现出色，但适应专业应用通常需要微调，这种方法内存和计算成本高。参数高效微调方法通过仅更新少量权重来缓解这一问题。

Method: HyperAdapt通过对预训练权重矩阵应用行列方向的缩放（使用对角矩阵），实现高秩更新。对于n×m矩阵，仅需n+m个可训练参数。

Result: 在GLUE、算术推理和常识推理基准测试中，使用高达140亿参数的模型，HyperAdapt在性能上匹配或接近全微调及最先进的PEFT方法，同时使用的可训练参数数量少几个数量级。

Conclusion: HyperAdapt在理论上建立了更新秩的上界，实证表明它在各模型层中持续诱导高秩变换，是一种高效的参数微调方法。

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [275] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 提出了一种新的高矩阵聚类框架SCoS，直接对矩阵数据进行子空间聚类，而非传统向量化方法


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法假设向量化数据，无法有效处理矩阵形式的数据样本，需要一种能直接对矩阵进行子空间聚类的通用框架

Method: 基于块项分解(BTD)构建三阶张量，联合估计聚类成员关系和部分共享子空间，并提供可识别性理论保证

Result: 在高光谱成像数据集上验证，相比现有方法具有更高的聚类准确性和鲁棒性，尤其在噪声和干扰严重时表现更优

Conclusion: 该框架在处理高维应用中具有重要潜力，能够有效捕捉超越单个数据向量的结构信息

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [276] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 本研究利用图机器学习进行理性农药设计，开发更安全环保的农用化学品，创建了最大的蜜蜂毒性数据集ApisTox，评估了多种机器学习模型，发现药物化学中的成功方法在农用化学品领域泛化能力不足，需要开发领域特定模型。


<details>
  <summary>Details</summary>
Motivation: 受药物发现中计算机模拟方法的启发，旨在加速开发更安全、环保的农用化学品，特别关注生态毒理学问题。

Method: 创建了最大的蜜蜂毒性数据集ApisTox，广泛评估了分子指纹、图核、图神经网络和预训练变换器等机器学习模型在分子图分类任务上的表现。

Result: 研究发现药物化学中成功的机器学习方法在农用化学品领域往往无法有效泛化，凸显了开发领域特定模型和基准的必要性。

Conclusion: 未来工作将专注于开发全面的基准测试套件，并设计专门针对农药发现独特挑战的机器学习模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [277] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量(GBSM)，用于计算马尔可夫决策过程(MDP)对之间的状态相似性，解决了传统BSM在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量(BSM)在单MDP中表现良好，但在多MDP场景（如策略迁移）中应用受限。现有方法缺乏对数学性质的严格分析，限制了理论进展。

Method: 正式建立了MDP对之间的广义双模拟度量(GBSM)，严格证明了其三个基本性质：对称性、跨MDP三角不等式和相同状态空间上的距离边界。

Result: GBSM在策略迁移、状态聚合和基于采样的估计中获得了比标准BSM更严格的理论边界，并提供了闭式样本复杂度估计。数值结果验证了理论发现。

Conclusion: GBSM为多MDP场景提供了理论基础，在理论和实践上都优于传统BSM方法。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [278] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 本文提出了一种将强化学习与大型语言模型相结合的新型电商支付欺诈检测方法，通过将交易风险建模为多步马尔可夫决策过程，利用LLM迭代优化奖励函数，提高了欺诈检测的准确性和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法在复杂多变的支付场景中效果有限，而强化学习的奖励函数设计需要大量人工专业知识。LLM具有强大的推理和编码能力，可以自动优化奖励函数，提高检测性能。

Method: 将交易风险建模为多步马尔可夫决策过程，利用强化学习优化多阶段支付风险检测。使用LLM迭代优化奖励函数设计，克服传统方法需要大量人工专业知识的限制。

Result: 在真实数据上的实验表明，LLM增强的强化学习框架在欺诈检测准确性、鲁棒性和抗干扰性方面表现优异，长期评估验证了其有效性。

Conclusion: 该方法展示了LLM在工业级强化学习应用中的巨大潜力，为电商支付欺诈检测提供了新的解决方案，具有零样本能力和持续改进的优势。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [279] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新型的周期性CNN架构，通过引入周期性边界条件，证明了其在d维输入空间中能够逼近依赖于d-1个线性变量的脊函数，而这是低维脊设置无法实现的。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在处理具有周期性结构的数据时存在局限性，特别是在图像分析、物理信息学习和材料科学等领域，数据往往具有高内在维度的脊状结构。

Method: 设计周期性CNN架构，在卷积层中引入周期性边界条件，并通过严格的数学定理证明其表达能力。

Result: 周期性CNN能够精确逼近高维脊函数，建立了CNN表达能力的新理论边界，为实际应用提供了理论基础。

Conclusion: 周期性CNN不仅扩展了CNN近似理论的数学基础，还展示了一类具有实用价值的架构，特别适用于具有周期性结构的高维数据问题。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [280] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO是一个基于时间序列基础模型的时间序列异常检测方法，通过引入基于patch的内存模块来缓解模型过度泛化问题，能够在多数据集上联合微调并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重构的深度模型在时间序列异常检测中容易过度泛化，准确重构未见过的异常；而现有的内存架构方法训练成本高且难以与时间序列基础模型有效集成。

Method: 提出MOMEMTO方法，包含基于patch的内存模块，从多领域捕获代表性正常模式；通过多领域训练策略实现单一模型在多数据集上的联合微调；内存项使用预训练编码器的潜在表示初始化，组织为patch级单元，通过注意力机制更新。

Result: 在23个单变量基准数据集上的实验表明，MOMEMTO作为单一模型在AUC和VUS指标上优于基线方法，并显著提升了其骨干时间序列基础模型的性能，特别是在少样本学习场景中。

Conclusion: MOMEMTO成功解决了时间序列异常检测中的过度泛化问题，通过内存模块和多领域训练策略实现了高效且高性能的检测效果。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [281] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 本文分析了对角线性网络的训练轨迹与lasso正则化路径之间的紧密联系，表明训练时间扮演了逆正则化参数的角色。


<details>
  <summary>Details</summary>
Motivation: 对角线性网络的理论兴趣在于其隐式正则化可以被严格分析。本文旨在深化这一分析，揭示训练轨迹与lasso路径的关系。

Method: 使用对角线性网络（具有线性激活和对角权重矩阵的神经网络），从小的初始化开始训练，并与lasso正则化路径进行比较。

Result: 在lasso正则化路径单调的假设下，这种联系是精确的；在一般情况下，显示出近似联系。提供了严格结果和模拟来支持这一结论。

Conclusion: 对角线性网络的完整训练轨迹与lasso正则化路径密切相关，训练时间起到了逆正则化参数的作用。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [282] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 本文提出了一个基于集成概率机器学习的诊断框架，通过量化和自动化预测不确定性来改进数据驱动的一致性诊断性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中应用广泛，但现有模型在置信度评估方面存在困难，这在基于一致性的诊断中尤为重要，因为决策逻辑对误报高度敏感。

Method: 使用集成概率机器学习方法来量化和自动化预测不确定性，构建了一个诊断框架。

Result: 通过消融分析和对比分析在多个案例研究中评估，结果显示在一系列诊断指标上都有持续改进。

Conclusion: 所提出的方法能够有效提高数据驱动一致性诊断的诊断特性，特别是在处理预测不确定性方面表现出色。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [283] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出一种基于预训练扩散模型的轻量级通用数据同化方法，无需额外训练即可应用于动力系统状态估计


<details>
  <summary>Details</summary>
Motivation: 数据同化在气象学、海洋学和机器人学等领域广泛应用，但现有方法可能计算复杂或需要专门训练。本文旨在利用预训练的扩散模型简化数据同化过程

Method: 基于粒子滤波算法，将预训练用于模拟动力系统的扩散模型（如GenCast）直接应用于数据同化，无需额外训练

Result: 开发了一种轻量级通用数据同化框架，特别展示了在GenCast（基于扩散模型的全球集合天气预报系统）上的应用

Conclusion: 该方法为利用预训练扩散模型进行高效数据同化提供了新途径，具有广泛的应用潜力

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [284] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 本文提出了一种低秩双随机聚类方法（LoRD），通过仅放宽正交约束来获得概率聚类结果，并引入块对角正则化（B-LoRD）进一步提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图聚类方法（如谱聚类、对称非负矩阵分解等）过度放宽了低秩、非负、双随机和正交约束，这可能限制了它们的聚类效果。

Method: 提出LoRD模型仅放宽正交约束，保持双随机性；引入块对角正则化（B-LoRD）增强聚类性能；通过引入类概率参数将非凸双随机约束转化为线性凸约束；设计了全局收敛的投影梯度下降算法。

Result: 理论分析证明了正交性与块对角性在双随机约束下的等价性，并证明了LoRD和B-LoRD的梯度Lipschitz连续性。大量实验验证了方法的有效性。

Conclusion: LoRD和B-LoRD方法在保持必要约束的同时有效提升了聚类性能，为图聚类提供了新的理论框架和实用算法。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [285] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出Shared-Weights Extender (SWE)和Steepest Voting Distributor (SVoD)两种方法，用于在神经网络训练过程中有效扩展网络容量，防止新添加神经元失效。


<details>
  <summary>Details</summary>
Motivation: 在训练过程中扩展神经网络是增加容量的有效方法，但新添加的神经元往往无法适应已训练网络而变得无效，无法真正提升网络能力。

Method: SWE方法通过将新神经元与现有神经元耦合来实现平滑集成，防止神经元失效；SVoD是基于梯度的神经元分配方法，用于在深度网络扩展时跨层分配神经元。

Result: 在四个数据集上的广泛测试表明，该方法能有效抑制神经元失效，相比其他扩展方法和基线获得更好的性能。

Conclusion: 提出的SWE和SVoD方法能够有效解决神经网络扩展中的神经元失效问题，实现更好的容量增长和性能提升。

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [286] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO算法解决了GRPO在组内响应全对或全错时无法学习的问题，通过优势校准和非对称裁剪技术，将同质错误转化为有效的学习信号，显著提升了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在组内响应全部正确或全部错误时无法有效学习，特别是在全错情况下优势函数为零，导致梯度消失和学习信号丢失。

Method: 1. 优势校准：在优势计算中假设存在虚拟最大奖励样本，改变组内奖励的均值和方差，确保同质错误样本的优势值不再为零。2. 非对称裁剪：放松正样本的更新幅度，同时对负样本施加更严格的约束，稳定探索压力。

Result: 在Qwen2.5-Math-7B模型上的实验表明，NGRPO在MATH500、AMC23和AIME2025等数学基准测试中显著优于PPO、GRPO、DAPO和PSR-NSR等基线方法。

Conclusion: NGRPO能够有效从同质错误中学习，实现了数学推理能力的稳定和显著提升。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [287] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 本文首次分析了异质性在图级学习中的影响，通过理论分析和实验验证发现：与节点级任务不同，图级任务（特别是基于motif的局部结构标记）需要混合频率动态而非频率主导机制。


<details>
  <summary>Details</summary>
Motivation: 异质性在节点级任务中已被广泛研究，但其在图级学习中的影响尚不清楚。本文旨在填补这一研究空白，特别关注图级任务中的异质性效应。

Method: 1) 提出图级标记方案的分类法；2) 聚焦局部结构标记中的motif检测任务；3) 使用基于能量的梯度流分析理论框架；4) 在合成数据集和真实分子属性预测数据集上进行实验验证。

Result: 理论分析表明motif检测需要混合频率动态，实验证明频率自适应模型优于频率主导模型。在合成数据集和真实分子数据集上的实验均支持这一发现。

Conclusion: 本研究建立了图级学习中异质性的新理论理解，为设计有效的GNN架构提供了指导，强调需要频率自适应机制而非频率主导策略。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [288] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 本文提出了一种联邦学习中的动态优化后门攻击方法，通过最小-最大框架解耦主任务和后门任务，提高攻击的持久性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性暴露了新的攻击面，现有后门攻击依赖固定模式或对抗性扰动作为触发器，容易受到诚实更新的稀释和联邦防御的影响。

Method: 采用动态优化后门触发器的min-max框架：内层最大化中毒样本与良性样本的性能差距，外层将自适应触发器注入本地模型。

Result: 在计算机视觉和自然语言任务上评估，与6种后门攻击方法和6种防御算法比较，该方法实现了良好的攻击性能且易于集成到现有技术中。

Conclusion: 提出的动态优化方法有效解耦了主任务和后门任务，显著提高了后门攻击在联邦学习环境中的持久性和鲁棒性。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [289] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: 本文提出了GNARL框架，将神经算法推理重新定义为马尔可夫决策过程，结合模仿学习和强化学习来解决传统NAR方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统神经算法推理存在无法构造有效解、无法处理多个正确解、在NP难问题上表现差、以及缺乏专家算法时无法应用等局限性。

Method: 将算法轨迹学习问题重新构建为马尔可夫决策过程，提出GNARL框架，结合模仿学习和强化学习技术，适用于广泛的图基问题。

Result: 在多个CLRS-30问题上获得很高的图精度，在NP难问题上匹配或超越传统NAR方法，甚至在缺乏专家算法时也能应用。

Conclusion: GNARL框架成功解决了NAR的关键限制，为算法学习提供了更强大和通用的方法。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [290] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 本文提出使用置信网络（CN）作为贝叶斯网络（BN）的隐私保护替代方案，通过模糊化而非添加噪声的方式，在保护隐私的同时保持模型的推理效用。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题日益严重，公开发布的贝叶斯网络模型需要保护训练数据中的敏感信息。现有的噪声添加方法虽然能提供强大的隐私保护，但会显著影响模型的推理效用。

Method: 将贝叶斯网络转换为置信网络，通过调整CN的超参数来调节隐私保护程度，同时识别需要隐藏的关键学习信息以防止攻击者恢复原始BN。

Result: 数值实验表明，置信网络能够有效降低追踪攻击的成功概率，同时保持有意义的推理能力，隐私增益可以通过超参数调节。

Conclusion: 置信网络为开发隐私感知的概率图模型提供了一种原则性、实用且有效的方法，能够在隐私保护和模型效用之间取得良好平衡。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [291] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出HEROS异构在线集成方法，通过马尔可夫决策过程平衡预测性能与可持续性约束，ζ策略在减少资源消耗的同时实现接近最优性能


<details>
  <summary>Details</summary>
Motivation: 现有流挖掘集成方法过度关注预测能力而忽视计算成本，需要更可持续的在线学习方法

Method: HEROS方法从多样化超参数模型池中选择子集进行训练，引入马尔可夫决策过程框架和ζ策略来优化资源分配

Result: 在11个基准数据集上的实验表明，ζ策略在保持高精度的同时显著降低资源消耗，部分情况下甚至优于竞争对手

Conclusion: HEROS的ζ策略是实现绿色在线学习的有效方法，在预测性能和可持续性之间取得了良好平衡

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [292] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文为异步更新的Polyak-Ruppert平均Q学习建立了中心极限定理，包括非渐近中心极限定理和函数中心极限定理。


<details>
  <summary>Details</summary>
Motivation: 研究Q学习算法的统计性质，特别是在异步更新设置下的收敛行为，为强化学习算法的理论分析提供基础。

Method: 使用Polyak-Ruppert平均技术分析Q学习算法，在异步更新框架下建立中心极限定理，包括收敛速率和函数极限性质。

Result: 得到了非渐近中心极限定理，收敛速率显式依赖于迭代次数、状态-动作空间大小、折扣因子和探索质量；同时得到了函数中心极限定理，证明部分和过程弱收敛于布朗运动。

Conclusion: 该工作为异步Q学习的统计推断提供了理论基础，揭示了算法收敛的精确统计性质。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [293] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Otters的新型SNN架构，利用光电设备中的自然信号衰减作为TTFS编码的核心计算，通过硬件-软件协同设计实现了更高的能效。


<details>
  <summary>Details</summary>
Motivation: 传统SNN虽然理论上具有高能效，但实际推理过程中需要评估时间衰减函数和突触权重乘法运算，这些数字操作成本高昂，限制了能效优势的实现。

Method: 1) 利用氧化铟光电突触设备的自然物理衰减直接实现时间函数；2) 将设备模拟输出视为突触权重和时间衰减的融合乘积；3) 开发量化神经网络到SNN的转换算法以支持复杂架构；4) 完整的硬件-软件协同设计。

Result: 在七个GLUE基准数据集上达到最先进精度，相比之前领先的SNN实现了1.77倍的能效提升（基于22nm商用工艺的能耗分析）。

Conclusion: 该工作建立了SNN能效的新范式，将基础设备物理特性直接转化为强大的计算原语，所有代码和数据均已开源。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [294] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: 本文提出了仿真基础神经网络(SGNNs)的理论基础，证明其在仿真先验下实现摊销贝叶斯推断，收敛到贝叶斯最优预测器，并能在模型错误设定下学习经验方法无法观测的科学量。


<details>
  <summary>Details</summary>
Motivation: SGNNs在真实标签有限或不可观测的领域取得了最先进性能，但缺乏理论基础。本文旨在为仿真基础学习建立正式的理论框架。

Method: 通过理论分析证明SGNNs实现摊销贝叶斯推断，推导模型错误设定下的泛化界限，并形式化SGNNs特有的机制可解释性方法。

Result: 数值实验验证了所有理论预测：SGNNs能够恢复潜在参数，在失配情况下保持鲁棒性，在模型选择任务中AIC误差减半。

Conclusion: SGNNs为数据受限场景下的科学预测提供了一个原则性和实用的框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [295] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net是一种创新的低秩参数高效框架，通过利用层间激活残差的低秩特性，采用双路径架构重建层激活，在减少参数复杂度和内存需求的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前低秩方法存在三个关键缺陷：模型性能受损、计算开销大、激活内存节省有限。需要解决这些限制来提升低秩架构的效率。

Method: 提出跨层低秩残差网络(CR-Net)，基于层间激活残差具有低秩特性的发现，采用双路径架构将前一层输出与其低秩差异相结合来高效重建层激活，并开发专门的激活重计算策略来减少内存需求。

Result: 在60M到7B参数规模的预训练实验中，CR-Net始终优于最先进的低秩框架，同时需要更少的计算资源和内存。

Conclusion: CR-Net成功解决了当前低秩方法的局限性，提供了一种高效的大语言模型预训练解决方案，在保持性能的同时显著降低了参数复杂度和资源需求。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [296] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 本文综述了无标签数据表示学习的最新理论进展，特别是深度学习方法与传统统计方法的差异，以及作者在该方向上的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型使用自监督、去噪/掩码自编码器等新原理进行无监督表示学习，但这些方法难以用经典理论分析。需要结合统计和优化的数学工具来理解这些模型学习到的表示及其性能。

Method: 结合统计学和优化的数学工具，对无监督表示学习的理论进行分析和解释。

Result: 提供了无标签数据表示学习最新理论进展的综述，并展示了作者在该方向的研究贡献。

Conclusion: 需要新的理论框架来理解和分析深度学习中的无监督表示学习方法，本文为此方向提供了理论综述和贡献。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [297] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 本文提出了一种完全可学习的神经奖励机（FLNRM），能够端到端学习符号接地函数和自动机，无需依赖先验知识，在非马尔可夫强化学习任务中表现优于基于RNN的方法。


<details>
  <summary>Details</summary>
Motivation: 解决非马尔可夫强化学习任务中传统方法对预定义符号接地函数和先验任务知识的依赖问题，使方法更易应用且更具可解释性。

Method: 提出完全可学习的神经奖励机（FLNRM），集成深度强化学习，端到端学习符号接地函数和自动机表示。

Result: FLNRM方法在非马尔可夫强化学习任务中优于基于循环神经网络的方法，同时保持了自动机的有限性和紧凑性带来的可解释性优势。

Conclusion: FLNRM提供了一种无需先验知识的端到端学习框架，在保持可解释性的同时实现了更好的性能，为非马尔可夫强化学习提供了更实用的解决方案。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [298] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge是一个统一的多模态框架，支持视觉语言理解、生成和检索任务，采用语言中心设计和轻量级双向潜在对齐模块，通过两阶段解耦训练策略实现多任务统一建模。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型解决方案往往孤立处理不同任务或需要从头训练，导致计算成本高且跨模态泛化能力有限。

Method: 采用语言中心设计重用预训练LLM，引入轻量级双向潜在对齐模块，提出两阶段解耦训练策略：监督微调和潜在空间对齐，以及语义引导的扩散训练。

Result: 在广泛基准测试中，OmniBridge在所有三个任务上都达到了竞争性或最先进的性能。

Conclusion: 潜在空间对齐在共享表示空间下统一多模态建模方面具有有效性，代码和模型已开源。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [299] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 本文提出了一种结合GAN和Transformer编码器的混合方法，用于生成高质量的信用卡欺诈交易样本，以解决数据集严重不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临数据集高度不平衡的挑战，传统过采样方法如SMOTE生成的样本过于简单，而现有的生成模型如CTGAN和TVAE在高维依赖建模方面仍有不足。

Method: 使用生成对抗网络（GAN）结合Transformer编码器块，通过对抗训练生成逼真的欺诈交易样本，Transformer的自注意力机制能够学习丰富的特征交互。

Result: 在公开的信用卡欺诈检测数据集上测试表明，该Transformer-based GAN方法在Recall、F1-score和AUC指标上显著优于传统和生成式重采样策略。

Conclusion: 该方法能有效克服欺诈检测任务中严重的类别不平衡问题，生成高质量的少数类样本。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [300] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一个基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略实现跨架构的对抗样本生成，提高了攻击泛化性和生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒对抗攻击方法通常依赖于特定网络架构或需要大量查询，导致跨架构迁移性差且查询成本高。

Method: JAD利用潜在扩散模型，通过从CNN和ViT模型中蒸馏注意力图来指导对抗样本生成，专注于跨架构的敏感图像区域。

Result: 实验表明JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法。

Conclusion: JAD为黑盒对抗攻击提供了一个有前景且有效的范式，实现了架构无关的高效攻击。

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [301] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 本文系统评估了三种无需反向传播的神经网络训练方法（FF、CaFo、MF），发现MF算法在MLP架构上不仅分类精度超越BP，还能降低41%能耗和34%训练时间，为可持续AI发展提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的计算和能耗需求不断增长，主要受反向传播驱动，这挑战了AI的可持续发展。研究旨在寻找更节能的训练方法。

Method: 建立了严格的比较框架，在各自原生架构上实现三种BP-free算法（FF、CaFo、MF），使用Optuna优化超参数，基于验证性能应用一致的早停标准，并通过NVML API和CodeCarbon测量硬件级能耗。

Result: MF算法在MLP上的分类精度超越BP，验证损失收敛到更优最小值，能耗降低41%，训练时间缩短34%，碳足迹显著减小。硬件分析揭示了FF的架构低效性和MF的计算精简设计。

Conclusion: MF算法通过结合精度和可持续性优势，挑战了全局优化必要性的假设，为未来节能深度学习提供了清晰的数据驱动路线图。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [302] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出了Diffusion Bridge Variational Inference (DBVI)方法，通过使用可学习的、数据依赖的初始分布来改进深度高斯过程的后验推断，相比DDVI方法在效率和精度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的DDVI方法使用固定的无条件起始分布，与复杂真实后验分布差距较大，导致推断轨迹效率低下且收敛缓慢。

Method: DBVI通过参数化的摊销神经网络生成数据依赖的初始分布，使用ELBO目标的梯度进行渐进式适应，减少后验差距并提高样本效率。网络设计为在诱导输入上操作，作为数据集的结构化低维摘要。

Result: 在回归、分类和图像重建任务中，DBVI在预测精度、收敛速度和后验质量方面持续优于DDVI和其他变分基线方法。

Conclusion: DBVI保持了DDVI的数学优雅性，同时通过可学习的初始分布显著提升了深度高斯过程后验推断的效率和性能。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [303] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: 提出了AxelGNN，一种基于Axelrod文化传播模型的新型图神经网络架构，通过相似性门控概率交互、特征级复制机制和全局极化保持，解决了GNN中的特征过度平滑、异构关系处理和特征向量不可分等问题。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络面临的三个基本限制：深层网络中的特征过度平滑导致节点表示难以区分；难以有效处理连接节点差异显著的异构关系；将整个特征向量作为不可分割单元处理，限制了灵活性。

Method: AxelGNN采用相似性门控概率交互机制，根据节点相似性自适应促进收敛或发散；实现特征级复制机制，在片段级别进行细粒度特征聚合；保持全局极化以维护多个表示簇中的节点区分性。

Result: 在节点分类和影响力估计基准测试中，AxelGNN在具有不同同质性-异质性特征的多样化图结构上，始终优于或匹配最先进的GNN方法。

Conclusion: AxelGNN的双稳态收敛动力学自然地在单一架构内处理同质性和异质性图，为图神经网络提供了更灵活和有效的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [304] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 该论文研究非上下文多臂老虎机问题在迁移学习场景下的表现，提出了KL-UCB-Transfer算法，在源分布和目标分布足够接近时显著优于无先验基线。


<details>
  <summary>Details</summary>
Motivation: 解决传统多臂老虎机问题中缺乏先验知识的问题，利用迁移学习框架，通过源分布样本和距离约束来提升目标分布上的学习效率。

Method: 首先推导了包含迁移参数的问题相关渐近遗憾下界，然后提出了KL-UCB-Transfer索引策略，该策略在高斯情况下能够匹配该下界。

Result: 理论分析表明KL-UCB-Transfer算法在渐近意义上是最优的，仿真验证了当源分布和目标分布足够接近时，该算法显著优于无先验基线方法。

Conclusion: 迁移学习框架可以有效提升多臂老虎机问题的性能，KL-UCB-Transfer算法为处理具有先验知识的强化学习问题提供了有效解决方案。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [305] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 该论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，涵盖了对抗性示例、领域泛化和大型语言模型越狱三个方面的最新进展，提出了新的算法、攻击和防御方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键应用中广泛使用，确保这些模型的决策能够抵御对抗性攻击具有根本重要性。论文旨在设计具有理想鲁棒性属性的算法。

Method: 1) 针对计算机视觉中的对抗性示例问题，引入了新的技术结果、训练范式和认证算法；2) 针对领域泛化问题，提出了在医学影像、分子识别和图像分类中实现最先进泛化能力的新算法；3) 针对大型语言模型越狱问题，提出了新的攻击和防御方法。

Result: 论文在多个领域取得了前沿进展：在对抗性示例方面提出了新的技术方案，在领域泛化方面实现了最先进的性能，在语言模型安全方面代表了设计鲁棒语言代理的最新进展。

Conclusion: 该研究为深度学习模型的安全性和鲁棒性提供了重要贡献，涵盖了从计算机视觉到自然语言处理的多个关键领域，推动了鲁棒人工智能系统的发展。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [306] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL是一个统一的鲁棒REBEL更新家族，通过Wasserstein、KL和χ²模糊集解决RLHF中的过优化问题，在保持可扩展性的同时实现最优参数化收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有的离线RLHF方法存在过优化问题，模型会过度拟合奖励错误设定并偏离训练期间观察到的偏好行为。

Method: 使用Fenchel对偶将更新简化为简单的相对奖励回归，避免了PPO风格的裁剪或辅助价值网络。推导了三种散度的实用SGD算法：梯度正则化（Wasserstein）、重要性加权（KL）和快速一维对偶求解（χ²）。

Result: 在Emotion Alignment、ArmoRM多目标基准和HH-Alignment上的实验表明，该方法在未见偏好混合、模型大小和数据规模上具有强大的最坏情况鲁棒性，其中χ²-REBEL表现出持续强大的经验性能。

Conclusion: 研究验证了无免费午餐权衡：半径收缩速度快于经验散度集中率可实现极小极大最优参数化率但会丧失覆盖保证，而覆盖保证半径则带来O(n^{-1/4})的收敛率。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [307] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一种可扩展的多标签因果发现方法，用于处理稀疏高维事件序列，通过预训练的因果Transformer作为基础模型，并行推断因果图并自适应融合来重建标签的全局马尔可夫边界。


<details>
  <summary>Details</summary>
Motivation: 理解事件序列中的因果关系（如症状导致疾病、错误代码导致系统故障）在医疗和车辆诊断等领域至关重要，但目前仍是一个未解决的挑战。

Method: 使用两个预训练的因果Transformer作为事件序列的领域特定基础模型，CARGO并行地为每个序列推断一次性因果图，并通过自适应频率融合来聚合这些图，以重建标签的全局马尔可夫边界。

Result: 在具有29,100多个唯一事件类型和474个不平衡标签的真实世界汽车故障预测数据集上，CARGO展示了其执行结构化推理的能力。

Conclusion: 这种两阶段方法能够在规模上实现高效的概率推理，同时避免了全数据集条件独立性测试的不可行成本。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [308] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一个结合信任和公平感知的联邦学习框架，通过基于适应度的客户端选举和分槽聚合来解决非IID数据、客户端不可靠性和对抗性攻击问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗等敏感领域的部署面临非IID数据、客户端不可靠性和对抗性操纵的持续挑战，需要更健壮和公平的解决方案。

Method: 采用三阶段参与策略（自由训练、自然选择、分槽团队参与），结合动态客户端评分、自适应阈值和基于队列的调度，平衡收敛效率与鲁棒性。

Result: 在医疗影像、视觉基准和表格农业数据上的实验表明，FedFiTS在准确性、达到目标时间和对投毒攻击的韧性方面持续优于FedAvg、FedRand和FedPow。

Conclusion: 通过整合信任感知聚合和公平导向的客户端选择，FedFiTS推进了可扩展和安全的联邦学习，适用于现实世界的医疗和跨领域部署。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [309] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 该论文提出了两种向量（标准差向量和聚类向量）来分析大语言模型的权重特性，能够有效区分不同模型并显示同系列模型间的相似性。研究发现LoRA微调后，标准差向量受数据集直接影响，而聚类向量保持与预训练模型的高度一致性。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的架构和参数特性是当前热点，本文关注权重特性以分析模型间的相关性和差异。

Method: 提出标准差向量（假设权重服从正态分布，对投影矩阵标准差进行归一化）和聚类向量（对权重投影矩阵奇异值进行K-Means聚类分组）来描述模型特征。

Result: 两种向量能有效区分不同模型，清晰显示同系列模型的相似性。LoRA微调后，标准差向量受数据集直接影响，聚类向量保持与预训练模型的高度一致性。

Conclusion: 标准差向量和聚类向量是分析大语言模型权重特性的有效工具，能揭示模型间的关系和微调过程中的变化规律。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [310] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL是一种针对大语言模型训练的强化学习方法，通过并发异步数据生成和模型训练，结合飞行中权重更新机制，在保持高硬件利用率的同时确保训练数据的时效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在扩展到大语言模型训练时面临硬件利用率低和生成过时数据的问题，这会影响RL算法的效果。

Method: 采用并发异步数据生成和模型训练，引入飞行中权重更新机制，让LLM生成引擎在生成token序列时能持续接收更新的模型权重。

Result: 在128个H100 GPU上的长形式推理任务实验中，PipelineRL相比传统RL基线实现了约2倍的学习速度提升，同时保持高度在策略的训练数据。

Conclusion: PipelineRL在硬件效率和数据时效性之间实现了优越的权衡，其开源实现为大规模LLM训练提供了可扩展的解决方案。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [311] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU是一个生成式时空框架，通过显式建模人类移动的语义和时间复杂性来推进移动性分析。该框架包含四个关键创新：时空概念编码器、认知轨迹记忆、生活方式概念库和任务导向生成头。在四个真实数据集上的实验表明，该方法在三个基准任务上均优于强基线。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹记录了签到序列，为研究短期访问模式和持久生活方式规律提供了独特窗口。现有方法未能充分建模移动数据的语义和时间复杂性，需要更先进的框架来提取语义规律。

Method: 提出GSTM-HMU框架：1）STCE编码器整合地理位置、POI类别语义和周期性时间节奏；2）CTM记忆模块自适应过滤历史访问，强调近期和行为显著事件；3）LCB概念库提供结构化人类偏好线索；4）任务导向生成头将学习表示转换为下游任务预测。

Result: 在Gowalla、WeePlace、Brightkite和FourSquare四个数据集上，对下一个位置预测、轨迹用户识别和时间估计三个任务进行评估。结果显示相比强基线获得一致且显著的性能提升。

Conclusion: 生成式建模为构建更鲁棒、可解释和可泛化的人类移动智能系统提供了有前景的基础。GSTM-HMU在提取复杂移动数据中的语义规律方面表现出有效性。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [312] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 本文研究了激活函数在神经网络训练动态中的作用及其对强化学习中灾难性遗忘的影响，发现除了稀疏表示外，激活函数的梯度稀疏性也对减少遗忘起重要作用，并提出了能够产生稀疏输出和稀疏梯度的新激活函数——大象激活函数。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是强化学习中长期存在的挑战，现有研究主要关注算法层面，而对神经网络架构特性如何导致灾难性遗忘的理解不足。本文旨在填补这一空白。

Method: 通过研究激活函数在神经网络训练动态中的作用，提出新的大象激活函数，该函数能同时产生稀疏输出和稀疏梯度，并在基于值的算法中替换传统激活函数。

Result: 实验表明，通过简单地用大象激活函数替换传统激活函数，可以显著提高神经网络对灾难性遗忘的抵抗能力，使强化学习更加样本高效和内存高效。

Conclusion: 激活函数的梯度稀疏性对减少灾难性遗忘具有重要作用，大象激活函数为缓解强化学习中的灾难性遗忘问题提供了有效的架构解决方案。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [313] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 本文提出了功能缩放定律（FSL），通过随机梯度下降的随机微分方程建模，描述了学习率调度对训练过程中损失动态的影响，为LLM预训练提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律主要关注最终损失，忽略了训练过程中的损失动态和学习率调度的影响，本文旨在填补这一空白。

Method: 使用师生核回归设置和在线随机梯度下降，通过内在时间视角和SDE建模，推导出功能缩放定律来分析不同学习率调度的影响。

Result: FSL能够捕捉学习率调度的影响，理论证明了高容量模型更高效、学习率衰减可提高效率、WSD调度优于直接衰减等经验实践。

Conclusion: FSL框架加深了对LLM预训练动态的理解，为大规模模型训练提供了优化insights，实验验证了其在0.1B到1B参数模型上的实用性。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [314] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 提出一种新的鲁棒性验证方法，通过从训练数据中提取"弱鲁棒"样本来评估模型对扰动的敏感性，从而指导模型性能提升


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动模型在干净数据集上表现良好，但对对抗性攻击和常见数据失真等扰动非常脆弱，需要更有效的鲁棒性验证方法

Method: 通过局部鲁棒性分析从训练数据集中提取最易受扰动影响的"弱鲁棒"样本，这些样本作为模型脆弱性的早期敏感指标

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上的实验表明，基于弱鲁棒样本的鲁棒性验证能有效提升模型在对抗性和常见失真场景下的可靠性

Conclusion: 该方法提供了一种更细致理解模型鲁棒性的途径，能够指导有针对性的性能改进，提高模型在现实扰动环境下的可靠性

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [315] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill是一个知识蒸馏框架，通过预测、特征和补丁级别的蒸馏，将大型PPG基础模型的知识转移到资源受限设备上，实现高效的心率估计和房颤检测。


<details>
  <summary>Details</summary>
Motivation: 光电容积脉搏波（PPG）在可穿戴健康监测中广泛应用，但大型PPG基础模型难以在资源受限的设备上部署。需要一种方法在保持性能的同时实现高效推理。

Method: PPG-Distill框架结合了形态学蒸馏（保留局部波形模式）和节律蒸馏（捕捉补丁间时间结构），通过预测级、特征级和补丁级蒸馏传递全局和局部知识。

Result: 在心率估计和房颤检测任务上，PPG-Distill将学生模型性能提升高达21.8%，同时实现7倍推理加速和19倍内存使用减少。

Conclusion: PPG-Distill成功实现了在可穿戴设备上高效PPG分析的目标，为资源受限环境下的健康监测提供了可行的解决方案。

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [316] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion是一个联邦迁移学习框架，通过统一领域适应和低标注需求学习，结合多样性/聚类感知编码器，解决联邦学习中的特征空间异构、严重非IID数据和标注稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的联邦学习面临异构特征空间、严重非IID数据和跨客户端标注稀缺等挑战，需要一种能够同时处理个性化、领域适应和标注效率的鲁棒解决方案。

Method: 使用标记的教师客户端通过置信度过滤的伪标签和领域自适应迁移指导学习者客户端，客户端维护针对本地数据个性化的编码器。采用相似性加权的分类器耦合（可选聚类平均）来保持全局一致性，减少数据丰富站点的支配作用。节俭标注流程结合自监督/半监督预训练和选择性微调。

Result: 在表格和图像基准测试中，FedFusion在IID、非IID和标注稀缺情况下，在准确性、鲁棒性和公平性方面始终优于最先进的基线方法，同时保持可比的通信和计算预算。

Conclusion: 协调个性化、领域适应和标注效率是解决现实世界约束下鲁棒联邦学习的有效方法。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [317] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文系统研究了最先进开源文本到视频生成模型的延迟和能耗，建立了计算受限的分析模型，验证了空间分辨率、时间长度和去噪步骤的缩放规律，并比较了六种不同T2V模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成系统虽然能生成高质量视频，但计算成本高昂，且能耗特性缺乏系统研究，需要为设计更可持续的生成视频系统提供基准参考和实践指导。

Method: 首先开发计算受限的分析模型预测缩放规律，然后通过WAN2.1-T2V模型进行细粒度实验验证，最后扩展到六种不同T2V模型比较其运行时和能耗特性。

Result: 验证了空间和时间维度呈二次增长，去噪步骤呈线性缩放，为不同T2V模型提供了基准性能数据。

Conclusion: 研究结果为设计和部署更可持续的生成视频系统提供了基准参考和实践见解，有助于优化计算效率和能源消耗。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [318] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过消融研究分析了在电力系统潮流计算中集成物理约束的混合机器学习模型，评估了不同混合策略在准确性、物理合规性、工业准备度和分布外泛化能力四个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和跨境电力交换的增加，电网面临更大的不确定性和运行风险。传统物理求解器虽然准确但速度慢，机器学习模型作为快速替代方案需要更好地遵守物理定律（如基尔霍夫定律）。

Method: 使用自定义基准测试管道LIPS，研究从简单多层感知器到先进图网络的模型架构，探索将物理约束作为正则化项或无监督损失的混合策略。

Result: 结果表明，集成物理知识在不同标准下对性能有显著影响，研究提供了可复现的实现。

Conclusion: 通过系统评估不同混合策略，为电力系统潮流计算的机器学习模型设计提供了实用指导，强调了物理约束集成的重要性。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [319] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文通过算法稳定性分析去中心化对抗训练在凸损失下的泛化性能，发现泛化误差随对抗扰动强度和训练步数增加而增长


<details>
  <summary>Details</summary>
Motivation: 虽然对抗训练能增强模型鲁棒性，但存在鲁棒过拟合和泛化差距扩大的问题。现有研究已证明去中心化网络中对抗训练的收敛性，但其泛化特性尚未被探索

Method: 基于扩散策略的稳定性泛化分析框架，针对凸损失函数进行理论推导，并通过逻辑回归进行数值实验验证

Result: 理论推导出泛化误差界，显示泛化误差随对抗扰动强度和训练步数增加而增长，这与单智能体情况一致但在去中心化设置中是新的发现

Conclusion: 数值实验验证了理论预测，为去中心化对抗训练的泛化性能提供了理论依据

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [320] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 本文研究发现，在大型推理模型中，更长的思维链（CoT）和更多的回顾步骤反而会降低准确性，提出了失败步骤分数（FSF）作为更有效的评估指标，并证明通过移除失败分支可以显著提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在测试时需要大量计算资源用于生成长思维链，但什么构成有效的思维链仍不清楚。虽然先前研究表明延长思维链和增加回顾步骤能带来收益，但近期研究显示更短的思考可能优于长思维链，因此需要系统评估思维链的有效性。

Method: 在10个大型推理模型上进行数学和科学推理的系统评估，引入思维链的图视图来提取结构，提出失败步骤分数（FSF）指标，并设计了两种干预实验：基于指标对候选思维链进行排序，以及编辑思维链移除失败分支。

Result: 研究发现思维链延长和回顾步骤增加与准确性降低相关。FSF指标在预测正确性方面优于长度和回顾比率，移除失败分支的干预显著提高了准确性，表明失败分支会干扰后续推理。

Conclusion: 有效的思维链是失败步骤较少的思维链，支持基于结构的测试时扩展策略，而不是盲目生成长思维链。失败步骤分数是评估思维链质量的关键指标。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [321] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文提出了一个成本效益分析框架，帮助组织评估本地部署开源大语言模型与使用商业订阅服务的经济可行性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，组织面临选择商业云服务还是本地部署的决策。商业服务虽然方便但存在数据隐私、供应商锁定和长期成本等问题，因此需要分析本地部署的经济可行性。

Method: 通过分析最新开源模型（如Qwen、Llama、Mistral等）的硬件需求、运营成本和性能基准，并与主要云服务提供商的订阅费用进行比较。

Result: 研究提供了基于使用水平和性能需求的盈亏平衡点估计，为组织决策提供量化依据。

Conclusion: 该框架为组织规划LLM战略提供了实用的决策工具，帮助他们在商业服务和本地部署之间做出经济合理的选择。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [322] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE是一个利用大型语言模型（ChatGPT-4.1）进行土壤湿度时间序列分析的集成框架，能够零样本检测灌溉模式和异常，无需任务特定标注或微调。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法要么依赖基于阈值的规则，要么需要大量数据的机器学习模型，存在适应性和可解释性限制。

Method: 将时间序列数据转换为文本表示，设计领域知识提示模板，利用ChatGPT-4.1的推理能力识别灌溉事件、估计净灌溉增益、检测分类异常并生成结构化报告。

Result: 在美国多个商业和实验农场的真实数据实验中，SPADE在异常检测方面优于现有方法，召回率和F1分数更高，灌溉事件检测精度和召回率也很高。

Conclusion: LLMs作为可扩展、适应性强的精准农业工具具有巨大潜力，能够整合定性知识和数据驱动推理，为土壤湿度监测和灌溉调度提供可操作的见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [323] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 本文提出了可解释不确定性估计（XUE）方法，将可解释性与不确定性量化相结合，以增强医疗AI的可信度和可用性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统未能以符合临床推理的方式明确量化或传达不确定性，现有可解释AI工作缺乏对预测置信度的捕捉，而不确定性估计技术又缺乏直观解释，这种脱节限制了AI在医学中的应用。

Method: 系统地将医学不确定性映射到AI不确定性概念，识别XUE实施的关键挑战，提出技术方向包括多模态不确定性量化、模型无关的可视化技术和不确定性感知决策支持系统。

Result: 提出了XUE框架和实现指导原则，强调需要开发既能生成可靠预测又能以临床有意义方式表达置信度的AI系统。

Conclusion: 这项工作通过桥接可解释性和不确定性，为开发与真实世界临床复杂性相一致的AI系统铺平了道路，有助于构建可信赖的医疗AI。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [324] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: HSGM是一种分层分段图记忆框架，通过将长文档分解为有意义的片段，构建局部语义图和全局图记忆，显著降低了长文档语义解析的计算复杂度和内存需求。


<details>
  <summary>Details</summary>
Motivation: 长文档语义解析面临二次复杂度增长和内存需求激增的挑战，传统方法难以处理超长文本。

Method: 将输入文档分解为M个片段，在每个片段上构建局部语义图，提取紧凑的摘要节点形成全局图记忆，支持增量更新和分层查询处理。

Result: 理论复杂度从O(N²)降至O(Nk + (N/k)²)，在三个基准测试中实现2-4倍推理加速、>60%内存减少，保持≥95%基线准确率。

Conclusion: HSGM为超长文本提供了可扩展、准确的语义建模方法，支持实时和资源受限的NLP应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [325] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过单一自然语言提示自动化整个OpenFOAM工作流程，显著降低CFD仿真的学习门槛。


<details>
  <summary>Details</summary>
Motivation: 解决CFD仿真学习曲线陡峭和手动设置复杂的问题，降低工程仿真的专业门槛。

Method: 采用多智能体框架，包含网格生成代理、HPC脚本自动生成和可视化功能；使用模型上下文协议(MCP)实现可组合服务架构；通过层次化多索引RAG实现高精度配置生成。

Result: 在110个仿真任务基准测试中，使用Claude 3.5 Sonnet达到88.2%的成功率，显著优于现有框架(MetaOpenFOAM的55.5%)。

Conclusion: Foam-Agent通过专业化多智能体系统有效降低了CFD仿真的专业知识门槛，为复杂科学计算的民主化提供了可行方案。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [326] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLMs）在运筹学（OR）中的集成应用，将方法分为自动建模、辅助优化和直接求解三个方向，并讨论了评估基准、领域应用及关键开放问题。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖专家建模和手动参数调整，难以处理大规模、动态和多约束问题。LLMs通过语义理解、结构化生成和推理控制，有望解决这些局限性。

Method: 将LLMs在OR中的方法组织为三个主要方向：自动建模（将自然语言描述转换为数学模型或可执行代码）、辅助优化（生成启发式方法、演化算法）和直接求解优化任务。

Result: 综述了LLMs在OR中的最新进展，包括评估基准和领域特定应用，并总结了关键开放问题，如语义到结构映射的不稳定性、研究进展的碎片化、泛化能力有限和评估体系不足。

Conclusion: 本文概述了推进LLMs在OR中作用的可能研究途径，强调了解决现有挑战的重要性。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [327] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本文提出了SAPA框架，利用大语言模型合成理论驱动的潜在态度来预测网约车选择，解决了现有模型无法捕捉心理因素和类别不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 现有网约车模式选择模型预测精度有限，无法捕捉关键心理因素，且面临严重的类别不平衡问题（网约车出行仅占日常出行的很小比例）。

Method: SAPA框架采用分层方法：首先使用LLM从原始出行调查数据生成定性旅行者画像，然后训练倾向得分模型；接着LLM为理论驱动的潜在变量分配定量分数；最后分类器整合倾向得分、潜在变量分数和可观测出行属性来预测网约车选择。

Result: 在大规模多年出行调查上的实验表明，SAPA显著优于最先进基线方法，在测试集上的PR-AUC指标将网约车选择预测提高了75.9%。

Conclusion: 该研究为准确预测网约车模式选择提供了强大工具，并提供了一种可轻松迁移到各种应用的方法论。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [328] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: OBER是一个基于学习成果的教育推荐系统，通过将学习成果和评估项目嵌入数据模式，使任何推荐算法都能根据其促进的掌握程度进行评估。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统仅基于点击或评分相关性进行调优和评估，其真正的教学影响不明确。需要一种能够直接衡量推荐系统对学习成果影响的方法。

Method: OBER采用简约的实体关系模型、基于日志的掌握度公式和插件架构，在非正式学习领域的电子学习系统中进行了为期两周的随机分组测试，比较了固定专家路径、协同过滤和基于知识的过滤三种方法。

Result: 协同过滤方法最大化了用户留存率，但固定路径方法实现了最高的掌握度。OBER框架可以从相同的日志数据中同时获得业务指标、相关性指标和学习指标。

Conclusion: OBER框架是方法无关的，可以轻松扩展到未来的自适应或情境感知推荐系统，让实践者能够在没有额外测试开销的情况下权衡相关性和参与度与成果掌握度。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [329] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 提出MMCD框架，通过多模态协作决策和跨模态知识蒸馏解决自动驾驶中传感器故障或协作车辆缺失时的鲁棒性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练和测试时所有数据模态和连接车辆都可用，这在实际中不现实，因为可能存在传感器故障或连接车辆缺失的情况

Method: 提出MMCD框架，融合自车和协作车辆的多模态观测数据，采用基于教师-学生模型的跨模态知识蒸馏方法，教师模型使用多模态数据训练，学生模型设计为在模态减少时仍能有效运行

Result: 在连接自动驾驶和空地车辆协作实验中，该方法将驾驶安全性提高了20.7%，在潜在事故检测和安全驾驶决策方面超越了现有最佳基线

Conclusion: MMCD框架通过多模态协作和知识蒸馏技术，显著提升了自动驾驶系统在挑战性条件下的鲁棒性和安全性

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [330] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出了一种形式化方法来解释定量双极论证框架（QBAFs）中的推理变化，通过追踪论证强度排序中的不一致性来识别解释原因。


<details>
  <summary>Details</summary>
Motivation: 在QBAF中，当更新框架并重新得出结论时，论证强度排序可能出现不一致性，需要系统化方法来解释这些变化的原因。

Method: 定义了强度不一致性概念，识别充分、必要和反事实解释，并开发了基于启发式的搜索方法来寻找解释。

Result: 证明了强度不一致性解释存在的充要条件，并提供了相应的实现。

Conclusion: 该方法能够有效追踪QBAF更新过程中的推理变化，为论证强度排序的不一致性提供形式化解释框架。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [331] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 提出了Neural DNA（nDNA）作为语义基因型表示，通过潜在几何结构捕捉AI基础模型的内部认知身份，包括谱曲率、热力学长度和信念向量场三个维度，用于追踪模型谱系、检测漂移和研究人工认知演化。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只衡量模型行为，但模型的本质在于其潜在几何结构。需要一种能够捕捉模型内部认知身份的方法，类似于生物DNA编码遗传信息。

Method: 提出nDNA表示方法，基于三个几何维度：谱曲率（揭示概念流曲率）、热力学长度（量化语义转换代价）、信念向量场（描述信念方向场）。这种方法将AI模型视为语义流体动力学系统。

Result: nDNA能够生成稳定的、坐标无关的神经网络DNA指纹，可用于追踪预训练、微调、对齐等过程中的模型谱系，检测模型漂移，比较不同模型。

Conclusion: 这项工作开创了神经基因组学新领域，将AI模型视为具有可追踪内部认知的数字语义有机体，为模型比较、风险诊断和演化研究提供了新工具。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [332] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文提出了相似性场理论，这是一个数学框架，用于形式化实体间相似性关系及其演化的原则。该理论定义了相似性场、系统演化、概念纤维和生成算子，并形式化了智能的生成定义。


<details>
  <summary>Details</summary>
Motivation: 作者认为持久化和转换相似性关系是任何可理解动态系统的结构基础，需要建立一个数学框架来形式化这些原则。

Method: 定义了相似性场S: U×U→[0,1]，系统演化序列Z_p=(X_p,S^(p))，概念纤维F_α(K)，以及生成算子G。通过定理证明约束相似性场的演化。

Result: 证明了两个定理：(i)非对称性阻止相互包含；(ii)稳定性需要锚坐标或在f的水平集内最终限制。这些结果确保相似性场演化既受约束又可解释。

Conclusion: 相似性场理论为表征、比较和构建智能系统提供了基础语言，可用于解释大语言模型并将其作为社会认知的实验探针。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [333] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer是一个用于预测个体健康风险的分层视觉-语言多模态Transformer框架，结合了医学影像、临床文本和可穿戴设备数据，在MIMIC-IV数据集上取得了0.90的AUROC。


<details>
  <summary>Details</summary>
Motivation: 随着慢性疾病负担增加和多模态临床数据的复杂性，需要统一的AI框架来主动预测个体健康风险。

Method: 采用分层堆叠的视觉-语言多模态Transformer架构，包含四个关键创新：跨模态预训练、时间融合块、疾病本体映射适配器和LLM推理头。

Result: 在MIMIC-IV纵向队列中，平均AUROC达到0.90，预期校准误差为2.7%。

Conclusion: VL-RiskFormer展示了在多模态临床数据上预测健康风险的有效性，为个性化医疗提供了有力工具。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [334] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind是一个结合CoE、KG、RAG和LLM的混合架构，用于解决个性化食谱推荐中的模糊意图、语义准确性和细节覆盖问题，在准确度、相关性、完整性和清晰度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 个性化食谱推荐面临模糊用户意图处理、语义准确性保证和充分细节覆盖的挑战。

Method: 提出ChefMind混合架构，结合Chain of Exploration(CoE)细化模糊查询为结构化条件，Knowledge Graph(KG)提供语义推理和可解释性，Retrieval-Augmented Generation(RAG)补充上下文烹饪细节，LLM整合输出为连贯推荐。

Result: 在小厨房数据集和手动标注查询上评估，ChefMind在准确度、相关性、完整性和清晰度方面优于LLM-only、KG-only和RAG-only基线，平均得分8.7 vs 6.4-6.7，未处理查询降至1.6%。

Conclusion: ChefMind在模糊需求处理上表现出鲁棒性，为个性化食谱推荐提供了有效的解决方案。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [335] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 本文提出了一种"N-Plus-1"GPT代理框架，通过多个独立求解代理和比较代理来提高机械工程问题分析的可靠性，解决了GPT单次求解成功率仅为85%的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: GPT在机械工程分析中表现出不稳定性，单次求解成功率仅为85%，这种不可靠性使其无法直接应用于教育或工程实践。需要一种机制来提高解决方案的可靠性。

Method: 采用"N-Plus-1"代理框架：首先启动N个独立的求解代理生成N个解决方案，然后通过比较代理对这些方案进行总结、比较，并推荐最佳解决方案。基于孔多塞陪审团定理，当单次求解成功率大于1/2且N足够大时，多数解决方案将正确。

Result: 该框架能够显著提高问题求解的可靠性，比较代理还能整合不同的问题解释和数学建模方法。与商业多代理模型Grok Heavy相比，该框架更注重透明度和教学价值。

Conclusion: 提出的多代理框架有效解决了GPT在机械工程分析中的不稳定性问题，通过集体决策机制提高了解决方案的质量和可靠性，特别适合教育应用场景。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [336] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: 本文提出了ComputerAgent，一个轻量级分层强化学习框架，用于桌面应用控制，相比大型多模态语言模型在推理延迟、样本效率和设备部署方面有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在桌面应用控制中存在推理延迟高、样本效率低、无法在设备上部署等问题，需要更实用的解决方案。

Method: 采用分层强化学习框架，将操作系统控制建模为两级选项过程（管理器和子策略），使用三模态状态编码器处理视觉和上下文多样性，集成元动作和早停机制减少无效交互。

Result: 在135个真实世界桌面任务测试中，简单任务成功率92.1%，困难任务成功率58.8%，模型参数仅1500万，推理时间减半，性能匹配或超过2000亿参数的大模型。

Conclusion: 分层强化学习为计算机控制提供了一个实用、可扩展的替代方案，优于基于单一大型多模态语言模型的自动化方法。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [337] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 论文揭示当前医疗AI基准测试存在严重问题，前沿模型虽然在基准测试中得分很高，但实际表现脆弱，容易受到输入变化影响，且存在走捷径学习现象。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭露当前医疗AI基准测试的局限性，指出高分并不代表真实医疗场景下的可靠性，需要更严格的评估标准。

Method: 通过压力测试评估六个旗舰模型在六个广泛使用的医疗基准上的表现，包括移除关键输入、改变提示词等方法，并采用临床医生指导的评分标准进行评估。

Result: 发现领先系统在关键输入被移除时仍能猜对答案，在微小提示变化下会改变答案，并生成看似合理但有缺陷的推理。基准测试之间差异很大，但被错误地等同对待。

Conclusion: 医疗基准测试分数不能直接反映真实世界的准备程度，需要关注模型的鲁棒性、合理推理能力以及与真实医疗需求的契合度，而不仅仅是排行榜分数。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [338] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 本文研究通过推理长度约束和模型量化两种计算约束策略来降低推理模型的计算需求，并分析它们对模型安全性能的影响。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展虽然能通过生成长链思维序列提升推理语言模型性能，但计算成本显著增加。本研究旨在探索在计算约束下保持模型性能的方法。

Method: 采用两种方法：1）基于长度控制策略优化的强化学习方法微调推理模型以满足用户定义的推理长度；2）应用量化技术，在用户定义的计算约束下最大化生成链式思维序列。

Result: 研究发现计算约束策略能有效降低推理模型的计算需求，但需要在计算效率与模型安全性之间进行权衡。

Conclusion: 计算约束策略是降低推理模型计算成本的有效方法，但需要仔细平衡计算效率与安全性能之间的关系。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [339] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 论文提出了Gödel测试，评估大语言模型能否为简单未解数学猜想生成正确证明。在组合优化领域的五个猜想上测试GPT-5，结果显示模型在常规推理上有进步，偶尔有原创性，但在跨论文综合推理方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型在中学和大学数学竞赛中表现优异，但能否解决更高级数学领域的新简单猜想尚不清楚。作者旨在评估大语言模型在未解数学猜想证明方面的能力。

Method: 设计了Gödel测试，选择五个组合优化领域的简单未解猜想，提供相关源论文但不透露作者自己的猜想，详细评估GPT-5的推理过程。

Result: 在三个较简单问题上，GPT-5产生接近正确的解；问题2中甚至推导出不同近似保证，推翻了作者猜想；问题4（需结合两篇论文结果）失败；问题5提出正确算法但分析失败。

Conclusion: GPT-5在常规数学推理上取得有意义的进展，偶尔展现原创性，但在跨论文综合推理方面仍有明显局限，可能是通过Gödel测试的早期步骤。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [340] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 该论文提出了首个HTS编码分类基准，基于美国海关在线搜索系统构建，并展示了微调的Atlas模型在10位和6位分类上的显著性能提升，同时成本更低且支持数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: HTS编码分类是全球贸易中的关键瓶颈，但机器学习社区对此关注不足。错误分类可能导致货物运输中断，因此需要开发准确的自动化分类系统。

Method: 基于美国海关CROSS系统构建基准数据集，使用微调的LLaMA-3.3-70B模型（Atlas）进行HTS编码分类，并与GPT-5-Thinking和Gemini-2.5-Pro-Thinking等领先模型进行比较。

Result: Atlas模型在10位分类上达到40%准确率，6位分类达到57.5%准确率，分别比GPT-5-Thinking提高15个百分点，比Gemini-2.5-Pro-Thinking提高27.5个百分点。同时成本降低5-8倍，且支持自托管保证数据隐私。

Conclusion: Atlas模型为HTS分类设立了强基线，但该任务仍具挑战性（仅40%的10位准确率）。通过发布数据集和模型，旨在将HTS分类确立为新的社区基准任务，促进检索、推理和对齐方面的未来研究。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [341] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个新的函数调用基准测试，专注于评估大语言模型对参数描述中格式指令的遵循能力，弥补了现有基准测试只关注参数正确性而忽略格式要求的不足。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准测试（如BFCL、tau^2-Bench、ACEBench）只评估参数正确性，但不测试格式指令遵循能力，这在现实AI代理系统中是一个重要缺陷。

Method: 在JSON schema描述中直接编码可验证的格式要求（如值不能包含标点符号），包含750个测试用例，每个用例包含一个函数及其输入参数的格式要求，以及相应的用户查询。评估完全基于算法，确保客观性和可重复性。

Result: 即使是GPT-5和Claude 4.1 Opus等最先进的专有模型也经常无法遵循基本的格式规则，这揭示了现实世界代理系统的实际局限性。

Conclusion: IFEval-FC基准测试揭示了当前大语言模型在精确指令遵循方面的不足，强调了在函数调用评估中考虑格式要求的重要性，为改进AI代理系统的实用性提供了重要见解。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [342] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA是一个新颖的视觉记忆问答任务，需要从多模态记忆中回答回忆性问题。Pensieve管道通过记忆增强、时空感知检索和多记忆问答微调来解决该任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中基于视觉记忆的问答任务面临的挑战，包括任务导向记忆创建、时空信息利用以及多记忆推理能力。

Method: 提出Pensieve综合管道，包含记忆特定增强、时空感知多信号检索和多记忆问答微调三个核心组件。

Result: 在创建的多模态基准测试中，Pensieve相比现有最优方法在问答准确率上提升高达14%。

Conclusion: Memory-QA任务具有重要的现实意义，Pensieve管道通过系统性的方法有效解决了该任务的独特挑战，展现了显著的性能优势。

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [343] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA是一个用于击剑裁判辅助的AI系统原型，通过姿态动作识别和规则推理来解决击剑裁判中的主观判断、人为错误等问题。


<details>
  <summary>Details</summary>
Motivation: 击剑运动面临裁判主观判断、人为错误、偏见以及训练环境中裁判资源有限等挑战，需要自动化裁判辅助系统。

Method: 系统从视频中提取2D关节位置，进行归一化处理，计算101维运动学特征，使用Transformer进行多标签动作和剑尖分类，并结合基于规则的推理来确定优先权和得分。

Result: 在有限的手动标注数据下，5折交叉验证的平均macro-F1得分为0.549，优于Temporal Convolutional Network、BiLSTM和普通Transformer等基线模型。

Conclusion: 虽然尚未达到部署水平，但结果表明在花剑击剑中实现自动化裁判辅助的可行路径，并为AI在击剑教练等领域的应用开辟了新机会。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [344] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: LLMZ+是一种基于提示白名单的防御机制，通过只允许上下文相关的安全消息与代理型LLM交互，有效防御越狱攻击，同时保证合法业务通信不受影响。


<details>
  <summary>Details</summary>
Motivation: 代理型AI相比传统模型具有更高的安全风险，因为它们拥有对数据源和API工具的特权访问权限，且依赖LLM的非确定性行为。现有的基于恶意意图检测的防御机制存在局限性。

Method: 提出LLMZ+方法，采用提示白名单机制，只允许符合预定义用例和操作边界的消息与代理型LLM进行交互。

Result: 实验评估显示LLMZ+对常见越狱提示具有强韧性，同时不影响合法业务通信。在实验设置中，误报率和漏报率均可降至0。

Conclusion: LLMZ+通过基于上下文的特异性保证安全性，简化了安全框架，增强了长期韧性，并减少了维持LLM信息安全所需的资源。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [345] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 提出了一种结合LLM和外部符号方程求解器的新方法，通过分解问题生成方程，然后进行答案验证和迭代修正，在数学应用题求解上达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: LLM在解决数学应用题时面临挑战，需要提升其推理和数学能力。现有方法通过改进提示词帮助LLM解决更复杂的数学问题。

Method: 首先提示LLM从问题分解中创建方程，使用外部符号方程求解器生成答案。然后让LLM第二次解决问题进行答案估计，通过比较估计值和生成答案来验证准确性，如果验证失败则进行迭代修正。

Result: 该方法在数值和代数数学应用题数据集上取得了新的最先进结果，平均比之前最佳结果提高了近2%。在三角函数应用题上也取得了令人满意的结果。

Conclusion: 该方法有效提升了LLM解决数学应用题的能力，并引入了两个新数据集SVAMPClean和Trig300来进一步测试LLM的推理能力。

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [346] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 提出了一种结合气候灾害数据和进化学习的地理空间代理模型，用于评估气候风险及其对经济系统的级联影响


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要建模空间异质性灾害与适应性经济系统之间的复杂相互作用

Method: 将Mesa空间建模与CLIMADA气候影响评估相结合，引入适应性学习行为，让企业通过基于适应度的选择和突变来演化预算分配、定价、工资和风险适应策略

Result: 使用RCP8.5情景下的河流洪水预测显示，进化适应使企业能够在经历数十年气候压力中断后恢复到基线生产水平；未直接暴露于洪水的代理人也面临供应链中断影响，RCP8.5情景下世纪末商品平均价格比基线高5.6%

Conclusion: 这个开源框架为金融机构和公司提供了量化直接和级联气候风险的工具，同时评估成本效益的适应策略

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [347] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG是一个低成本图增强检索生成框架，通过个性化PageRank实现高效检索，在仅使用3%-11%输出token的情况下达到主流图RAG方法80%以上的准确率


<details>
  <summary>Details</summary>
Motivation: 现有图增强检索生成系统忽视了LLM在图构建过程中的高token成本，限制了大规模应用

Method: 受HippoRAG启发，在检索阶段引入个性化PageRank(PPR)算法，构建信息丰富的图结构

Result: TERAG在显著降低成本的同时，实现了至少80%的准确率，仅消耗主流方法的3%-11%输出token

Conclusion: TERAG证明了图增强检索生成可以在大幅降低LLM使用成本的情况下保持高性能

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [348] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 该论文探讨了机器学习在航空系统中的安全合规性，提出了机器学习模型描述（MLMD）的概念，并细化了语义保持的关键概念，以确保模型在目标环境中的准确复制。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在航空系统中的应用增加，需要确保这些系统的安全运行并符合相关指导标准（如EASA概念文件和ED-324）。论文旨在澄清ML模型与其明确描述之间的区别，并确保模型在目标环境中保持训练性能。

Method: 论文通过定义机器学习模型描述（MLMD）的概念，细化了语义保持的关键要求，以确保模型的准确复制。此外，通过多个工业用例来构建和比较不同的目标模型，验证所提出方法的有效性。

Result: 通过应用MLMD和语义保持的概念，论文在多个工业用例中成功构建和比较了目标模型，证明了该方法能够确保ML模型在目标环境中保持其预期功能和性能。

Conclusion: 论文强调了ML模型与其明确描述之间的区别的重要性，并通过语义保持的概念确保了模型在目标环境中的准确复制。这一方法为ML在航空系统中的安全合规性提供了可行的解决方案，并有助于满足EASA等监管机构的要求。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [349] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型在医学领域的最新研究进展，包括训练技术、医疗应用、优缺点分析，并对医学LLMs进行了创新性分类和评估方法分类，提出了现有挑战的解决方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，大型语言模型在医学领域展现出巨大应用潜力，需要系统梳理当前研究进展，为后续研究提供指导。

Method: 采用系统性文献综述方法，深入分析医学大模型的训练技术、医疗场景适应、相关应用及其优缺点，并对医学LLMs进行基于训练方法的分类。

Result: 将医学LLMs分为三种类型，评估方法分为两类，系统总结了医学LLMs的发展现状和应用效果。

Conclusion: 通过系统综述，强调了开发医学LLMs的必要性，为后续研究提供了清晰的发展方向，有助于推动医学AI领域的进一步发展。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [350] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: DataAgents是一种结合LLM推理与任务分解、行动推理和工具调用的自主数据代理，能够将复杂非结构化数据转化为可操作知识，代表数据到知识系统的范式转变


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性增长，数据准备、转换和分析仍然劳动密集且难以扩展。数据与AI的对齐至关重要，但数据通常未以最优方式结构化以支持AI利用

Method: DataAgents通过整合LLM推理、任务分解、行动推理和工具调用，能够自主解释数据任务描述，将任务分解为子任务，推理行动，将行动转化为Python代码或工具调用，并执行操作

Result: DataAgents能够处理数据收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等任务，动态规划工作流并适应各种数据任务

Conclusion: DataAgents代表了向自主数据到知识系统的范式转变，需要推进行动工作流优化、建立开放数据集和基准生态系统、保护隐私、平衡效率与可扩展性，并开发可信的防护机制

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [351] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 提出经验扩展框架，通过自主环境交互和协作经验共享实现LLM的持续进化，突破静态人类生成数据的限制


<details>
  <summary>Details</summary>
Motivation: 传统通过扩大模型规模、训练数据和计算能力的LLM发展方式已接近饱和，人类生成文本资源耗尽且收益递减

Method: 经验扩展框架：捕获原始交互、提炼为紧凑可重用知识、定期优化存储内容以保持相关性和效率

Result: 在模拟真实场景中验证，包括泛化到未见但相关任务、重复查询和过饱和知识存储，经验扩展提高了准确性，维持了长期性能，并在新情境中保持增益

Conclusion: 结构化部署后学习可以扩展LLM能力超越静态人类生成数据的限制，为持续智能进步提供可扩展路径

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [352] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS是一个分布式目录服务，用于发现AI代理的能力、元数据和来源，通过内容寻址存储、层次化分类和加密签名实现高效、可验证的多维度发现。


<details>
  <summary>Details</summary>
Motivation: 解决异构多代理系统中代理能力发现和验证的挑战，提供标准化的代理注册和互操作性解决方案。

Method: 基于Open Agentic Schema Framework构建，采用两级映射的Kademlia分布式哈希表，重用OCI/ORAS基础设施进行工件分发，集成Sigstore进行来源验证。

Result: 实现了高效、可验证的代理发现系统，支持多种新兴代理模式（LLM提示代理、MCP服务器、A2A组件）。

Conclusion: ADS为新兴代理注册和互操作性倡议提供了重要的架构模型，具有可扩展的安全和性能特性。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [353] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于模型检验的方法，用于验证LLM文本生成过程的概率计算树逻辑（PCTL）属性。该方法通过α-k有界文本生成来限制生成过程中的token选择，并支持多种文本量化评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本生成过程缺乏形式化验证方法，无法保证生成文本的一致性和可靠性。作者发现文本生成过程中通常只选择有限数量的token，但选择并不总是相同的，这促使开发形式化验证方法。

Method: 提出α-k有界文本生成方法：在每个生成步骤中，只考虑累积概率最高的前k个token，并通过阈值α进一步筛选。LLMCHECKER基于此构建模型检验框架，验证PCTL属性。

Result: 该方法在多个LLM（Llama、Gemma、Mistral、Genstruct、BERT）上进行了验证，证明能够有效检查文本生成过程的一致性。

Conclusion: 这是首次将PCTL模型检验应用于LLM文本生成过程验证，为LLM的形式化验证提供了新途径，有助于提高生成文本的质量和可靠性。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [354] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 提出一个模块化框架用于ICD-10-CM编码预测，通过原则性模型选择、冗余感知数据采样和结构化输入设计来解决现有LLM在医疗编码中的挑战。


<details>
  <summary>Details</summary>
Motivation: ICD编码对临床文档、计费和医疗分析至关重要，但目前仍是劳动密集型且易出错的任务。LLM在自动化ICD编码方面有潜力，但在基础模型选择、输入上下文化和训练数据冗余方面存在挑战。

Method: 采用模块化框架，包括LLM作为评估器的评估协议与Plackett-Luce聚合来评估开源LLM，引入基于嵌入的相似性度量、冗余感知采样策略去除语义重复的出院摘要，利用台湾医院的结构化出院摘要评估上下文效果。

Result: 在两个机构数据集上的实验表明，经过微调的选择基础模型在内部和外部评估中始终优于基线LLM。包含更多临床部分持续提高预测性能。

Conclusion: 该研究使用开源LLM建立了实用且原则性的ICD-10-CM编码预测方法，提出的框架通过结合知情模型选择、高效数据优化和上下文感知提示，为自动化医疗编码系统的实际部署提供了可扩展的机构就绪解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [355] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为混合优势策略优化（MAPO）的新方法，解决了GRPO中存在的优势反转和优势镜像问题，通过动态重新加权优势函数来适应不同轨迹确定性的样本。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在优势函数分配上存在优势反转和优势镜像问题，阻碍了不同查询样本间合理的优势分配。

Method: 提出MAPO方法，引入优势百分比偏差来处理高确定性轨迹样本，并动态重新加权优势函数以适应不同轨迹确定性的样本特征。

Result: 通过与相关最先进方法的比较以及对不同优势变体的消融研究，验证了所提方法的有效性。

Conclusion: MAPO是一种简单但有效的GRPO策略，能够自适应地配置优势函数以考虑样本特定特征，显著提升了基础模型在推理任务上的性能。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [356] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 提出了ProfileBench基准和Conf-Profile框架，用于解决LLM在用户画像任务中缺乏基准和标签数据的问题，通过置信度驱动的两阶段方法实现标签自由且可靠的用户画像。


<details>
  <summary>Details</summary>
Motivation: 用户画像作为用户理解的核心技术，LLMs提供了有前景的解决方案，但缺乏全面基准和大规模真实标签数据限制了进展，且异构噪声用户信息会影响LLMs的可靠性。

Method: 提出Conf-Profile框架：1）利用高级LLMs合成高质量标签，通过置信度加权投票和校准；2）将多结果、推理和置信度蒸馏到轻量LLM；3）通过置信度引导的无监督强化学习增强推理能力。

Result: 实验结果表明，Conf-Profile通过两阶段训练显著提升性能，在Qwen3-8B模型上F1分数提高了13.97。

Conclusion: 该方法有效解决了用户画像任务中的标签稀缺和可靠性问题，为LLM在用户画像领域的应用提供了实用解决方案。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [357] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的LLM记忆定义和四部分分类法（参数化、上下文、外部、程序性/情景性），以及记忆四元组（位置、持久性、写入/访问路径、可控性）。通过三设置协议和分层评估框架，建立了可重现、可比较和可治理的坐标系统。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM记忆研究中缺乏统一操作定义和评估标准的问题，避免异构设置下的扭曲比较，并整合机制、评估和治理。

Method: 提出记忆四元组分类法，采用三设置协议（仅参数化、离线检索、在线检索），构建分层评估框架，包括参数化、上下文、外部和程序性/情景性记忆评估，并开发DMM Gov更新和遗忘协调系统。

Result: 建立了一个包含时间治理和泄漏审计的完整评估框架，支持记忆更新、遗忘和检索性能的比较分析。

Conclusion: 该框架为LLM记忆研究提供了可重现、可比较和可治理的坐标系统，支持研究部署和测试命题验证。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [358] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个高效的5600亿参数开源MoE推理模型，通过精心设计的训练流程（包括长链思维数据冷启动和大规模强化学习）实现先进推理能力，在复杂推理任务上达到开源模型的最优性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效处理复杂推理任务的开源模型，特别是在STEM、代码和代理推理等领域实现卓越性能，同时显著降低计算资源消耗。

Method: 采用长链思维数据冷启动训练策略增强推理潜力，然后通过领域并行训练方案将不同领域的专家模型解耦优化并融合为单一模型，使用DORA系统进行大规模RL训练实现3倍以上的训练加速。

Result: 在复杂推理任务上达到开源模型的最优性能，在AIME-25上代理推理的平均token消耗减少64.5%（从19,653降至6,965），且不降低任务准确率。

Conclusion: LongCat-Flash-Thinking展示了在推理系统和代理AI研究方面的显著进展，该模型的发布将促进相关领域的进一步发展。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [359] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文系统研究了视觉语言模型中的视觉空间推理能力，提出了空间智能的三级分类体系，并创建了包含20个开源数据集、23个任务设置的SIBench基准测试。实验发现当前模型在基础感知任务上表现良好，但在理解和规划任务上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理是人类核心认知能力，对推进具身智能和自主系统至关重要。尽管视觉语言模型取得进展，但由于三维空间表示和推理的复杂性，实现人类水平的视觉空间推理仍极具挑战。

Method: 系统回顾现有方法（输入模态、模型架构、训练策略、推理机制），将空间智能分为基础感知、空间理解、空间规划三个能力层级，并构建SIBench基准测试。

Result: 实验表明，最先进的视觉语言模型在感知和推理之间存在显著差距：在基础感知任务上表现良好，但在理解规划任务上持续表现不佳，特别是在数值估计、多视角推理、时间动态和空间想象方面。

Conclusion: 研究揭示了实现空间智能仍面临的重大挑战，同时为未来研究提供了系统路线图和全面基准。SIBench基准和相关资源已开源。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [360] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: DEAL是一个新颖的框架，将低秩适应（LoRA）与连续微调策略相结合，通过知识保留和自适应参数更新模块解决传统微调方法的灾难性遗忘和数据效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法存在灾难性遗忘和次优数据效率的问题，限制了其在实际应用中的适用性。需要一种能够在隐私保护设置下保持效率的同时，缓解现有微调方法局限性的新方法。

Method: 提出DEAL框架，整合LoRA与连续微调策略，包含知识保留和自适应参数更新模块，在15个多样化数据集上进行实验验证。

Result: DEAL在15个数据集上持续优于基线方法，在任务准确性和资源效率方面取得了显著提升。

Conclusion: 该方法通过增强任务性能同时提高资源效率，展示了在LLMs中推进持续适应的潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [361] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本文首次对基于大语言模型（LLM）的智能代理中的幻觉问题进行了全面调查，提出了新的分类法来识别不同阶段出现的幻觉类型，深入分析了18种触发原因，并总结了幻觉缓解和检测方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM-based代理在人类认知、推理和交互方面表现出强大能力，但它们仍然容易产生幻觉问题，这可能导致错误的任务执行并削弱系统可靠性。需要系统性地理解和整合相关研究进展。

Method: 通过仔细分析代理的完整工作流程，提出了新的分类法识别不同阶段的幻觉类型；深入研究了18种触发原因；通过大量现有研究的详细回顾，总结了幻觉缓解和检测方法。

Result: 建立了基于LLM代理幻觉的全面调查框架，提出了系统性的分类方法和触发原因分析，为未来研究提供了理论基础和方法指导。

Conclusion: 这项调查将为解决LLM-based代理中的幻觉问题提供启发，最终促进更稳健可靠的代理系统发展，并指出了未来研究的有前景方向。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [362] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究LLMs能否从数学可解释的推荐模型生成有效的用户导向解释，通过用户研究评估不同解释策略的质量。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI工作多依赖自动评估指标，但无法捕捉用户实际需求和感知，因此采用用户中心方法进行真实评估。

Method: 基于约束矩阵分解的可解释推荐模型，通过精心设计的LLM提示将模型结构转化为自然语言解释，对326名参与者进行多维度质量评估。

Result: 所有解释类型都获得良好接受，不同策略间存在中等统计差异，用户评论提供了量化结果之外的补充见解。

Conclusion: LLMs能够从数学可解释模型生成有效的用户导向解释，用户中心评估方法为解释质量提供了更全面的理解。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [363] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种剩余时间预测方法在真实物流公司仓库流程中的表现，发现深度学习模型精度最高，但浅层方法在计算资源需求上更具优势。


<details>
  <summary>Details</summary>
Motivation: 预测性流程监控旨在预测正在进行的流程执行的未来情况，其中剩余时间预测是一个常见目标。本文旨在在真实物流场景中比较不同预测方法的性能。

Method: 在物流公司航空业务的出库仓库流程中，使用包含169,523条轨迹的原始事件日志，比较了四种剩余时间预测方法，包括深度学习模型和浅层方法（如传统提升技术）。

Result: 深度学习模型达到最高精度，但浅层方法（如传统提升技术）在计算资源需求显著减少的情况下仍能达到竞争性精度。

Conclusion: 虽然深度学习在精度上表现最佳，但浅层方法在计算效率方面具有明显优势，为实际应用提供了有价值的权衡选择。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [364] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 该论文提出了基于游戏研究和游戏AI的嵌套概念（地标、纪念碑、灯塔）来解决程序生成内容评估中难以找到与人类体验一致的度量标准的问题


<details>
  <summary>Details</summary>
Motivation: 算法评估程序生成内容时难以找到与人类体验一致的度量标准，特别是对于复合人工制品。自动分解作为一种可能的解决方案需要满足一系列属性的概念

Method: 引入基于玩家视角的嵌套概念：地标、纪念碑和灯塔，这些概念基于人工制品的可感知性、唤起性和行动召唤性，适用于各种游戏类型

Result: 这些实体可以通过当前研究和工业中使用的技术来发现和评估，为程序生成内容的完全自动分解和重要子组件评估开辟了道路

Conclusion: 该方法旨在在人文科学和技术游戏研究之间建立联系，实现更好的计算程序生成内容评估，虽然重点强调混合主动程序生成和组合程序生成，但相信其应用范围更广

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [365] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新的因果表示学习框架，使用可观测源作为辅助变量来识别潜在变量，通过体积保持编码器实现子空间级别的变换和排列识别。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习方法通常依赖外部辅助变量，但忽略了可以从数据中观测或提取的系统驱动潜在因子作为有效辅助变量的可能性。

Method: 引入可观测源作为辅助变量的框架，使用体积保持编码器进行潜在变量识别，并提供变量选择方案来最大化潜在因子的可恢复性。

Result: 实验证明该框架在合成图和图像数据上有效，能够识别整个潜在变量到子空间级别的变换和排列。

Conclusion: 该方法扩展了当前因果表示学习的边界，为利用可观测源作为辅助变量提供了新的理论框架和实践方案。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [366] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC通过生成高级规划程序和使用领域自适应批评器来改进LLM任务规划，显著降低查询成本并提高长期奖励对齐


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为任务规划器时通用知识与环境特定需求之间的差距，以及现有方法依赖频繁查询和短期反馈导致的成本高和长期奖励对齐不足的问题

Method: 使用LLM生成多样化高级规划程序迭代产生候选计划，训练领域自适应批评器评估候选计划并选择最符合长期奖励的方案

Result: 在ALFWorld、NetHack和StarCraft II Unit Building任务中，CoPiC相比AdaPlanner和Reflexion基线平均成功率提高23.33%，查询成本降低91.27%

Conclusion: CoPiC通过程序化规划和领域自适应评估机制，有效提升了LLM任务规划的性能和效率，实现了更好的长期奖励对齐

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [367] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一种多智能体系统初始化方法，通过优化智能体团队结构来提升系统性能，结合自然语言到格式机制和帕累托平衡选择策略，在多个任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统初始化方法未能充分考虑后续阶段智能体的协作需求，需要一种能够优化智能体团队结构的方法来提升系统效率和效果。

Method: AgentInit包含多轮智能体交互和反思，采用自然语言到格式机制确保一致性，并应用帕累托原则进行平衡团队选择，同时考虑智能体团队多样性和任务相关性。

Result: 实验表明AgentInit在多个框架和任务中持续优于最先进的初始化方法和预定义策略，性能提升分别达到1.2和1.6倍，同时显著减少令牌消耗。

Conclusion: AgentInit具有良好的可迁移性，关键组件有效性得到验证，证明其作为可靠多智能体系统初始化方法的能力和适应性。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [368] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 本文研究了LLMs在阿拉伯世界的跨文化常识推理迁移，发现仅需12个文化特定示例即可平均提升10%性能，且来自印尼和美国的跨文化演示也能实现类似效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型往往反映西方中心偏见，限制了其在多元文化背景下的有效性。虽然已有研究探索文化对齐，但跨文化迁移潜力（利用一种文化的对齐来提升其他文化性能）仍未充分探索。

Method: 使用覆盖13个阿拉伯国家的文化基础常识推理数据集，评估轻量级对齐方法（上下文学习、基于演示的强化学习DITTO）以及监督微调和直接偏好优化等基线方法。

Result: 结果显示，在多语言模型中，仅从一个国家的12个文化特定示例就能在其他国家平均提升10%性能。此外，来自印尼和美国背景的跨文化演示在多项选择推理中能够匹配或超越文化内对齐效果。

Conclusion: 这些发现表明高效的跨文化对齐是可能的，为将LLMs适应低资源文化环境提供了一种有前景的方法。

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [369] [Tensor Train Completion from Fiberwise Observations Along a Single Mode](https://arxiv.org/abs/2509.18149)
*Shakir Showkat Sofi,Lieven De Lathauwer*

Main category: math.NA

TL;DR: 本文提出了一种针对特定"纤维式"观测张量的张量补全方法，利用张量链分解和标准线性代数运算，在特定观测模式下实现高效且具有确定性恢复保证的张量补全。


<details>
  <summary>Details</summary>
Motivation: 现有的张量补全方法主要基于随机均匀观测和不相干性假设，但当观测模式本身具有可被利用的低秩结构时，可以设计更高效的确定性恢复算法。本文针对纤维式观测（沿特定模式的部分纤维完全观测或完全缺失）这一实际应用场景进行研究。

Method: 使用标准线性代数运算计算特定类型纤维观测张量的张量链分解，该方法快速且基于观测模式的确定性条件提供恢复保证。

Result: 通过数值实验验证了所提方法的有效性，展示了在实际应用场景中的优势。

Conclusion: 提出的张量补全方法在纤维式观测模式下具有高效性和确定性恢复保证，为实际应用中沿特定模式采样的多路数据张量补全提供了有效解决方案。

Abstract: Tensor completion is an extension of matrix completion aimed at recovering a
multiway data tensor by leveraging a given subset of its entries (observations)
and the pattern of observation. The low-rank assumption is key in establishing
a relationship between the observed and unobserved entries of the tensor. The
low-rank tensor completion problem is typically solved using numerical
optimization techniques, where the rank information is used either implicitly
(in the rank minimization approach) or explicitly (in the error minimization
approach). Current theories concerning these techniques often study
probabilistic recovery guarantees under conditions such as random uniform
observations and incoherence requirements. However, if an observation pattern
exhibits some low-rank structure that can be exploited, more efficient
algorithms with deterministic recovery guarantees can be designed by leveraging
this structure. This work shows how to use only standard linear algebra
operations to compute the tensor train decomposition of a specific type of
``fiber-wise" observed tensor, where some of the fibers of a tensor (along a
single specific mode) are either fully observed or entirely missing, unlike the
usual entry-wise observations. From an application viewpoint, this setting is
relevant when it is easier to sample or collect a multiway data tensor along a
specific mode (e.g., temporal). The proposed completion method is fast and is
guaranteed to work under reasonable deterministic conditions on the observation
pattern. Through numerical experiments, we showcase interesting applications
and use cases that illustrate the effectiveness of the proposed approach.

</details>


### [370] [A mixed formulation for the fractional Poisson problem](https://arxiv.org/abs/2509.18348)
*Juan Pablo Borthagaray,Nahuel de León*

Main category: math.NA

TL;DR: 本文提出了一种分数阶泊松问题的混合公式，通过稳定化方法确保离散化的稳定性，并证明了收敛性和收敛速率。


<details>
  <summary>Details</summary>
Motivation: 经典泊松问题的混合公式引入通量作为额外变量，但直接离散化分数阶泊松问题存在困难，需要开发稳定的数值方法。

Method: 使用分数阶微积分恒等式建立混合公式的适定性，采用Hughes和Masud的稳定化方法获得强制性和适定公式，确保任何符合有限元离散化的稳定性。

Result: 证明了离散化的收敛性，推导了收敛速率，并通过数值实验验证了稳定化的重要性和理论结果的准确性。

Conclusion: 提出的稳定化混合公式为分数阶泊松问题提供了有效的数值求解框架，理论分析和数值实验均验证了方法的可靠性和精度。

Abstract: The mixed formulation of the classical Poisson problem introduces the flux as
an additional variable, leading to a system of coupled equations. Using
fractional calculus identities, in this work we explore a mixed formulation of
the fractional Poisson problem and establish its well-posedness. Since a direct
discretization of this problem appears to be out of reach, we adapt a
stabilized approach by Hughes and Masud, which yields a coercive and well-posed
formulation. The coercivity ensures the stability of any conforming finite
element discretization. We further prove the convergence of this
discretization, derive convergence rates, and discuss implementation aspects.
Finally, we present numerical experiments that highlight both the importance of
stabilization and the accuracy of our theoretical results.

</details>


### [371] [ff-bifbox: A scalable, open-source toolbox for bifurcation analysis of nonlinear PDEs](https://arxiv.org/abs/2509.18429)
*Christopher M. Douglas,Pierre Jolivet*

Main category: math.NA

TL;DR: ff-bifbox是一个新的开源工具箱，用于对大型非线性偏微分方程进行数值分支追踪、稳定性/分岔分析、解析分析和时间积分，特别适用于自适应网格上的2D和3D问题。


<details>
  <summary>Details</summary>
Motivation: 非线性偏微分方程产生复杂的动力学行为，但由于自由度数量大、算子条件数差以及空间和参数分辨率需求变化，在状态空间中分析这些行为通常很困难。

Method: 使用FreeFEM进行有限元空间离散化，通过PETSc在分布式框架中操作离散化算子，提供分支追踪、稳定性分析、解析分析和时间积分等功能。

Result: 通过三个示例验证了实现并展示了工具箱的能力：3D Brusselator系统、3D板屈曲系统和2D可压缩Navier-Stokes系统，除了重现先前研究结果外，还为每个系统提供了新的结果。

Conclusion: ff-bifbox是一个功能强大的开源工具箱，能够有效处理大型非线性偏微分方程的复杂动力学分析，为研究人员提供了实用的数值分析工具。

Abstract: Nonlinear PDEs give rise to complex dynamics that are often difficult to
analyze in state space due to their relatively large numbers of degrees of
freedom, ill-conditioned operators, and changing spatial and parameter
resolution requirements. This work introduces ff-bifbox: a new open-source
toolbox for performing numerical branch tracing, stability/bifurcation
analysis, resolvent analysis, and time integration of large, time-dependent
nonlinear PDEs discretized on adaptively refined meshes in two and three
spatial dimensions. Spatial discretization is handled using finite elements in
FreeFEM, with the discretized operators manipulated in a distributed framework
via PETSc. Following a summary of the underlying theory and numerics, results
from three examples are presented to validate the implementation and
demonstrate its capabilities. The considered examples, which are provided with
runnable ff-bifbox code, include: a 3-D Brusselator system, a 3-D plate
buckling system, and a 2-D compressible Navier--Stokes system. In addition to
reproducing results from prior studies, novel results are presented for each
system.

</details>


### [372] [A new cross approximation for Tucker tensors and its application in Tucker-Anderson Acceleration](https://arxiv.org/abs/2509.18554)
*Daniel Appelö,Yingda Cheng*

Main category: math.NA

TL;DR: 本文提出了两种新的Tucker张量格式算法：Cross²-DEIM（基于DEIM的纤维采样Tucker张量交叉逼近方法）和Tucker-AA（基于Anderson加速的非线性张量方程低秩求解器）。


<details>
  <summary>Details</summary>
Motivation: 现有Tucker张量分解方法在计算效率和内存使用方面存在不足，需要开发更高效的逼近算法和求解器来处理高维非线性问题。

Method: Cross²-DEIM采用迭代纤维采样策略，每模态采样O(r)个纤维；Tucker-AA将Anderson加速扩展到Tucker格式，结合Cross²-DEIM处理非线性项并控制中间秩增长。

Result: Cross²-DEIM在迭代次数和中间内存使用方面表现良好，可用于构建快速泊松求解器；Tucker-AA成功应用于3D非线性偏微分方程的近似求解。

Conclusion: 两种新算法在Tucker张量计算中显示出优越性能，为高维非线性问题提供了有效的数值求解工具。

Abstract: This paper proposes two new algorithms related to the Tucker tensor format.
The first method is a new cross approximation for Tucker tensors, which we call
Cross$^2$-DEIM. Cross$^2$-DEIM is an iterative method that uses a fiber
sampling strategy, sampling $O(r)$ fibers in each mode, where $r$ denotes the
target rank. The fibers are selected based on the discrete empirical
interpolation method (DEIM). Cross$^2$-DEIM resemblances the Fiber Sampling
Tucker Decomposition (FSTD)2 approximation, and has favorable computational
scaling compared to existing methods in the literature. We demonstrate good
performance of Cross$^2$-DEIM in terms of iteration count and intermediate
memory. First we design a fast direct Poisson solver based on Cross$^2$-DEIM
and the fast Fourier transform. This solver can be used as a stand alone or as
a preconditioner for low-rank solvers for elliptic problems.
  The second method is a low-rank solver for nonlinear tensor equation in
Tucker format by Anderson acceleration (AA), which we call Tucker-AA. Tucker-AA
is an extension of low-rank AA (lrAA) proposed in our prior work for low-rank
solution to nonlinear matrix equation. We apply Cross$^2$-DEIM with warm-start
in Tucker-AA to deal with the nonlinearity in the equation. We apply low-rank
operations in AA, and by an appropriate rank truncation strategy, we are able
to control the intermediate rank growth. We demonstrated the performance for
Tucker-AA for approximate solutions nonlinear PDEs in 3D.

</details>


### [373] [Skew Gradient Embedding for Thermodynamically Consistent Systems](https://arxiv.org/abs/2509.18601)
*Xuelong Gu,Qi Wang*

Main category: math.NA

TL;DR: 提出了新的Skew Gradient Embedding (SGE)框架，将热力学一致的偏微分方程模型重新表述为广义梯度流，并开发了统一的数值稳定化策略。


<details>
  <summary>Details</summary>
Motivation: 为了系统性地处理包含可逆和不可逆过程的复杂物理模型，提供统一的数值方法框架，提高计算效率和稳定性。

Method: 利用广义梯度流模型的固有结构，特别是由外2形式表示的斜对称分量，开发统一的稳定化策略，构建保持能量耗散率或确保离散能量稳定性的数值格式。

Result: 数值实验证实了所提方案的鲁棒性、准确性和性能优势，能够自然解耦多物理系统控制方程，提高计算效率。

Conclusion: SGE框架为算法开发提供了灵活性和通用性，支持一阶和二阶格式设计，在保持稳定性和精度的同时显著提升计算效率。

Abstract: We propose a novel Skew Gradient Embedding (SGE) framework for systematically
reformulating thermodynamically consistent partial differential equation (PDE)
models-capturing both reversible and irreversible processes-as generalized
gradient flows. These models include a wide spectrum of models in classical
electrodynamics, fluid mechanics, quantum mechanics, rheology of complex
fluids, solid mechanics, and statistical physics. Exploiting the intrinsic
structure of generalized gradient flow models, especially, the skew symmetric
component expressed by the exterior 2-form, we develop a unified stabilization
strategy for constructing numerical schemes that either preserve the energy
dissipation rate or ensure discrete energy stability. This stabilization
strategy enables the design of both first- and second-order schemes,
highlighting the flexibility and generality of the SGE approach in algorithm
development. A key strength of SGE is its flexible treatment of skew-gradient
(zero-energy-contribution) terms arising from reversible dynamics either
implicitly or explicitly. While treated explicitly, it often leads to a natural
decoupling of the governing equations in multiphysics systems, thereby
improving computational efficiency without compromising stability or accuracy.
Numerical experiments confirm the robustness, accuracy, and performance
advantages of the proposed schemes.

</details>


### [374] [Optimality of quasi-Monte Carlo methods and suboptimality of the sparse-grid Gauss--Hermite rule in Gaussian Sobolev spaces](https://arxiv.org/abs/2509.18712)
*Yoshihito Kazashi,Yuya Suzuki,Takashi Goda*

Main category: math.NA

TL;DR: 本文证明了在混合主导光滑度的Sobolev空间中，几种拟蒙特卡洛方法的最优性以及基于单变量Gauss-Hermite规则的稀疏网格求积法的次优性。稀疏网格Gauss-Hermite求积法的收敛率仅为N^{-α/2}，而最优率为N^{-α}(ln N)^{(d-1)/2}。


<details>
  <summary>Details</summary>
Motivation: 研究不同数值积分方法在Sobolev空间中的收敛性能，特别关注稀疏网格求积法与拟蒙特卡洛方法的比较。

Method: 通过分析Gauss-Hermite节点的结构建立上下界，证明权重修改无法改善收敛率；同时研究带变量变换的拟蒙特卡洛方法。

Result: 稀疏网格Gauss-Hermite求积法只能达到N^{-α/2}的收敛率，而几种拟蒙特卡洛方法能够达到最优收敛率N^{-α}(ln N)^{(d-1)/2}。

Conclusion: 基于Gauss-Hermite规则的稀疏网格求积法在收敛率上存在本质限制，而合适的拟蒙特卡洛方法能够达到理论最优性能。

Abstract: Optimality of several quasi-Monte Carlo methods and suboptimality of the
sparse-grid quadrature based on the univariate Gauss--Hermite rule is proved in
the Sobolev spaces of mixed dominating smoothness of order $\alpha$, where the
optimality is in the sense of worst-case convergence rate. For sparse-grid
Gauss--Hermite quadrature, lower and upper bounds are established, with rates
coinciding up to a logarithmic factor. The dominant rate is found to be only
$N^{-\alpha/2}$ with $N$ function evaluations, although the optimal rate is
known to be $N^{-\alpha}(\ln N)^{(d-1)/2}$. The lower bound is obtained by
exploiting the structure of the Gauss--Hermite nodes and is independent of the
quadrature weights; consequently, no modification of the weights can improve
the rate $N^{-\alpha/2}$. In contrast, several quasi-Monte Carlo methods with a
change of variables are shown to achieve the optimal rate, some up to, and one
including, the logarithmic factor.

</details>


### [375] [Novel Adaptive Schemes for Hyperbolic Conservation Laws](https://arxiv.org/abs/2509.18908)
*Shaoshuai Chu,Pingyao Feng,Vadim A. Kolotilov,Alexander Kurganov,Vladimir V. Ostapenko*

Main category: math.NA

TL;DR: 本文提出了针对一维和二维双曲守恒律系统的新型自适应方案，结合了低耗散中心迎风格式和五阶有限差分格式，通过平滑度指示器自动检测解的不同区域并采用相应数值方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统数值方法在双曲守恒律系统中遇到的挑战：直接应用高阶格式会在粗糙区域产生虚假振荡，而过度使用过压缩限制器会在平滑区域产生阶梯状非物理结构。

Method: 使用平滑度指示器自动识别解的不同区域：在粗糙区域采用二阶有限体积低耗散中心迎风格式（配合过压缩限制器处理接触间断，Minmod2限制器处理其他粗糙区域），在平滑区域采用五阶有限差分格式。

Result: 通过多个具有挑战性的数值算例验证了所提自适应方案的优势，能够锐利地分辨非线性冲击波和线性退化接触间断，同时在平滑区域保持高精度。

Conclusion: 提出的自适应方案有效解决了双曲守恒律系统数值模拟中的精度与稳定性平衡问题，为复杂流动问题的数值模拟提供了可靠工具。

Abstract: We introduce new adaptive schemes for the one- and two-dimensional hyperbolic
systems of conservation laws. Our schemes are based on an adaption strategy
recently introduced in [{\sc S. Chu, A. Kurganov, and I. Menshov}, Appl. Numer.
Math., 209 (2025)]. As there, we use a smoothness indicator (SI) to
automatically detect ``rough'' parts of the solution and employ in those areas
the second-order finite-volume low-dissipation central-upwind scheme with an
overcompressive limiter, which helps to sharply resolve nonlinear shock waves
and linearly degenerate contact discontinuities. In smooth parts, we replace
the limited second-order scheme with a quasi-linear fifth-order (in space and
third-order in time) finite-difference scheme, recently proposed in [{\sc V. A.
Kolotilov, V. V. Ostapenko, and N. A. Khandeeva}, Comput. Math. Math. Phys., 65
(2025)]. However, direct application of this scheme may generate spurious
oscillations near ``rough'' parts, while excessive use of the overcompressive
limiter may cause staircase-like nonphysical structures in smooth areas. To
address these issues, we employ the same SI to distinguish contact
discontinuities, treated with the overcompressive limiter, from other ``rough''
regions, where we switch to the dissipative Minmod2 limiter. Advantage of the
resulting adaptive schemes are clearly demonstrated on a number of challenging
numerical examples.

</details>


### [376] [A Divergence-free Preserving Mixed Finite Element Method for Thermally Driven Active Fluid Model](https://arxiv.org/abs/2509.19053)
*Nan Zheng,Qingguang Guan,Wenlong Pei,Wenju Zhao*

Main category: math.NA

TL;DR: 本文提出了一种保持无散度的混合有限元方法，用于求解非线性四阶热驱动活性流体方程系统。通过引入两个辅助变量降低模型复杂度，结合DLN时间积分器实现全离散近似，证明了能量有界性和误差估计，并通过数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统四阶系统需要H2-相容有限元空间，正则性约束较强。为了降低计算复杂度并增强算法鲁棒性，需要将四阶系统转化为二阶系统，同时保持模型的无散度条件。

Method: 引入两个辅助变量：w=Δu将四阶系统转化为二阶系统，η类似压力变量保持无散度条件。结合两步DLN时间积分器（无条件非线性稳定、二阶精度）和混合有限元方法，采用基于最小耗散准则的自适应时间步长策略。

Result: 在适当正则性假设和温和时间限制下，证明了模型能量的有界性和相关误差估计。数值实验验证了理论结果，表明该方法在模拟复杂活性流体动力学方面具有有效性和准确性。

Conclusion: 所提出的混合有限元方法成功降低了四阶系统的计算复杂度，保持了无散度条件，DLN时间积分器提供了良好的稳定性，自适应策略平衡了计算成本和时间效率，为活性流体模拟提供了有效的数值工具。

Abstract: In this report, we propose a divergence-free preserving mixed finite element
method (FEM) for the system of nonlinear fourth-order thermally driven active
fluid equations. By introducing two auxiliary variables, we lower the
complexity of the model and enhance the robustness of the algorithm. The
auxiliary variable $w = \Delta u$ is used to convert the original fourth-order
system to an equivalent system of second-order equations, thereby easing the
regularity constraints imposed on standard $H^2$-conforming finite element
space. The second variable $\eta$, analogous to the pressure, helps the scheme
preserve the divergence-free condition arising from the model. The two-step
Dahlquist-Liniger-Nevanlinna (DLN) time integrator, unconditionally non-linear
stable and second-order accurate under non-uniform time grids, is combined with
the mixed FEM for fully discrete approximation. Due to the fine properties of
the DLN scheme, we prove the boundedness of model energy and the associated
error estimates under suitable regularity assumptions and mild time
restrictions. Additionally, an adaptive time-stepping strategy based on a
minimum-dissipation criterion is to balance computational costs and time
efficiency. Several numerical experiments validate the theoretical findings and
demonstrate the method's effectiveness and accuracy in simulating complex
active fluid dynamics.

</details>


### [377] [Regularity estimate and sparse approximation of pathwise robust Duncan-Mortensen-Zakai equation](https://arxiv.org/abs/2509.19093)
*Yuhua Meng,Zhongjian Wang,Stephen S. T. Yau,Zhiwen Zhang*

Main category: math.NA

TL;DR: 本文建立了路径稳健Duncan-Mortensen-Zakai方程解在加权Sobolev空间框架下的任意阶导数先验估计，改进了正则性边界，为量化张量列车方法提供收敛保证，并在高维立方传感器和多模态问题上展示了优越性能


<details>
  <summary>Details</summary>
Motivation: 解决非线性滤波问题中DMZ方程的正则性估计问题，为量化张量列车方法提供理论支撑，提升实时滤波计算能力

Method: 使用加权Sobolev空间建立先验估计，利用Sobolev不等式及其加权版本锐化正则性边界，在函数多态状态漂移和观测假设下降低方法复杂度

Result: 数值模拟显示，在高维立方传感器问题上，该方法比粒子滤波和扩展卡尔曼滤波具有更高的效率和精度；在多模态问题上，能准确重构多模态条件密度函数

Conclusion: 改进的正则性估计增强了量化张量列车方法的可行性，该方法在解决高维非线性滤波问题时表现出优越性能，为实时滤波应用提供了有效工具

Abstract: In this paper, we establish an \textit{a priori} estimate for arbitrary-order
derivatives of the solution to the pathwise robust Duncan-Mortensen-Zakai (DMZ)
equation within the framework of weighted Sobolev spaces. The weight function,
which vanishes on the physical boundary, is crucial for the \textit{a priori}
estimate, but introduces a loss of regularity near the boundary. Therefore, we
employ the Sobolev inequalities and their weighted analogues to sharpen the
regularity bound, providing improvements in both classical Sobolev spaces and
H{\"o}lder continuity estimates. The refined regularity estimate reinforces the
plausibility of the quantized tensor train (QTT) method in [S. Li, Z. Wang, S.
S.-T. Yau, and Z. Zhang, IEEE Trans. Automat. Control, 68 (2023), pp.
4405--4412] and provides convergence guarantees of the method. To further
enhance the capacity of the method to solve the nonlinear filtering problem in
a real-time manner, we reduce the complexity of the method under the assumption
of a functional polyadic state drift $f$ and observation $h$. Finally, we
perform numerical simulations to reaffirm our theory. For high-dimensional
cubic sensor problems, our method demonstrates superior efficiency and accuracy
in comparison to the particle filter (PF) and the extended Kalman filter (EKF).
Beyond this, for multi-mode problems, while the PF exhibits a lack of precision
due to its stochastic nature and the EKF is constrained by its Gaussian
assumption, the enhanced method provides an accurate reconstruction of the
multi-mode conditional density function.

</details>


### [378] [A noise-robust Monte Carlo method for electric field calculations in EMC3](https://arxiv.org/abs/2509.19178)
*William De Deyn,Ruben De Wolf,Vince Maes,Giovanni Samaey*

Main category: math.NA

TL;DR: 本文提出了一种新的噪声鲁棒方法，用于计算EMC3等离子体边缘输运代码中的E×B漂移所需的电场梯度，避免了传统有限差分方法对噪声的放大问题。


<details>
  <summary>Details</summary>
Motivation: EMC3代码目前缺乏对E×B漂移的自洽处理，而该漂移对等离子体边缘的粒子和热分布有显著影响。传统有限差分方法在计算电场梯度时会放大代码输出的噪声。

Method: 将之前提出的1D噪声鲁棒导数计算方法扩展到2D，并推导出描述电场演化的偏微分方程，通过蒙特卡洛模拟直接近似电场，避免有限差分近似。

Result: 通过测试案例验证了该方法的准确性和噪声鲁棒性。

Conclusion: 新方法能够有效计算电场梯度，提高EMC3代码对E×B漂移处理的预测能力。

Abstract: One of the main codes to analyze and optimize stellarator configurations is
the EMC3 code, which implements a state-of-the-art 3D Monte Carlo plasma edge
transport code. However, so far, a self-consistent treatment of the E x B drift
is absent. This plasma drift is known to significantly impact the particle and
heat distribution in the plasma edge. It is desirable to incorporate this drift
into EMC3 to improve the predictive capabilities of the code. The calculation
of the E x B drift requires the approximation of the electric field E, which is
proportional to the gradient of the electric potential $ \varphi $. In previous
work, the gradient was calculated with a least squares method based on a finite
difference approximation of the electric potential. However, due to the
stochastic nature of EMC3, the output plasma fields computed by the code are
inherently noisy. The finite difference method further amplifies the noise,
with the amplification growing as the grid size decreases. We continue from,
which introduced a new noise-robust method for 1D derivatives. We extend the
noise-robust method to 2D and apply it to the electric potential. We show that
a PDE can be derived that describes the evolution of the electric field in case
of a uniform diffusion coefficient. This PDE allows us to approximate the
electric field directly with a Monte Carlo simulation, thus avoiding the need
for a finite difference approximation. We illustrate the accuracy of the method
and the noise robustness with a test case.

</details>


### [379] [Reconstruction of a potential parameter in time-fractional diffusion problems via a Kohn--Vogelius type functional: Theoretical aspects](https://arxiv.org/abs/2509.19260)
*Hamza Kahlaoui,Mourad Hrizi,Abdessamad Oulmelk,Xiangcheng Zheng,Ahmed Hendy*

Main category: math.NA

TL;DR: 本文提出了一种基于Kohn-Vogelius成本函数的稳定鲁棒方法，用于从边界观测数据重构时间分数阶扩散方程中的空间相关势函数。


<details>
  <summary>Details</summary>
Motivation: 解决时间分数阶扩散方程中从边界观测数据重构空间相关势函数的逆问题，需要开发稳定且鲁棒的恢复方法。

Method: 将逆势函数问题转化为优化问题，最小化Kohn-Vogelius型泛函。建立了优化问题的适定性，证明了极小解的存在唯一性和稳定性。分析了KV泛函的Fréchet可微性及其梯度的Lipschitz连续性，并开发了收敛的共轭梯度算法进行数值重构。

Result: 通过一维和二维数值实验验证了所提方法的有效性和鲁棒性，包括含噪声数据的情况。

Conclusion: 该方法为时间分数阶扩散方程中的势函数重构提供了一种理论严谨且数值有效的解决方案。

Abstract: Of concern is the problem of reconstructing a space-dependent potential from
boundary observations in the Caputo time-fractional diffusion equation,
utilizing a stable and robust recovery method. We develop an algorithm to
minimize the Kohn-Vogelius (KV) cost function, which measures the difference
between the solutions of two excitations. The inverse potential problem is
recast into an optimization problem, where the objective is to minimize a
Kohn-Vogelius-type functional within a set of admissible potentials. We
establish the well-posedness of this optimization problem by proving the
existence and uniqueness of a minimizer and demonstrating its stability with
respect to perturbations in the boundary data. Furthermore, we analyze the
Fr\'echet differentiability of the KV functional and prove the Lipschitz
continuity of its gradient. These theoretical results enable the development of
a convergent conjugate gradient algorithm for numerical reconstruction. The
effectiveness and robustness of the proposed method are confirmed through
several numerical examples in both one and two dimensions, including cases with
noisy data.

</details>


### [380] [RGDBEK: Randomized Greedy Double Block Extended Kaczmarz Algorithm with Hybrid Parallel Implementation and Applications](https://arxiv.org/abs/2509.19267)
*Aneesh Panchal,Ratikanta Behera*

Main category: math.NA

TL;DR: 本文提出了RGDBEK算法，一种改进的Kaczmarz迭代求解器，通过随机化贪婪双块扩展策略解决传统方法的跷跷板效应，并实现了高效的CPU-GPU混合并行计算。


<details>
  <summary>Details</summary>
Motivation: 传统Kaczmarz算法存在跷跷板效应阻碍收敛稳定性，且现有并行架构无法有效支持初始化无关的并行计算来充分利用CPU和GPU资源。

Method: RGDBEK算法采用基于残差概率分布的随机化列块和行块选择策略，结合优化的稀疏矩阵-向量乘法存储格式，实现了混合CPU-GPU并行实现。

Result: 理论分析证明该算法具有线性收敛性，数值实验表明RGDBEK在迭代次数和计算时间上均优于现有Kaczmarz变体，在有限元离散化、图像去模糊和噪声人口建模等应用中表现出色。

Conclusion: RGDBEK算法有效解决了传统Kaczmarz方法的收敛稳定性问题，并通过混合并行架构显著提升了大规模稀疏线性系统的求解效率，未来将扩展到张量系统和优化并行参数选择。

Abstract: Kaczmarz is one of the most prominent iterative solvers for linear systems of
equations. Despite substantial research progress in recent years, the
state-of-the-art Kaczmarz algorithms have not fully resolved the seesaw effect,
a major impediment to convergence stability. Furthermore, while there have been
advances in parallelizing the inherently sequential Kaczmarz method, no
existing architecture effectively supports initialization-independent
parallelism that fully leverages both CPU and GPU resources. This paper
proposes the Randomized Greedy Double Block Extended Kaczmarz (RGDBEK)
algorithm, a novel Kaczmarz approach designed for efficient large-scale linear
system solutions. RGDBEK employs a randomized selection strategy for column and
row blocks based on residual-derived probability distributions, thereby
mitigating the traditional seesaw effect and enhancing convergence robustness.
Theoretical analysis establishes linear convergence of the method under
standard assumptions. Extensive numerical experiments on synthetic random
matrices and real-world sparse matrices from the SuiteSparse collection
demonstrate that RGDBEK outperforms existing Kaczmarz variants, including GRK,
FDBK, FGBK, and GDBEK, in both iteration counts and computational time. In
addition, a hybrid parallel CPU-GPU implementation utilizing optimized sparse
matrix-vector multiplications via the state-of-the-art storage format improves
scalability and performance on large sparse problems. Applications in finite
element discretizations, image deblurring, and noisy population modeling
demonstrate the algorithm's versatility and effectiveness. Future work will
explore extending RGDBEK to tensor systems, optimizing parallel parameter
selection, and reducing communication overhead to further enhance efficiency
and applicability.

</details>
