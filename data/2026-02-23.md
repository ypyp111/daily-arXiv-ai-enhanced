<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 27]
- [cs.LG](#cs.LG) [Total: 104]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 10]
- [math.NA](#math.NA) [Total: 11]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文提出了KPM-Bench数据集和MoPE算法，用于解决视频描述中细粒度运动细节不足和幻觉问题，通过运动解析和提取技术提升运动中心视频描述的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述模型在准确描述细粒度运动细节方面存在局限，且存在严重幻觉问题，特别是在运动中心视频中，对复杂肢体动态的精确描述至关重要但常被忽视。

Method: 1) 开发自动化标注流程，结合运动学计算和语言解析；2) 构建KPM-Bench数据集，包含细粒度视频-描述对、运动理解问答对和幻觉评估集；3) 提出MoPE算法从文本描述中提取运动属性；4) 基于MoPE开发独立于大模型的幻觉评估指标；5) 将MoPE集成到GRPO后训练框架中。

Result: 创建了KPM-Bench开源数据集，提出了MoPE算法和相应的幻觉评估指标，通过GRPO框架有效缓解了幻觉问题，显著提升了运动中心视频描述模型的可靠性。

Conclusion: 该工作通过数据集构建和算法创新，系统性地解决了视频描述中的细粒度运动理解和幻觉问题，为运动中心视频理解提供了新的基准和方法。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出3D-HIW数据集和CLUTCH系统，用于野外环境下的手部动作建模，在文本到动作和动作到文本任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有手部动作生成方法依赖工作室采集的数据集，动作和场景有限，难以扩展到野外环境，且现有模型在动画保真度和文本-动作对齐方面存在不足。

Method: 1) 构建3D-HIW数据集：结合视觉语言模型和3D手部追踪器处理大量第一人称动作视频；2) 提出CLUTCH系统：包含SHIFT（部件模态分解的VQ-VAE架构）和几何精炼阶段，通过重建损失微调LLM。

Result: 在文本到动作和动作到文本任务上实现了最先进的性能，建立了首个可扩展的野外手部动作建模基准。

Conclusion: 通过3D-HIW数据集和CLUTCH系统，成功解决了野外环境下手部动作建模的挑战，为可扩展的手部动画生成和描述建立了新基准。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: PRISM是一个用于结肠镜导航的自监督单目深度和姿态估计框架，通过边缘检测和亮度解耦利用解剖和光照先验指导几何学习，在真实和合成数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜辅助导航中的单目深度和姿态估计对改善筛查很重要，但面临纹理缺失表面、复杂光照模式、变形以及缺乏可靠真实标注数据等挑战。

Method: 提出PRISM自监督学习框架，结合边缘检测（使用DexiNed或HED等学习型边缘检测器）和亮度解耦（通过内在分解模块分离着色和反射），利用解剖和光照先验指导几何学习。

Result: 在多个真实和合成数据集上实现最先进性能。消融研究发现：1）真实数据的自监督训练优于幻影数据的监督训练；2）视频帧率对模型性能至关重要，需要特定数据集的视频帧采样策略。

Conclusion: PRISM通过结合边缘检测和亮度解耦有效解决了结肠镜深度和姿态估计的挑战，为结肠镜导航提供了实用的自监督学习框架，并建立了训练数据选择的最佳实践。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 提出LGD-Net框架，通过跨模态特征幻觉而非像素级图像生成，从H&E切片直接预测HER2表达水平，避免计算开销和重建伪影。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色评估HER2表达水平资源密集、昂贵且耗时，许多地区无法获得。从H&E切片预测HER2水平成为潜在替代方案，但现有像素级虚拟染色方法计算昂贵且易产生重建伪影，可能传播诊断错误。

Method: 提出Latent-Guided Dual-Stream Network (LGD-Net)，采用跨模态特征幻觉而非显式像素级图像生成。模型学习将形态学H&E特征直接映射到分子潜在空间，在训练时由教师IHC编码器指导。通过轻量级辅助正则化任务，利用特定任务领域知识（核分布和膜染色强度）正则化模型训练。

Result: 在公开BCI数据集上的广泛实验表明，LGD-Net实现了最先进的性能，显著优于基线方法，同时能够使用单模态H&E输入进行高效推理。

Conclusion: LGD-Net通过跨模态特征幻觉方法有效解决了传统像素级虚拟染色的局限性，为从H&E切片准确预测HER2表达水平提供了高效可靠的解决方案。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一种无需额外训练或仅需轻量LoRA调优的遥感图像文本引导分割方法，结合对比式和生成式视觉语言模型与SAM，在19个基准测试中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型和视觉基础模型为零样本遥感图像分割提供了新机会，但大多数方法仍依赖额外可训练组件，限制了泛化能力和实际应用。本研究探索如何在不进行额外训练的情况下，仅利用现有基础模型实现基于文本的遥感分割。

Method: 提出两种方法：1) 对比式方法使用CLIP作为SAM网格提议的掩码选择器，实现完全零样本的开放词汇语义分割；2) 生成式方法使用GPT-5（零样本）和LoRA调优的Qwen-VL模型生成点击提示给SAM，实现推理和指代分割。

Result: 在19个遥感基准测试（包括开放词汇、指代和基于推理的任务）上进行了广泛实验，对比式方法在完全零样本设置下实现了最先进的开放词汇语义分割，生成式方法（特别是LoRA调优的Qwen-VL）在推理和指代分割中表现最佳。

Conclusion: 研究表明，通过巧妙结合现有基础模型，可以在不进行额外训练或仅需轻量调优的情况下，实现强大的遥感图像文本引导分割，为实际应用提供了高效解决方案。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT是一个简单的仅编码器视频分割模型，通过轻量级查询传播机制实现跨帧信息传递，无需专用跟踪模块，在保持竞争力的同时实现5-10倍加速，达到160 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割模型通常结合逐帧分割器和复杂的专用跟踪模块，虽然有效但引入了显著的架构复杂性和计算开销。受Vision Transformer在大规模预训练下无需专用模块即可进行准确图像分割的启发，作者希望开发一个简单高效的仅编码器视频分割模型。

Method: 提出Video Encoder-only Mask Transformer (VidEoMT)，采用轻量级查询传播机制：重用前一帧的查询来跨帧传递信息，同时结合查询融合策略，将传播查询与一组时间无关的学习查询相结合，以平衡对新内容的适应性。

Result: VidEoMT在保持竞争力的准确率的同时，实现了5-10倍的加速，使用ViT-L骨干网络时运行速度可达160 FPS，达到了跟踪器的效果而无需增加复杂性。

Conclusion: 通过简单的仅编码器架构和轻量级查询传播机制，VidEoMT证明了可以在视频分割中实现高效的时空建模，无需复杂的专用跟踪模块，为实时视频分割提供了高效解决方案。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，探索了多种预检索和后检索性能预测器，并展示了VQPP在查询重写任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在文本和图像检索中已有广泛研究，但在基于内容的视频检索（CBVR）领域仍未被充分探索，需要建立专门的基准来推动该领域发展。

Method: 创建了包含56K文本查询和51K视频的VQPP基准，包含官方训练、验证和测试划分。探索了多种预检索和后检索性能预测器，并使用最佳预检索预测器作为奖励模型，通过直接偏好优化（DPO）训练LLM进行查询重写。

Result: 预检索预测器取得了有竞争力的性能，能够在执行检索步骤前实现应用。VQPP基准为视频领域的QPP研究提供了可复现比较的基础。

Conclusion: VQPP是首个视频查询性能预测基准，填补了CBVR领域QPP研究的空白，为未来视频检索中的查询性能预测研究提供了标准化评估框架和实用工具。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文批判性分析了Liu和Szirányi的手势识别方法，指出其评估协议存在严重数据泄露问题，导致报告的近乎完美准确率指标无效


<details>
  <summary>Details</summary>
Motivation: 针对Liu和Szirányi提出的手势识别方法，作者发现其评估协议存在严重缺陷，特别是帧级别的随机训练-测试分割导致同一受试者的样本混合在训练集和测试集中，造成数据泄露，无法真正评估模型对未见个体的泛化能力

Method: 通过分析已发表的混淆矩阵、学习曲线和数据集构建方式，系统性地检查评估协议的合理性，特别关注数据分割方法是否遵循主体独立原则

Result: 研究发现报告的近乎完美准确率是由于数据泄露导致的，评估协议未能测量模型对未见个体的泛化能力，特别是在无人机-人交互等需要识别新用户手势的实际应用中

Conclusion: 在基于视觉的手势识别研究中，必须采用主体独立的数据分割方法，以确保评估结果能够真实反映模型对未见个体的识别能力，这对于实际应用至关重要

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出一种用于长视频理解的新框架，包含自适应视频采样器和时空视频压缩器，结合多模态大语言模型，有效处理长视频冗余问题


<details>
  <summary>Details</summary>
Motivation: 随着视频主干架构和大型语言模型的发展，分析长达数十分钟的长视频变得可行且普遍。然而，视频序列固有的冗余性给当前最先进模型带来两大挑战：1) 在内存限制内高效处理更多帧；2) 从大量输入数据中提取判别性信息。

Method: 提出端到端的长视频理解框架，包含基于信息密度的自适应视频采样器(AVS)和基于自动编码器的时空视频压缩器(SVC)，与多模态大语言模型(MLLM)集成。AVS自适应捕捉不同时长视频的关键信息，SVC实现高压缩率同时保留重要判别信息。

Result: 该框架在多个基准测试中表现出色，在长视频理解任务和标准视频理解基准上都取得优异性能，证明了其在处理长视频序列复杂性方面的有效性和通用性。

Conclusion: 提出的框架通过自适应采样和高效压缩机制，有效解决了长视频理解中的冗余问题，为处理长时间视频序列提供了有前景的解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型在细粒度图像分类任务上表现不佳，通过实验发现更好的视觉编码器和预训练阶段对提升细粒度分类性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种视觉问答基准上取得了显著进展，但最近的研究表明这些模型在传统的图像分类基准（特别是细粒度分类）上表现落后。本文旨在探索视觉语言模型在细粒度视觉知识方面表现不佳的原因。

Method: 对大量最新的视觉语言模型在细粒度分类基准上进行测试，通过一系列消融实验分析影响性能的因素，包括不同LLM、视觉编码器以及预训练阶段的影响。

Result: 发现使用更好的LLM对所有基准分数都有同等提升，而更好的视觉编码器则不成比例地提高细粒度分类性能。预训练阶段对细粒度性能至关重要，特别是在预训练期间语言模型权重未冻结的情况下。

Conclusion: 这些发现为增强视觉语言模型的细粒度视觉理解和视觉中心能力提供了方向，强调了视觉编码器质量和预训练策略的重要性。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态深度重建框架，利用极稀疏的雷达或LiDAR测距数据生成密集深度图，作为基于扩散模型的新视角合成的几何条件，显著提升单图像新视角合成的几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的单图像新视角合成方法依赖于单目深度估计的几何信息，但在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计的可靠性有限，导致合成视图的质量和一致性受到限制。

Method: 提出多模态深度重建框架，利用极稀疏的测距数据（如汽车雷达或LiDAR），在角度域中使用局部高斯过程建模深度，实现计算高效推理并显式量化有限观测区域的不确定性。重建的深度和不确定性可直接替换现有扩散渲染流程中的单目深度估计器。

Result: 在真实世界多模态驾驶场景实验中，用稀疏测距重建深度替换纯视觉深度，显著提升了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 研究强调了可靠几何先验对基于扩散的视角合成的重要性，并展示了即使在极端稀疏情况下，多模态传感的实际优势。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑的视觉Transformer，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像任务中表现出色，特别适合资源受限的临床环境。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer依赖位置嵌入和类别标记编码固定的空间先验，这在医学成像和边缘临床系统中可能阻碍泛化能力，因为空间布局信息可能较弱或不一致。

Method: 提出ZACH-ViT，移除位置嵌入和[CLS]标记，通过全局平均池化处理补丁表示实现排列不变性，使用自适应残差投影保持训练稳定性，同时严格控制参数预算。

Result: 在7个MedMNIST数据集上的评估显示：ZACH-ViT（0.25M参数）在BloodMNIST上表现最强，在PathMNIST上与TransMIL竞争，但在具有强解剖先验的数据集（OCTMNIST, OrganAMNIST）上优势减弱，符合架构假设。

Conclusion: 将架构归纳偏置与数据结构对齐比追求通用基准主导更重要。ZACH-ViT尽管尺寸极小且无预训练，仍能实现竞争性性能，支持在资源受限临床环境中的部署。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET提出了一种残差导向的多层表示对齐框架，通过共享投影器将VLA模型的多个层与3D视觉基础模型的多个层对齐，显著提升了3D空间理解能力，同时大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language-Action模型通常在2D数据上预训练，缺乏3D空间理解能力。现有的表示对齐方法通常只在单层进行监督，无法充分利用深度分布的信息，而简单的多层对齐又会导致梯度干扰。

Method: ROCKET采用残差导向的多层表示对齐框架，将多层对齐形式化为将一个残差流对齐到另一个残差流。具体包括：1）使用共享投影器通过层不变映射将VLA骨干的多个层与强大的3D视觉基础模型的多个层对齐；2）提出Matryoshka风格的稀疏激活方案来平衡多个对齐损失；3）结合免训练层选择策略。

Result: 实验表明，ROCKET仅需约4%的计算预算，就在LIBERO上达到了98.5%的最先进成功率。在LIBERO-Plus和RoboTwin等多个基准测试中，以及多个VLA模型上都表现出优越性能。

Conclusion: ROCKET通过残差导向的多层表示对齐框架，有效解决了现有VLA模型缺乏3D空间理解的问题，在显著降低计算成本的同时实现了优越的性能，为机器人操作任务提供了更高效的解决方案。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出基于记忆驱动的质量感知框架MQAF，通过建立记忆库存储失真模式，动态切换双模式质量评估策略，减少对高质量参考图像的依赖，同时支持有参考和无参考图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估方法依赖参考图像质量，限制了在实际应用中的使用，因为理想参考源往往不可得。受人类视觉系统能够积累视觉记忆、基于长期记忆存储进行质量评估的启发，需要开发减少对高质量参考图像依赖的方法。

Method: 提出记忆驱动的质量感知框架MQAF：1）建立记忆库存储失真模式；2）采用双模式质量评估策略：有参考时通过自适应加权参考信息并与记忆库中的失真模式比较获得质量分数；无参考时依赖记忆库中的失真模式推断图像质量。

Result: 实验结果表明，该方法在多个数据集上优于现有最先进方法，同时能够适应无参考和有参考两种任务。

Conclusion: 提出的记忆驱动框架通过模拟人类视觉记忆机制，有效减少了对高质量参考图像的依赖，在保持评估精度的同时增强了实际应用能力，为图像质量评估提供了更灵活的解决方案。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: 提出了首个伪多模态水下目标跟踪基准MUOT_3M（300万帧）和基于SAM的多模态到单模态跟踪器MUTrack，通过视觉几何对齐、视觉语言融合和四级知识蒸馏实现高效水下跟踪。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪对海洋机器人、生态监测和海洋探索至关重要，但现有基准数据集规模小且仅支持RGB模态，限制了在颜色失真、浑浊和低能见度条件下的鲁棒性。

Method: 1) 构建MUOT_3M基准：包含300万帧、3030个视频，提供RGB、增强RGB、深度和语言四种同步模态；2) 提出MUTrack跟踪器：基于SAM架构，采用视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识迁移到单模态学生模型中。

Result: 在五个UOT基准测试中，MUTrack比最强SOTA基线AUC提升8.40%，精度提升7.80%，同时保持24 FPS的实时运行速度。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新基础，解决了水下跟踪领域的数据稀缺和模态限制问题。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出L-AVC任务和EPEM方法，通过多模态LLM高效精确地编辑图像中的主观情感内容


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注客观对齐（如语言、布局等控制信号），忽视了主观情感内容，且缺乏通用的情感视觉定制基础模型

Method: 提出EPEM方法，包含高效情感间语义转换模块（EIC）和精确情感无关内容保留模块（PER），通过多模态LLM实现情感编辑

Result: 在构建的L-AVC数据集上，EPEM方法在情感视觉定制任务上显著优于多个先进基线方法

Conclusion: 情感信息对L-AVC任务至关重要，EPEM方法能够高效精确地操控图像中的情感信息

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 本文提出深度安全导向视频理解任务DeepSVU，不仅检测威胁还要分析威胁原因，并提出统一物理世界正则化MoE方法UPRM来解决该任务


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注检测和定位威胁，但缺乏分析和评估威胁原因的能力。为填补这一空白，本文提出DeepSVU任务，要求不仅识别威胁，还要归因和评估威胁原因

Method: 提出统一物理世界正则化MoE方法UPRM，包含两个关键组件：统一物理世界增强MoE块用于建模粗到细的物理世界信息，物理世界权衡正则化器用于自适应权衡这些因素

Result: 在DeepSVU指令数据集上的实验表明，UPRM优于多个先进的视频大语言模型和非VLM方法，验证了粗到细物理世界信息的重要性以及UPRM的有效性

Conclusion: DeepSVU任务需要更深入的安全视频理解，UPRM方法通过建模物理世界信息和自适应权衡机制，有效提升了安全视频理解能力

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出UAOR（不确定性感知观测重注入）模块，一种无需训练、即插即用的VLA模型增强方法，通过注意力检索在语言模型层不确定性高时重注入关键观测信息，提升动作生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型增强方法通常需要额外的观测线索（深度图、点云）或辅助模块（目标检测器、编码器），这些方法需要昂贵的数据收集和额外训练。需要一种更高效、无需训练的增强方案。

Method: 提出UAOR模块，当语言模型层的不确定性（通过动作熵衡量）较高时，通过注意力检索机制将关键观测信息重注入到下一层的FFN中，帮助VLA模型在推理时更好地关注观测信息。

Result: 实验表明，UAOR能持续提升多种VLA模型在仿真和真实世界任务中的性能，且开销极小。无需额外观测线索或模块，成为现有VLA管道的通用实用插件。

Conclusion: UAOR是一种有效、无需训练、即插即用的VLA模型增强模块，通过不确定性感知的观测重注入机制，显著提升模型在机器人操作任务中的表现，具有很好的实用性和通用性。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出Dual-Channel Attention Guidance (DCAG)框架，通过同时操纵DiT架构中的Key和Value通道实现无需训练的图像编辑强度控制，相比仅使用Key通道的方法能提供更精确的编辑-保真度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力操纵方法仅关注Key空间来调节注意力路由，而完全忽略了控制特征聚合的Value空间。需要一种无需训练的方法来更精确地控制基于DiT架构的扩散模型的编辑强度。

Method: 首先发现DiT多模态注意力层中的Key和Value投影都表现出明显的偏置-增量结构，基于此提出DCAG框架，同时操纵Key通道（控制注意力位置）和Value通道（控制特征聚合）。理论分析表明Key通道通过非线性softmax函数作为粗粒度控制，Value通道通过线性加权求和作为细粒度补充。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）上，DCAG在所有保真度指标上始终优于仅使用Key通道的指导方法，在局部编辑任务中改进最显著：对象删除（LPIPS降低4.9%）和对象添加（LPIPS降低3.2%）。

Conclusion: DCAG通过同时利用Key和Value通道，创建了一个二维参数空间(δ_k, δ_v)，能够实现比任何单通道方法更精确的编辑-保真度权衡，为基于DiT的扩散模型提供了更灵活和精确的编辑控制。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST提出一个分解-融合框架，利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型，以提升小样本动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统小样本动作识别方法仅使用语义粗糙的类别名称作为辅助上下文，这种上下文过于有限，无法为捕捉动作中的新颖空间和时间概念提供足够的背景知识。

Method: 1. 分解阶段：将普通动作名称解耦为多样的时空属性描述（动作相关知识）；2. 融合阶段：提出空间/时间知识补偿器（SKC/TKC）分别发现判别性对象级和帧级原型；SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 实验结果表明DiST在五个标准FSAR数据集上取得了最先进的结果。

Conclusion: DiST通过利用大语言模型提供的解耦空间和时间知识来学习表达性多粒度原型，有效提升了小样本动作识别的性能，为捕捉细粒度空间细节和多样化时间模式提供了透明度。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard：一种用于分散式监控的拓扑感知Transformer框架，通过自适应度量学习、空间条件注意力和差分隐私嵌入，实现隐私保护的跨摄像头行人重识别。


<details>
  <summary>Details</summary>
Motivation: 城市级行人重识别面临视角变化、遮挡和域偏移等挑战，同时需要遵守数据保护法规，防止原始图像共享。现有方法在隐私保护和跨视图对齐方面存在不足。

Method: 1. 分散自适应度量学习：根据特征分布调整实例级边界，增强类内紧凑性；2. 空间条件注意力：将GPS或部署平面图等粗略几何信息注入图自注意力，实现投影一致的跨视图对齐；3. 差分隐私嵌入映射与紧凑近似索引：支持安全高效部署。

Result: 在Market-1501等公开基准测试中，检索精度和查询吞吐量均优于强基线方法。数据库级检索研究证实了框架在隐私关键城市身份匹配中的实用性。

Conclusion: CityGuard框架通过集成度量学习、几何感知注意力和差分隐私技术，实现了对视角变化、遮挡和域偏移具有鲁棒性的描述符，并在隐私保护与效用之间提供了可调节的平衡。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M提出了一种时序一致性感知的文本到动作生成框架，通过TCaS-VQ-VAE实现跨序列时序对齐，结合掩码运动Transformer和运动学约束块，在HumanML3D和KIT-ML基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到动作生成框架通常忽略跨序列的时序一致性（即相同动作在不同实例中共享的时序结构），导致语义错位和物理上不合理的动作。

Method: 提出TCA-T2M框架：1) 时序一致性感知的空间VQ-VAE（TCaS-VQ-VAE）用于跨序列时序对齐；2) 掩码运动Transformer用于文本条件动作生成；3) 运动学约束块减轻离散化伪影，确保物理合理性。

Result: 在HumanML3D和KIT-ML基准测试中，TCA-T2M实现了最先进的性能，证明了时序一致性对鲁棒和连贯的文本到动作生成的重要性。

Conclusion: TCA-T2M通过引入时序一致性感知机制，有效解决了现有文本到动作生成中的语义错位和物理不合理问题，为鲁棒的动作生成提供了新思路。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 3DMedAgent是一个统一代理系统，让2D多模态大语言模型能够执行3D CT分析而无需3D特定微调，通过协调异构工具和结构化记忆实现从感知到理解的渐进式分析。


<details>
  <summary>Details</summary>
Motivation: 现有3D分析方法要么采用孤立的任务特定建模，要么采用任务无关的端到端范式，阻碍了感知证据的系统性积累。同时，多模态大语言模型虽然改进了视觉感知能力，但其主要面向2D设计，限制了处理体积医学数据的能力。

Method: 提出3DMedAgent统一代理，通过灵活的MLLM代理协调异构视觉和文本工具，将复杂3D分析逐步分解为可处理的子任务：从全局到局部视图、从3D体积到信息丰富的2D切片、从视觉证据到结构化文本表示。核心设计包括维护长期结构化记忆，聚合中间工具输出，支持查询自适应、证据驱动的多步推理。

Result: 在超过40个任务上的实验表明，3DMedAgent在3D胸部成像的DeepChestVQA基准测试中，持续优于通用、医学和3D特定的MLLMs，展示了向通用3D临床助手扩展的可行路径。

Conclusion: 3DMedAgent为2D MLLMs执行通用3D CT分析提供了一种无需3D特定微调的统一代理方法，通过工具协调和结构化记忆实现了从感知到理解的渐进式分析，为通用3D临床助手开辟了可扩展的路径。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出两阶段训练策略用于BEV语义地图分割，通过自监督预训练减少对昂贵BEV标注的依赖，在微调阶段仅需50%数据即可超越全监督基线


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV语义地图方法依赖昂贵且标注不一致的地面真值，限制了自动驾驶感知系统的可扩展性

Method: 两阶段训练：1) 自监督预训练阶段，将BEVFormer预测可微分重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练，加入时序一致性损失；2) 监督微调阶段仅需50%数据集

Result: 在nuScenes数据集上，微调后性能提升达+2.5pp mIoU超越全监督基线，同时减少50%标注数据使用，总训练时间减少三分之二

Conclusion: 可微分重投影加相机视角伪标签能产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了可扩展路径

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 提出一个结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据的高分辨率（10米）土壤湿度估算框架，用于欧洲植被区域，通过机器学习方法实现农场级应用。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤湿度产品分辨率过低（>1公里），无法满足农场级应用需求。需要开发高分辨率土壤湿度估算方法，以支持精准农业、水资源管理和气候监测。

Method: 结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，通过机器学习方法构建土壤湿度估算框架。使用113个国际土壤湿度网络站点进行验证，比较不同模态组合和时间参数化策略，采用空间交叉验证确保地理泛化能力。同时评估IBM-NASA Prithvi基础模型嵌入特征与传统手工特征的效果。

Result: 混合时间匹配策略（Sentinel-2当日采集与Sentinel-1降轨数据）达到R²=0.514，结合10天ERA5回溯窗口可提升至R²=0.518。Prithvi基础模型嵌入特征相比传统手工特征改进有限（R²=0.515 vs. 0.514），表明传统特征工程在稀疏数据回归任务中仍具竞争力。

Conclusion: 领域特定的光谱指数结合基于树的集成方法为欧洲田间尺度土壤湿度监测提供了实用且计算高效的解决方案。传统特征工程在稀疏数据回归任务中仍保持高度竞争力。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript：首个大规模、多书写者、平行风格的印地语手写数据集，包含531名书写者抄写的相同6首传统对句，支持手写识别、书写者识别、风格分析和生成建模等任务。


<details>
  <summary>Details</summary>
Motivation: 尽管印地语有数亿使用者，但天城体手写文本在公开基准数据集中严重不足。现有资源规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和书写者多样性，无法捕捉天城体手写连续、融合和结构复杂的特性。

Method: 收集531名独特贡献者的手写印地语文本，构建为平行风格语料库，所有书写者转录相同的6首传统印地语对句。数据集包含去识别化的人口统计元数据，基于客观清晰度和分辨率标准的严格质量筛选，以及页面级布局难度标注。

Result: 基线实验显示清晰的质量分离和对未见书写者的强泛化能力，证明了数据集的可靠性和实用价值。DohaScript旨在作为标准化、可复现的基准，推动低资源脚本环境下连续手写天城体文本的研究。

Conclusion: DohaScript填补了天城体手写文本数据集的空白，通过受控设计支持系统性的书写者特定变异分析，为手写识别、书写者识别、风格分析和生成建模等任务提供了有价值的资源。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT：一种无需训练的DiT加速框架，通过线性多步方法预测模型输出，实现5.54倍延迟降低


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiT）的迭代去噪过程计算成本高，现有基于特征缓存和重用的加速方法可能导致潜在漂移和视觉质量下降

Method: 提出PrediT框架，将特征预测建模为线性多步问题，使用经典线性多步方法从历史信息预测未来输出，结合校正器在高动态区域激活防止误差累积，并采用动态步长调制机制自适应调整预测范围

Result: 在各种基于DiT的图像和视频生成模型上实现高达5.54倍的延迟降低，同时质量下降可忽略不计

Conclusion: PrediT是一种有效的无需训练加速方法，通过预测而非简单重用的策略，在保持生成质量的同时显著提升DiT推理效率

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench：首个用于全面评估视觉语言模型处理分布外数据能力的自动化基准，包含4万个实例-类别对，发现当前VLM在常见类别上仍存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用中数据往往不满足独立同分布假设，分布外数据可能带来安全风险，但现有研究缺乏全面评估VLM处理OOD数据能力的有效基准。

Method: 提出OODBench方法，通过最小化人工验证的自动化流程构建新基准，包含4万个实例级OOD实例-类别对，并设计从基础到进阶的提示问题渐进式自动评估指标。

Result: 当前VLM在OODBench上表现出显著性能下降，即使底层图像类别是常见的；提出的自动化评估指标能更全面地评估OOD数据对不同难度问题的影响。

Conclusion: OODBench填补了VLM处理分布外数据评估的空白，总结了重要发现和见解，为未来OOD数据获取和评估研究提供了基础。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化感知任务中表现不如CNN，与人类感知对齐有限


<details>
  <summary>Details</summary>
Motivation: 虽然ViTs在多种图像任务中表现出色，但其在可视化图形感知任务中的能力尚未被充分探索，而这类任务对解释可视化至关重要

Method: 基于Cleveland和McGill的经典研究，设计了一系列受控的图形感知任务，将ViTs与CNNs和人类参与者进行对比评估

Result: ViTs在通用视觉任务中表现强劲，但在可视化领域的图形感知任务中与人类感知的对齐程度有限，存在明显的感知差距

Conclusion: 研究揭示了ViTs在可视化感知方面的局限性，为在可视化系统和图形感知建模中应用ViTs提供了重要考量

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个用于短视频广告内容审核的框架，结合了思维链推理、基于规则的策略原则和批评者引导的奖励机制，通过强化学习优化模型，在多模态欺骗性内容检测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告包含欺骗性的视觉、语音和字幕内容，需要比社区安全过滤器更细粒度、基于策略的审核机制。

Method: 1. 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签；2. 采用强化学习，使用平衡因果一致性和策略遵从性的复合奖励优化模型；3. 多任务架构同时建模模态内操纵（如夸张图像）和跨模态不匹配（如字幕-语音漂移）。

Result: 在真实短视频广告上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面均优于强基线模型。

Conclusion: BLM-Guard框架通过结合规则驱动数据合成、强化学习优化和多任务架构，为商业广告内容审核提供了有效的解决方案，显著降低了标注成本并提升了检测性能。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [31] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督学习从故意扭曲的运动中学习物理合理性，在保持语义一致性的同时改善文本生成运动的物理真实性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到运动生成方法在语义对齐方面取得进展，但难以同时保证语义一致性和物理合理性（如脚部漂浮等问题）。

Method: 提出Distortion-aware Motion Calibrator (DMC)，一个自监督数据驱动的后处理模块，通过输入故意扭曲的运动和原始文本描述，学习生成物理合理的运动。

Result: DMC显著提升物理合理性：在T2M上FID降低42.74%，在T2M-GPT上降低13.20%，R-Precision最高；应用于MoMask时穿透减少33.0%，漂浮伪影更接近真实参考。

Conclusion: DMC作为一个有前景的后处理运动精炼框架，能够为各种文本到运动模型同时提升语义一致性和物理合理性。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [32] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 首次研究离散图像分词器的对抗攻击脆弱性，提出高效攻击方法，并通过无监督对抗训练提升其鲁棒性


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中应用日益广泛，但其对抗攻击脆弱性尚未被研究，而CLIP编码器的脆弱性已有探索，需要填补这一空白

Method: 1) 提出针对离散分词器的对抗攻击方法，旨在扰动特征提取并改变分词结果；2) 基于鲁棒CLIP编码器研究，通过无监督对抗训练微调流行分词器，保持其他组件不变

Result: 攻击方法计算高效、应用无关，在分类、多模态检索和字幕生成任务中均有效；防御方法显著提升对无监督和端到端监督攻击的鲁棒性，并能泛化到未见任务和数据

Conclusion: 研究揭示了分词器鲁棒性在下游任务中的关键作用，为开发安全的多模态基础模型迈出了重要一步，无监督方法比监督对抗训练更具通用性

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [33] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的框架，通过实例细节提取器和细节融合模块解决属性泄漏问题，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例生成方法在细粒度语义理解方面面临挑战，特别是在处理复杂文本描述时，难以精确控制多个实例的属性和空间关系。

Method: 提出DEIG框架，包含实例细节提取器（将文本编码器嵌入转换为紧凑的实例感知表示）和细节融合模块（应用基于实例的掩码注意力防止属性跨实例泄漏）。构建高质量数据集和DEIG-Bench基准，支持细粒度监督。

Result: DEIG在多个基准测试中在空间一致性、语义准确性和组合泛化方面一致优于现有方法，并且可以作为即插即用模块轻松集成到标准扩散管道中。

Conclusion: DEIG通过创新的实例细节提取和融合机制，实现了细粒度可控的多实例生成，解决了属性泄漏问题，为复杂场景生成提供了有效解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [34] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS是一个结合全局草图引导与多个局部草图-文本对的时尚图像生成框架，通过多级条件编码和扩散对引导实现结构保持与语义增强。


<details>
  <summary>Details</summary>
Motivation: 草图是时尚设计早期构思的重要媒介，能表达结构、轮廓和空间关系，而文本描述则补充材质、颜色和风格细节。有效结合文本和视觉模态需要在利用文本局部属性指导时保持草图视觉结构。

Method: 提出LOTS框架：1）多级条件编码阶段：在共享潜在空间中独立编码局部特征，同时保持全局结构协调；2）扩散对引导阶段：通过基于注意力的引导在扩散模型的多步去噪过程中整合局部和全局条件。

Result: 创建了首个多文本-草图对的时尚数据集Sketchy，包含高质量专业草图和非专家草图两个版本。实验表明该方法在增强全局结构保持的同时利用更丰富的局部语义指导，优于现有最先进方法。

Conclusion: LOTS框架通过结合全局草图引导与多个局部草图-文本对，有效提升了时尚图像生成的质量，在保持结构一致性的同时增强了语义控制能力。数据集、平台和代码均已公开。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [35] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS：两阶段框架，通过扩散模型修复被器械遮挡的组织，结合2D高斯泼溅和可学习变形模型，实现手术场景的实时3D重建，在图像质量和几何精度上均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 手术场景的实时3D重建对机器人手术至关重要，但现有方法在遮挡区域重建质量有限，且缺乏深度精度评估。EndoNeRF和StereoMIS等基准数据集缺少3D真值，难以全面评估重建质量。

Method: 提出Diff2DGS两阶段框架：第一阶段使用具有时序先验的扩散模型修复被手术器械遮挡的组织区域；第二阶段采用2D高斯泼溅（2DGS）结合可学习变形模型（LDM）捕捉动态组织变形和解剖几何。

Result: 在EndoNeRF数据集上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB，均超越现有方法。实验表明仅优化图像质量不一定能获得最佳3D重建精度，因此进一步优化深度质量以确保几何保真度。

Conclusion: Diff2DGS在图像质量和几何精度上均优于现有方法，为手术场景的可靠3D重建提供了有效解决方案，并强调了同时优化外观和几何的重要性。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [36] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++ 是一个基于3D高斯泼溅的框架，通过全局自适应亮度调整和局部像素级残差细化来解决多视角采集中的光照不一致问题，提升3D新视角合成的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图像采集面临复杂光照变化和相机成像管道的限制，多视角采集中的光照、传感器响应和ISP配置差异导致光度不一致，这违反了现代3D新视角合成方法（如NeRF和3DGS）所依赖的光度一致性假设，导致重建和渲染质量下降。

Method: 结合全局视角自适应亮度调整和局部像素级残差细化进行精确色彩校正，设计无监督目标函数联合强制亮度校正以及多视角几何和光度一致性，同时保持显式的3DGS表示形式。

Result: 在低光照、过曝和复杂亮度/色彩变化等挑战性场景中实现了最先进的性能，提高了重建保真度，同时保持了实时渲染效率。

Conclusion: Luminance-GS++ 通过创新的光照校正方法有效解决了多视角采集中的光度不一致问题，在保持3DGS实时渲染优势的同时显著提升了在各种复杂光照条件下的重建和渲染质量。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [37] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 提出基于高斯-拉普拉斯算子(G-LoG)的双参数过滤方法，用于医学图像拓扑特征提取，在MedMNIST数据集上表现优于单参数过滤，且MLP模型在拓扑特征上达到与复杂深度学习模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的过滤方法来检测拓扑和几何特征至关重要。本文旨在利用拉普拉斯高斯算子增强医学图像边界的能力，定义更适合多参数持久性模块的特征。

Method: 提出G-LoG(高斯-拉普拉斯高斯)双参数过滤方法：1) 将体积图像建模为有界函数；2) 利用拉普拉斯高斯算子增强图像边界；3) 构建双参数过滤生成拓扑特征；4) 证明持久性模块的交互距离相对于有界函数的最大范数是稳定的。

Result: 1) 在MedMNIST数据集上的实验表明，双参数过滤显著优于单参数过滤；2) 简单的多层感知器(MLP)在拓扑特征上达到与Google AutoML Vision、ResNet、AutoKeras和auto-sklearn等复杂深度学习模型相当的性能；3) 证明了方法的稳定性。

Conclusion: G-LoG双参数过滤为医学图像分析提供了有效的拓扑特征提取方法，不仅性能优于传统单参数方法，而且基于拓扑特征的简单MLP模型能达到与复杂深度学习模型相当的分类性能，展示了拓扑数据分析在医学图像处理中的潜力。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [38] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出基于退化流形的退化感知自感知框架，通过对比学习在特征空间中显式结构化图像退化信息，实现无需退化标签的检测器自感知能力


<details>
  <summary>Details</summary>
Motivation: 目标检测器在正常成像条件下表现良好，但在模糊、噪声、压缩、恶劣天气或分辨率变化等退化条件下可能无声失败。在安全关键场景中，仅产生预测而不评估输入是否保持在检测器名义工作范围内是不够的，需要自感知目标检测能力

Method: 基于退化流形的退化感知自感知框架：1）在标准检测骨干网络上添加轻量级嵌入头，通过多层对比学习训练；2）相同退化组成的图像被拉近，不同退化配置被推远，形成几何组织的表示；3）从干净训练嵌入中估计原始原型，定义名义工作点；4）自感知表现为与该参考的几何偏差

Result: 在合成损坏基准、跨数据集零样本迁移和自然天气引起的分布偏移上表现出强大的原始-退化可分性，在多种检测器架构上行为一致，在语义偏移下具有鲁棒泛化能力

Conclusion: 退化感知表示几何为自感知目标检测提供了实用且检测器无关的基础，能够独立于检测置信度提供退化引起的偏移的内在图像级信号

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [39] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 该论文提出通过从对称变换示例中学习潜在空间中的等变算子，以解决传统神经网络和等变网络在罕见对称变换下泛化能力不足的问题，在旋转和平移的噪声MNIST数据集上验证了方法的有效性，但指出了扩展到更复杂数据集的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在计算机视觉中虽然成功，但在处理训练中罕见的群对称变换（如异常姿态、尺度、位置等）时仍存在困难。等变神经网络虽然能解决对称变换的泛化问题，但需要先验知识。因此需要一种能从对称变换示例中学习等变算子的方法。

Method: 提出一种架构，从对称变换的示例中学习潜在空间中的等变算子。使用旋转和平移的噪声MNIST数据集作为简单测试平台，验证该方法在分布外分类任务中的有效性。

Result: 该方法在旋转和平移的噪声MNIST数据集上成功实现了分布外分类，克服了传统神经网络和等变网络的局限性，证明了从示例中学习等变算子的可行性。

Conclusion: 虽然概念上很有吸引力，但将这种架构扩展到更复杂的数据集仍面临挑战。该方法为解决对称变换泛化问题提供了有前景的方向，但需要进一步研究以应对实际应用中的复杂性。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [40] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 提出一种基于头部和手部姿态控制的人为中心视频世界模型，用于扩展现实(XR)中的沉浸式交互体验


<details>
  <summary>Details</summary>
Motivation: 当前视频世界模型仅接受文本或键盘等粗粒度控制信号，无法响应用户真实世界动作追踪，限制了在具身交互中的应用

Method: 评估现有扩散变换器条件策略，提出有效的3D头部和手部控制机制；训练双向视频扩散模型教师，蒸馏为因果交互系统生成第一人称虚拟环境

Result: 通过人类受试者评估显示，相比基线方法，该系统显著提升了任务表现和用户对执行动作的感知控制水平

Conclusion: 提出的以人为中心的视频世界模型能够有效响应真实世界动作追踪，为扩展现实中的具身交互提供了更自然的控制方式

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [41] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力约束下进行室内导航的新基准，包含5种代表性智能体、45个真实室内场景和473个导航任务，发现现有VLM在严格移动约束下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 真实世界导航需要考虑智能体的物理能力约束（如扫地机器人不能爬楼梯），而现有视觉语言导航研究缺乏对智能体能力条件的评估，需要建立专门基准来测试VLM在考虑具体物理和操作能力下的导航性能。

Method: 定义了5种代表性人类和机器人智能体，描述其物理尺寸、移动能力和环境交互能力；构建了包含45个真实室内场景、473个导航任务和2365个问答对的基准数据集；评估了13个现代视觉语言模型在能力约束导航任务上的表现。

Result: 当前VLM的导航性能随着移动约束的收紧而急剧下降；即使是state-of-the-art模型在处理需要空间维度推理的障碍物类型时也表现不佳；模型在考虑智能体具体能力条件下的导航能力有限。

Conclusion: 需要开发能力感知的导航系统，未来VLM需要加强具身空间推理能力；CapNav基准为评估和改进VLM在真实世界约束条件下的导航性能提供了重要工具。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 提出首个实时、完全因果的时空感知对话动作生成方法，可在VR头显上实时部署，结合用户位置和音频生成全身动作，实现自然的空间对齐和眼神接触控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏空间感知能力，无法让虚拟代理自然地转向用户、响应用户移动并保持自然注视。随着具身代理在VR、远程呈现和数字人应用中日益重要，需要超越语音对齐手势的完整空间感知能力。

Method: 结合因果Transformer VAE与流匹配模型，使用交错潜在token进行流式推理，通过用户轨迹和音频条件生成动作。引入注视评分机制和分类器自由引导，分离学习与控制，让用户可在推理时调整眼神接触强度。

Result: 在Embody 3D数据集上达到最先进的动作质量，推理速度超过300 FPS（比非因果基线快3倍），能捕捉自然对话的细微空间动态。在实时VR系统中验证了方法的可行性。

Conclusion: 实现了首个实时、完全因果的时空感知对话动作生成系统，可在VR头显上实时部署，为虚拟代理提供了自然的空间感知能力，支持用户可调节的眼神接触控制。

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过扩展token预算、自适应选择策略和训练免费检索专家混合，提升流式视频理解性能


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token数，导致细粒度视觉细节丢失，且在处理密集视频流时存在查询-帧相似度随时间增加的问题，偏向检索后期帧

Method: 1) 扩展token预算以实现更细粒度的时空理解；2) 引入自适应选择策略减少token冗余同时保留局部时空信息；3) 提出训练免费的检索专家混合，利用外部模型更好识别相关帧

Result: 在CG-Bench上提升8.0%，LVBench上提升8.5%，VideoMME(Long)上提升2.4%（相比ReKV with Qwen2.5-VL-7B）

Conclusion: MemStream通过扩展token预算、自适应选择和训练免费检索专家混合，显著提升了流式视频理解的性能，解决了现有方法在处理密集视频流时的偏差问题

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: QueryPlot是一个语义检索和制图框架，通过自然语言处理技术整合地质文本和地图数据，实现基于语义相似度的矿产远景预测。


<details>
  <summary>Details</summary>
Motivation: 传统矿产远景预测需要人工整合异构地质知识（包括文本矿床模型和地理空间数据），过程繁琐且知识密集。需要自动化工具来高效整合文本和空间数据，支持自然语言查询的矿产预测。

Method: 1. 整理120多种矿床类型的描述性模型；2. 将州地质图汇编多边形转换为结构化文本表示；3. 使用预训练嵌入模型编码用户查询和区域描述；4. 计算语义相似度得分进行区域排序和空间可视化；5. 支持组合查询和多标准远景分析。

Result: 在钨矽卡岩矿床案例研究中，基于嵌入的检索实现了已知矿床的高召回率，生成的远景区域与专家定义的许可区域高度一致。相似度得分作为监督学习特征可显著提升分类性能。

Conclusion: QueryPlot成功整合了地质文本和空间数据，通过语义检索支持自然语言查询的矿产远景预测，提高了预测效率和准确性，并可作为web系统支持交互式查询和GIS兼容层导出。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [45] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: 该研究探索了LLM之间的神经同步性作为其"社会心智"的证据，发现LLM在社交互动中表现出与人脑类似的神经同步现象，且这种同步性与社交表现相关。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为社会心智是生物特有属性，虽然LLM被广泛接受为人类行为的近似模型，但关于LLM是否具有可比拟人类社会心智的能力仍存在争议。该研究旨在通过神经同步性这一神经科学指标来探讨LLM的社会性。

Method: 引入神经同步性作为分析LLM社会性的新代理指标，通过精心设计的社交模拟实验，在表征层面分析LLM之间的神经同步现象，考察其是否反映社交参与度和时间对齐。

Result: 实验表明，LLM之间的神经同步性可靠地反映了它们互动中的社交参与度和时间对齐。神经同步性与LLM的社交表现呈强相关，揭示了神经同步性与LLM社交行为之间的重要联系。

Conclusion: LLM在社交互动中表现出与人脑类似的神经同步现象，为理解LLM的"社会心智"提供了新视角，揭示了人类与LLM社交互动内部动态的惊人相似性。

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [46] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 大型语言模型在眼动和阅读时间预测上表现更好，但会低估人类反应概率；模型越大，对完形填空数据的预测质量越高，因为更少依赖词汇共现统计，更符合人类语义选择


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在预测人类眼动和阅读时间数据方面的表现差异，探索模型规模如何影响对语言处理的预测能力，特别是模型在语义理解和词汇统计敏感性方面的权衡

Method: 通过比较不同规模的语言模型在眼动数据、阅读时间数据和完形填空任务上的表现，分析模型对下一个词的预测质量及其与人类反应的匹配程度

Result: 更大的语言模型在眼动和阅读时间预测上表现更好，但对人类反应的概率分配仍然不足；模型规模增大时，对完形填空数据的预测质量提高，更少依赖词汇共现统计，更符合人类语义选择

Conclusion: 大型语言模型更强的记忆能力帮助它们猜测更语义合适的词，但使它们对词汇识别相关的低层次信息不那么敏感，这支持了模型规模与语义理解能力正相关的观点

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [47] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 研究发现：控制语言模型行为的转向向量可靠性取决于训练激活差异的余弦相似度、正负激活在转向方向上的分离程度，以及转向方向是否能有效近似潜在行为表示。


<details>
  <summary>Details</summary>
Motivation: 转向向量虽然能平均有效地控制语言模型行为，但其效果在不同样本间差异很大，对许多目标行为不可靠。本研究旨在探究转向可靠性在不同行为间差异的原因，以及训练数据如何影响转向效果。

Method: 通过分析转向向量训练数据，研究三个关键因素：1) 训练激活差异的余弦相似度；2) 正负激活在转向方向上的分离程度；3) 不同提示变体训练的转向向量的方向差异和效果相关性。

Result: 发现三个关键结果：1) 训练激活差异的余弦相似度越高，转向越可靠；2) 正负激活在转向方向上分离越好的行为数据集，转向越可靠；3) 不同提示变体训练的转向向量方向不同但效果相似，且在不同数据集上效果相关。

Conclusion: 转向向量不可靠的根本原因是潜在行为表示无法被线性转向方向有效近似。这些发现为诊断转向不可靠性提供了实用方法，并激励开发更鲁棒的转向方法，需要显式考虑非线性潜在行为表示。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [48] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出一种基于语言模型生成语义软标签的新神经主题模型方法，通过重构上下文丰富的监督信号来提升主题质量。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型通常通过重构文档的词袋表示进行优化，忽略了上下文信息且面临数据稀疏性问题，需要更有效的监督信号来捕捉语义结构。

Method: 使用语言模型通过专用提示词生成下一个词的概率分布，将其投影到预定义词汇表上获得语义软标签，然后训练主题模型重构这些软标签，利用语言模型的隐藏状态作为监督信号。

Result: 在三个数据集上的实验表明，该方法在主题连贯性和纯度方面显著优于现有基线，同时新提出的检索指标也显示在识别语义相似文档方面有显著优势。

Conclusion: 通过语言模型生成语义软标签的方法能够有效提升神经主题模型的质量，产生更符合语料主题结构的主题，特别适用于检索导向的应用场景。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [49] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 提出首个条件性生物医学问答基准CondMedQA和条件门控推理框架CGR，解决现有系统忽略患者特定条件的问题


<details>
  <summary>Details</summary>
Motivation: 现有生物医学QA系统假设医学知识普遍适用，但真实临床推理本质上是条件性的，几乎所有决策都依赖于患者特定因素（如并发症、禁忌症）。现有基准无法评估这种条件推理，检索增强或基于图的方法缺乏确保检索知识适用于给定上下文的明确机制。

Method: 提出CondMedQA基准（首个条件性生物医学QA基准），包含答案随患者条件变化的多跳问题。提出条件门控推理（CGR）框架，构建条件感知知识图谱，基于查询条件选择性激活或剪枝推理路径。

Result: CGR能更可靠地选择条件适当的答案，同时在生物医学QA基准上达到或超越最先进性能，突显了显式建模条件性对稳健医学推理的重要性。

Conclusion: 条件性建模对于稳健的医学推理至关重要，CGR框架通过条件感知知识图谱和选择性路径激活，有效解决了现有系统忽略患者特定条件的问题，为临床决策支持系统提供了更可靠的解决方案。

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [50] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 本文首次系统比较了基于DSPy优化框架的指令优化方法在表格事实验证任务中的表现，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有特定优势。


<details>
  <summary>Details</summary>
Motivation: 指令优化为提升大语言模型推理性能提供了一种轻量级、模型无关的方法，但缺乏在表格事实验证任务中的系统比较研究。

Method: 基于DSPy优化框架，评估了四种现成的提示技术（直接预测、思维链、带SQL工具的ReAct、带Python执行的CodeAct），并研究了三种优化器（COPRO、MiPROv2、SIMBA）在四个基准测试和三个模型家族上的表现。

Result: 指令优化持续提升验证准确率：MiPROv2在思维链上获得最稳定的增益，SIMBA为ReAct智能体提供最大收益（尤其在大模型规模下）。行为分析显示SIMBA通过应用启发式方法鼓励更直接的推理路径。

Conclusion: 思维链在表格事实检查中仍然有效（尤其对小模型），而基于大模型的ReAct智能体虽能达到竞争性性能，但需要仔细的指令优化。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [51] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate是一个基于图检索增强生成（GraphRAG）的框架，用于自动构建UMLS概念集，结合知识图谱检索和LLM推理，显著减少人工工作量。


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别工具通常将自由文本映射到UMLS概念唯一标识符（CUI），但许多下游任务需要的是包含相关同义词、子类型和超类型的概念集。目前构建这样的概念集是劳动密集型的，执行不一致，现有工具支持不足。

Method: 构建UMLS知识图谱并进行嵌入以实现语义检索。针对每个目标概念，从知识图谱中检索候选CUI，然后通过大型语言模型（比较GPT-5和GPT-5-mini）进行过滤和分类。在五个词汇异质性临床概念上评估框架。

Result: CUICurate产生的概念集比人工基准更大更完整，同时保持与人工相当的精确度。GPT-5-mini在过滤阶段召回率更高，而GPT-5的分类结果更符合临床医生判断。输出稳定且计算成本低。

Conclusion: CUICurate提供了一个可扩展且可重复的方法来支持UMLS概念集构建，显著减少人工工作量。通过整合基于图的检索和LLM推理，该框架生成聚焦的候选概念集，可适应不同表型和分析需求的临床NLP流程。

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [52] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 论文研究金融问答中检索增强生成的失败模式：正确文档被检索到但包含答案的具体页面或块被遗漏，导致生成器基于不完整上下文推断。作者提出领域微调的页面评分器来改善页面级检索。


<details>
  <summary>Details</summary>
Motivation: 在金融监管文件问答中，检索增强生成虽然常用，但可靠性依赖于检索到确切的上下文来证明答案。研究发现一个常见失败模式：正确文档被检索到，但包含答案的具体页面或块被遗漏，导致生成器基于不完整上下文推断。尽管这在实践中很重要，但金融问答文献中对此缺乏系统研究。

Method: 1) 在多粒度级别（文档、页面、块）评估检索性能；2) 引入基于oracle的分析来提供检索和生成性能的实证上限；3) 在FinanceBench的150个问题子集上复现和比较多种检索策略（稠密、稀疏、混合、分层方法，包括重排序和查询重构）；4) 提出领域微调的页面评分器，将页面作为文档和块之间的中间检索单元，专门针对金融文件微调双编码器进行页面级相关性判断。

Result: 1) 文档发现性能的提升通常能转化为更强的页面召回率；2) oracle性能显示页面和块级检索仍有改进空间；3) 提出的领域微调页面评分器显著改善了页面召回率和块检索性能。

Conclusion: 针对金融问答中"正确文档被检索但具体页面/块被遗漏"的失败模式，通过引入专门为金融文件微调的页面级评分器，能够有效改善页面召回和块检索性能，填补了现有分层检索方法的不足。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [53] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 论文认为传统基于静态基准测试的评估方法已不适用于AI代理系统，需要将评估重新定义为建立信任、迭代和治理的核心测量学科。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从静态模型发展为复合型、使用工具的代理系统，传统评估方法（静态基准、聚合分数、一次性成功标准）已无法有效衡量系统在变化和规模化下的行为可信度，反而可能掩盖系统真实问题。

Method: 论文通过分析评估流程本身引入的隐性故障模式、高基准分数误导团队的原因，以及代理系统如何从根本上改变性能测量的意义，来重新定义评估的角色。

Result: 研究发现传统评估方法在代理系统时代存在严重局限性，评估不应只是性能展示，而应成为建立信任、支持迭代和治理的核心测量学科。

Conclusion: 评估需要从模型中心的静态检查点转变为系统级的持续控制功能，特别是在非确定性系统中，评估应作为信任、迭代和治理的基础，而非简单的性能指标。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [54] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究发现，当用户认为对话AI存在党派偏见时，其纠正错误观念的效果会显著降低28%，表明AI的说服力受政治中立性感知影响


<details>
  <summary>Details</summary>
Motivation: 对话AI被提议作为纠正公众误解和传播错误信息的可扩展方式，但其效果可能取决于对其政治中立性的感知。随着大型语言模型进入党派冲突，精英们越来越多地将其描绘为意识形态对齐的工具，需要测试这些可信度攻击是否会降低基于LLM的说服力

Method: 在美国进行了一项预注册的调查实验（N=2144），参与者与ChatGPT进行了三轮关于个人持有的经济政策误解的对话。实验设置了中性对照组和显示LLM对参与者党派有偏见的警告信息组

Result: 与中性对照组相比，显示LLM对参与者党派有偏见的简短警告信息使说服力降低了28%。文本分析表明，警告改变了互动方式：受访者更多地反驳，参与度更不开放

Conclusion: 对话AI的说服影响具有政治依赖性，受党派对齐感知的限制。当用户认为AI存在党派偏见时，其纠正错误观念的效果会显著减弱

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [55] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 提出对抗性问题生成框架，通过对比目标模型与专家模型的输出，生成紧凑的语义挑战性问题，以少量合成数据提升LLM在专业领域的适应能力。


<details>
  <summary>Details</summary>
Motivation: LLM在专业领域适应能力不足，现有合成数据生成方法（如改写或知识提取）存在两个关键缺陷：1) 对解释性推理能力支持有限；2) 生成的合成语料库通常过大且冗余，样本效率低下。

Method: 提出对抗性问题生成框架，通过迭代反馈驱动过程，比较待适应模型与基于参考文档的专家模型的输出，构建紧凑的语义挑战性问题集，揭示并解决理解差距。

Result: 在LegalBench语料库的专业子集上评估，该方法以显著更少的合成样本实现了更高的准确率。

Conclusion: 对抗性问题生成框架能够有效提升LLM在专业领域的适应效率，通过生成紧凑的挑战性问题集，解决了现有合成数据方法的局限性。

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [56] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 论文提出基于频率分析的注意力机制检测大语言模型幻觉，通过分析注意力分布的高频成分识别不稳定的基础行为，实现轻量级幻觉检测


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的幻觉检测方法通常依赖粗粒度摘要，无法捕捉注意力中的细粒度不稳定性。需要更精细的方法来分析注意力信号以检测幻觉

Method: 将注意力分布建模为离散信号，提取反映注意力快速局部变化的高频成分。基于高频注意力特征开发轻量级幻觉检测器

Result: 在RAGTruth和HalluRAG基准测试中，该方法在模型和任务上均优于基于验证、内部表示和注意力的现有方法

Conclusion: 注意力高频成分能有效反映幻觉相关的碎片化和不稳定基础行为，基于频率分析的注意力特征为幻觉检测提供了有效且轻量的解决方案

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [57] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 论文通过无损压缩分析发现LLM生成文本比人类文本具有更高的结构规律性和可压缩性，但在小规模交互环境中这种差异减弱。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型通过概率采样生成文本时，如何改变语言的结构统计组织，以及如何从表面文本区分生成机制。

Method: 使用无损压缩作为模型无关的统计规律性度量，分析三个渐进复杂的信息生态系统：受控的人类-LLM延续、知识基础设施的生成中介（维基百科 vs Grokipedia）、完全合成的社交互动环境（Moltbook vs Reddit）。

Result: 压缩分析揭示了概率生成的结构特征：在受控和中介环境中，LLM生成语言比人类文本具有更高的结构规律性和可压缩性；但在碎片化交互环境中差异减弱，表明小尺度下表面可区分性存在基本限制。

Conclusion: 无损压缩提供了一个简单稳健的框架来量化生成系统如何重塑文本生产，为通信演化复杂性提供了结构视角。

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [58] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: FENCE是一个用于金融领域多模态越狱检测的双语（韩语-英语）数据集，包含金融相关查询和基于图像的威胁，可用于训练和评估越狱检测器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视觉语言模型（VLMs）的越狱风险对部署构成重大威胁，特别是VLMs因处理文本和图像而攻击面更广。金融领域缺乏越狱检测资源，需要针对性的解决方案。

Method: 创建FENCE双语多模态数据集，包含金融相关查询和图像基础威胁，强调领域真实性。使用商业和开源VLMs进行实验验证漏洞，并训练基线检测器。

Result: 实验显示商业和开源VLMs均存在漏洞，GPT-4o有可测量的攻击成功率，开源模型暴露更大。基于FENCE训练的基线检测器在分布内准确率达到99%，在外部基准测试中保持强性能。

Conclusion: FENCE为金融领域多模态越狱检测提供了专注资源，支持在敏感领域构建更安全可靠的AI系统。数据集展示了在训练可靠检测模型方面的鲁棒性。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [59] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 提出结合Transformer文本嵌入与语言学特征信息的混合方法检测点击诱饵标题，最佳模型F1分数达91%，优于多种基线方法


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题降低在线信息质量并损害用户信任，需要有效的检测方法来应对这一问题

Method: 采用混合方法：结合基于Transformer的文本嵌入与语言学特征（如第二人称代词、最高级、数字、注意力导向标点等15个显式特征），使用XGBoost树基分类器

Result: 最佳模型（XGBoost结合增强特征）F1分数达91%，优于TF-IDF、Word2Vec、GloVe、LLM提示分类和仅特征基线

Conclusion: 提出的特征集通过突出显著语言学线索增强了可解释性，实现了透明且校准良好的点击诱饵预测，并发布了代码和训练模型支持可重复研究

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [60] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 提出Info-Gain Sampler，一种新的MDM解码框架，通过平衡即时不确定性和对未来掩码标记的信息增益，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有MDM采样器采用贪心启发式方法，只关注局部确定性，忽略了当前解码选择对后续步骤的下游影响，未能充分利用MDM的非因果特性来最小化累积不确定性。

Method: 提出Info-Gain Sampler，一个原则性的解码框架，不仅考虑当前位置的不确定性，还评估解码决策如何重塑所有剩余掩码位置的概率/不确定性，平衡即时不确定性和对未来标记的信息增益。

Result: 在推理、编码、创意写作和图像生成等多样化任务上，Info-Gain Sampler始终优于现有MDM采样器：推理任务平均准确率提升3.6%，创意写作任务胜率63.1%，推理任务累积不确定性从78.4降至48.6。

Conclusion: Info-Gain Sampler通过系统性地考虑当前解码决策对未来的影响，充分利用MDM的非因果特性，显著提升了生成质量，为MDM解码提供了更有效的原则性框架。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [61] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本文提出了一种基于信息论的句子理解处理存储成本度量方法，使用预训练神经语言模型估计先前词语对未来上下文的信息量，相比传统离散语法指标具有连续性和理论中立性。


<details>
  <summary>Details</summary>
Motivation: 实时句子理解对工作记忆产生显著负荷，但现有基于符号语法的处理负荷度量方法采用离散、统一的句法预测成本，无法准确反映实际处理过程。需要一种连续、理论中立的度量方法来更好地量化处理存储成本。

Method: 提出基于信息论的处理存储成本度量：在不确定性条件下，先前词语对未来上下文携带的信息量。该度量是连续的、理论中立的，可以通过预训练神经语言模型进行估计。

Result: 在英语中的三项分析验证了该方法的有效性：1）恢复了中心嵌入和关系从句中已知的处理不对称性；2）与语法注释语料库中基于语法的存储成本相关；3）在两个大规模自然数据集中预测阅读时间方差，优于包含传统信息预测因子的基线模型。

Conclusion: 基于信息论的处理存储成本度量方法为句子理解研究提供了连续、理论中立的量化工具，能够有效预测阅读行为，为心理语言学理论提供了新的形式化基础。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [62] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: 提出Confidence-Driven Contrastive Decoding方法，通过检测低置信度token并选择性干预，提升LLM推理可靠性，同时减少输出长度


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法假设均匀分配推理计算能提升正确性，但研究表明推理不确定性高度局部化：少数低置信度token对推理错误和不必要输出扩展贡献最大

Method: 提出Thinking by Subtraction方法，包含置信度驱动的对比解码：检测解码过程中的低置信度token，选择性干预；构建对比参考（将高置信度token替换为最小占位符），在低置信度位置通过减去参考分布来精炼预测

Result: CCD显著提升数学推理基准的准确性，同时大幅减少输出长度，KV缓存开销最小；作为无需训练的方法，通过针对性低置信度干预提升推理可靠性，避免计算冗余

Conclusion: 通过置信度驱动的对比解码，实现了针对性的token级干预，在提升LLM推理可靠性的同时减少了不必要的输出扩展，为测试时推理优化提供了新思路

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [63] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式Web应用，通过集成多种分析技术并利用视觉语言模型自动生成自然语言解释，降低了大型语言模型机制可解释性分析的门槛，使非专家也能理解复杂的模型内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 当前机制可解释性分析工具虽然强大，但由于复杂性过高，主要限于专家使用，存在可访问性差距。需要设计更易用的工具来让更广泛的受众理解语言模型的内部工作机制。

Method: 开发ELIA交互式Web应用，集成归因分析、函数向量分析和电路追踪三种关键技术，并创新性地使用视觉语言模型为这些方法产生的复杂可视化结果自动生成自然语言解释。

Result: 通过混合方法用户研究验证，用户明显偏好交互式可探索界面而非静态可视化。AI生成的解释帮助非专家弥合知识差距：统计分析显示用户先前LLM经验与理解分数无显著相关性，表明系统降低了不同经验水平用户的理解障碍。

Conclusion: AI系统确实可以简化复杂的模型分析，但其真正潜力在于与以用户为中心的设计相结合，优先考虑交互性、具体性和叙事引导，从而降低机制可解释性分析的门槛。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [64] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 创建了首个罗马尼亚语抑郁和焦虑语料库PsihoRo，包含205名受访者的文本数据，填补了罗马尼亚语心理健康NLP资源的空白。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语目前没有开源的心理健康语料库，而现有的心理NLP资源主要集中在英语等语言上。心理健康数据从社交媒体收集存在假设偏差问题，需要更可靠的数据收集方法。

Method: 通过包含6个开放式问题的表格收集数据，同时使用标准化的PHQ-9和GAD-7筛查问卷进行评估。对收集的205份文本进行统计分析、罗马尼亚语LIWC文本分析、情感检测和主题建模。

Result: 成功创建了首个罗马尼亚语抑郁和焦虑语料库PsihoRo，虽然样本量较小（205名受访者），但为分析罗马尼亚人口的心理健康文本迈出了第一步。

Conclusion: PsihoRo填补了罗马尼亚语心理健康NLP资源的空白，为理解罗马尼亚人口的心理健康特征提供了重要资源，展示了该语料库在心理文本分析中的应用潜力。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [65] [Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning](https://arxiv.org/abs/2602.18326)
*Tao Wu,Adam Kapelner*

Main category: cs.CL

TL;DR: 该研究开发了一个深度学习系统，用于自动筛选适合高中生词汇教学的信息化语境示例，通过对比三种建模方法，发现结合人工特征监督的模型能高效生成大量优质教学语境。


<details>
  <summary>Details</summary>
Motivation: 为高中生词汇教学自动筛选高质量语境示例，解决传统方法效率低、成本高的问题，实现大规模优质教学资源的低成本供给。

Method: 比较三种建模方法：1) 基于MPNet统一上下文嵌入的无监督相似性策略；2) 基于指令感知微调Qwen3嵌入和监督非线性回归头的框架；3) 在方法2基础上加入人工设计语境特征。引入新的评估指标"保持能力曲线"来可视化模型性能。

Result: 模型(iii)表现最佳，在仅丢弃70%优质语境的情况下，实现了440:1的优质-劣质语境比例，显著优于其他方法。监督学习结合人工特征能高效生成大量近乎完美的教学语境。

Conclusion: 现代嵌入模型结合神经网络架构，在人类监督指导下，能够低成本地大规模生成近乎完美的词汇教学语境，为语言教育提供了有效的技术解决方案。

Abstract: We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.

</details>


### [66] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个针对印度司法系统的AI框架，用于预测和解释上诉判决，通过结构化决策点分析和IRAC框架解释，在多个数据集上超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 印度法院面临大量案件积压，特别是上诉案件，需要AI技术来帮助预测判决并提高司法效率。

Method: Vichara框架处理英文上诉案件文件，将其分解为决策点（包含法律问题、裁决机构、结果、推理和时间背景），并使用基于IRAC框架的结构化解释格式。

Result: 在PredEx和ILDC_expert数据集上，Vichara超越了现有基准，GPT-4o mini表现最佳（F1：81.5和80.3）。人工评估显示GPT-4o mini的解释在清晰度、关联性和实用性方面最优。

Conclusion: Vichara框架能够准确预测上诉判决并提供可解释的推理，有助于提高印度司法系统的效率和透明度。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [67] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 提出双尺度验证框架，结合点对点和成对人工标注，用于政治立场预测的主观连续知识表示，构建大规模政治立场知识库，证明从主观话语中提取序数结构的可行性。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识表示常需捕捉主观连续属性（如政治立场），这与广泛接受的成对验证黄金标准相冲突，需要解决主观连续知识验证的挑战。

Method: 采用双尺度验证框架，结合点对点和成对人工标注；使用22个语言模型构建包含23,228个论点的大规模政治立场预测知识库，数据来自30场英国政治电视节目辩论。

Result: 点对点评估显示中等程度的人机一致性（α=0.578），反映内在主观性；成对验证显示人机排名对齐更强（最佳模型α=0.86），证明从点对点预测中提取序数结构的可行性。

Conclusion: 提出了主观连续知识的实用验证方法，构建了验证的结构化论证知识库，证明可从主观话语中提取序数结构，推进了传统符号或分类方法不足领域的知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [68] [SPQ: An Ensemble Technique for Large Language Model Compression](https://arxiv.org/abs/2602.18420)
*Jiamin Yao,Eren Gultepe*

Main category: cs.CL

TL;DR: SPQ是一种集成压缩技术，结合SVD、剪枝和量化三种方法，针对LLM的不同低效源进行压缩，在保持性能的同时显著减少内存占用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临内存占用大、推理速度慢的问题，现有单一压缩方法各有局限，需要一种集成多种互补技术的综合压缩方案。

Method: 提出SPQ集成压缩框架：1) 基于激活的剪枝去除MLP层冗余神经元；2) 保留方差的SVD将注意力投影分解为紧凑低秩因子；3) 8位线性量化统一压缩所有线性层。

Result: 在LLaMA-2-7B上实现75%内存减少，困惑度从5.47降至4.91，下游任务精度保持，内存占用6.86GB优于GPTQ的7.16GB，推理速度提升1.9倍。

Conclusion: SPQ通过层感知和互补压缩技术实现了鲁棒的模型压缩，为内存受限环境中的LLM实际部署提供了实用解决方案。

Abstract: This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/

</details>


### [69] [RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering](https://arxiv.org/abs/2602.18425)
*Deniz Qian,Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: RVR是一个多轮检索框架，通过检索-验证-检索的迭代过程最大化答案覆盖率，在多个数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对需要广泛有效答案的查询，现有检索方法难以全面覆盖所有可能的答案，需要一种能够最大化答案覆盖率的检索框架。

Method: 提出检索-验证-检索（RVR）多轮框架：首轮检索器处理原始查询获得候选文档，验证器筛选高质量子集；后续轮次将已验证文档加入查询，发现之前未覆盖的答案。

Result: 在QAMPARI多答案检索数据集上，相对基线获得至少10%相对提升和3%绝对提升；在QUEST和WebQuestionsSP两个域外数据集上，不同基础检索器均表现一致提升。

Conclusion: RVR通过验证器和检索器适应新推理场景的迭代方法，为全面答案召回提供了有前景的解决方案，即使使用现成检索器也有效，微调后效果更佳。

Abstract: Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.

</details>


### [70] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: 论文提出了VIRAASAT数据集和SCoM框架，用于解决LLMs在印度文化推理任务上的不足。VIRAASAT是一个半自动生成的多跳问答数据集，覆盖印度所有邦和中央直辖区；SCoM框架通过模拟知识图谱操作来提升文化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要丰富社会文化知识和本地背景的任务中表现不佳，特别是在印度文化相关任务上。现有的文化基准测试存在三个问题：(1)手工制作，(2)只包含测试事实记忆的单跳问题，(3)扩展成本过高，导致这一缺陷未被充分衡量。

Method: 1. 提出VIRAASAT：半自动多跳方法，基于包含700多个专家策划文化实体的知识图谱（涵盖13个印度文化属性），生成覆盖印度所有28个邦和8个中央直辖区的3200多个多跳问题。2. 提出SCoM框架：适应Chain-of-Manipulation范式，训练模型在内部模拟原子知识图谱操作，可靠地遍历图谱拓扑结构。

Result: 评估当前SOTA LLMs在VIRAASAT上发现关键推理限制：基于CoT微调无法有效处理和综合低概率事实。实验显示，SCoM在监督微调上比标准CoT基线提升高达20%。

Conclusion: VIRAASAT数据集和SCoM框架为构建文化感知推理模型奠定了坚实基础。通过知识图谱操作模拟，模型能够更好地进行文化推理，填补了LLMs在文化特定任务上的能力空白。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving](https://arxiv.org/abs/2602.17677)
*Sutej Kulgod,Sean Ye,Sanchit Tanwar,Christoffer Heckman*

Main category: cs.LG

TL;DR: 该论文发现VLM在驾驶任务MCQA基准测试中会利用文本线索而非视觉信息，提出了一种消除文本捷径的方法，使模型必须依赖视觉基础


<details>
  <summary>Details</summary>
Motivation: 现有的MCQA基准测试存在严重问题：合成生成的MCQA数据包含隐藏的文本线索，导致视觉语言模型可以通过利用语言模式而非视觉上下文来获得高分，这使得评估结果不能准确反映模型的视觉理解能力

Method: 通过将正确答案与语言伪影解耦，并采用课程学习策略，强制模型依赖视觉基础。具体方法包括消除可被利用的文本捷径，使模型必须基于视觉信息进行推理

Result: 提出的方法显著降低了模型的"盲目准确率"（无视觉输入时的表现），从比随机猜测高66.9%降至仅高2.9%，消除了绝大多数可被利用的文本捷径，确保性能准确反映感知理解能力

Conclusion: 合成MCQA基准测试存在严重缺陷，需要更严格的评估方法来确保VLM真正依赖视觉信息。提出的方法有效解决了文本捷径问题，使模型性能评估更加可靠

Abstract: Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.

</details>


### [72] [Joint Parameter and State-Space Bayesian Optimization: Using Process Expertise to Accelerate Manufacturing Optimization](https://arxiv.org/abs/2602.17679)
*Saksham Kiroriwal,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.LG

TL;DR: 提出POGPN-JPSS框架，结合部分可观测高斯过程网络与联合参数状态空间建模，利用专家知识提取高维中间观测的低维特征，显著提升多阶段生物乙醇生产过程优化效率。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在处理高维多阶段制造过程时性能受限，传统方法忽略中间观测和过程结构。现有POGPN方法虽然建模为有向无环图，但难以处理高维状态空间时间序列的中间观测。

Method: 提出POGPN-JPSS框架，将POGPN与联合参数状态空间建模结合，利用过程专家知识从高维状态空间数据中提取低维潜在特征，有效利用中间提取信息进行优化。

Result: 在多阶段生物乙醇生产过程的挑战性高维仿真中，POGPN-JPSS显著优于现有方法，达到期望性能阈值的速度提高两倍，可靠性更高，大幅节省时间和资源。

Conclusion: 将专家知识与结构化概率模型结合对于快速过程成熟至关重要，POGPN-JPSS框架为高维多阶段制造过程优化提供了有效解决方案。

Abstract: Bayesian optimization (BO) is a powerful method for optimizing black-box manufacturing processes, but its performance is often limited when dealing with high-dimensional multi-stage systems, where we can observe intermediate outputs. Standard BO models the process as a black box and ignores the intermediate observations and the underlying process structure. Partially Observable Gaussian Process Networks (POGPN) model the process as a Directed Acyclic Graph (DAG). However, using intermediate observations is challenging when the observations are high-dimensional state-space time series. Process-expert knowledge can be used to extract low-dimensional latent features from the high-dimensional state-space data. We propose POGPN-JPSS, a framework that combines POGPN with Joint Parameter and State-Space (JPSS) modeling to use intermediate extracted information. We demonstrate the effectiveness of POGPN-JPSS on a challenging, high-dimensional simulation of a multi-stage bioethanol production process. Our results show that POGPN-JPSS significantly outperforms state-of-the-art methods by achieving the desired performance threshold twice as fast and with greater reliability. The fast optimization directly translates to substantial savings in time and resources. This highlights the importance of combining expert knowledge with structured probabilistic models for rapid process maturation.

</details>


### [73] [BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs](https://arxiv.org/abs/2602.17680)
*Yujia Wang,Jihong Guan,Wengen Li,Shuigeng Zhou,Xuhong Wang*

Main category: cs.LG

TL;DR: BioBridge是一个蛋白质理解领域自适应持续预训练框架，通过结合蛋白质语言模型和通用大语言模型的优势，实现蛋白质序列理解和通用推理能力


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型在多任务适应性和跨生物环境泛化能力有限，而通用大语言模型缺乏蛋白质序列解释能力和领域专业知识，无法进行有效的生物语义推理

Method: 采用领域增量持续预训练框架，通过PLM-Projector-LLM管道实现跨模态对齐，将蛋白质序列嵌入映射到语言模型的语义空间，并进行端到端优化

Result: 在EC和BindingDB等蛋白质基准测试中性能与主流PLMs相当，在MMLU和RACE等通用理解任务中与LLMs表现相当

Conclusion: BioBridge成功结合了领域特定适应性和通用语言能力，展示了将蛋白质领域知识与通用推理语料同时注入LLM的创新优势

Abstract: Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.

</details>


### [74] [LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs](https://arxiv.org/abs/2602.17681)
*Ofir Gordon,Lior Dikstein,Arnon Netzer,Idan Achituve,Hai Victor Habi*

Main category: cs.LG

TL;DR: 本文提出LATMiX方法，通过可学习的可逆仿射变换优化MX低比特量化，在多种模型大小和零样本基准测试中取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有激活变换方法主要局限于旋转或Hadamard变换，且多数研究关注传统量化方案，而现代硬件越来越多支持微缩放(MX)数据格式。尝试结合两者会导致严重性能下降，先前工作不得不引入变换假设。

Method: 首先对MX量化下的变换进行理论分析，推导量化误差边界，强调同时考虑激活分布和底层量化结构的重要性。基于此提出LATMiX方法，将异常值减少推广到可学习的可逆仿射变换，使用标准深度学习工具进行优化。

Result: 实验显示，在广泛的零样本基准测试中，LATMiX在MX低比特量化上相比强基线取得一致的平均准确率提升，且适用于多种模型大小。

Conclusion: LATMiX通过理论指导的可学习变换，有效解决了MX量化中的性能下降问题，为后训练量化提供了更通用的异常值减少方法。

Abstract: Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.

</details>


### [75] [Duality Models: An Embarrassingly Simple One-step Generation Paradigm](https://arxiv.org/abs/2602.17682)
*Peng Sun,Xinyi Shang,Tao Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 提出Duality Models (DuMo)，通过"一个输入，双输出"范式同时预测速度场和流映射，解决了传统一致性模型在训练目标分配上的权衡问题，在ImageNet 256×256上仅用2步达到SOTA FID 1.79。


<details>
  <summary>Details</summary>
Motivation: 传统基于一致性的生成模型（如Shortcut和MeanFlow）采用"一个输入，一个输出"范式，需要在多步目标（稳定性）和少步目标（生成质量）之间分配训练预算，导致训练效率低下和收敛问题。

Method: 提出Duality Models (DuMo)，采用共享主干网络和双头架构，从单一输入x_t同时预测速度场v_t和流映射u_t，将多步目标的几何约束应用于每个样本，无需分离训练目标。

Result: 在ImageNet 256×256数据集上，使用679M参数的Diffusion Transformer和SD-VAE，仅用2步推理就达到了1.79的FID，创下了新的SOTA记录。

Conclusion: DuMo通过"一个输入，双输出"范式有效解决了传统一致性模型训练目标分配的权衡问题，显著提高了稳定性和效率，实现了高质量少步生成。

Abstract: Consistency-based generative models like Shortcut and MeanFlow achieve impressive results via a target-aware design for solving the Probability Flow ODE (PF-ODE). Typically, such methods introduce a target time $r$ alongside the current time $t$ to modulate outputs between a local multi-step derivative ($r = t$) and a global few-step integral ($r = 0$). However, the conventional "one input, one output" paradigm enforces a partition of the training budget, often allocating a significant portion (e.g., 75% in MeanFlow) solely to the multi-step objective for stability. This separation forces a trade-off: allocating sufficient samples to the multi-step objective leaves the few-step generation undertrained, which harms convergence and limits scalability. To this end, we propose Duality Models (DuMo) via a "one input, dual output" paradigm. Using a shared backbone with dual heads, DuMo simultaneously predicts velocity $v_t$ and flow-map $u_t$ from a single input $x_t$. This applies geometric constraints from the multi-step objective to every sample, bounding the few-step estimation without separating training objectives, thereby significantly improving stability and efficiency. On ImageNet 256 $\times$ 256, a 679M Diffusion Transformer with SD-VAE achieves a state-of-the-art (SOTA) FID of 1.79 in just 2 steps. Code is available at: https://github.com/LINs-lab/DuMo

</details>


### [76] [Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates](https://arxiv.org/abs/2602.17683)
*Irene Iele,Giulia Romoli,Daniele Molino,Elena Mulero Ayllón,Filippo Ruffini,Paolo Soda,Matteo Tortora*

Main category: cs.LG

TL;DR: 提出基于Transformer的概率预测框架，用于卫星NDVI植被指数预测，通过分离历史植被动态与未来外生信息建模，结合气象特征工程，在云层遮挡的不规则采样条件下实现准确预测。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要准确的植被动态短期预测，但卫星NDVI预测面临挑战：云层遮挡导致采样稀疏不规则，作物生长的异质性气候条件复杂。

Method: 提出概率预测框架，采用Transformer架构分离历史植被动态与未来外生信息建模；引入时间距离加权分位数损失处理不规则重访模式；结合累积和极端天气特征工程捕捉延迟气象效应。

Result: 在欧洲卫星数据上的实验表明，该方法在点预测和概率评估指标上均优于统计、深度学习和近期时间序列基线；消融研究显示目标历史信息起核心作用，气象协变量提供互补增益。

Conclusion: 提出的框架能有效处理卫星NDVI预测中的不规则采样和异质性气候条件，为精准农业提供可靠的数据驱动决策支持，代码已开源。

Abstract: Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.

</details>


### [77] [CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models](https://arxiv.org/abs/2602.17684)
*Xiao Zhu,Xinyu Zhou,Boyu Zhu,Hanxu Hu,Mingzhe Du,Haotian Zhang,Huiming Wang,Zhijiang Guo*

Main category: cs.LG

TL;DR: CodeScaler是一个无需执行的奖励模型，用于代码生成的强化学习和推理阶段，通过语法感知的代码提取和保持有效性的奖励塑造，在多个代码基准上显著提升性能，同时大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）依赖单元测试的执行反馈，但其可扩展性受到高质量测试用例可用性和可靠性的限制。需要一种无需执行的奖励模型来扩展代码生成的强化学习训练和推理。

Method: CodeScaler是一个无需执行的奖励模型，基于从已验证代码问题中精心策划的偏好数据进行训练。采用语法感知的代码提取和保持有效性的奖励塑造技术，确保稳定和鲁棒的优化。

Result: 在五个代码基准上，CodeScaler将Qwen3-8B-Base平均提升11.72分，优于基于执行的强化学习1.82分。在推理时，延迟降低10倍，性能与单元测试方法相当。在RM-Bench上，不仅在代码领域（+3.3分），在通用和推理领域（平均+2.7分）也超越现有奖励模型。

Conclusion: CodeScaler提供了一种无需执行的奖励建模方法，能够扩展代码生成的强化学习训练和推理，解决了传统基于测试的强化学习的可扩展性限制，同时在性能和效率上都有显著提升。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).

</details>


### [78] [Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling](https://arxiv.org/abs/2602.17685)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.LG

TL;DR: 论文提出统一共椭圆机动框架，结合霍曼转移、安全椭圆接近操作和显式加油逻辑，用于LEO多目标主动碎片清除任务规划，并比较贪婪启发式、MCTS和Masked PPO三种算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道多目标主动碎片清除（ADR）的挑战，需要高效、可扩展且资源优化的任务规划方法，以应对随机碎片场、禁飞区和燃料约束等现实约束条件。

Method: 提出统一共椭圆机动框架，整合霍曼转移、安全椭圆接近操作和显式加油逻辑。在具有随机碎片场、禁飞区和ΔV约束的真实轨道仿真环境中，对贪婪启发式、蒙特卡洛树搜索（MCTS）和基于Masked PPO的深度强化学习三种规划算法进行基准测试。

Result: 在100个测试场景中，Masked PPO在任务效率和计算性能方面表现最优，访问的碎片数量是贪婪算法的两倍，并在运行时间上显著优于MCTS。

Conclusion: 现代强化学习方法在可扩展、安全和资源高效的空间任务规划中具有巨大潜力，为ADR自主性的未来发展铺平了道路。

Abstract: This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.

</details>


### [79] [Curriculum Learning for Efficient Chain-of-Thought Distillation via Structure-Aware Masking and GRPO](https://arxiv.org/abs/2602.17686)
*Bowen Yu,Maolin Wang,Sheng Zhang,Binhao Wang,Yi Wen,Jingtong Gao,Bowen Liu,Zimo Zhao,Wanyu Wang,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 提出三阶段课程学习框架，通过渐进式技能获取解决大模型与小模型在思维链推理蒸馏中的能力不匹配问题，使小模型在保持准确性的同时显著缩短输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有思维链蒸馏方法存在核心问题：大模型生成的推理过程通常过于冗长，小模型难以忠实复现。现有方法要么压缩为单步推理失去可解释性，要么无法平衡准确性和简洁性。

Method: 三阶段课程学习框架：1) 通过掩码乱序重建建立结构理解；2) 在掩码补全任务上应用组相对策略优化，让模型自主发现准确性与简洁性的平衡；3) 识别持续失败案例，通过针对性重写引导学生模型内化教师知识，同样使用GRPO优化。

Result: 在GSM8K数据集上，该方法使Qwen2.5-3B-Base模型准确率提升11.29%，同时输出长度减少27.4%，超越了指令调优变体和先前蒸馏方法。

Conclusion: 该框架有效解决了思维链蒸馏中的能力不匹配问题，使小模型能够生成既准确又简洁的推理过程，在保持可解释性的同时显著提升性能。

Abstract: Distilling Chain-of-Thought (CoT) reasoning from large language models into compact student models presents a fundamental challenge: teacher rationales are often too verbose for smaller models to faithfully reproduce. Existing approaches either compress reasoning into single-step, losing the interpretability that makes CoT valuable. We present a three-stage curriculum learning framework that addresses this capacity mismatch through progressive skill acquisition. First, we establish structural understanding via masked shuffled reconstruction. Second, we apply Group Relative Policy Optimization (GRPO) on masked completion tasks, enabling the model to discover its own balance between accuracy and brevity. Third, we identify persistent failure cases and guide the student to internalize teacher knowledge through targeted rewriting, again optimized with GRPO. Experiments on GSM8K demonstrate that our approach enables Qwen2.5-3B-Base to achieve an 11.29 percent accuracy improvement while reducing output length by 27.4 percent, surpassing both instruction-tuned variants and prior distillation methods.

</details>


### [80] [Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver](https://arxiv.org/abs/2602.18248)
*Pietro Sittoni,Emanuele Zangrando,Angelo A. Casulli,Nicola Guglielmi,Francesco Tudisco*

Main category: cs.LG

TL;DR: Neural-HSS：基于分层半可分矩阵结构的参数高效神经网络架构，专为椭圆型PDE设计，在低数据量下具有理论保证的数据效率


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在求解PDE方面表现出色，但许多关键应用仍受限于大规模高质量数据集生成和模型训练的高计算成本。需要开发在低数据量下仍能高效学习的架构

Method: 受椭圆型PDE格林函数结构启发，提出Neural-HSS架构，基于分层半可分矩阵结构，具有参数效率优势。理论分析证明其在低数据量下的精确性，并探讨其与傅里叶神经算子层、卷积层等架构原语的联系

Result: 在200万网格点的三维泊松方程上验证了数据效率，在低数据量下优于基线方法。展示了在电磁学、流体动力学、生物学等多个领域PDE数据上的学习能力

Conclusion: Neural-HSS为椭圆型PDE提供了一种参数高效、数据高效的深度学习架构，在低数据量下具有理论保证的精确性，适用于多种物理领域

Abstract: Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.

</details>


### [81] [AnCoder: Anchored Code Generation via Discrete Diffusion Models](https://arxiv.org/abs/2602.17688)
*Anton Xue,Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: AnchorTree框架通过抽象语法树锚定扩散过程，优先解析语法和语义关键token，提升代码生成质量


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在代码生成中不尊重编程语言的刚性结构，常产生无法执行的破碎程序

Method: 引入AnchorTree框架，利用抽象语法树作为结构化先验，优先解析关键词和标识符等关键token，建立结构脚手架指导后续生成

Result: 基于AnchorTree的AnCoder模型家族验证了结构锚定扩散在参数效率下实现高质量代码生成的有效性

Conclusion: 结构化锚定扩散为代码生成提供了参数高效的路径，能够更好地尊重编程语言的结构约束

Abstract: Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.

</details>


### [82] [Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction](https://arxiv.org/abs/2602.17689)
*Melika Filvantorkaman,Mohsen Piri*

Main category: cs.LG

TL;DR: 提出Robust-MMR自监督预训练框架，通过非对称扰动感知掩码、领域一致性正则化和模态弹性约束，提升医学视觉语言模型在领域偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学视觉语言模型在成像设备、采集协议和报告风格等变化导致的领域偏移下性能下降，现有多模态预训练方法忽视鲁棒性，将其视为下游适应问题。

Method: 提出Robust Multi-Modal Masked Reconstruction (Robust-MMR)自监督预训练框架，整合非对称扰动感知掩码、领域一致性正则化和模态弹性约束，鼓励领域不变表示。

Result: 在VQA-RAD上达到78.9%跨域准确率，比最强基线提升3.8个百分点；SLAKE和VQA-2019分别达到74.6%和77.0%；扰动评估下VQA-RAD准确率从69.1%提升至75.6%；MELINDA跨域准确率从70.3%提升至75.2%；检索任务中平均排名退化从超过16降至4.1。

Conclusion: 在预训练阶段显式建模鲁棒性能够获得更可靠、可迁移的医学视觉语言表示，适用于真实世界部署。

Abstract: Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.

</details>


### [83] [Tethered Reasoning: Decoupling Entropy from Hallucination in Quantized LLMs via Manifold Steering](https://arxiv.org/abs/2602.17691)
*Craig Atkinson*

Main category: cs.LG

TL;DR: HELIX框架通过几何约束解决量化语言模型在高温度采样时产生的轨迹发散问题，保持语义一致性同时提升输出多样性。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型面临两难困境：低温采样导致重复输出，高温采样（T>2.0）则引起轨迹发散和语义不连贯。需要一种方法既能保持输出多样性又能避免语义崩溃。

Method: HELIX几何框架将隐藏状态轨迹约束到预计算的真实性流形上，计算统一真实性分数（UTS）结合语义熵和马氏距离。当UTS指示轨迹发散时，使用渐进式引导向量将激活重定向到结构一致区域，仅影响0.2-2.5%的token。

Result: 在4位量化Granite 4.0 H Small模型上：GSM8K在T=3.0时保持88.84%准确率（仅比T=0.5下降2.81pp）；MMLU在14,042个问题上保持72.49%（下降1.24pp）。高温下引导输出仅5-20%想法重复，而保守设置下为70-80%。跨架构验证显示概念生成增加46.7%。

Conclusion: HELIX作为语法约束，使模型能够探索语义多样性而不违反逻辑骨架，揭示先前被掩盖的高熵创造性储备。该方法支持多温度合成，生成比单温度推理多200%的独特概念。

Abstract: Quantized language models face a fundamental dilemma: low sampling temperatures yield repetitive, mode-collapsed outputs, while high temperatures (T > 2.0) cause trajectory divergence and semantic incoherence. We present HELIX, a geometric framework that decouples output entropy from hallucination by tethering hidden-state trajectories to a pre-computed truthfulness manifold. HELIX computes a Unified Truth Score (UTS) combining token-level semantic entropy with Mahalanobis distance from the manifold. When UTS indicates trajectory divergence, graduated steering vectors redirect activations toward structurally coherent regions while affecting only 0.2-2.5% of tokens.
  On 4-bit quantized Granite 4.0 H Small (32B/9B active, hybrid Mamba-Transformer): GSM8K maintains 88.84% accuracy at T = 3.0 (2.81pp degradation from T = 0.5); MMLU maintains 72.49% across 14,042 questions (1.24pp degradation). This demonstrates that high-temperature hallucination is primarily trajectory divergence rather than semantic collapse. Notably, steering the sparse Transformer attention layers (~10% of layers) is sufficient to correct drift in the Mamba-2 state-space formulation.
  Geometric tethering reveals a previously-masked High-Entropy Creative Reservoir. At T > 2.0, steered outputs exhibit 5-20% idea duplication versus 70-80% at conservative settings. Cross-architecture validation (Qwen3-30B-A3B MOE) confirms this phenomenon is architecture-independent, with 46.7% higher unique concept generation. HELIX acts as a syntax tether, enabling exploration of semantic diversity without violating the logical backbone required for valid output. This enables Multi-Temperature Synthesis, generating 200% more unique concepts than single-temperature inference.

</details>


### [84] [Agentic Unlearning: When LLM Agent Meets Machine Unlearning](https://arxiv.org/abs/2602.17692)
*Bin Wang,Fan Wang,Pingping Wang,Jinyu Cong,Yang Yu,Yilong Yin,Zhongyi Han,Benzheng Wei*

Main category: cs.LG

TL;DR: 提出agentic unlearning框架SBU，通过同步双更新协议同时从模型参数和持久记忆中删除指定信息，解决参数-记忆回流问题


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法仅针对模型参数，存在两个关键问题：(1) 参数-记忆回流问题 - 检索会重新激活参数残留或记忆伪影重新引入敏感内容；(2) 缺乏覆盖参数和记忆路径的统一策略

Method: 提出同步回流遗忘(SBU)框架：记忆路径采用基于依赖闭包的遗忘方法，修剪孤立实体并逻辑上使共享伪影无效；参数路径采用随机参考对齐，将模型输出导向高熵先验；通过同步双更新协议集成两条路径，形成闭环机制

Result: 在医疗QA基准测试中，SBU显著减少了目标隐私信息在两条路径上的痕迹，同时对保留数据的性能退化有限

Conclusion: SBU通过联合参数和记忆路径的遗忘，解决了agentic系统中信息删除的关键挑战，防止跨路径再污染

Abstract: In this paper, we introduce \textbf{agentic unlearning} which removes specified information from both model parameters and persistent memory in agents with closed-loop interaction. Existing unlearning methods target parameters alone, leaving two critical gaps: (i) parameter-memory backflow, where retrieval reactivates parametric remnants or memory artifacts reintroduce sensitive content, and (ii) the absence of a unified strategy that covers both parameter and memory pathways. We present Synchronized Backflow Unlearning (SBU), a framework that unlearns jointly across parameter and memory pathways. The memory pathway performs dependency closure-based unlearning that prunes isolated entities while logically invalidating shared artifacts. The parameter pathway employs stochastic reference alignment to guide model outputs toward a high-entropy prior. These pathways are integrated via a synchronized dual-update protocol, forming a closed-loop mechanism where memory unlearning and parametric suppression reinforce each other to prevent cross-pathway recontamination. Experiments on medical QA benchmarks show that SBU reduces traces of targeted private information across both pathways with limited degradation on retained data.

</details>


### [85] [A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU](https://arxiv.org/abs/2602.17693)
*Yuchen Luo,Fangyue Zhu,Ruining Zhou,Mingzhe Huang,Jian Zhu,Fanyu Fan,Wei Shao*

Main category: cs.LG

TL;DR: 本文研究了在昇腾NPU上对推理导向模型进行后训练量化的可行性与限制，发现4位权重量化对大型模型可行，但4位权重-激活量化在长上下文推理中不稳定，而8位量化数值稳定。


<details>
  <summary>Details</summary>
Motivation: 后训练量化对高效模型部署至关重要，但现有研究主要关注GPU架构，在昇腾NPU上的有效性尚未充分探索。本文旨在填补这一空白，为在昇腾NPU上部署量化推理模型提供实践参考。

Method: 对DeepSeek-R1-Distill-Qwen系列(1.5B/7B/14B)和QwQ-32B等推理导向模型进行案例研究，评估四种代表性量化算法：AWQ、GPTQ、SmoothQuant和FlatQuant，涵盖从仅权重压缩到基于旋转的高级方法。

Result: 实验结果显示显著的平台敏感性：4位仅权重量化对大型模型可行，但4位权重-激活量化在NPU上存在逐层校准不稳定性，导致长上下文推理任务中的逻辑崩溃；标准8位量化保持数值稳定。INT8实际部署表明，尽管优化内核减少了延迟，但动态量化开销目前限制了端到端加速。

Conclusion: 研究为在昇腾NPU上部署量化推理模型提供了可行性参考和限制分析，指出4位权重-激活量化在NPU上的不稳定性问题，建议在实际部署中考虑8位量化的稳定性优势。

Abstract: Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.

</details>


### [86] [AsynDBT: Asynchronous Distributed Bilevel Tuning for efficient In-Context Learning with Large Language Models](https://arxiv.org/abs/2602.17694)
*Hui Ma,Shaoyu Dou,Ya Liu,Fei Xing,Li Feng,Feng Pi*

Main category: cs.LG

TL;DR: 提出异步分布式双层调优算法AsynDBT，通过优化上下文学习样本和提示片段来提升LLM下游任务性能，同时解决联邦学习中的异构数据和掉队者问题。


<details>
  <summary>Details</summary>
Motivation: 云端LLM API使用成本低但参数不可知，用户需要手动调整提示；上下文学习(ICL)需要高质量数据但数据敏感难以共享；现有联邦学习结合ICL的方法面临严重的掉队者问题和异构数据挑战。

Method: 提出异步分布式双层调优算法AsynDBT，通过分布式架构优化上下文学习样本和提示片段，基于LLM反馈进行调优，提供隐私保护并适应异构计算环境。

Result: 在多个基准数据集上的实验证明了AsynDBT的有效性和效率，同时提供了理论收敛保证。

Conclusion: AsynDBT算法成功解决了联邦学习中结合上下文学习时的掉队者问题和异构数据挑战，在保护隐私的同时提升了LLM下游任务性能。

Abstract: With the rapid development of large language models (LLMs), an increasing number of applications leverage cloud-based LLM APIs to reduce usage costs. However, since cloud-based models' parameters and gradients are agnostic, users have to manually or use heuristic algorithms to adjust prompts for intervening LLM outputs, which requiring costly optimization procedures. In-context learning (ICL) has recently emerged as a promising paradigm that enables LLMs to adapt to new tasks using examples provided within the input, eliminating the need for parameter updates. Nevertheless, the advancement of ICL is often hindered by the lack of high-quality data, which is often sensitive and different to share. Federated learning (FL) offers a potential solution by enabling collaborative training of distributed LLMs while preserving data privacy. Despite this issues, previous FL approaches that incorporate ICL have struggled with severe straggler problems and challenges associated with heterogeneous non-identically data. To address these problems, we propose an asynchronous distributed bilevel tuning (AsynDBT) algorithm that optimizes both in-context learning samples and prompt fragments based on the feedback from the LLM, thereby enhancing downstream task performance. Benefiting from its distributed architecture, AsynDBT provides privacy protection and adaptability to heterogeneous computing environments. Furthermore, we present a theoretical analysis establishing the convergence guarantees of the proposed algorithm. Extensive experiments conducted on multiple benchmark datasets demonstrate the effectiveness and efficiency of AsynDBT.

</details>


### [87] [EXACT: Explicit Attribute-Guided Decoding-Time Personalization](https://arxiv.org/abs/2602.17695)
*Xin Yu,Hanwen Xing,Lingzhou Xue*

Main category: cs.LG

TL;DR: EXACT：基于可解释属性的解码时个性化方法，通过离线学习用户偏好属性和在线检索相关属性来引导生成，解决上下文偏好漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有解码时个性化方法依赖隐式、不可解释的偏好表示，且使用僵化的上下文无关用户表示，无法处理不同提示下偏好的变化

Method: EXACT使用预定义的可解释属性集，离线阶段通过最大化偏好响应似然识别用户特定属性子集，在线阶段检索与提示最相关的属性注入上下文引导生成

Result: 在人工标注偏好数据集上的实验表明，EXACT在偏好建模准确性和个性化生成质量方面持续优于强基线方法

Conclusion: EXACT提供了一种可扩展、可解释的解码时个性化方法，能有效处理上下文偏好漂移，适应不同任务而无需合并冲突偏好

Abstract: Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.

</details>


### [88] [Can LLM Safety Be Ensured by Constraining Parameter Regions?](https://arxiv.org/abs/2602.17696)
*Zongmin Li,Jian Su,Farah Benamara,Aixin Sun*

Main category: cs.LG

TL;DR: 当前的安全区域识别方法在不同数据集和模型上表现不稳定，无法可靠识别出稳定、与数据集无关的安全区域


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常被认为包含"安全区域"——参数子集，其修改直接影响安全行为。研究旨在系统评估现有安全区域识别方法的可靠性和稳定性

Method: 系统评估了四种安全区域识别方法，涵盖从单个权重到整个Transformer层的不同参数粒度，在四个不同规模的LLM家族上进行测试，使用十个安全识别数据集

Result: 识别出的安全区域仅表现出低到中等的重叠度（IoU测量），当使用效用数据集（非有害查询）进一步细化时，重叠度显著下降

Conclusion: 当前技术无法可靠地识别出稳定、与数据集无关的安全区域，表明现有方法存在局限性

Abstract: Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.

</details>


### [89] [Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters](https://arxiv.org/abs/2602.17697)
*Nada Zine,Clément Quinton,Romain Rouvoy*

Main category: cs.LG

TL;DR: 该论文提出将LLM视为可配置系统，应用变异性管理技术系统分析推理配置选择，以解决LLM推理能耗优化问题。


<details>
  <summary>Details</summary>
Motivation: LLM推理计算需求巨大，能耗问题突出，但配置空间庞大导致经验评估不可行，需要系统化的配置分析方法。

Method: 将LLM视为可配置系统，使用基于特征的变异性模型表示生成超参数及其约束，采样代表性配置，测量能耗、延迟、准确性，并学习预测模型。

Result: 变异性建模有效管理LLM推理配置复杂性，系统分析超参数效应和交互，揭示权衡关系，并能从有限测量中准确预测推理行为。

Conclusion: 这项工作开辟了软件工程与机器学习交叉的新研究方向，通过变异性建模实现LLM的高效可持续配置。

Abstract: Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.

</details>


### [90] [ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs](https://arxiv.org/abs/2602.17698)
*Xinlin Li,Timothy Chou,Josh Fromm,Zichang Liu,Yunjie Pan,Christina Fragouli*

Main category: cs.LG

TL;DR: ScaleBITS：一个混合精度量化框架，通过硬件对齐的块级权重分区和双向通道重排序，在内存预算下实现自动化细粒度比特分配，在超低比特量化中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 后训练权重量化对减少大语言模型的内存和推理成本至关重要，但将平均精度推到4比特以下仍然具有挑战性，主要因为权重敏感度高度不均匀且缺乏原则性的精度分配方法。现有解决方案要么使用不规则细粒度混合精度导致高运行时开销，要么依赖启发式或高度受限的精度分配策略。

Method: 提出ScaleBITS混合精度量化框架：1）基于新的敏感度分析指导；2）引入硬件对齐的块级权重分区方案，通过双向通道重排序实现；3）将全局比特分配建模为约束优化问题，开发可扩展的贪婪算法近似方法，实现端到端原则性分配。

Result: 实验表明，ScaleBITS在超低比特量化中显著优于均匀精度量化（提升高达36%），并优于最先进的敏感度感知基线方法（提升高达13%），且不增加运行时开销。

Conclusion: ScaleBITS框架成功解决了超低比特量化的挑战，通过硬件高效的混合精度分配实现了更好的性能，为大语言模型的高效部署提供了有效的解决方案。

Abstract: Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.

</details>


### [91] [MIDAS: Mosaic Input-Specific Differentiable Architecture Search](https://arxiv.org/abs/2602.17700)
*Konstanty Subbotko*

Main category: cs.LG

TL;DR: MIDAS提出了一种新颖的可微分神经架构搜索方法，通过自注意力机制计算动态、输入特定的架构参数，并引入局部化架构选择和拓扑感知搜索空间，在多个基准上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 尽管可微分神经架构搜索（NAS）提供了高效的梯度优化方法，但在实际应用中采用仍然有限。现有方法如DARTS使用静态架构参数，缺乏对不同输入的适应性。

Method: MIDAS通过自注意力机制计算动态、输入特定的架构参数，取代DARTS中的静态参数。采用局部化架构选择，为激活图的每个空间补丁单独计算架构参数。引入参数免费、拓扑感知的搜索空间，建模节点连接性并简化每个节点的两条入边选择。

Result: 在DARTS搜索空间上，CIFAR-10达到97.42% top-1准确率，CIFAR-100达到83.38%。在NAS-Bench-201上始终能找到全局最优架构。在RDARTS上，在CIFAR-10的四个搜索空间中的两个上创造了最先进水平。

Conclusion: MIDAS通过动态、输入特定的架构参数和局部化选择机制，显著提升了可微分NAS的性能和鲁棒性。补丁级注意力提高了候选操作间的区分度，产生的参数分布具有类别感知性和单峰性，为解码提供了可靠指导。

Abstract: Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.

</details>


### [92] [Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure](https://arxiv.org/abs/2602.17699)
*Chandrasekhar Gokavarapu,Sudhakar Gadde,Y. Rajasekhar,S. R. Bhargava*

Main category: cs.LG

TL;DR: 提出一个统一框架，用于在分布偏移下对预测器风险进行可验证的认证，通过可计算的偏移度量提供显式上界。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，机器学习模型经常面临训练分布和测试分布不匹配的问题（分布偏移）。现有方法缺乏对分布偏移下风险的可验证保证，需要建立理论框架来提供显式风险边界和可验证的认证。

Method: 开发统一框架，在可验证的正则性和复杂性约束下，通过可计算的偏移度量（如分布距离）和模型参数，推导出分布偏移下超额风险的显式上界。框架强调可验证性、非平凡规模下的可靠性，以及通过可识别性条件而非事后解释来保证可解释性。

Result: 建立了分布偏移下风险认证的显式不等式，证明了学习模型在非平凡规模下的可验证性，提供了可解释性的形式化保证，并明确指出了失效模式和非可认证机制的特征。

Conclusion: 该框架为分布偏移下的机器学习提供了理论基础，通过可计算的度量实现了风险的可验证认证，为实际应用中的模型可靠性和可解释性提供了理论保证。

Abstract: Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.

</details>


### [93] [Investigating Target Class Influence on Neural Network Compressibility for Energy-Autonomous Avian Monitoring](https://arxiv.org/abs/2602.17751)
*Nina Brolich,Simon Geis,Maximilian Kasper,Alexander Barnhill,Axel Plinge,Dominik Seuß*

Main category: cs.LG

TL;DR: 该论文提出在微控制器单元(MCU)上运行机器学习模型进行鸟类监测，通过模型压缩技术实现高效边缘计算，显著降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类监测方法成本高、效率低，现有机器学习解决方案需要复杂模型和大量计算资源。作者希望开发能在现场直接运行的节能监测系统，利用MCU实现边缘计算。

Method: 训练并压缩针对不同数量目标类别的模型，评估在边缘设备上检测多种鸟类的能力，研究物种数量对神经网络可压缩性的影响。

Result: 实现了显著的压缩率且性能损失最小，提供了不同硬件平台的基准测试结果，并评估了部署能量自主设备的可行性。

Conclusion: 在MCU上运行压缩模型进行鸟类监测是可行的，能够实现节能的边缘计算，为野生动物监测提供了高效、低成本的解决方案。

Abstract: Biodiversity loss poses a significant threat to humanity, making wildlife monitoring essential for assessing ecosystem health. Avian species are ideal subjects for this due to their popularity and the ease of identifying them through their distinctive songs. Traditionalavian monitoring methods require manual counting and are therefore costly and inefficient. In passive acoustic monitoring, soundscapes are recorded over long periods of time. The recordings are analyzed to identify bird species afterwards. Machine learning methods have greatly expedited this process in a wide range of species and environments, however, existing solutions require complex models and substantial computational resources. Instead, we propose running machine learning models on inexpensive microcontroller units (MCUs) directly in the field. Due to the resulting hardware and energy constraints, efficient artificial intelligence (AI) architecture is required. In this paper, we present our method for avian monitoring on MCUs. We trained and compressed models for various numbers of target classes to assess the detection of multiple bird species on edge devices and evaluate the influence of the number of species on the compressibility of neural networks. Our results demonstrate significant compression rates with minimal performance loss. We also provide benchmarking results for different hardware platforms and evaluate the feasibility of deploying energy-autonomous devices.

</details>


### [94] [Parallel Complex Diffusion for Scalable Time Series Generation](https://arxiv.org/abs/2602.17706)
*Rongyao Cai,Yuxi Wan,Kexin Zhang,Ming Jin,Zhiqiang Ge,Qingsong Wen,Yong Liu*

Main category: cs.LG

TL;DR: PaCoDi是一个谱域原生架构，通过傅里叶变换将时间信号转换为解耦的频谱分量，实现了长程依赖建模，同时减少50%注意力计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统时间扩散模型存在局部纠缠问题和注意力机制的O(L²)计算成本，难以高效建模时间序列中的长程依赖关系。

Method: 引入并行复扩散(PaCoDi)架构，在频域进行生成建模；利用傅里叶变换作为对角化算子；提出正交前向扩散和条件反向分解定理；使用平均场理论近似和交互校正机制；利用实值信号的厄米对称性压缩序列长度；推导异方差损失处理压缩流形上的非各向同性噪声分布。

Result: PaCoDi在生成质量和推理速度上均优于现有基线方法，实现了理论严谨且计算高效的时间序列建模解决方案。

Conclusion: PaCoDi通过频域解耦方法，从根本上改变了问题拓扑结构，在保持理论严谨性的同时，显著提升了时间序列生成的计算效率和建模能力。

Abstract: Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.

</details>


### [95] [Financial time series augmentation using transformer based GAN architecture](https://arxiv.org/abs/2602.17865)
*Andrzej Podobiński,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 使用基于Transformer的GAN生成合成数据增强金融时间序列，显著提升LSTM预测模型的准确性


<details>
  <summary>Details</summary>
Motivation: 金融时间序列数据稀缺且动态变化，导致深度学习模型训练不充分、泛化能力差，需要可靠的数据增强方法来提升预测准确性

Method: 提出使用基于Transformer的GAN（TTS-GAN）生成合成数据来增强真实金融时间序列，然后训练LSTM预测模型；同时提出结合动态时间规整（DTW）和改进的深度数据集相似性度量（DeD-iMs）的时间序列质量评估指标

Result: 在比特币和标普500价格数据上，使用GAN增强数据训练的LSTM模型比仅使用真实数据训练的模型预测准确性显著提高，且在不同预测时间范围内都有效

Conclusion: GAN作为数据增强工具能有效克服金融领域数据稀缺问题，提升预测能力，提出的时间序列特定质量指标能可靠评估生成数据质量

Abstract: Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.

</details>


### [96] [Provable Adversarial Robustness in In-Context Learning](https://arxiv.org/abs/2602.17743)
*Di Zhang*

Main category: cs.LG

TL;DR: 论文提出一个分布鲁棒的元学习框架，为ICL在Wasserstein分布偏移下提供最坏情况性能保证，发现模型鲁棒性随容量平方根增长，对抗设置带来与扰动幅度平方成正比的样本复杂度惩罚。


<details>
  <summary>Details</summary>
Motivation: 现有ICL理论假设测试任务分布与预训练相似，忽略了对抗性分布偏移对实际可靠性的威胁，需要填补这一理论空白。

Method: 引入分布鲁棒的元学习框架，针对线性自注意力Transformer，推导非渐近边界，分析对抗扰动强度、模型容量和上下文示例数量之间的关系。

Result: 模型鲁棒性随容量平方根增长（ρ_max ∝ √m），对抗设置带来样本复杂度惩罚与扰动幅度平方成正比（N_ρ - N_0 ∝ ρ²），合成任务实验验证了这些缩放规律。

Conclusion: 这些发现推进了对ICL在对抗条件下极限的理论理解，表明模型容量是分布鲁棒性的基本资源。

Abstract: Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($ρ$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($ρ_{\text{max}} \propto \sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_ρ- N_0 \propto ρ^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.

</details>


### [97] [MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies](https://arxiv.org/abs/2602.17868)
*Vasilii Feofanov,Songkang Wen,Jianfeng Zhang,Lujia Pan,Ievgen Redko*

Main category: cs.LG

TL;DR: MantisV2和Mantis+时间序列基础模型在零样本特征提取方面取得显著进步，通过合成数据预训练、架构优化和测试时方法改进，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发时间序列分类基础模型具有重要实践意义，可作为通用特征提取器用于多样化下游任务。尽管早期模型如Mantis已显示出潜力，但冻结编码器和微调编码器之间存在显著性能差距，需要改进零样本特征提取能力。

Method: 1) 提出Mantis+，完全在合成时间序列上预训练的Mantis变体；2) 通过受控消融研究改进架构，获得更轻量化的MantisV2编码器；3) 提出增强的测试时方法，利用中间层表示并改进输出标记聚合；4) 通过自集成和跨模型嵌入融合进一步提升性能。

Result: 在UCR、UEA、人类活动识别(HAR)基准和EEG数据集上的广泛实验表明，MantisV2和Mantis+始终优于先前的时间序列基础模型，实现了最先进的零样本性能。

Conclusion: 通过合成数据预训练、架构优化和测试时方法改进，显著增强了时间序列基础模型的零样本特征提取能力，为多样化下游任务提供了强大的通用特征提取器。

Abstract: Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.

</details>


### [98] [Bayesian Optimality of In-Context Learning with Selective State Spaces](https://arxiv.org/abs/2602.17744)
*Di Zhang,Jiaqi Xing*

Main category: cs.LG

TL;DR: 论文提出贝叶斯最优序列预测作为理解上下文学习的新原则，证明选择性状态空间模型在特定任务中能实现贝叶斯最优预测器，优于基于梯度下降的Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 当前对Transformer上下文学习的解释主要基于"隐式梯度下降"框架，但作者认为这不能完全解释某些架构（如选择性状态空间模型）的优越性能。需要一个新的理论框架来理解上下文学习，特别是解释为什么某些架构在统计效率上更优。

Method: 将上下文学习形式化为元学习问题，在线性高斯状态空间模型任务中，证明元训练的选择性状态空间模型能渐近实现贝叶斯最优预测器。通过构造具有时间相关噪声的任务，展示贝叶斯预测器严格优于任何经验风险最小化估计器。

Result: 在合成LG-SSM任务和字符级马尔可夫基准测试中，选择性状态空间模型更快收敛到贝叶斯最优风险，在结构化噪声设置中具有更好的样本效率，比线性Transformer更鲁棒地跟踪潜在状态。

Conclusion: 应将上下文学习从"隐式优化"重新框架为"最优推断"，这解释了选择性状态空间模型的效率优势，并为架构设计提供了原则性基础。贝叶斯最优序列预测为理解上下文学习提供了新的理论视角。

Abstract: We propose Bayesian optimal sequential prediction as a new principle for understanding in-context learning (ICL). Unlike interpretations framing Transformers as performing implicit gradient descent, we formalize ICL as meta-learning over latent sequence tasks. For tasks governed by Linear Gaussian State Space Models (LG-SSMs), we prove a meta-trained selective SSM asymptotically implements the Bayes-optimal predictor, converging to the posterior predictive mean. We further establish a statistical separation from gradient descent, constructing tasks with temporally correlated noise where the optimal Bayesian predictor strictly outperforms any empirical risk minimization (ERM) estimator. Since Transformers can be seen as performing implicit ERM, this demonstrates selective SSMs achieve lower asymptotic risk due to superior statistical efficiency. Experiments on synthetic LG-SSM tasks and a character-level Markov benchmark confirm selective SSMs converge faster to Bayes-optimal risk, show superior sample efficiency with longer contexts in structured-noise settings, and track latent states more robustly than linear Transformers. This reframes ICL from "implicit optimization" to "optimal inference," explaining the efficiency of selective SSMs and offering a principled basis for architecture design.

</details>


### [99] [Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs](https://arxiv.org/abs/2602.17778)
*Zachary Coalson,Bo Fang,Sanghyun Hong*

Main category: cs.LG

TL;DR: 论文发现对话LLMs存在"轮次放大"故障模式，攻击者可系统利用澄清寻求行为来延长多轮对话而不完成任务，这种攻击源于对话动态机制，能跨提示和任务持续存在。


<details>
  <summary>Details</summary>
Motivation: 多轮交互长度是对话LLMs运营成本的主要因素，需要研究LLMs中可能被利用来延长对话的新故障模式，以降低运营成本和提升系统安全性。

Method: 采用机制视角，识别与澄清寻求响应相关的查询无关通用激活子空间；通过微调进行供应链攻击和通过低级参数损坏进行运行时攻击，将模型转向抽象澄清寻求行为；在多个指令调优LLMs和基准测试上验证攻击效果。

Result: 攻击能显著增加对话轮次同时保持合规性；现有防御措施对这种新兴故障类别的保护有限；攻击源于对话动态机制，能跨提示和任务持续存在。

Conclusion: 对话LLMs存在"轮次放大"这一新故障模式，攻击者可系统利用澄清寻求行为来延长对话，这需要新的防御机制来应对这种基于对话动态的成本放大攻击。

Abstract: Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.

</details>


### [100] [Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data](https://arxiv.org/abs/2602.17888)
*Sayeed Shafayet Chowdhury,Karen D'Souza,V. Siva Kakumani,Snehasis Mukhopadhyay,Shiaofen Fang,Rodney J. Schlosser,Daniel M. Beswick,Jeremiah A. Alt,Jess C. Mace,Zachary M. Soler,Timothy L. Smith,Vijay R. Ramakrishnan*

Main category: cs.LG

TL;DR: 使用监督机器学习模型预测慢性鼻窦炎患者手术获益，在观察性临床试验的前瞻性数据上达到85%准确率，优于临床专家预测


<details>
  <summary>Details</summary>
Motivation: 慢性鼻窦炎（CRS）手术决策复杂，需要权衡已知手术风险与不确定的个体化结果。虽然AI在医疗预后中应用广泛，但在观察性临床试验的前瞻性标准化数据上应用机器学习预测仍未被充分探索，尽管这有潜力降低成本并改善患者预后。

Method: 使用观察性干预试验的前瞻性收集队列数据，所有患者均接受手术。训练监督机器学习模型仅基于术前数据预测手术获益，以Sino-Nasal Outcome Test-22（SNOT-22）作为主要患者报告结果。采用多种算法包括集成方法，并在30个混合难度病例的保留集上评估模型性能。

Result: 最佳模型达到约85%的分类准确率，能够准确且可解释地预测手术候选资格。在30个病例的保留集上，模型达到80%准确率，超过临床专家平均预测准确率（75.6%）。

Conclusion: 机器学习模型能够有效预测慢性鼻窦炎患者的手术获益，性能优于临床专家，有潜力增强临床决策支持，实现个性化CRS治疗。

Abstract: Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.

</details>


### [101] [Multi-material Multi-physics Topology Optimization with Physics-informed Gaussian Process Priors](https://arxiv.org/abs/2602.17783)
*Xiangyu Sun,Shirin Hosseinmardi,Amin Yousefpour,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出基于物理信息高斯过程（PIGP）的拓扑优化框架，解决多材料、多物理场问题的计算成本高、谱偏差等挑战，通过神经网络参数化GP均值函数，同时优化目标函数、多物理场势能泛函和设计约束。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习拓扑优化方法大多局限于简化基准问题，存在计算成本高、谱偏差、难以处理复杂物理等限制，这些限制在多材料、多物理场非自伴随问题中尤为突出。

Method: 使用独立高斯过程先验表示主变量、伴随变量和设计变量，其均值函数通过神经网络参数化，通过最小化基于目标函数、多物理场势能泛函和设计约束的损失函数同时估计所有参数，并引入加速训练的微分和积分方案。

Result: 框架在单/多材料合规性最小化、热传导优化、柔顺机构设计等基准问题上有效，并成功应用于热机械拓扑优化等代表性多物理场问题，生成具有锐利界面和物理解释性材料分布的超分辨率拓扑结构。

Conclusion: 提出的PIGP框架能有效同时解决耦合多物理场和设计问题，生成高质量拓扑结构，结果通过开源代码和COMSOL商业软件验证。

Abstract: Machine learning (ML) has been increasingly used for topology optimization (TO). However, most existing ML-based approaches focus on simplified benchmark problems due to their high computational cost, spectral bias, and difficulty in handling complex physics. These limitations become more pronounced in multi-material, multi-physics problems whose objective or constraint functions are not self-adjoint. To address these challenges, we propose a framework based on physics-informed Gaussian processes (PIGPs). In our approach, the primary, adjoint, and design variables are represented by independent GP priors whose mean functions are parametrized via neural networks whose architectures are particularly beneficial for surrogate modeling of PDE solutions. We estimate all parameters of our model simultaneously by minimizing a loss that is based on the objective function, multi-physics potential energy functionals, and design-constraints. We demonstrate the capability of the proposed framework on benchmark TO problems such as compliance minimization, heat conduction optimization, and compliant mechanism design under single- and multi-material settings. Additionally, we leverage thermo-mechanical TO with single- and multi-material options as a representative multi-physics problem. We also introduce differentiation and integration schemes that dramatically accelerate the training process. Our results demonstrate that the proposed PIGP framework can effectively solve coupled multi-physics and design problems simultaneously -- generating super-resolution topologies with sharp interfaces and physically interpretable material distributions. We validate these results using open-source codes and the commercial software package COMSOL.

</details>


### [102] [Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds](https://arxiv.org/abs/2602.17798)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出GrMoE：基于Grassmann流形的MoE路由框架，用矩阵Bingham分布的集中参数控制路由稀疏性，实现连续可调的稀疏机制，避免专家崩溃。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型的softmax门控缺乏控制稀疏性与利用率权衡的理论机制，且存在专家崩溃问题。需要一种几何原理明确、可连续控制稀疏性的路由框架。

Method: 在Grassmann流形子空间上构建路由框架，使用矩阵Bingham分布的集中参数Λ作为门控权重，开发摊销变分推理方法估计后验路由分布，实现不确定性感知的专家分配。

Result: 在多个规模的MoE语言模型上实现0%路由崩溃，困惑度相当或更好，负载均衡提升15-30%，集中参数与有效稀疏度呈单调关系，支持训练后稀疏度调整。专家学习到与语言专业化相关的异质集中值。

Conclusion: GrMoE提供了基于几何原理的路由框架，通过集中参数连续控制稀疏性，建立了首个浓度控制稀疏性的形式理论，实现了可解释、无崩溃的路由行为。

Abstract: Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $Λ$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\% routing collapse across all seeds, comparable or better perplexity with 15--30\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.

</details>


### [103] [Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.17809)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: SBA是一种贝叶斯参数高效微调框架，在Stiefel流形上使用Matrix Langevin先验，通过切空间拉普拉斯近似进行后验推断，提供校准的不确定性估计，在领域偏移下表现优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）缺乏原则性的不确定性估计，导致预测校准不佳，在领域偏移下行为不可靠。需要一种能提供校准预测不确定性的贝叶斯PEFT框架。

Method: 在Stiefel流形上放置Matrix Langevin先验于正交适配器因子，通过切空间拉普拉斯近似与测地线回缩进行近似后验推断，避免环境空间投影的结构方差膨胀。

Result: 在多个模型和基准测试中，SBA在任务性能上与LoRA和DoRA相当，同时将预期校准误差降低18-34%，在领域偏移下将选择性预测AUROC提高12-25%，以更少参数成本优于五个LoRA模型的深度集成。

Conclusion: 将不确定性放置在正确的几何结构上比简单地对适配器添加任何贝叶斯处理更重要，SBA在流形上的内在推理具有严格的理論优势。

Abstract: Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.

</details>


### [104] [MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance](https://arxiv.org/abs/2602.17930)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: MIRA通过构建结构化记忆图来整合LLM先验知识和智能体经验，减少对实时LLM监督的依赖，在稀疏奖励环境中提高早期学习效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏或延迟奖励环境中样本效率低，而过度依赖LLM监督存在可扩展性限制和不可靠信号问题。

Method: 构建结构化记忆图存储高回报轨迹片段和LLM输出的子目标结构，从中推导效用信号来软调整优势估计，随着训练进展效用项衰减。

Result: MIRA在稀疏奖励环境中优于RL基线，达到与频繁LLM监督方法相当的回报，同时显著减少在线LLM查询。

Conclusion: MIRA通过记忆图整合LLM先验与智能体经验，在减少LLM依赖的同时提高早期学习效率，并保持标准收敛保证。

Abstract: Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/

</details>


### [105] [Avoid What You Know: Divergent Trajectory Balance for GFlowNets](https://arxiv.org/abs/2602.17827)
*Pedro Dall'Antonia,Tiago da Silva,Daniel Csillag,Salem Lahlou,Diego Mesquita*

Main category: cs.LG

TL;DR: 提出ACE算法，通过训练专门的探索GFlowNet来搜索未被充分探索的高奖励区域，显著提升GFlowNets的学习效率和探索效果。


<details>
  <summary>Details</summary>
Motivation: GFlowNets在训练过程中探索效率受限，现有方法（如好奇心驱动搜索和自监督随机网络蒸馏）会浪费样本在已经充分探索的区域，需要更有效的探索策略来发现新颖且高概率的区域。

Method: 提出自适应互补探索（ACE）算法，引入一个专门的探索GFlowNet，该网络被训练来搜索标准GFlowNet未充分探索区域中的高奖励状态，而标准GFlowNet则专注于学习目标分布采样。

Result: 通过大量实验表明，ACE在目标分布近似精度和多样化高奖励状态发现率方面显著优于先前工作。

Conclusion: ACE为GFlowNets提供了一种原则性的有效探索算法，能够更好地平衡探索与利用，提高学习效率和采样质量。

Abstract: Generative Flow Networks (GFlowNets) are a flexible family of amortized samplers trained to generate discrete and compositional objects with probability proportional to a reward function. However, learning efficiency is constrained by the model's ability to rapidly explore diverse high-probability regions during training. To mitigate this issue, recent works have focused on incentivizing the exploration of unvisited and valuable states via curiosity-driven search and self-supervised random network distillation, which tend to waste samples on already well-approximated regions of the state space. In this context, we propose Adaptive Complementary Exploration (ACE), a principled algorithm for the effective exploration of novel and high-probability regions when learning GFlowNets. To achieve this, ACE introduces an exploration GFlowNet explicitly trained to search for high-reward states in regions underexplored by the canonical GFlowNet, which learns to sample from the target distribution. Through extensive experiments, we show that ACE significantly improves upon prior work in terms of approximation accuracy to the target distribution and discovery rate of diverse high-reward states.

</details>


### [106] [Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2602.17931)
*Narjes Nourzad,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出一种结合LLM指导与智能体自身经验的方法，通过构建记忆图编码子目标和轨迹，从中推导效用函数来指导强化学习，减少对频繁LLM调用的依赖，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏或延迟奖励环境中，强化学习需要大量交互导致样本复杂度高。虽然大语言模型可用于子目标发现和轨迹指导，但频繁调用LLM存在可扩展性和可靠性问题。需要一种方法既能利用LLM指导，又不过度依赖持续监督。

Method: 构建记忆图编码来自LLM指导和智能体自身成功轨迹的子目标和轨迹。从图中推导效用函数，评估智能体轨迹与先前成功策略的匹配程度。该效用函数塑造优势函数，为critic提供额外指导而不改变奖励。主要依赖离线输入，仅偶尔进行在线查询，避免持续LLM监督。

Result: 在基准环境中的初步实验显示，相比基线RL方法，该方法提高了样本效率并加速了早期学习。最终回报与需要频繁LLM交互的方法相当，但减少了LLM调用需求。

Conclusion: 通过构建记忆图结合LLM指导和智能体自身经验，能够有效指导强化学习探索，提高样本效率，同时减少对频繁LLM调用的依赖，平衡了LLM指导的益处与可扩展性需求。

Abstract: In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.

</details>


### [107] [Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models](https://arxiv.org/abs/2602.17829)
*Preetom Biswas,Giulia Pedrielli,K. Selçuk Candan*

Main category: cs.LG

TL;DR: ruleXplain：利用大语言模型从仿真驱动动力系统中提取可验证因果规则的解释框架


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理具有延迟效应的时序数据因果推断时，难以产生泛化且可解释的解释，特别是当多个不同的输入轨迹产生几乎无法区分的输出时。需要一种能够提取形式化解释的方法来理解复杂动力系统中的输入-输出关系。

Method: ruleXplain框架利用大语言模型，通过引入具有时序算子和延迟语义的约束符号规则语言，使用结构化提示让LLM生成可验证的因果规则。利用仿真器生成产生相似目标输出的多样化反事实输入轨迹作为候选解释，对这些输入进行聚类并提供给LLM，LLM负责生成编码联合时序趋势的符号规则，并通过闭环精炼过程确保规则一致性和语义有效性。

Result: 在PySIRTEM流行病仿真器（测试率输入到每日感染数）和EnergyPlus建筑能耗仿真器（温度和太阳辐照度输入到电力需求）上验证了框架有效性。进行了三类实验：1）通过输入重构评估规则集效能；2）消融研究评估规则集的因果编码；3）提取规则在具有不同相位动态的未见输出趋势上的泛化测试。

Conclusion: ruleXplain能够从仿真驱动动力系统中提取形式化、可验证的因果解释，解决了传统方法在处理具有延迟效应的复杂时序数据因果推断时的局限性，为理解复杂系统中的输入-输出关系提供了新的解释框架。

Abstract: Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.

</details>


### [108] [Causal Neighbourhood Learning for Invariant Graph Representations](https://arxiv.org/abs/2602.17934)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong*

Main category: cs.LG

TL;DR: CNL-GNN是一个新颖的因果图神经网络框架，通过因果干预和反事实邻域生成来解决图数据中的虚假相关性问题，提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 图数据中经常存在噪声和虚假相关性，这些相关性掩盖了真实的因果关系。传统的GNN依赖虚假连接，难以在不同图之间有效泛化，传统聚合方法会放大这些虚假模式，限制了模型在分布变化下的鲁棒性。

Method: 提出CNL-GNN框架，通过因果干预图结构来识别和保留因果相关连接，减少虚假影响。具体方法包括：生成反事实邻域、基于可学习重要性掩码和注意力机制的自适应边扰动、结合结构级干预与因果特征从混杂因素中解耦，学习不变节点表示。

Result: 在四个公开数据集（包括一个数据集的多个领域变体）上的广泛实验表明，CNL-GNN优于最先进的GNN模型，超越了传统的基于特征的方法。

Conclusion: CNL-GNN通过因果干预和结构解耦，能够学习鲁棒的节点表示，有效提升图模型的因果学习能力和泛化性能，为图数据中的因果学习提供了新思路。

Abstract: Graph data often contain noisy and spurious correlations that mask the true causal relationships, which are essential for enabling graph models to make predictions based on the underlying causal structure of the data. Dependence on spurious connections makes it challenging for traditional Graph Neural Networks (GNNs) to generalize effectively across different graphs. Furthermore, traditional aggregation methods tend to amplify these spurious patterns, limiting model robustness under distribution shifts. To address these issues, we propose Causal Neighbourhood Learning with Graph Neural Networks (CNL-GNN), a novel framework that performs causal interventions on graph structure. CNL-GNN effectively identifies and preserves causally relevant connections and reduces spurious influences through the generation of counterfactual neighbourhoods and adaptive edge perturbation guided by learnable importance masking and an attention-based mechanism. In addition, by combining structural-level interventions with the disentanglement of causal features from confounding factors, the model learns invariant node representations that are robust and generalize well across different graph structures. Our approach improves causal graph learning beyond traditional feature-based methods, resulting in a robust classification model. Extensive experiments on four publicly available datasets, including multiple domain variants of one dataset, demonstrate that CNL-GNN outperforms state-of-the-art GNN models.

</details>


### [109] [MePoly: Max Entropy Polynomial Policy Optimization](https://arxiv.org/abs/2602.17832)
*Hang Liu,Sangli Teng,Maani Ghaffari*

Main category: cs.LG

TL;DR: 提出MePoly策略参数化方法，基于多项式能量模型，提供显式可处理的概率密度，解决扩散策略缺乏显式密度的问题，在多个基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随机最优控制为复杂决策问题提供统一框架，但传统参数化策略难以表示解的多模态特性。扩散策略虽能恢复多模态，但缺乏显式概率密度，使策略梯度优化复杂化。

Method: 提出MePoly策略参数化方法，基于多项式能量模型，提供显式可处理的概率密度，支持精确熵最大化。理论基于经典矩问题，利用对任意分布的通用逼近能力。

Result: MePoly能有效捕捉复杂的非凸流形，在多样化基准测试中表现优于基线方法。

Conclusion: MePoly通过多项式能量模型提供显式概率密度，解决了扩散策略的局限性，在捕捉多模态分布和策略优化方面表现出色。

Abstract: Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.

</details>


### [110] [Optimizing Graph Causal Classification Models: Estimating Causal Effects and Addressing Confounders](https://arxiv.org/abs/2602.17941)
*Simi Job,Xiaohui Tao,Taotao Cai,Haoran Xie,Jianming Yong,Xin Wang*

Main category: cs.LG

TL;DR: CCAGNN是一个结合因果推理的图神经网络框架，通过考虑混杂因素实现鲁棒预测和反事实推理，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据越来越多，传统图机器学习方法（如图神经网络）依赖相关性，对虚假模式和分布变化敏感。因果学习能理解真正的因果关系，在干预下保持稳定，但需要将因果推理融入图学习。

Method: 提出CCAGNN（Confounder-Aware Causal GNN）框架，将因果推理整合到图学习中，考虑混杂因素，支持反事实推理，确保预测反映真实的因果关系。

Result: 在六个公开数据集上的综合实验表明，CCAGNN始终优于领先的现有模型，显示出更好的鲁棒性和预测可靠性。

Conclusion: CCAGNN成功地将因果推理与图学习结合，解决了传统方法对虚假相关性的依赖问题，为实际应用提供了更可靠、鲁棒的预测模型。

Abstract: Graph data is becoming increasingly prevalent due to the growing demand for relational insights in AI across various domains. Organizations regularly use graph data to solve complex problems involving relationships and connections. Causal learning is especially important in this context, since it helps to understand cause-effect relationships rather than mere associations. Since many real-world systems are inherently causal, graphs can efficiently model these systems. However, traditional graph machine learning methods including graph neural networks (GNNs), rely on correlations and are sensitive to spurious patterns and distribution changes. On the other hand, causal models enable robust predictions by isolating true causal factors, thus making them more stable under such shifts. Causal learning also helps in identifying and adjusting for confounders, ensuring that predictions reflect true causal relationships and remain accurate even under interventions. To address these challenges and build models that are robust and causally informed, we propose CCAGNN, a Confounder-Aware causal GNN framework that incorporates causal reasoning into graph learning, supporting counterfactual reasoning and providing reliable predictions in real-world settings. Comprehensive experiments on six publicly available datasets from diverse domains show that CCAGNN consistently outperforms leading state-of-the-art models.

</details>


### [111] [Influence-Preserving Proxies for Gradient-Based Data Selection in LLM Fine-tuning](https://arxiv.org/abs/2602.17835)
*Sirui Chen,Yunzhe Qi,Mengting Ai,Yifan Sun,Ruizhong Qiu,Jiaru Zou,Jingrui He*

Main category: cs.LG

TL;DR: Iprox是一个两阶段框架，通过低秩压缩和对齐梯度/逻辑值，从目标大模型中构建保留影响力的代理模型，使基于梯度的数据选择方法能更高效地应用于大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的数据选择方法（如TracIn和Influence Functions）计算成本高，难以应用于数十亿参数的大语言模型。使用现成的小模型作为代理效果不佳，因为它们的学习动态不明确、大小无法灵活调整，且无法与目标模型在梯度影响力估计上对齐。

Method: Iprox采用两阶段框架：1）低秩压缩阶段，保留目标模型的影响力信息；2）对齐阶段，同时对齐模型梯度和逻辑值，构建能灵活控制计算成本且保留目标模型影响力的代理模型。

Result: 在多种LLM家族和评估任务上，Iprox始终优于现成代理和基线方法。在Qwen3-4B上，Iprox构建的1.5B代理比更大的1.7B现成代理表现更好。在Llama3.2上，Iprox在性能优于基线的同时，计算成本比完整的3B模型减少一半以上。

Conclusion: Iprox提供了有效的保留影响力代理模型，使基于梯度的数据选择方法能更可扩展地应用于大语言模型，解决了现有方法计算成本高和代理模型效果不佳的问题。

Abstract: Supervised fine-tuning (SFT) relies critically on selecting training data that most benefits a model's downstream performance. Gradient-based data selection methods such as TracIn and Influence Functions leverage influence to identify useful samples, but their computational cost scales poorly, making them impractical for multi-billion-parameter large language models (LLMs). A common alternative is to use off-the-shelf smaller models as proxies, but they remain suboptimal since their learning dynamics are unclear, their sizes cannot be flexibly adjusted, and they cannot be further aligned with the target model in terms of gradient-based influence estimation. To address these challenges, we introduce Iprox, a two-stage framework that derives influence-preserving proxies directly from the target model. It first applies a low-rank compression stage to preserve influence information of the target model, and then an aligning stage to align both model gradients and logits, thereby constructing proxies that flexibly control computational cost while retaining the target model's influence. Experimental results across diverse LLM families and evaluation tasks show that Iprox consistently outperforms off-the-shelf proxies and baseline methods. On Qwen3-4B, a 1.5B proxy constructed with Iprox achieves stronger performance than the larger 1.7B off-the-shelf proxy. Notably, on Llama3.2, Iprox achieves better performance than baselines while reducing computational cost by more than half relative to the full 3B model. These results show that Iprox provides effective influence-preserving proxies, making gradient-based data selection more scalable for LLMs.

</details>


### [112] [Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models](https://arxiv.org/abs/2602.17846)
*Nick Dodson,Xinyu Gao,Qingsong Wang,Yusu Wang,Zhengchao Wan*

Main category: cs.LG

TL;DR: 扩散模型存在记忆训练数据的隐私风险，作者提出几何框架将噪声调度分为三个区域，发现中噪声区域记忆风险最高，并提出几何干预方法缓解记忆问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型能生成高质量样本，但也会记忆训练数据，引发严重隐私担忧。目前对记忆与泛化机制的理解不足，特别是噪声调度中记忆发生的位置、数据几何的影响以及不同噪声尺度现象的相互作用。

Method: 引入几何框架，基于高斯壳覆盖训练数据的特性和后验集中行为，将噪声调度分为三个区域：小噪声、中噪声和大噪声。识别中噪声区域的危险区，并提出几何条件指导的针对性干预方法。

Result: 记忆风险在噪声水平上高度不均匀，中噪声区域记忆最显著。小噪声区域因训练覆盖有限而避免记忆，大噪声区域后验集中度低，呈现近似线性高斯去噪行为。提出的几何干预能有效缓解记忆问题。

Conclusion: 通过几何框架揭示了扩散模型中记忆与泛化的机制差异，识别了记忆风险最高的中噪声区域，并提出了基于几何条件的针对性干预方法，为理解和管理扩散模型的隐私风险提供了新视角。

Abstract: Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.

</details>


### [113] [ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization](https://arxiv.org/abs/2602.17867)
*João N. Cardoso,Arlindo L. Oliveira,Bruno Martins*

Main category: cs.LG

TL;DR: ADAPT：一种结合束搜索初始化和自适应梯度引导突变的混合方法，用于优化LLM激活空间中特征可视化的离散文本输入


<details>
  <summary>Details</summary>
Motivation: 理解LLM激活空间中学习方向编码的特征需要识别强烈激活这些方向的输入。特征可视化通过优化输入来最大化激活目标方向，为昂贵的数据库搜索方法提供了替代方案，但由于文本的离散性，在LLM中仍未充分探索。现有的提示优化技术不适合这个领域，容易陷入局部最小值。

Method: ADAPT是一种混合方法，结合了束搜索初始化和自适应梯度引导突变，专门针对这些失败模式设计。该方法在Gemma 2 2B的稀疏自编码器潜在空间上进行评估。

Result: ADAPT在不同层和潜在类型上始终优于先前的方法。研究提出了基于数据集激活统计的度量标准，以支持严格比较。

Conclusion: LLM的特征可视化是可行的，但需要针对该领域量身定制的设计假设。ADAPT方法为理解LLM激活空间中的特征编码提供了有效的工具。

Abstract: Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.

</details>


### [114] [Dual Length Codes for Lossless Compression of BFloat16](https://arxiv.org/abs/2602.17849)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 论文提出Dual Length Codes，一种平衡压缩效率和解码速度的混合编码方案，用于加速LLM训练和推理中的网络通信。


<details>
  <summary>Details</summary>
Motivation: LLM训练和推理严重依赖并行化和集体操作，这些操作经常受网络带宽限制。现有方法如霍夫曼编码解码慢且硬件复杂，而通用编码如指数哥伦布编码解码快但不利用符号频率分布。

Method: 提出双长度编码：分析Gemma模型的BFloat16张量，发现前8个最频繁符号占约50%概率，为这8个符号分配4位短码，其余248个符号分配9位长码，使用单个前缀位区分两种码长，仅需8个条目的查找表进行编解码。

Result: 压缩率达到18.6%（霍夫曼编码为21.3%），但显著加快了解码速度并简化了硬件复杂度。

Conclusion: Dual Length Codes在压缩效率和解码速度之间取得了良好平衡，特别适合LLM训练和推理中的网络通信优化。

Abstract: Training and serving Large Language Models (LLMs) relies heavily on parallelization and collective operations, which are frequently bottlenecked by network bandwidth. Lossless compression using e.g., Huffman codes can alleviate the issue, however, Huffman codes suffer from slow, bit-sequential decoding and high hardware complexity due to deep tree traversals. Universal codes e.g., Exponential-Golomb codes are faster to decode but do not exploit the symbol frequency distributions. To address these limitations, this paper introduces Dual Length Codes, a hybrid approach designed to balance compression efficiency with decoding speed. Analyzing BFloat16 tensors from the Gemma model, we observed that the top 8 most frequent symbols account for approximately 50% of the cumulative probability. These 8 symbols are assigned a short 4 bit code. The remaining 248 symbols are assigned a longer 9 bit code. The coding scheme uses a single prefix bit to distinguish between the two code lengths. The scheme uses a small Look Up Table with only 8 entries for encoding and decoding. The scheme achieves a compressibility of 18.6% in comparison to 21.3% achieved by Huffman codes, but it significantly speeds up the decoding and simplifies the hardware complexity.

</details>


### [115] [NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs](https://arxiv.org/abs/2602.18008)
*Zihan Guan,Rituparna Datta,Mengxuan Hu,Shunshun Liu,Aiying Zhang,Prasanna Balachandran,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 提出NIMM评估框架测试LLM生成机理模型在真实场景下的可靠性，并开发NIMMgen框架通过迭代优化提升模型代码正确性和实用性


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机理模型的方法过于简化现实条件，无法确定在实际应用中的可靠性，需要更真实的评估框架

Method: 提出NIMM评估框架，在部分观测和多样化任务目标的真实场景下评估LLM生成模型；并设计NIMMgen代理框架，通过迭代优化提升代码正确性和实用性

Result: 评估发现现有基线方法存在从模型有效性到代码正确性的根本挑战；NIMMgen在三个不同科学领域的数据集上表现优异，且学习到的机理模型支持反事实干预模拟

Conclusion: NIMM框架揭示了当前LLM生成机理模型的局限性，NIMMgen通过迭代优化显著提升了模型可靠性和实用性，为科学建模提供了新方法

Abstract: Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.

</details>


### [116] [In-Context Learning for Pure Exploration in Continuous Spaces](https://arxiv.org/abs/2602.17976)
*Alessio Russo,Yin-Ching Lee,Ryan Welch,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出C-ICPE-TS算法，用于连续空间中的主动顺序测试（纯探索），通过元训练深度神经网络策略学习可迁移的测试策略，无需参数更新或手工信息模型。


<details>
  <summary>Details</summary>
Motivation: 许多现代设置中假设空间是连续的且与查询/动作空间自然重合，如连续臂老虎机中的最优动作识别、目标区域内的ε球定位、未知函数极小值估计等，需要解决连续空间中的纯探索问题。

Method: 提出C-ICPE-TS算法，元训练深度神经网络策略，将观察历史映射到(i)下一个连续查询动作和(ii)预测假设，直接从数据中学习可迁移的顺序测试策略。推理时在未见任务上主动收集证据推断真实假设。

Result: 在连续最优臂识别、区域定位和函数极小值识别等多个基准测试中验证了C-ICPE-TS的有效性。

Conclusion: C-ICPE-TS为连续空间中的纯探索问题提供了有效解决方案，能够学习可迁移的测试策略并在未见任务上无需参数更新或手工信息模型即可推断真实假设。

Abstract: In active sequential testing, also termed pure exploration, a learner is tasked with the goal to adaptively acquire information so as to identify an unknown ground-truth hypothesis with as few queries as possible. This problem, originally studied by Chernoff in 1959, has several applications: classical formulations include Best-Arm Identification (BAI) in bandits, where actions index hypotheses, and generalized search problems, where strategically chosen queries reveal partial information about a hidden label. In many modern settings, however, the hypothesis space is continuous and naturally coincides with the query/action space: for example, identifying an optimal action in a continuous-armed bandit, localizing an $ε$-ball contained in a target region, or estimating the minimizer of an unknown function from a sequence of observations. In this work, we study pure exploration in such continuous spaces and introduce Continuous In-Context Pure Exploration for this regime. We introduce C-ICPE-TS, an algorithm that meta-trains deep neural policies to map observation histories to (i) the next continuous query action and (ii) a predicted hypothesis, thereby learning transferable sequential testing strategies directly from data. At inference time, C-ICPE-TS actively gathers evidence on previously unseen tasks and infers the true hypothesis without parameter updates or explicit hand-crafted information models. We validate C-ICPE-TS across a range of benchmarks, spanning continuous best-arm identification, region localization, and function minimizer identification.

</details>


### [117] [Neural Prior Estimation: Learning Class Priors from Latent Representations](https://arxiv.org/abs/2602.17853)
*Masoud Yavari,Payman Moallem*

Main category: cs.LG

TL;DR: 提出NPE框架，通过学习特征条件化的对数先验估计来缓解类别不平衡导致的系统偏差，无需显式类别计数或分布特定超参数。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡会导致深度神经网络产生系统性偏差，因为倾斜的有效类别先验会影响模型性能，特别是对少数类别的识别能力。

Method: 提出神经先验估计器(NPE)框架，通过一个或多个先验估计模块与主干网络联合训练，使用单向逻辑损失学习特征条件化的对数先验估计。在神经坍缩机制下，NPE理论上能恢复类别对数先验（相差一个加性常数）。将学习到的估计融入logit调整，形成NPE-LA进行偏差感知预测。

Result: 在长尾CIFAR和不平衡语义分割基准（STARE、ADE20K）上的实验显示，NPE-LA能带来一致的性能提升，特别是对代表性不足的类别效果显著。

Conclusion: NPE提供了一种轻量级且理论上有依据的学习先验估计方法，能够实现不平衡感知预测，无需显式类别计数或分布特定超参数。

Abstract: Class imbalance induces systematic bias in deep neural networks by imposing a skewed effective class prior. This work introduces the Neural Prior Estimator (NPE), a framework that learns feature-conditioned log-prior estimates from latent representations. NPE employs one or more Prior Estimation Modules trained jointly with the backbone via a one-way logistic loss. Under the Neural Collapse regime, NPE is analytically shown to recover the class log-prior up to an additive constant, providing a theoretically grounded adaptive signal without requiring explicit class counts or distribution-specific hyperparameters. The learned estimate is incorporated into logit adjustment, forming NPE-LA, a principled mechanism for bias-aware prediction. Experiments on long-tailed CIFAR and imbalanced semantic segmentation benchmarks (STARE, ADE20K) demonstrate consistent improvements, particularly for underrepresented classes. NPE thus offers a lightweight and theoretically justified approach to learned prior estimation and imbalance-aware prediction.

</details>


### [118] [Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards](https://arxiv.org/abs/2602.18037)
*Johannes Ackermann,Michael Noukhovitch,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出使用梯度正则化（GR）替代传统的KL惩罚，通过引导策略更新到奖励模型更准确的区域来解决RLHF中的奖励黑客问题。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的后训练中，RLHF和RLVR存在奖励黑客问题，即策略可能利用奖励模型的不准确性学习到非预期的行为。传统方法使用KL惩罚限制策略更新，但效果有限。

Method: 提出梯度正则化（GR）方法：1）理论推导奖励模型准确性与收敛最优值平坦度的关系；2）使用GR引导训练到更平坦区域以保持奖励模型准确性；3）提出高效的有限差分估计实现GR。

Result: 实验表明：1）梯度范数与奖励准确性在RLHF中确实相关；2）GR在多种RL实验中优于KL惩罚；3）GR在RLHF中获得更高的GPT评判胜率；4）避免基于规则的数学奖励中对格式的过度关注；5）防止LLM-as-a-Judge数学任务中的评判黑客攻击。

Conclusion: 梯度正则化是一种有效的替代KL惩罚的方法，能够更好地解决RLHF中的奖励黑客问题，通过引导策略更新到奖励模型更准确的区域，提高训练稳定性和最终性能。

Abstract: Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.

</details>


### [119] [Learning Optimal and Sample-Efficient Decision Policies with Guarantees](https://arxiv.org/abs/2602.17978)
*Daqian Shao*

Main category: cs.LG

TL;DR: 该论文提出了一种从存在隐藏混杂因素的离线数据集中学习决策策略的方法，使用工具变量解决因果推断问题，并在模仿学习和时序逻辑目标学习方面进行了扩展。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量在线环境交互，这在成本高、危险或不可行的场景中存在问题。离线学习面临隐藏混杂因素的挑战，这些混杂因素会导致虚假相关性和次优决策。

Method: 1. 使用工具变量识别因果效应，将其建模为条件矩限制问题；2. 受双重/去偏机器学习启发，开发了具有收敛和最优性保证的样本高效算法；3. 将方法扩展到模仿学习，放宽对隐藏混杂因素的条件要求；4. 开发了学习线性时序逻辑表达的高层目标的算法。

Result: 提出的CMR求解算法优于现有最先进算法，模仿学习算法具有收敛速率保证，时序逻辑学习算法提高了样本效率。在强化学习基准测试和合成/半合成数据集上验证了方法的有效性。

Conclusion: 该论文开发的方法能够从存在隐藏混杂因素的离线数据中学习有效的决策策略，为实际高风险应用中的决策制定提供了理论保证和实用工具。

Abstract: The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.

</details>


### [120] [JAX-Privacy: A library for differentially private machine learning](https://arxiv.org/abs/2602.17861)
*Ryan McKenna,Galen Andrew,Borja Balle,Vadym Doroshenko,Arun Ganesh,Weiwei Kong,Alex Kurakin,Brendan McMahan,Mikhail Pravilov*

Main category: cs.LG

TL;DR: JAX-Privacy是一个用于简化差分隐私机器学习机制部署的库，旨在提供强大且高性能的解决方案。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是解决差分隐私机器学习中机制部署的复杂性，为研究人员和从业者提供一个统一的工具，整合最新研究成果，简化实现过程。

Method: JAX-Privacy库基于可用性、灵活性和效率的设计原则，提供经过验证的模块化原语，涵盖批量选择、梯度裁剪、噪声添加、会计和审计等关键组件。

Result: 开发了一个综合性的差分隐私机器学习库，能够同时满足需要深度定制的研究人员和希望开箱即用的从业者的需求。

Conclusion: JAX-Privacy成功整合了差分隐私机器学习领域的大量最新研究，为研究人员和从业者提供了一个强大、灵活且易于使用的工具，促进了差分隐私技术的实际应用。

Abstract: JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.

</details>


### [121] [Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory](https://arxiv.org/abs/2602.18297)
*Usman Anwar,Tim Bakker,Dana Kianfar,Cristina Pinneri,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过信息论分析CoT监控性，提出两种训练方法提升监控准确性并防止CoT退化


<details>
  <summary>Details</summary>
Motivation: 现有CoT监控器在实践中存在性能问题，需要理论分析其监控性的必要条件，并提出改进方法

Method: 1. 信息论分析CoT监控性必要条件；2. 识别两种近似误差源；3. 提出两种训练方法：基于oracle的方法和标签无关的条件互信息最大化方法

Result: 两种方法在多种环境中显著提升监控准确性，防止CoT退化，缓解奖励黑客问题

Conclusion: CoT与输出间的非零互信息是监控的必要非充分条件，通过针对性训练可系统性提升CoT监控性

Abstract: Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.

</details>


### [122] [Turbo Connection: Reasoning as Information Flow from Higher to Lower Layers](https://arxiv.org/abs/2602.17993)
*Mohan Tang,Sidi Lu*

Main category: cs.LG

TL;DR: TurboConn是一种新型Transformer架构，通过将高层隐藏状态路由到后续token的低层，突破固定计算深度限制，显著提升LLM在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 人类通过多步骤序列推理解决复杂问题，但Transformer的计算能力受限于固定的最大计算步数。现有架构无法支持token间的深层信息传递，限制了模型的多步推理能力。

Method: 提出TurboConn架构，将每个token的高层隐藏状态通过多个残差连接路由到下一个token的低层。这种密集的向后连接机制允许信息在token间更深入地流动，突破了传统Transformer的固定深度限制。

Result: 在GSM8K、Parity和多步算术等基准测试上获得0.9%到超过10%的准确率提升。特别是Parity任务中，Qwen-3-1.7B从53.78%提升到100%准确率。密集连接显著优于稀疏连接方案。

Conclusion: 计算路径深度是推理能力的关键因素，TurboConn提供了一种无需从头训练或复杂课程学习就能增强LLM推理能力的新机制，且不影响生成延迟。

Abstract: Complex problems, whether in math, logic, or planning, are solved by humans through a sequence of steps where the result of one step informs the next. In this work, we adopt the perspective that the reasoning power of Transformers is fundamentally limited by a fixed maximum number of steps along any latent path of computation. To address this, we introduce Turbo Connection (TurboConn), a novel architecture that overcomes the fixed-depth constraint by routing multiple residual connections from the higher-layer hidden states of each token $t$ to the lower layers of token $t+1$. Fine-tuning pre-trained LLMs with our method not only yields accuracy gains of 0.9% to over 10% on benchmarks like GSM8K, Parity, and multi-step arithmetic, but also demonstrates that the density of these backward connections is critical; our dense interaction significantly outperforms "sparse" alternatives that only pass a single hidden state or vector. Notably, TurboConn can be integrated into pre-trained LLMs to overcome task-specific plateaus: while a fine-tuned Qwen-3-1.7B achieves only 53.78% on Parity, adding our architectural modification enables the model to reach 100% accuracy, all without the necessity to retrain the full model from scratch or sophisticated curriculum learning. Our results provide strong empirical evidence that the depth of the computational path is a key factor in reasoning ability, also offering a new mechanism to enhance LLMs without significantly affecting generation latency.

</details>


### [123] [On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction](https://arxiv.org/abs/2602.18301)
*Ivan Bondarenko,Egor Palkin,Fedor Tikunov*

Main category: cs.LG

TL;DR: 本文研究LLM中用于一步文本重建的两个学习原型令牌（m-token和e-token）的信息编码特性，探索语义与句法内容的分离、稳定性分析，并测试通过教师嵌入施加语义结构的正则化方案。


<details>
  <summary>Details</summary>
Motivation: 传统自回归LLM需要n次前向传递生成n个token序列，效率低下。最近研究表明冻结LLM可以通过两个学习原型令牌在单次前向传递中重建数百个token，这为超越自回归范式提供了可能。本文旨在深入理解这些原型令牌编码的信息特性和行为。

Method: 进行一系列实验：1）分离两个原型令牌中的语义和句法内容；2）分析e-token的稳定性特性；3）可视化重建过程中对e-token的注意力模式；4）测试两种正则化方案：基于锚点的损失和关系蒸馏目标，使用教师嵌入在e-token上"施加"语义结构。

Result: 1）标准优化下，m-token比e-token更强烈地捕获语义信息；2）基于锚点的约束与重建准确性之间存在明显权衡；3）关系蒸馏可以在不牺牲重建质量的情况下将批次级语义关系转移到原型令牌空间。

Conclusion: 关系蒸馏支持将语义关系转移到原型令牌空间而不损害重建质量，这为未来非自回归序列到序列系统预测原型令牌作为中间表示提供了可行性证据。

Abstract: Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for "imposing" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.

</details>


### [124] [PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting](https://arxiv.org/abs/2602.17998)
*Shubham Bhardwaj,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: PHAST：一种用于位置观测数据的端口哈密顿架构，通过分解哈密顿量实现稳定长期预测和物理参数恢复


<details>
  <summary>Details</summary>
Motivation: 真实物理系统是耗散的，从部分观测（仅位置数据）预测其动力学是科学机器学习中的核心挑战。现有方法难以同时实现稳定长期预测和物理参数恢复。

Method: 提出PHAST架构，基于端口哈密顿框架，将哈密顿量分解为势能、质量和阻尼三个部分，对应三种知识状态（已知、部分已知、未知）。使用高效低秩PSD/SPD参数化，并通过Strang分裂推进动力学。

Result: 在13个仅位置数据的基准测试中（涵盖机械、电气、分子、热学、引力和生态系统），PHAST在长期预测方面优于竞争基线，并在有足够锚点时实现物理参数恢复。

Conclusion: PHAST成功解决了仅位置数据的动力学学习问题，实现了稳定预测和参数恢复。研究表明，没有足够锚点时参数识别是病态的（规范自由度），需要将预测稳定性与可识别性分开评估。

Abstract: Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\dot{x}=(J-R)\nabla H(x)$, guaranteeing $dH/dt\le 0$ when $R\succeq 0$. We introduce \textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.

</details>


### [125] [On the "Induction Bias" in Sequence Models](https://arxiv.org/abs/2602.18333)
*M. Reza Ebrahimi,Michaël Defferrard,Sunny Panchal,Roland Memisevic*

Main category: cs.LG

TL;DR: Transformer在状态追踪任务上存在根本性挑战，即使在训练和测试分布匹配的情况下，其数据效率远低于RNN，且难以在不同序列长度间共享学习到的机制。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在语言模型实践中取得了显著成功，但近期研究对其状态追踪能力提出了质疑。现有研究主要关注其在分布外泛化（如长度外推）的失败，而本文转向研究这些限制在分布内的影响。

Method: 进行大规模实验研究，比较Transformer和RNN在多种监督机制下的数据效率。分析所需训练数据随状态空间大小和序列长度的增长情况，并研究学习到的状态追踪机制在不同序列长度间的共享程度。

Result: 1. Transformer所需训练数据随状态空间大小和序列长度增长的速度远快于RNN；2. Transformer在不同序列长度间表现出可忽略甚至有害的权重共享，表明它们学习的是长度特定的孤立解决方案；3. 循环模型则表现出有效的摊销学习，通过跨长度共享权重，使得一个序列长度的数据能提升其他长度的性能。

Conclusion: 状态追踪仍然是Transformer面临的根本性挑战，即使在训练和评估分布匹配的情况下。Transformer缺乏有效的跨长度权重共享机制，导致数据效率低下，而RNN在这方面表现更优。

Abstract: Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.

</details>


### [126] [Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures](https://arxiv.org/abs/2602.18417)
*Joshua Nunley*

Main category: cs.LG

TL;DR: 提出一个在U(d)闭子群上构建隐藏状态序列模型的直接框架，通过最小公理设置从共享骨架推导出RNN和Transformer模板，并在O(d)上实验验证


<details>
  <summary>Details</summary>
Motivation: 为序列模型中的隐藏状态提供一个统一的数学框架，使其能够在李群（特别是U(d)的闭子群）上定义，从而获得更好的几何结构和数值稳定性

Method: 1. 建立最小公理设置，从共享骨架推导出循环和Transformer模板；2. 子群选择作为状态空间、切投影和更新映射的即插即用组件；3. 专门化到O(d)正交群；4. 提出切空间中的一般线性混合扩展

Result: 在Tiny Shakespeare和Penn Treebank数据集上评估了正交状态RNN和Transformer模型，在参数匹配设置下表现良好；切空间线性混合扩展提高了有限预算下的性能

Conclusion: 该框架为序列模型提供了统一的李群基础，子群选择具有灵活性，切空间扩展能提升性能，为几何深度学习中的序列建模开辟了新方向

Abstract: This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.

</details>


### [127] [Flow Actor-Critic for Offline Reinforcement Learning](https://arxiv.org/abs/2602.18015)
*Jongseong Chae,Jongeui Park,Yongjae Shin,Gyeongmin Kim,Seungyul Han,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出Flow Actor-Critic方法，通过流模型同时改进策略和保守价值函数估计，在离线RL中处理复杂多模态数据分布，在D4RL和OGBench基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的数据集分布通常呈现复杂多模态特性，传统高斯策略难以充分捕捉这种分布。需要更表达性强的策略来处理复杂多模态数据集。

Method: 提出Flow Actor-Critic方法：1）使用流模型作为策略（actor）；2）利用表达性流模型进行保守价值函数（critic）估计，防止在数据外区域的Q值爆炸；3）基于流行为代理模型设计新的critic正则化器，该模型是流策略设计的副产品。

Result: 在D4RL和最新的OGBench基准测试数据集上取得了新的最先进性能。

Conclusion: 通过联合利用流模型同时改进策略和保守价值函数估计，能够有效处理离线RL中的复杂多模态数据分布，实现性能提升。

Abstract: The dataset distributions in offline reinforcement learning (RL) often exhibit complex and multi-modal distributions, necessitating expressive policies to capture such distributions beyond widely-used Gaussian policies. To handle such complex and multi-modal datasets, in this paper, we propose Flow Actor-Critic, a new actor-critic method for offline RL, based on recent flow policies. The proposed method not only uses the flow model for actor as in previous flow policies but also exploits the expressive flow model for conservative critic acquisition to prevent Q-value explosion in out-of-data regions. To this end, we propose a new form of critic regularizer based on the flow behavior proxy model obtained as a byproduct of flow-based actor design. Leveraging the flow model in this joint way, we achieve new state-of-the-art performance for test datasets of offline RL including the D4RL and recent OGBench benchmarks.

</details>


### [128] [COMBA: Cross Batch Aggregation for Learning Large Graphs with Context Gating State Space Models](https://arxiv.org/abs/2602.17893)
*Jiajun Shen,Yufei Jin,Yi He,xingquan Zhu*

Main category: cs.LG

TL;DR: COMBA：使用状态空间模型进行大图学习，通过图上下文门控和跨批次聚合解决SSMs在图上应用的挑战


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在处理长距离依赖方面表现出色且计算成本较低，但将其应用于图结构数据（特别是大图）面临挑战，因为SSMs是序列模型，而将图转换为序列进行学习成本过高

Method: 提出COMBA框架，包含两个关键创新：1）图上下文门控 - 利用不同跳数的邻居上下文学习最佳邻居聚合控制；2）跨批次聚合 - 对每个图上下文采样节点作为批次，训练图神经网络并通过跨批次聚合信息，实现大图扩展

Result: 理论研究表明跨批次聚合相比无聚合的GNN训练保证更低的误差；在基准网络上的实验显示相比基线方法有显著的性能提升

Conclusion: COMBA成功将状态空间模型应用于大图学习，通过图上下文门控和跨批次聚合解决了序列模型处理图数据的挑战，为大规模图学习提供了有效的解决方案

Abstract: State space models (SSMs) have recently emerged for modeling long-range dependency in sequence data, with much simplified computational costs than modern alternatives, such as transformers. Advancing SMMs to graph structured data, especially for large graphs, is a significant challenge because SSMs are sequence models and the shear graph volumes make it very expensive to convert graphs as sequences for effective learning. In this paper, we propose COMBA to tackle large graph learning using state space models, with two key innovations: graph context gating and cross batch aggregation. Graph context refers to different hops of neighborhood for each node, and graph context gating allows COMBA to use such context to learn best control of neighbor aggregation. For each graph context, COMBA samples nodes as batches, and train a graph neural network (GNN), with information being aggregated cross batches, allowing COMBA to scale to large graphs. Our theoretical study asserts that cross-batch aggregation guarantees lower error than training GNN without aggregation. Experiments on benchmark networks demonstrate significant performance gains compared to baseline approaches. Code and benchmark datasets will be released for public access.

</details>


### [129] [Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors](https://arxiv.org/abs/2602.17898)
*Jingquan Yan,Yuwei Miao,Peiran Yu,Junzhou Huang*

Main category: cs.LG

TL;DR: 论文分析了注意力回归模型中PCC停滞现象，揭示了MSE优化与PCC梯度冲突、softmax注意力限制，并提出ECA方法突破PCC瓶颈。


<details>
  <summary>Details</summary>
Motivation: 注意力回归模型训练中常出现PCC停滞现象：即使MSE持续下降，PCC早期就停止改善。这一现象缺乏理论解释，阻碍了模型在形状匹配任务上的性能提升。

Method: 1. 理论分析PCC停滞原因：揭示MSE优化与PCC梯度冲突，softmax注意力在数据同质化时的限制；2. 推导凸聚合器的PCC改进极限；3. 提出外推相关性注意力(ECA)，包含新机制改善PCC优化并突破凸包限制。

Result: 在多样化基准测试中，ECA能持续突破PCC停滞，在保持MSE性能的同时显著提升相关性表现，特别是在具有挑战性的同质数据设置中效果明显。

Conclusion: PCC停滞现象源于优化动态和模型容量的根本限制。ECA通过理论指导的机制成功解决了这些问题，为注意力回归模型提供了更有效的形状匹配能力。

Abstract: Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.

</details>


### [130] [Distribution-Free Sequential Prediction with Abstentions](https://arxiv.org/abs/2602.17918)
*Jialin Yu,Moïse Blanchard*

Main category: cs.LG

TL;DR: 本文研究了一种半对抗性序列预测问题，其中学习者可以在实例被污染时弃权而不受惩罚，提出了无需先验分布知识的分布无关学习算法AbstainBoost，为VC类实现了次线性误差保证。


<details>
  <summary>Details</summary>
Motivation: 研究在流数据中对抗性实例注入的预测问题，学习者可以在实例被污染时弃权而不受惩罚。现有工作假设已知干净样本分布，这在理论和实践中都是强假设，需要探索无需先验分布知识的分布无关学习框架。

Method: 提出了基于弱学习器提升的算法AbstainBoost，采用分布无关的弃权学习方法，适用于一般VC类。算法通过提升过程处理对抗性实例，同时保证对遗忘型对手的有效性，并对线性分类器等结构化函数类扩展到自适应对手。

Result: AbstainBoost算法为一般VC类在分布无关的弃权学习中实现了次线性误差保证。对于自适应对手，结构化函数类（包括线性分类器）也能获得类似保证。同时提供了相应的下界，揭示了误分类错误与错误弃权次数之间的多项式权衡关系。

Conclusion: 无需先验分布知识也能在半对抗性序列预测中实现有效学习，AbstainBoost算法为VC类提供了分布无关的学习保证，填补了已知分布假设与完全对抗性学习之间的空白，并揭示了错误类型之间的基本权衡关系。

Abstract: We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\ instances, but at each round, the learner may also \emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $μ$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $μ$ is \emph{unknown} and propose an algorithm \textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.

</details>


### [131] [The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning](https://arxiv.org/abs/2602.18428)
*Mojtaba Sahraee-Ardakan,Mauricio Delbracio,Peyman Milanfar*

Main category: cs.LG

TL;DR: 该论文揭示了噪声无关自主生成模型（如均衡匹配和盲扩散）的工作原理，通过形式化边际能量概念，证明这些模型实际上是在边际能量上进行黎曼梯度流，而非简单的盲去噪。


<details>
  <summary>Details</summary>
Motivation: 解决自主生成模型的一个根本悖论：当噪声水平被视为随机变量时，模型优化的底层景观是什么？以及有界的噪声无关网络如何在梯度通常发散的数据流形附近保持稳定？

Method: 形式化边际能量概念，提出相对能量分解，证明自主生成模型是边际能量上的黎曼梯度流。通过分析噪声预测参数化和速度参数化的稳定性条件，揭示不同参数化方法的性能差异。

Result: 证明自主生成模型通过局部共形度量抵消几何奇异性，将无限深的势阱转化为稳定吸引子。发现噪声预测参数化存在"Jensen Gap"导致确定性盲模型灾难性失败，而速度参数化因满足有界增益条件而固有稳定。

Conclusion: 自主生成模型不是简单的盲去噪，而是边际能量上的特定黎曼梯度流。速度参数化因其吸收后验不确定性的能力而比噪声预测参数化更稳定，这解释了实践中观察到的性能差异。

Abstract: Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.

</details>


### [132] [Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere](https://arxiv.org/abs/2602.17940)
*Shogo Iwazaki*

Main category: cs.LG

TL;DR: 本文研究了高斯过程（GP）赌博机问题在频率论设置下的算法无关最坏情况下界，特别关注平方指数（SE）核函数。论文部分解决了维度相关对数因子在上下界之间的差距问题，给出了累积遗憾和简单遗憾的下界，并改进了SE核的最大信息增益上界。


<details>
  <summary>Details</summary>
Motivation: 高斯过程赌博机问题中，对于平方指数核函数，维度相关对数因子在上下界之间存在差距，这是一个尚未解决的开放性问题。本文旨在部分解决这个问题，特别是在超球形输入域下，为GP赌博机问题提供更精确的理论界限。

Method: 采用算法无关的最坏情况分析方法，在频率论设置下研究GP赌博机问题。聚焦于平方指数核函数和超球形输入域，通过理论分析推导累积遗憾和简单遗憾的下界，同时改进最大信息增益的上界。

Result: 1. 任何算法都遭受Ω(√[T(ln T)^d(ln ln T)^{-d}])的累积遗憾；2. 任何算法需要Ω(ε^{-2}(ln 1/ε)^d(ln ln 1/ε)^{-d})时间步长来找到ε-最优解；3. 改进了SE核的最大信息增益上界为O((ln T)^{d+1}(ln ln T)^{-d})。

Conclusion: 本文部分解决了GP赌博机问题中维度相关对数因子的开放性问题，在超球形输入域下证明了现有最佳算法在维度无关对数因子意义下的最优性，为平方指数核函数的理论分析提供了更精确的界限。

Abstract: We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $Ω(\sqrt{T (\ln T)^{d} (\ln \ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $Ω(ε^{-2}(\ln \frac{1}ε)^d (\ln \ln \frac{1}ε)^{-d})$ time steps to find an $ε$-optimal point. We also provide the improved $O((\ln T)^{d+1}(\ln \ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \emph{dimension-independent} logarithmic factors under a hyperspherical input domain.

</details>


### [133] [Cut Less, Fold More: Model Compression through the Lens of Projection Geometry](https://arxiv.org/abs/2602.18116)
*Olga Saukh,Dong Wang,Haris Šikić,Yun Cheng,Lothar Thiele*

Main category: cs.LG

TL;DR: 论文提出模型折叠作为无需重新校准的神经网络压缩方法，在几何理论上优于结构化剪枝，并在大规模实验中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 研究无需重新训练的神经网络压缩方法对于大规模部署至关重要。当前的结构化剪枝方法存在局限性，需要探索更优的几何感知压缩方法。

Method: 从投影几何角度分析压缩方法：结构化剪枝是轴对齐投影，而模型折叠通过权重聚类进行低秩投影。将两者形式化为正交算子，证明在一定条件下折叠具有更小的参数重构误差和功能扰动。

Result: 在超过1000个检查点上评估了ResNet18、ViT-B/32、CLIP ViT-B/32和LLaMA模型，结果显示折叠通常获得更高的压缩后精度，在中等高压缩率下优势最大。在某些特定训练设置下差距会缩小或偶尔反转。

Conclusion: 模型折叠作为一种几何感知、无需重新校准的压缩方法，在实践中通常优于剪枝，在理论上更具原则性，为神经网络压缩提供了新的替代方案。

Abstract: Compressing neural networks without retraining is vital for deployment at scale. We study calibration-free compression through the lens of projection geometry: structured pruning is an axis-aligned projection, whereas model folding performs a low-rank projection via weight clustering. We formalize both as orthogonal operators and show that, within a rank distance of one, folding provably yields smaller parameter reconstruction error, and under mild smoothness assumptions, smaller functional perturbations than pruning. At scale, we evaluate >1000 checkpoints spanning ResNet18, PreActResNet18, ViT-B/32, and CLIP ViT-B/32 on CIFAR-10 and ImageNet-1K, covering diverse training hyperparameters (optimizers, learning rates, augmentations, regularization, sharpness-aware training), as well as multiple LLaMA-family 60M and 130M parameter models trained on C4. We show that folding typically achieves higher post-compression accuracy, with the largest gains at moderate-high compression. The gap narrows and occasionally reverses at specific training setups. Our results position folding as a geometry-aware, calibration-free alternative to pruning that is often superior in practice and principled in theory.

</details>


### [134] [Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2602.18117)
*Yongjae Shin,Jongseong Chae,Jongeui Park,Youngchul Sung*

Main category: cs.LG

TL;DR: FINO：一种基于流匹配的离线到在线强化学习方法，通过注入噪声增强探索，结合熵引导采样平衡探索与利用，在有限在线预算下实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在离线RL中表现出色，但扩展到在线微调时面临挑战。当前方法通常将在线微调视为离线预训练的直接延续，未能解决关键问题，特别是如何有效探索离线数据集之外的动作空间。

Method: 提出FINO方法：1）使用流匹配策略，在策略训练中注入噪声以鼓励探索离线数据集之外的动作；2）结合熵引导采样机制，在在线微调过程中动态平衡探索与利用。

Result: 在多种具有挑战性的任务上进行实验，结果表明FINO在有限的在线预算下始终能够实现优越的性能表现。

Conclusion: FINO通过噪声注入和熵引导采样有效解决了离线到在线RL中的探索挑战，显著提高了样本效率，为生成模型在RL中的在线应用提供了有效解决方案。

Abstract: Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.

</details>


### [135] [Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition](https://arxiv.org/abs/2602.17947)
*Yubo Zhou,Jun Shu,Junmin Liu,Deyu Meng*

Main category: cs.LG

TL;DR: 该论文分析了超参数优化中梯度估计的偏差-方差分解，提出了降低方差的集成策略，并在多个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的超参数优化方法主要关注估计偏差，而忽略了数据分布带来的方差误差，这影响了性能表现。论文旨在分析超梯度估计的方差项，并提出降低方差的方法。

Method: 1. 对超梯度估计误差进行偏差-方差分解；2. 提供超梯度估计误差界的全面分析；3. 提出集成超梯度策略来有效降低HPO算法中的方差。

Result: 实验在正则化超参数学习、数据超清洗和少样本学习等任务上表明，方差降低策略改进了超梯度估计。理论分析建立了超额误差与超梯度估计之间的联系，解释了实证观察。

Conclusion: 论文通过偏差-方差分解深入分析了超梯度估计误差，提出的集成策略有效降低了方差，为理解实践中常见的验证集过拟合等现象提供了理论解释。

Abstract: Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.

</details>


### [136] [A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion](https://arxiv.org/abs/2602.17948)
*Yu Bai,Zhe Wang,Jiarui Zhang,Dong-Xiao Zhang,Yinjun Gao,Jun-Jie Zhang*

Main category: cs.LG

TL;DR: 通过对称性破缺维度扩展(SBDE)研究准确性与鲁棒性权衡的几何机制，发现插入辅助维度提升准确率但降低对抗鲁棒性，可通过掩码投影恢复鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习中的准确性与对抗鲁棒性权衡现象普遍存在，但其几何起源尚不明确，需要探究其根本机制

Method: 使用对称性破缺维度扩展(SBDE)作为受控探针，通过在输入图像中插入常数值像素来打破平移对称性；采用测试时掩码投影将插入的辅助像素重置为训练值

Result: SBDE显著提升干净准确率（如CIFAR-10上从90.47%到95.63%），但降低了对迭代白盒攻击的鲁棒性；掩码投影能有效中和攻击并恢复鲁棒性，表明脆弱性主要来自插入维度

Conclusion: 准确性与鲁棒性权衡的几何解释：优化景观加深吸引盆地以提升准确性，但不可避免地沿辅助自由度建立陡峭边界，导致对离流形扰动的脆弱敏感性

Abstract: The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\%$ to $95.63\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.

</details>


### [137] [Hardware-Friendly Input Expansion for Accelerating Function Approximation](https://arxiv.org/abs/2602.17952)
*Hu Lou,Yin-Jun Gao,Dong-Xiao Zhang,Tai-Jiao Du,Jun-Jie Zhang,Jia-Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出一种通过输入空间扩展来打破参数对称性的硬件友好方法，用于一维函数逼近，能显著加速训练并提高精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽然具有强大的通用逼近能力，但优化过程常受参数空间对称性导致的平坦损失景观阻碍，导致收敛缓慢、泛化能力差，特别是对高频分量。受物理学中对称性破缺原理启发，需要一种硬件友好的方法来改善函数逼近性能。

Method: 提出输入空间扩展方法：将原始一维输入（如x）与常数值（如π）组合形成高维向量（如[π, π, x, π, π]），在不增加网络参数数量的情况下有效打破参数对称性。评估了不同扩展维度和常数选择的影响。

Result: 在10个代表性一维函数（包括平滑、不连续、高频和不可微函数）上的实验表明，输入空间扩展显著加速训练收敛（LBFGS迭代平均减少12%），并提高逼近精度（最优5D扩展使最终MSE减少66.3%）。消融研究显示π常数表现最佳。

Conclusion: 本文提出了一种低成本、高效且硬件友好的算法设计技术，通过输入空间扩展打破参数对称性，改善神经网络在函数逼近任务中的训练效率和精度。

Abstract: One-dimensional function approximation is a fundamental problem in scientific computing and engineering applications. While neural networks possess powerful universal approximation capabilities, their optimization process is often hindered by flat loss landscapes induced by parameter-space symmetries, leading to slow convergence and poor generalization, particularly for high-frequency components. Inspired by the principle of \emph{symmetry breaking} in physics, this paper proposes a hardware-friendly approach for function approximation through \emph{input-space expansion}. The core idea involves augmenting the original one-dimensional input (e.g., $x$) with constant values (e.g., $π$) to form a higher-dimensional vector (e.g., $[π, π, x, π, π]$), effectively breaking parameter symmetries without increasing the network's parameter count. We evaluate the method on ten representative one-dimensional functions, including smooth, discontinuous, high-frequency, and non-differentiable functions. Experimental results demonstrate that input-space expansion significantly accelerates training convergence (reducing LBFGS iterations by 12\% on average) and enhances approximation accuracy (reducing final MSE by 66.3\% for the optimal 5D expansion). Ablation studies further reveal the effects of different expansion dimensions and constant selections, with $π$ consistently outperforming other constants. Our work proposes a low-cost, efficient, and hardware-friendly technique for algorithm design.

</details>


### [138] [Bayesian Online Model Selection](https://arxiv.org/abs/2602.17958)
*Aida Afshar,Yuke Zhang,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 提出一种贝叶斯在线模型选择算法，用于随机多臂老虎机问题，能在多个基础学习器中自适应选择最优者，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯老虎机中的在线模型选择问题：当环境实例从先验分布中采样时，如何设计自适应策略来探索多个老虎机学习器，并与事后最佳学习器竞争？

Method: 引入新的贝叶斯算法用于随机老虎机的在线模型选择，允许基础学习器之间共享数据以缓解先验误设问题。

Result: 理论证明贝叶斯遗憾界为O(d*M√T + √(MT))，其中M为基础学习器数量，d*为最优学习器的遗憾系数，T为时间范围。实验验证在多种随机老虎机设置中表现与最佳基础学习器相当。

Conclusion: 该贝叶斯在线模型选择算法能有效探索多个基础学习器并竞争事后最优者，数据共享机制有助于缓解先验误设问题。

Abstract: Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\left( d^* M \sqrt{T} + \sqrt{(MT)} \right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.

</details>


### [139] [Capabilities Ain't All You Need: Measuring Propensities in AI](https://arxiv.org/abs/2602.18182)
*Daniel Romero-Alvarado,Fernando Martínez-Plumed,Lorenzo Pacchiardi,Hugo Save,Siddhesh Milind Pawar,Behzad Mehrbakhsh,Pablo Antonio Moreno Casares,Ben Slater,Paolo Bova,Peter Romero,Zachary R. Tyler,Jonathan Prunty,Luning Sun,Jose Hernandez-Orallo*

Main category: cs.LG

TL;DR: 提出了首个测量AI倾向性的形式化框架，使用双逻辑函数描述模型在"理想区间"内的成功概率，结合倾向性和能力评估能更好地预测AI行为。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估主要关注能力测量，但倾向性（模型展现特定行为的趋势）对性能和安全性结果有核心影响。传统IRT方法不适合测量倾向性，因为倾向性过高或过低都可能有问题。

Method: 引入双逻辑公式化模型成功概率，当模型倾向性在"理想区间"内时成功概率高。使用配备新开发的任务无关评估标准的LLM来估计理想区间的界限。在六个LLM模型家族上应用该框架。

Result: 能够测量倾向性偏移程度及其对任务的影响。使用一个基准估计的倾向性成功预测了保留任务上的行为。结合倾向性和能力评估比单独使用任一种方法获得更强的预测能力。

Conclusion: 该框架展示了如何进行严格的倾向性测量，并证明结合倾向性和能力评估比仅使用能力评估能更好地预测AI行为，为AI评估提供了更全面的方法。

Abstract: AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an "ideal band". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.

</details>


### [140] [Improving Generalizability of Hip Fracture Risk Prediction via Domain Adaptation Across Multiple Cohorts](https://arxiv.org/abs/2602.17962)
*Shuo Sun,Meiling Zhou,Chen Zhao,Joyce H. Keyak,Nancy E. Lane,Jeffrey D. Deng,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Kui Zhang,Weihua Zhou*

Main category: cs.LG

TL;DR: 该研究评估了三种域适应方法（MMD、CORAL、DANN）及其组合在跨三个大型队列（SOF、MrOS、UKB）的髋部骨折风险预测中的表现，发现组合方法能显著提升模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型在不同队列间的泛化能力差，因为数据分布因临床站点、地区、人口统计和测量协议而异。这在髋部骨折风险预测中尤为突出，模型在一个队列（源队列）上训练后，在其他队列（目标队列）上部署时性能会显著下降。

Method: 使用三个大型队列（SOF、MrOS、UKB）共享的临床和DXA衍生特征，系统评估三种域适应方法：最大均值差异（MMD）、相关对齐（CORAL）和域对抗神经网络（DANN）及其组合。采用无结果方法，不依赖目标队列样本的已知结果。

Result: 域适应方法相比无适应基线（仅源训练）持续表现出改进性能。组合多种域适应方法带来最大且最稳定的增益。MMD、CORAL和DANN的组合方法获得最高区分度：仅男性源队列AUC为0.88，仅女性源队列AUC为0.95。

Conclusion: 整合多种域适应方法可以产生对数据集差异不敏感的特征表示。与依赖监督调优或假设目标队列样本已知结果的现有方法不同，这种无结果方法能在实际部署条件下进行模型选择，提高髋部骨折风险预测模型的泛化能力。

Abstract: Clinical risk prediction models often fail to be generalized across cohorts because underlying data distributions differ by clinical site, region, demographics, and measurement protocols. This limitation is particularly pronounced in hip fracture risk prediction, where the performance of models trained on one cohort (the source cohort) can degrade substantially when deployed in other cohorts (target cohorts). We used a shared set of clinical and DXA-derived features across three large cohorts - the Study of Osteoporotic Fractures (SOF), the Osteoporotic Fractures in Men Study (MrOS), and the UK Biobank (UKB), to systematically evaluate the performance of three domain adaptation methods - Maximum Mean Discrepancy (MMD), Correlation Alignment (CORAL), and Domain - Adversarial Neural Networks (DANN) and their combinations. For a source cohort with males only and a source cohort with females only, domain-adaptation methods consistently showed improved performance than the no-adaptation baseline (source-only training), and the use of combinations of multiple domain adaptation methods delivered the largest and most stable gains. The method that combines MMD, CORAL, and DANN achieved the highest discrimination with the area under curve (AUC) of 0.88 for a source cohort with males only and 0.95 for a source cohort with females only), demonstrating that integrating multiple domain adaptation methods could produce feature representations that are less sensitive to dataset differences. Unlike existing methods that rely heavily on supervised tuning or assume known outcomes of samples in target cohorts, our outcome-free approaches enable the model selection under realistic deployment conditions and improve generalization of models in hip fracture risk prediction.

</details>


### [141] [LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification](https://arxiv.org/abs/2602.18195)
*Hairong Chen,Yicheng Feng,Ziyu Jia,Samir Bhatt,Hengguan Huang*

Main category: cs.LG

TL;DR: LERD：一种端到端贝叶斯电生理神经动力学系统，可从多通道EEG推断潜在神经事件及其关系结构，无需事件或交互标注，用于阿尔茨海默病诊断。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病改变大脑电生理并破坏多通道EEG动力学，现有方法依赖黑盒分类器且未明确建模生成观察信号的基础动力学，需要更生理学对齐的诊断方法。

Method: 提出LERD系统，结合连续时间事件推断模块和随机事件生成过程，采用电生理启发的动力学先验指导学习，提供理论分析确保可训练性和稳定性保证。

Result: 在合成基准和两个真实世界AD EEG队列上的实验表明，LERD持续优于强基线，并产生生理学对齐的潜在摘要，有助于表征群体水平的动力学差异。

Conclusion: LERD为EEG动力学建模提供了可解释的贝叶斯框架，能够从多通道EEG中推断潜在神经事件和关系结构，在AD诊断中表现出优越性能并提供生理学洞见。

Abstract: Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.

</details>


### [142] [Student Flow Modeling for School Decongestion via Stochastic Gravity Estimation and Constrained Spatial Allocation](https://arxiv.org/abs/2602.17972)
*Sebastian Felipe R. Bundoc,Paula Joy B. Martinez,Sebastian C. Ibañez,Erika Fille T. Legara*

Main category: cs.LG

TL;DR: 菲律宾教育服务合同项目是世界上最大的教育补贴项目之一，但未能有效缓解公立学校拥挤问题。研究提出计算框架模拟学生流动模式，发现地理距离比学费成本对学校选择的限制更强四倍，且学校容量而非补贴金额是主要约束。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家面临学校拥挤问题，严重影响学习成果并加剧教育不平等。虽然补贴项目将学生从公立学校转移到私立学校可以缓解拥挤，但菲律宾教育服务合同项目等大型补贴项目因数据系统分散而效果不佳，缺乏基于科学和数据的分析来理解学生入学流动模式。

Method: 提出计算框架模拟学生流动模式和政策情景。整合近3000个机构的异构政府数据，使用负二项回归估计的随机重力模型推导距离、净学费成本和社会经济决定因素的行为弹性。这些弹性为双重约束空间分配机制提供信息，模拟不同补贴金额下的学生重新分配，同时尊重生源候选池和目的地容量限制。

Result: 研究发现地理距离对学校选择的限制比学费成本强四倍，学校容量而非补贴金额是主要约束因素。这表明仅靠补贴项目无法解决系统性过度拥挤问题。

Conclusion: 计算建模可以帮助教育政策制定者做出公平、数据驱动的决策，揭示影响有效资源分配的结构性约束，即使在资源有限的情况下也能实现更有效的资源配置。

Abstract: School congestion, where student enrollment exceeds school capacity, is a major challenge in low- and middle-income countries. It highly impacts learning outcomes and deepens inequities in education. While subsidy programs that transfer students from public to private schools offer a mechanism to alleviate congestion without capital-intensive construction, they often underperform due to fragmented data systems that hinder effective implementation. The Philippine Educational Service Contracting program, one of the world's largest educational subsidy programs, exemplifies these challenges, falling short of its goal to decongest public schools. This prevents the science-based and data-driven analyses needed to understand what shapes student enrollment flows, particularly how families respond to economic incentives and spatial constraints. We introduce a computational framework for modeling student flow patterns and simulating policy scenarios. By synthesizing heterogeneous government data across nearly 3,000 institutions, we employ a stochastic gravity model estimated via negative binomial regression to derive behavioral elasticities for distance, net tuition cost, and socioeconomic determinants. These elasticities inform a doubly constrained spatial allocation mechanism that simulates student redistribution under varying subsidy amounts while respecting both origin candidate pools and destination slot capacities. We find that geographic proximity constrains school choice four times more strongly than tuition cost and that slot capacity, not subsidy amounts, is the binding constraint. Our work demonstrates that subsidy programs alone cannot resolve systemic overcrowding, and computational modeling can empower education policymakers to make equitable, data-driven decisions by revealing the structural constraints that shape effective resource allocation, even when resources are limited.

</details>


### [143] [[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games](https://arxiv.org/abs/2602.18230)
*Jorge Carrasco Pollo,Ioannis Kapetangeorgis,Joshua Rosenthal,John Hua Yao*

Main category: cs.LG

TL;DR: 该论文对Abdelnabi等人(2024)提出的基于可评分游戏的LLM谈判基准进行了可重复性研究，发现基准虽复杂但模型比较存在模糊性，揭示了实验设置的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多智能体谈判任务中表现出巨大潜力，但该领域缺乏稳健且可泛化的评估基准。Abdelnabi等人(2024)提出了基于可评分游戏的谈判基准，但需要验证其可重复性、可用性和泛化性。

Method: 1) 复制原始实验并在更多模型上测试；2) 引入额外指标验证谈判质量和评估公平性；3) 在扩展版基准上分析更广泛模型的行为；4) 检查信息泄露检测和消融研究的完整性。

Result: 发现基准确实复杂，但模型比较存在模糊性，质疑其客观性；识别出实验设置的局限性，特别是在信息泄露检测和消融研究方面；通过分析更广泛模型的行为，为潜在用户提供了额外背景信息。

Conclusion: 研究强调了模型比较评估中上下文的重要性，指出当前谈判基准虽复杂但需要改进以提供更客观、可靠的评估框架。

Abstract: Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.

</details>


### [144] [Generating adversarial inputs for a graph neural network model of AC power flow](https://arxiv.org/abs/2602.17975)
*Robert Parker*

Main category: cs.LG

TL;DR: 论文提出优化方法生成对抗性输入点，使神经网络交流潮流预测与真实方程解产生显著误差，在14节点测试系统中验证了CANOS-PF图神经网络模型的脆弱性


<details>
  <summary>Details</summary>
Motivation: 神经网络作为交流潮流方程的代理模型在实际应用中存在安全隐患，需要验证其鲁棒性和可靠性。当前缺乏对这类模型对抗性攻击的系统研究，特别是针对电力系统特定场景的对抗样本生成方法

Method: 建立优化问题框架，生成使神经网络预测与真实交流潮流方程解误差最大的输入点。采用最小化扰动方法，在满足对抗性约束条件下寻找与训练点距离最小的对抗样本

Result: 在14节点测试系统中，生成的对抗点导致无功功率误差高达3.4标幺值，电压幅值误差达0.08标幺值。最小扰动实验显示，仅需在单个节点施加0.04标幺值的电压幅值扰动即可满足对抗约束

Conclusion: 神经网络交流潮流代理模型存在对抗性脆弱性，需要开发严格的验证方法和鲁棒训练技术来确保电力系统应用的安全性

Abstract: This work formulates and solves optimization problems to generate input points that yield high errors between a neural network's predicted AC power flow solution and solutions to the AC power flow equations. We demonstrate this capability on an instance of the CANOS-PF graph neural network model, as implemented by the PF$Δ$ benchmark library, operating on a 14-bus test grid. Generated adversarial points yield errors as large as 3.4 per-unit in reactive power and 0.08 per-unit in voltage magnitude. When minimizing the perturbation from a training point necessary to satisfy adversarial constraints, we find that the constraints can be met with as little as an 0.04 per-unit perturbation in voltage magnitude on a single bus. This work motivates the development of rigorous verification and robust training methods for neural network surrogate models of AC power flow.

</details>


### [145] [Learning Without Training](https://arxiv.org/abs/2602.17985)
*Ryan O'Dowd*

Main category: cs.LG

TL;DR: 该博士论文包含三个机器学习理论项目：1）改进监督学习函数逼近方法；2）研究部分域数据上的迁移学习；3）将信号分离技术应用于主动学习分类任务


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在大规模问题上的成功，机器学习研究蓬勃发展。本文旨在从数学理论角度解决机器学习中的三个核心问题：监督学习的函数逼近理论缺陷、部分域数据上的迁移学习、以及分类任务的效率提升。

Method: 1）提出改进监督学习函数逼近的新方法；2）研究函数提升在部分已知数据域上的理论，分析提升可定义子集和局部光滑性关系；3）将信号分离技术应用于分类任务，提出统一理论和快速算法。

Result: 1）解决了当前监督学习范式的理论缺陷；2）建立了部分域数据上迁移学习的理论框架；3）提出的主动学习分类算法在保持竞争力的准确率同时，显著提高了计算速度。

Conclusion: 该论文从数学理论角度为机器学习提供了三个重要贡献：改进了监督学习的函数逼近方法，建立了部分域迁移学习的理论框架，并将信号分离技术成功应用于主动学习分类任务，实现了准确率与效率的平衡。

Abstract: Machine learning is at the heart of managing the real-world problems associated with massive data. With the success of neural networks on such large-scale problems, more research in machine learning is being conducted now than ever before. This dissertation focuses on three different projects rooted in mathematical theory for machine learning applications.
  The first project deals with supervised learning and manifold learning. In theory, one of the main problems in supervised learning is that of function approximation: that is, given some data set $\mathcal{D}=\{(x_j,f(x_j))\}_{j=1}^M$, can one build a model $F\approx f$? We introduce a method which aims to remedy several of the theoretical shortcomings of the current paradigm for supervised learning.
  The second project deals with transfer learning, which is the study of how an approximation process or model learned on one domain can be leveraged to improve the approximation on another domain. We study such liftings of functions when the data is assumed to be known only on a part of the whole domain. We are interested in determining subsets of the target data space on which the lifting can be defined, and how the local smoothness of the function and its lifting are related.
  The third project is concerned with the classification task in machine learning, particularly in the active learning paradigm. Classification has often been treated as an approximation problem as well, but we propose an alternative approach leveraging techniques originally introduced for signal separation problems. We introduce theory to unify signal separation with classification and a new algorithm which yields competitive accuracy to other recent active learning algorithms while providing results much faster.

</details>


### [146] [PRISM: Parallel Reward Integration with Symmetry for MORL](https://arxiv.org/abs/2602.18277)
*Finn van der Knaap,Kejiang Qian,Zheng Xu,Fengxiang He*

Main category: cs.LG

TL;DR: PRISM算法解决异构多目标强化学习中目标时间频率差异问题，通过反射对称性对齐奖励通道，显著提升稀疏长时程奖励的学习效率。


<details>
  <summary>Details</summary>
Motivation: 异构多目标强化学习中，不同目标的时间频率差异巨大，导致密集目标主导学习过程，而稀疏长时程奖励的信用分配弱，样本效率低下。

Method: 提出PRISM算法：1) 使用ReSymNet模型通过残差块学习缩放机会值，调和目标间时间频率不匹配；2) 引入SymReg反射等变正则化器，强制智能体镜像并约束策略搜索到反射等变子空间。

Result: 在MuJoCo基准测试中，PRISM持续优于稀疏奖励基线和全密集奖励的oracle模型，提升Pareto覆盖和分布平衡：超体积增益超过基线100%，比oracle提升达32%。

Conclusion: PRISM通过反射对称性作为归纳偏置，有效解决异构多目标强化学习中的时间频率不匹配问题，显著改善稀疏长时程奖励的学习效率和泛化能力。

Abstract: This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\% over the baseline and up to 32\% over the oracle. The code is at \href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.

</details>


### [147] [Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers](https://arxiv.org/abs/2602.18292)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou-Ammar*

Main category: cs.LG

TL;DR: 该论文提出将解码视为一个原则性的优化层，通过正则化问题统一现有解码方法，并基于此框架设计了新的Best-of-K解码器以提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前解码方法被视为启发式的参数调整过程，缺乏理论框架。作者认为解码应该被理解为一个原则性的优化层，能够统一解释现有方法并便于设计新解码器。

Method: 提出一个统一的解码框架：在每个token位置，解决一个在概率单纯形上的正则化优化问题，平衡模型分数与结构偏好和约束。该框架能恢复贪婪解码、Softmax采样、Top-K、Top-P、Sparsemax等现有方法作为特例。

Result: 基于该框架设计了Best-of-K解码器，这是一种KL锚定的覆盖目标，针对多样本管道。实验显示BoK能显著提升性能，例如在高温采样下，Qwen2.5-Math-7B在MATH500上的准确率提升了+18.6%。

Conclusion: 解码应该被视为一个原则性的优化层而非启发式调整，统一的优化框架不仅能解释现有解码方法，还能方便地设计新的解码器，如BoK所示，这为解码方法的设计提供了理论基础。

Abstract: Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.

</details>


### [148] [Whole-Brain Connectomic Graph Model Enables Whole-Body Locomotion Control in Fruit Fly](https://arxiv.org/abs/2602.17997)
*Zehao Jin,Yaoye Zhu,Chen Zhang,Yanan Sui*

Main category: cs.LG

TL;DR: 将果蝇完整脑连接组作为神经网络控制器，用于具身强化学习中的全身运动控制，无需任务特定架构调整即可实现稳定控制。


<details>
  <summary>Details</summary>
Motivation: 探索使用真实生物脑连接组作为神经网络控制器在具身强化学习中的应用，验证静态脑连接组能否转化为有效的运动控制策略。

Method: 开发FlyGM模型，其静态结构与成年果蝇完整连接组相同，将静态连接组表示为有向消息传递图，建立从感觉输入到运动输出的生物信息流，并与生物力学果蝇模型集成。

Result: FlyGM在多样运动任务中实现稳定控制，无需任务特定架构调整。与度保持重连图、随机图和多层感知机相比，FlyGM具有更高的样本效率和优越性能。

Conclusion: 静态脑连接组可以转化为有效的神经策略，用于具身学习中的运动控制，为生物启发的神经网络设计提供了新思路。

Abstract: Whole-brain biological neural networks naturally support the learning and control of whole-body movements. However, the use of brain connectomes as neural network controllers in embodied reinforcement learning remains unexplored. We investigate using the exact neural architecture of an adult fruit fly's brain for the control of its body movement. We develop Fly-connectomic Graph Model (FlyGM), whose static structure is identical to the complete connectome of an adult Drosophila for whole-body locomotion control. To perform dynamical control, FlyGM represents the static connectome as a directed message-passing graph to impose a biologically grounded information flow from sensory inputs to motor outputs. Integrated with a biomechanical fruit fly model, our method achieves stable control across diverse locomotion tasks without task-specific architectural tuning. To verify the structural advantages of the connectome-based model, we compare it against a degree-preserving rewired graph, a random graph, and multilayer perceptrons, showing that FlyGM yields higher sample efficiency and superior performance. This work demonstrates that static brain connectomes can be transformed to instantiate effective neural policy for embodied learning of movement control.

</details>


### [149] [JPmHC Dynamical Isometry via Orthogonal Hyper-Connections](https://arxiv.org/abs/2602.18308)
*Biswa Sengupta,Jinhua Wang,Leo Brunswic*

Main category: cs.LG

TL;DR: JPmHC 提出了一种保持雅可比谱特性的流形约束超连接框架，解决了现有超连接方法中身份映射丢失导致的训练不稳定、可扩展性有限和内存开销增加的问题。


<details>
  <summary>Details</summary>
Motivation: 超连接等深度学习方法虽然通过更宽的残差流和多样化的连接模式提升了性能，但破坏了残差连接的身份映射特性，导致训练不稳定、可扩展性受限和内存开销增加。需要一种既能保持性能优势又能解决这些问题的框架。

Method: JPmHC 使用可训练的线性混合器替代身份跳跃连接，在n个并行流上操作，同时显式控制梯度条件。通过将混合器约束在算子范数有界的流形上（如双随机、Stiefel、Grassmann流形），防止梯度病态并增强稳定性。具体包括：自由概率分析预测结构化跳跃的雅可比谱；内存高效的隐式微分用于定点投影；通过Cayley变换实现Stiefel约束混合器。

Result: 在ARC-AGI上的实证评估表明，JPmHC相比双随机基线实现了更快的收敛速度、更高的准确率和更低的计算成本。该框架在保持性能的同时显著提升了训练稳定性和效率。

Conclusion: JPmHC作为HC的灵活可扩展扩展，推进了谱感知、稳定且高效的深度学习，为拓扑架构设计和基础模型演化提供了新的见解。

Abstract: Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.

</details>


### [150] [Asynchronous Heavy-Tailed Optimization](https://arxiv.org/abs/2602.18002)
*Junfei Sun,Dixi Yao,Xuchen Gong,Tahseen Rabbani,Manzil Zaheer,Tian Li*

Main category: cs.LG

TL;DR: 研究提出针对异步优化中重尾梯度噪声的改进方法，通过延迟感知学习率调度和延迟补偿提升性能，在图像和语言任务中表现优于现有同步和异步方法。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中常见的重尾随机梯度噪声会破坏优化过程稳定性。现有研究主要关注集中式或分布式同步设置下的重尾噪声处理，而重尾噪声与异步优化之间的相互作用尚未充分探索。

Method: 提出两种处理异步更新中掉队者的通信方案，基于延迟感知学习率调度和延迟补偿进行算法改进，理论分析这些修改对异步算法性能的增强效果。

Result: 在重尾噪声下的收敛保证与同步对应方法速率匹配，相比现有异步方法提高了延迟容忍度。实验表明，在图像和语言任务中，该方法在准确率/运行时间权衡和超参数鲁棒性方面优于现有同步和异步方法。

Conclusion: 提出的延迟感知学习率调度和延迟补偿方法有效解决了异步优化中的重尾梯度噪声问题，在理论和实证上都表现出优越性能，为处理Transformer等模型中的重尾噪声提供了有效的异步优化方案。

Abstract: Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.

</details>


### [151] [FedZMG: Efficient Client-Side Optimization in Federated Learning](https://arxiv.org/abs/2602.18384)
*Fotios Zantalis,Evangelos Zervas,Grigorios Koulouras*

Main category: cs.LG

TL;DR: FedZMG是一种无需参数、客户端侧的联邦学习优化算法，通过将本地梯度投影到零均值超平面来缓解非IID数据导致的客户端漂移问题，无需额外通信或超参数调优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据通常是非独立同分布的，这会导致客户端漂移，降低收敛速度和模型性能。现有的自适应优化器虽然能缓解这些问题，但通常计算复杂或通信开销大，不适合资源受限的物联网环境。

Method: 提出FedZMG算法，基于梯度中心化的思想，将本地梯度投影到零均值超平面上，有效中和异构数据分布中的"强度"或"偏差"偏移，无需额外通信或超参数调优。

Result: 理论分析证明FedZMG能降低有效梯度方差，提供比标准FedAvg更紧的收敛界。在EMNIST、CIFAR100和Shakespeare数据集上的实验表明，FedZMG在高度非IID设置下比FedAvg和FedAdam具有更好的收敛速度和最终验证准确率。

Conclusion: FedZMG是一种高效、无需参数的客户端优化算法，能有效解决联邦学习中的客户端漂移问题，特别适合资源受限的物联网环境，在非IID数据分布下表现优异。

Abstract: Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the "intensity" or "bias" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.

</details>


### [152] [Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay](https://arxiv.org/abs/2602.18401)
*Josue Casco-Rodriguez,Nanda H. Krishna,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 本文重新审视噪声循环神经网络中的重放现象，证明其符合时变梯度采样，提出隐藏状态泄漏和动量机制改善重放性能，在路径整合任务中验证了理论。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络（如海马体）能够内部生成类似刺激驱动活动的"重放"现象。现有计算模型将重放描述为朗之万采样，但新的噪声RNN重放改进方法已超越这一描述。本文旨在重新审视噪声RNN重放作为采样过程，以理解和改进其性能。

Method: 1) 在简单假设下证明重放活动应遵循的梯度是时变的且难以估计，但可激励在RNN中使用隐藏状态泄漏；2) 验证隐藏状态适应（负反馈）促进重放探索，但会导致非马尔可夫采样并减慢重放；3) 提出首个通过隐藏状态动量实现时间压缩重放的噪声路径整合RNN模型，将其与欠阻尼朗之万采样联系起来，并与适应机制结合以对抗缓慢同时保持探索。

Result: 在2D三角形路径、T型迷宫路径以及合成大鼠位置细胞活动的高维路径的路径整合任务中验证了理论发现。动量机制与适应机制结合能够有效对抗重放缓慢问题，同时保持探索能力。

Conclusion: 噪声路径整合RNN中的重放可理解为时变梯度采样过程。隐藏状态泄漏、适应和动量机制是改善重放性能的关键要素，其中动量机制能够实现时间压缩重放并解决适应机制导致的缓慢问题，为理解生物重放机制提供了新的计算框架。

Abstract: Biological neural networks (like the hippocampus) can internally generate "replay" resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.

</details>


### [153] [Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework](https://arxiv.org/abs/2602.18055)
*Jingyang Qiao,Zhizhong Zhang,Xin Tan,Jingyu Gong,Yanyun Qu,Yuan Xie*

Main category: cs.LG

TL;DR: 本文提出了Continual-NExT框架和MAGE方法，用于解决Dual-to-Dual多模态大语言模型在持续学习中的遗忘、幻觉等问题，实现了跨模态知识迁移和遗忘缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管Dual-to-Dual多模态大语言模型在即时学习和泛化方面表现出色，但在持续学习能力上存在不足，难以适应动态现实场景。传统灾难性遗忘之外，还存在幻觉、指令不遵循、跨模态知识迁移失败等挑战，且缺乏标准化持续学习框架。

Method: 提出Continual-NExT持续学习框架，包含精心设计的评估指标。并提出MAGE（通用LoRA和专家LoRA的混合与聚合）方法，通过混合通用和专家LoRA适配器来促进跨模态知识迁移并缓解遗忘。

Result: 大量实验表明，MAGE方法优于其他持续学习方法，达到了最先进的性能水平。

Conclusion: 本文为Dual-to-Dual多模态大语言模型建立了首个标准化持续学习框架，提出的MAGE方法有效解决了持续学习中的关键挑战，实现了更好的知识保留和跨模态迁移能力。

Abstract: Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.

</details>


### [154] [Unifying approach to uniform expressivity of graph neural networks](https://arxiv.org/abs/2602.18409)
*Huan Luo,Jonni Virtema*

Main category: cs.LG

TL;DR: 提出Template GNNs (T-GNNs)框架，通过聚合图模板嵌入来增强GNN表达能力，建立与Graded template modal logic (GML(T))的等价性，为分析GNN表达能力提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 标准GNN仅限于聚合直接邻居或全局信息，表达能力有限。为了增强表达能力，最近研究尝试纳入子结构信息（如环计数和子图属性），本文旨在形式化这一架构趋势。

Method: 提出Template GNNs (T-GNNs)框架，通过聚合指定图模板集合中的有效模板嵌入来更新节点特征。引入相应的Graded template modal logic (GML(T))逻辑、基于模板的互模拟概念和WL算法。

Result: 建立了T-GNNs与GML(T)逻辑之间的表达能力等价性，并展示了标准AC-GNNs及其变体可以作为T-GNNs的实例化，为分析GNN表达能力提供了统一方法。

Conclusion: T-GNNs框架统一了现有GNN变体，通过模板聚合机制增强了表达能力，为理解和分析GNN的表达能力提供了形式化基础。

Abstract: The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.

</details>


### [155] [Deepmechanics](https://arxiv.org/abs/2602.18060)
*Abhay Shinde,Aryan Amit Barsainyan,Jose Siguenza,Ankita Vaishnobi Bisoi,Rakshit Kr. Singh,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 对三种物理信息深度学习模型（HNN、LNN、SRNN）在六种动力系统上进行基准测试，发现这些模型在混沌或非保守系统中难以保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 物理信息深度学习模型已成为学习动力系统的强大工具，但缺乏跨不同物理现象的系统性基准测试，特别是在保守和耗散系统中，且现有基准测试未检查完整轨迹的稳定性。

Method: 使用DeepChem框架对三种物理信息架构（哈密顿神经网络、拉格朗日神经网络、辛循环神经网络）进行基准测试，评估六种动力系统：经典保守力学（质量弹簧系统、单摆、双摆、三体问题、弹簧摆）和非保守接触系统（弹跳球）。

Result: 所有基准测试模型在混沌或非保守系统中都难以保持稳定性，预测轨迹存在误差，需要定量和定性评估。

Conclusion: 物理信息深度学习模型需要更多研究来学习经典力学系统的鲁棒模型，特别是在处理混沌和非保守系统时。

Abstract: Physics-informed deep learning models have emerged as powerful tools for learning dynamical systems. These models directly encode physical principles into network architectures. However, systematic benchmarking of these approaches across diverse physical phenomena remains limited, particularly in conservative and dissipative systems. In addition, benchmarking that has been done thus far does not integrate out full trajectories to check stability. In this work, we benchmark three prominent physics-informed architectures such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Symplectic Recurrent Neural Networks (SRNN) using the DeepChem framework, an open-source scientific machine learning library. We evaluate these models on six dynamical systems spanning classical conservative mechanics (mass-spring system, simple pendulum, double pendulum, and three-body problem, spring-pendulum) and non-conservative systems with contact (bouncing ball). We evaluate models by computing error on predicted trajectories and evaluate error both quantitatively and qualitatively. We find that all benchmarked models struggle to maintain stability for chaotic or nonconservative systems. Our results suggest that more research is needed for physics-informed deep learning models to learn robust models of classical mechanical systems.

</details>


### [156] [Balancing Symmetry and Efficiency in Graph Flow Matching](https://arxiv.org/abs/2602.18084)
*Benjamin Honoré,Alba Carballo-Castro,Yiming Qin,Pascal Frossard*

Main category: cs.LG

TL;DR: 研究图生成模型中严格等变性的权衡：通过可控对称性调制方案放松等变性，可加速早期训练但可能导致过拟合；适当调制可在加速收敛的同时延迟过拟合，用19%的训练轮数达到更强性能。


<details>
  <summary>Details</summary>
Motivation: 图生成模型中的等变性（equivariance）虽然能确保模型尊重图的置换对称性，但严格的等变性会增加计算成本并减缓收敛速度，因为模型必须在大量可能的节点置换中保持一致。本文研究这种权衡关系。

Method: 从等变的离散流匹配模型出发，通过基于正弦位置编码和节点置换的可控对称性调制方案，在训练过程中放松模型的等变性约束。

Result: 实验表明：对称性破坏可以加速早期训练（提供更简单的学习信号），但代价是鼓励捷径解，导致过拟合（模型重复生成训练集中的图）。相反，适当调制对称性信号可以延迟过拟合同时加速收敛，使模型仅用基线训练轮数的19%就能达到更强的性能。

Conclusion: 在图生成模型中，严格等变性并非总是最优选择。通过可控的对称性调制方案，可以在保持模型性能的同时显著加速训练过程，平衡等变性的优势与计算效率之间的关系。

Abstract: Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\%$ of the baseline training epochs.

</details>


### [157] [TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs](https://arxiv.org/abs/2602.18109)
*Rong Fu,Yibo Meng,Guangzhen Yao,Jiaxuan Lu,Zeyu Zhang,Zhaolu Kang,Ziming Guo,Jia Yee Tan,Xiaojing Du,Simon James Fong*

Main category: cs.LG

TL;DR: TempoNet：基于Transformer的强化学习实时调度器，通过Urgency Tokenizer离散化时间余量，稀疏注意力机制实现近线性扩展，在工业混合关键性场景中优于传统调度器。


<details>
  <summary>Details</summary>
Motivation: 实时调度器需要在严格计算预算下处理紧截止期限，传统调度器难以应对复杂动态环境，需要更智能的调度决策框架。

Method: 结合置换不变Transformer和深度Q近似，使用Urgency Tokenizer将时间余量离散化为可学习嵌入，采用延迟感知稀疏注意力堆栈（块状top-k选择和局部敏感分块），多核映射层将Q分数转换为处理器分配。

Result: 在工业混合关键性追踪和大规模多处理器设置中，相比分析调度器和神经基线，在截止期限满足率上取得一致提升，优化稳定性更好，推理时间亚毫秒级。

Conclusion: 为基于Transformer的高吞吐量实时调度决策建立了实用框架，展示了强化学习在实时系统中的实际应用潜力。

Abstract: Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.

</details>


### [158] [Non-Stationary Online Resource Allocation: Learning from a Single Sample](https://arxiv.org/abs/2602.18114)
*Yiding Feng,Jiashuo Jiang,Yige Wang*

Main category: cs.LG

TL;DR: 研究非平稳需求下的在线资源分配问题，仅需每期一个历史样本，提出基于分位数的元策略，在两种样本信息设置下分别获得次线性和对数级遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下在线资源分配的关键挑战：需求分布任意变化且无法预测，同时算法仅需最少的历史数据（每期一个样本）即可有效运行。现有方法通常需要大量离线数据或对非平稳性有严格假设，本文旨在突破这些限制。

Method: 提出类型依赖的分位数元策略，将问题解耦为三个模块：奖励分布估计、通过流体松弛优化目标服务概率、通过动态接受阈值进行实时决策。针对两种样本信息设置（奖励观测样本和仅类型样本）设计不同策略。

Result: 对于奖励观测样本，静态阈值策略获得$\tilde{O}(\sqrt{T})$遗憾；对于仅类型样本，在最小到达概率假设下，部分自适应策略达到$\tilde{O}(\sqrt{T})$，完全自适应解析策略获得$O((\log T)^3)$的多对数遗憾，这是非平稳多资源分配的首个对数级遗憾保证。

Conclusion: 该框架在最小离线数据需求（每期一个样本）、处理任意非平稳性（无需变化预算假设）和支持多资源约束方面超越了先前工作，为非平稳在线资源分配提供了强大而实用的解决方案。

Abstract: We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.
  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\tilde{O}(\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.

</details>


### [159] [Learning Long-Range Dependencies with Temporal Predictive Coding](https://arxiv.org/abs/2602.18131)
*Tom Potter,Oliver Rhodes*

Main category: cs.LG

TL;DR: 本文提出了一种结合时间预测编码(tPC)和近似实时循环学习(RTRL)的新方法，用于训练循环神经网络，在保持预测编码局部化、可并行化特性的同时，有效处理长程时间依赖任务。


<details>
  <summary>Details</summary>
Motivation: 预测编码(PC)具有局部化、可并行化操作的特点，适合在神经形态硬件上实现能效学习。然而，将PC有效扩展到循环神经网络(RNNs)处理长程时间依赖任务一直具有挑战性。传统的BPTT方法存在非局部计算、缺乏空间并行性、需要存储大量激活历史等问题，导致高能耗。

Method: 提出了一种新颖的方法，将时间预测编码(tPC)与近似实时循环学习(RTRL)相结合，实现有效的时空信用分配。该方法保留了PC框架的局部化、可并行化和灵活特性。

Result: 在合成基准和实际任务上，该方法性能接近BPTT。在具有1500万参数的机器翻译任务中，测试困惑度为7.62（BPTT为7.49），这是tPC首次应用于如此规模的任务。

Conclusion: 该方法展示了在保持原始PC框架特性的同时学习复杂时间依赖关系的潜力，为更节能的学习系统铺平了道路。

Abstract: Predictive Coding (PC) is a biologically-inspired learning framework characterised by local, parallelisable operations, properties that enable energy-efficient implementation on neuromorphic hardware. Despite this, extending PC effectively to recurrent neural networks (RNNs) has been challenging, particularly for tasks involving long-range temporal dependencies. Backpropagation Through Time (BPTT) remains the dominant method for training RNNs, but its non-local computation, lack of spatial parallelism, and requirement to store extensive activation histories results in significant energy consumption. This work introduces a novel method combining Temporal Predictive Coding (tPC) with approximate Real-Time Recurrent Learning (RTRL), enabling effective spatio-temporal credit assignment. Results indicate that the proposed method can closely match the performance of BPTT on both synthetic benchmarks and real-world tasks. On a challenging machine translation task, with a 15-million parameter model, the proposed method achieves a test perplexity of 7.62 (vs. 7.49 for BPTT), marking one of the first applications of tPC to tasks of this scale. These findings demonstrate the potential of this method to learn complex temporal dependencies whilst retaining the local, parallelisable, and flexible properties of the original PC framework, paving the way for more energy-efficient learning systems.

</details>


### [160] [Advection-Diffusion on Graphs: A Bakry-Emery Laplacian for Spectral Graph Neural Networks](https://arxiv.org/abs/2602.18141)
*Pierre-Gabriel Berlureau,Ali Hariri,Victor Kawasaki-Borruat,Mia Zosso,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: 提出基于Bakry-Emery图拉普拉斯算子的mu-ChebNet架构，通过可学习的节点势能集成扩散和对流，在不改变图拓扑的情况下实现任务相关的传播动力学，解决GNN中的长距离信息传播问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）由于过度平滑和过度压缩问题，难以在长距离上传播信息。现有解决方案如图变换器或重布线通常计算成本高或需要改变图结构，需要一种不改变拓扑结构且高效的方法来改善长距离信息传播。

Method: 引入Bakry-Emery图拉普拉斯算子，通过可学习的节点势能集成扩散和对流，诱导任务相关的传播动力学。基于此开发mu-ChebNet谱架构，联合学习势能和切比雪夫滤波器，桥接消息传递的自适应性和谱效率。

Result: mu-ChebNet在合成长距离推理任务和真实世界基准测试中均取得一致性能提升，同时提供可解释的路由场，揭示信息如何在图中流动。理论分析显示势能如何调制谱特性，实现对关键图属性的控制。

Conclusion: Bakry-Emery拉普拉斯算子为自适应谱图学习提供了原则性和高效的基础，mu-ChebNet架构有效解决了GNN中的长距离信息传播问题，同时保持计算效率和拓扑不变性。

Abstract: Graph Neural Networks (GNNs) often struggle to propagate information across long distances due to oversmoothing and oversquashing. Existing remedies such as graph transformers or rewiring typically incur high computational cost or require altering the graph structure. We introduce a Bakry-Emery graph Laplacian that integrates diffusion and advection through a learnable node-wise potential, inducing task-dependent propagation dynamics without modifying topology. This operator has a well-behaved spectral decomposition and acts as a drop-in replacement for standard Laplacians in spectral GNNs. Building on this insight, we develop mu-ChebNet, a spectral architecture that jointly learns the potential and Chebyshev filters, effectively bridging message-passing adaptivity and spectral efficiency. Our theoretical analysis shows how the potential modulates the spectrum, enabling control of key graph properties. Empirically, mu-ChebNet delivers consistent gains on synthetic long-range reasoning tasks, as well as real-world benchmarks, while offering an interpretable routing field that reveals how information flows through the graph. This establishes the Bakry-Emery Laplacian as a principled and efficient foundation for adaptive spectral graph learning.

</details>


### [161] [Stable Long-Horizon Spatiotemporal Prediction on Meshes Using Latent Multiscale Recurrent Graph Neural Networks](https://arxiv.org/abs/2602.18146)
*Lionel Salesses,Larbi Arbaoui,Tariq Benamara,Arnaud Francois,Caroline Sainvitu*

Main category: cs.LG

TL;DR: 提出一种用于复杂几何体上时空场长期预测的深度学习框架，采用多时间尺度架构和图神经网络，在增材制造温度预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 复杂几何体上时空场的长期预测是科学机器学习中的基本挑战，特别是在增材制造中，温度历史直接影响缺陷形成和机械性能。高保真模拟计算成本高，现有机器学习方法在长期温度和梯度预测方面仍有局限。

Method: 采用时间多尺度架构，包含两个在互补时间尺度上运行的耦合模型。两个模型都依赖潜在循环图神经网络来捕捉网格上的时空动态，同时使用变分图自编码器提供紧凑的潜在表示，减少内存使用并提高训练稳定性。

Result: 在模拟粉末床熔融数据上的实验表明，该框架能够跨不同几何体进行准确且时间稳定的长期预测，优于现有基线方法。

Conclusion: 该框架虽然是在二维中评估的，但具有通用性和可扩展性，适用于具有多尺度动态的物理驱动系统和三维几何体。

Abstract: Accurate long-horizon prediction of spatiotemporal fields on complex geometries is a fundamental challenge in scientific machine learning, with applications such as additive manufacturing where temperature histories govern defect formation and mechanical properties. High-fidelity simulations are accurate but computationally costly, and despite recent advances, machine learning methods remain challenged by long-horizon temperature and gradient prediction. We propose a deep learning framework for predicting full temperature histories directly on meshes, conditioned on geometry and process parameters, while maintaining stability over thousands of time steps and generalizing across heterogeneous geometries. The framework adopts a temporal multiscale architecture composed of two coupled models operating at complementary time scales. Both models rely on a latent recurrent graph neural network to capture spatiotemporal dynamics on meshes, while a variational graph autoencoder provides a compact latent representation that reduces memory usage and improves training stability. Experiments on simulated powder bed fusion data demonstrate accurate and temporally stable long-horizon predictions across diverse geometries, outperforming existing baseline. Although evaluated in two dimensions, the framework is general and extensible to physics-driven systems with multiscale dynamics and to three-dimensional geometries.

</details>


### [162] [Unifying Formal Explanations: A Complexity-Theoretic Perspective](https://arxiv.org/abs/2602.18160)
*Shahaf Bassan,Xuanxiang Huang,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出统一框架分析ML解释的复杂度，发现全局解释具有组合优化性质（单调性、次模性、超模性），可多项式时间计算；而局部解释即使简化版本也是NP难的。


<details>
  <summary>Details</summary>
Motivation: 先前研究分别探讨了充分原因和对比原因在不同上下文（非概率vs概率、局部vs全局）中的计算复杂度，但缺乏统一分析框架。本文旨在建立统一框架，系统研究这些解释的计算复杂度特性。

Method: 提出统一概率价值函数框架，将各种解释问题都表征为该函数的最小化问题。然后分析价值函数的三个关键组合优化性质（单调性、次模性、超模性）如何影响计算复杂度。

Result: 发现全局价值函数具有单调性、次模性/超模性，使得在全局可解释性设置中，对于神经网络、决策树、树集成等多种ML模型，可以多项式时间计算各种解释。而局部价值函数缺乏这些性质，即使简化版本也是NP难计算的。

Conclusion: 通过统一框架揭示了全局和局部解释在计算复杂度上的根本差异：全局解释具有组合优化性质，可实现高效计算；而局部解释本质上是计算困难的。这为ML解释的算法设计提供了理论指导。

Abstract: Previous work has explored the computational complexity of deriving two fundamental types of explanations for ML model predictions: (1) *sufficient reasons*, which are subsets of input features that, when fixed, determine a prediction, and (2) *contrastive reasons*, which are subsets of input features that, when modified, alter a prediction. Prior studies have examined these explanations in different contexts, such as non-probabilistic versus probabilistic frameworks and local versus global settings. In this study, we introduce a unified framework for analyzing these explanations, demonstrating that they can all be characterized through the minimization of a unified probabilistic value function. We then prove that the complexity of these computations is influenced by three key properties of the value function: (1) *monotonicity*, (2) *submodularity*, and (3) *supermodularity* - which are three fundamental properties in *combinatorial optimization*. Our findings uncover some counterintuitive results regarding the nature of these properties within the explanation settings examined. For instance, although the *local* value functions do not exhibit monotonicity or submodularity/supermodularity whatsoever, we demonstrate that the *global* value functions do possess these properties. This distinction enables us to prove a series of novel polynomial-time results for computing various explanations with provable guarantees in the global explainability setting, across a range of ML models that span the interpretability spectrum, such as neural networks, decision trees, and tree ensembles. In contrast, we show that even highly simplified versions of these explanations become NP-hard to compute in the corresponding local explainability setting.

</details>


### [163] [A Deep Surrogate Model for Robust and Generalizable Long-Term Blast Wave Prediction](https://arxiv.org/abs/2602.18168)
*Danning Jing,Xinhai Chen,Xifeng Pu,Jie Hu,Chao Huang,Xuguang Chen,Qinglin Wang,Jie Liu*

Main category: cs.LG

TL;DR: RGD-Blast：一种用于高保真长期爆炸波预测的鲁棒可泛化深度代理模型，通过多尺度模块和动态-静态特征耦合机制，在保持精度的同时实现两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 爆炸波传播的时空动力学建模面临高度非线性、陡峭梯度和计算成本高的挑战。现有机器学习代理模型在复杂城市布局或分布外场景下精度下降，且自回归预测策略在长期预测中容易累积误差。

Method: 提出RGD-Blast模型，包含多尺度模块捕捉全局流动模式和局部边界交互，缓解自回归预测中的误差累积。引入动态-静态特征耦合机制，融合时变压力场与静态源和布局特征，增强分布外泛化能力。

Result: 相比传统数值方法实现两个数量级的加速，同时保持可比精度。在未见建筑布局的泛化测试中，280个连续时间步的平均RMSE低于0.01，R2超过0.89。在不同爆炸源位置和炸药重量的评估中进一步验证了泛化能力。

Conclusion: RGD-Blast显著推进了长期爆炸波建模的技术水平，为高保真、鲁棒且可泛化的爆炸波预测提供了有效的深度代理模型解决方案。

Abstract: Accurately modeling the spatio-temporal dynamics of blast wave propagation remains a longstanding challenge due to its highly nonlinear behavior, sharp gradients, and burdensome computational cost. While machine learning-based surrogate models offer fast inference as a promising alternative, they suffer from degraded accuracy, particularly evaluated on complex urban layouts or out-of-distribution scenarios. Moreover, autoregressive prediction strategies in such models are prone to error accumulation over long forecasting horizons, limiting their robustness for extended-time simulations. To address these limitations, we propose RGD-Blast, a robust and generalizable deep surrogate model for high-fidelity, long-term blast wave forecasting. RGD-Blast incorporates a multi-scale module to capture both global flow patterns and local boundary interactions, effectively mitigating error accumulation during autoregressive prediction. We introduce a dynamic-static feature coupling mechanism that fuses time-varying pressure fields with static source and layout features, thereby enhancing out-of-distribution generalization. Experiments demonstrate that RGD-Blast achieves a two-order-of-magnitude speedup over traditional numerical methods while maintaining comparable accuracy. In generalization tests on unseen building layouts, the model achieves an average RMSE below 0.01 and an R2 exceeding 0.89 over 280 consecutive time steps. Additional evaluations under varying blast source locations and explosive charge weights further validate its generalization, substantially advancing the state of the art in long-term blast wave modeling.

</details>


### [164] [SeedFlood: A Step Toward Scalable Decentralized Training of LLMs](https://arxiv.org/abs/2602.18181)
*Jihun Kim,Namhoon Lee*

Main category: cs.LG

TL;DR: SeedFlood是一种去中心化训练新方法，通过利用零阶更新的种子可重构结构，使消息大小接近零，从而消除模型大小对通信开销的影响，实现大规模模型的高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统基于gossip的方法存在两个主要问题：1）消息通信成本随模型大小增长；2）网络跳数导致信息衰减，使得全局共识效率低下。这些问题限制了去中心化训练在大规模模型上的应用。

Method: SeedFlood利用零阶更新的种子可重构结构，使消息大小接近零，然后将这些微小消息"淹没"到网络中的每个客户端。这种方法使通信开销变得可忽略且与模型大小无关。

Result: 实验表明，SeedFlood在去中心化LLM微调中始终优于基于gossip的基线方法，在泛化性能和通信效率方面都有更好表现。在大规模设置中，甚至能达到与一阶方法相当的结果。

Conclusion: SeedFlood通过消除模型大小对通信开销的影响，解决了去中心化训练的主要可扩展性瓶颈，使得在数百个客户端上训练数十亿参数模型成为可能，这在以前被认为是不切实际的。

Abstract: This work presents a new approach to decentralized training-SeedFlood-designed to scale for large models across complex network topologies and achieve global consensus with minimal communication overhead. Traditional gossip-based methods suffer from message communication costs that grow with model size, while information decay over network hops renders global consensus inefficient. SeedFlood departs from these practices by exploiting the seed-reconstructible structure of zeroth-order updates and effectively making the messages near-zero in size, allowing them to be flooded to every client in the network. This mechanism makes communication overhead negligible and independent of model size, removing the primary scalability bottleneck in decentralized training. Consequently, SeedFlood enables training in regimes previously considered impractical, such as billion-parameter models distributed across hundreds of clients. Our experiments on decentralized LLM fine-tuning demonstrate thatSeedFlood consistently outperforms gossip-based baselines in both generalization performance and communication efficiency, and even achieves results comparable to first-order methods in large scale settings.

</details>


### [165] [RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference](https://arxiv.org/abs/2602.18196)
*Xiuying Wei,Caglar Gulcehre*

Main category: cs.LG

TL;DR: RAT+是一种通过密集预训练结合全序列循环的注意力架构，可在推理时灵活切换为扩张注意力模式，仅需少量适应即可实现高效推理，同时保持接近密集注意力的性能。


<details>
  <summary>Details</summary>
Motivation: 结构化扩张注意力在推理时具有效率优势（减少FLOPs和KV缓存大小），但直接将预训练注意力模型稀疏化为扩张模式会导致严重的精度下降。需要一种既能保持密集预训练优势，又能在推理时灵活切换为高效稀疏模式的解决方案。

Method: RAT+架构在密集预训练时增强注意力机制，加入全序列循环和主动循环学习。单个模型只需密集预训练一次，推理时可灵活切换为扩张注意力（可选加局部窗口）或混合层/头组合，仅需10亿token的短适应而非重新训练单独的稀疏模型。

Result: 在1.5B参数、100B token训练下，RAT+在16倍扩张时接近密集注意力精度，64倍扩张时在常识推理和LongBench任务上仅下降2-3个点。RAT+在稀疏化为top-k块注意力时甚至优于原始注意力。在2.6B参数、200B token规模下观察到相同趋势。

Conclusion: RAT+通过密集预训练结合循环机制，实现了推理时灵活切换为高效稀疏模式的能力，在保持接近密集模型性能的同时显著提升推理效率，为大规模语言模型的高效部署提供了新方案。

Abstract: Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.

</details>


### [166] [Generative Model via Quantile Assignment](https://arxiv.org/abs/2602.18216)
*Georgi Hrusanov,Oliver Y. Chén,Julien S. Bodelet*

Main category: cs.LG

TL;DR: NeuroSQL是一种无需辅助网络的新型生成范式，通过最优传输问题隐式学习低维潜在表示，在图像质量和训练效率上优于传统生成模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度生成模型（如VAE、GAN）依赖编码器或判别器等辅助网络，导致训练不稳定、计算开销大、存在模式崩溃风险。需要一种更稳定高效的生成方法。

Method: NeuroSQL通过渐近近似将潜在变量表示为最优传输问题的解，通过求解线性分配问题学习潜在变量，然后将潜在信息传递给独立生成器，无需辅助网络。

Result: 在MNIST、CelebA、AFHQ、OASIS四个数据集上，NeuroSQL相比VAE、GAN和扩散模型：1）图像质量更好（像素距离更小，感知/结构保真度更高）；2）训练时间最短；3）在有限训练样本下仍能有效生成合成数据。

Conclusion: NeuroSQL通过分位数分配而非编码器，提供了一种快速、稳定、鲁棒的合成数据生成方法，信息损失最小，为生成建模提供了新范式。

Abstract: Deep Generative models (DGMs) play two key roles in modern machine learning: (i) producing new information (e.g., image synthesis) and (ii) reducing dimensionality. However, traditional architectures often rely on auxiliary networks such as encoders in Variational Autoencoders (VAEs) or discriminators in Generative Adversarial Networks (GANs), which introduce training instability, computational overhead, and risks like mode collapse. We present NeuroSQL, a new generative paradigm that eliminates the need for auxiliary networks by learning low-dimensional latent representations implicitly. NeuroSQL leverages an asymptotic approximation that expresses the latent variables as the solution to an optimal transportation problem. Specifically, NeuroSQL learns the latent variables by solving a linear assignment problem and then passes the latent information to a standalone generator. We benchmark its performance against GANs, VAEs, and a budget-matched diffusion baseline on four datasets: handwritten digits (MNIST), faces (CelebA), animal faces (AFHQ), and brain images (OASIS). Compared to VAEs, GANs, and diffusion models: (1) in terms of image quality, NeuroSQL achieves overall lower mean pixel distance between synthetic and authentic images and stronger perceptual/structural fidelity; (2) computationally, NeuroSQL requires the least training time; and (3) practically, NeuroSQL provides an effective solution for generating synthetic data with limited training samples. By embracing quantile assignment rather than an encoder, NeuroSQL provides a fast, stable, and robust way to generate synthetic data with minimal information loss.

</details>


### [167] [Parameter-Efficient Domain Adaptation of Physics-Informed Self-Attention based GNNs for AC Power Flow Prediction](https://arxiv.org/abs/2602.18227)
*Redwanul Karim,Changhun Kim,Timon Conrad,Nora Gourmelon,Julian Oelhaf,David Riebesel,Tomás Arias-Vergara,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 论文提出了一种参数高效的领域自适应方法，将LoRA与选择性解冻预测头结合，用于物理信息自注意力GNN，以解决交流潮流预测在电压域迁移中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有物理信息图神经网络求解器在跨电压域迁移时通常需要完全微调，这带来高重训练成本，且在目标域适应和源域保持之间的稳定性-可塑性权衡上控制有限。

Method: 采用参数高效的领域自适应方法，将LoRA应用于注意力投影层，并选择性解冻预测头，通过物理损失鼓励基尔霍夫一致性行为，同时限制适应为低秩更新。

Result: 提出的LoRA+PHead方法在多个电网拓扑中恢复了接近完全微调的精度（目标域RMSE差距为2.6×10^-4），同时减少了85.46%的可训练参数。物理残差与完全微调相当，但源域保持降低了4.7个百分点。

Conclusion: 该方法实现了可控的效率-精度权衡，能够在电压域迁移下实现参数高效且物理一致的交流潮流估计，在保持物理一致性的同时显著减少训练参数。

Abstract: Accurate AC-PF prediction under domain shift is critical when models trained on medium-voltage (MV) grids are deployed on high-voltage (HV) networks. Existing physics-informed graph neural solvers typically rely on full fine-tuning for cross-regime transfer, incurring high retraining cost and offering limited control over the stability-plasticity trade-off between target-domain adaptation and source-domain retention. We study parameter-efficient domain adaptation for physics-informed self-attention based GNN, encouraging Kirchhoff-consistent behavior via a physics-based loss while restricting adaptation to low-rank updates. Specifically, we apply LoRA to attention projections with selective unfreezing of the prediction head to regulate adaptation capacity. This design yields a controllable efficiency-accuracy trade-off for physics-constrained inverse estimation under voltage-regime shift. Across multiple grid topologies, the proposed LoRA+PHead adaptation recovers near-full fine-tuning accuracy with a target-domain RMSE gap of $2.6\times10^{-4}$ while reducing the number of trainable parameters by 85.46%. The physics-based residual remains comparable to full fine-tuning; however, relative to Full FT, LoRA+PHead reduces MV source retention by 4.7 percentage points (17.9% vs. 22.6%) under domain shift, while still enabling parameter-efficient and physically consistent AC-PF estimation.

</details>


### [168] [Variational Distributional Neuron](https://arxiv.org/abs/2602.18250)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出变分分布神经元的证明概念：将神经元构建为VAE模块，显式携带先验、摊销后验和局部ELBO，使计算单元从确定性标量变为分布，计算过程变为在约束下收缩可能性空间。


<details>
  <summary>Details</summary>
Motivation: 解决结构性张力：在序列生成中，因果性主要在符号空间组织，潜在变量常为辅助角色；而概率潜在模型捕获变化因素和不确定性，但不确定性通常由全局或参数机制承担，计算单元仍传播标量。核心问题：如果不确定性是计算的内在属性，为何计算单元不显式携带它？

Method: 提出变分分布神经元：每个神经元参数化后验分布，传播重参数化样本，通过局部ELBO的KL项正则化。分析"崩溃"模式和"活神经元"条件，通过自回归先验在时间上扩展贡献。

Result: 提出概念证明：神经元成为分布计算单元，计算变为在约束下收缩可能性空间。局部约束可测试，通过内部度量监控。单元携带的上下文信息量和时间持续性可通过不同约束局部调节。

Conclusion: 分布神经元为计算单元显式携带不确定性提供框架，解决概率约束组合的稳定、可解释和可控性问题，探讨计算原语应从确定性转向分布性的粒度问题。

Abstract: We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This "contraction" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze "collapse" modes and the conditions for a "living neuron", then extend the contribution over time via autoregressive priors over the latent, per unit.

</details>


### [169] [MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data](https://arxiv.org/abs/2602.18253)
*Xabier de Zuazo,Vincenzo Verbeni,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.LG

TL;DR: 首次展示MEG语音模型的跨任务迁移学习，通过预训练-微调框架实现数据高效的神经解码，证实感知与产生任务共享神经表征。


<details>
  <summary>Details</summary>
Motivation: 解决语音脑机接口中的数据效率问题，探索跨任务（感知与产生）的神经解码可能性，验证是否存在共享的神经处理机制。

Method: 使用Conformer架构模型，在单个被试的50小时听觉数据上预训练，然后在18名被试上仅用每人5分钟数据进行微调。评估同一任务内和跨任务（感知↔产生）的解码性能。

Result: 迁移学习带来一致改进：同一任务内准确率提升1-4%，跨任务提升达5-6%。关键发现：语音产生训练的模型能解码被动听觉任务，表明学习到的表征反映共享神经过程而非任务特异性运动活动。

Conclusion: 证明了MEG语音模型的跨任务迁移学习可行性，预训练显著提升数据效率，且感知与产生任务共享神经表征，为数据高效的语音脑机接口开辟新途径。

Abstract: Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.

</details>


### [170] [A Probabilistic Framework for LLM-Based Model Discovery](https://arxiv.org/abs/2602.18266)
*Stefan Wahl,Raphaela Schenk,Ali Farnoud,Jakob H. Macke,Daniel Gedon*

Main category: cs.LG

TL;DR: 将模型发现重新定义为概率推理问题，提出基于序列蒙特卡洛采样的ModelSMC算法，通过LLM迭代提出和优化候选模型


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的模型发现方法通常采用手工设计的启发式流程，缺乏明确的概率框架。需要将模型发现重新形式化为概率推理问题，为模型提出、细化和选择提供统一框架

Method: 提出ModelSMC算法，基于序列蒙特卡洛采样。将候选模型表示为粒子，由LLM迭代提出和细化，并使用基于似然的标准进行加权

Result: 在真实科学系统实验中，该方法发现了具有可解释机制的模型，并改进了后验预测检查。为理解和开发基于LLM的模型发现方法提供了概率视角

Conclusion: 将模型发现视为概率推理问题提供了统一的框架，ModelSMC算法展示了这一视角的有效性，为LLM在科学发现中的应用提供了新的理论基础

Abstract: Automated methods for discovering mechanistic simulator models from observational data offer a promising path toward accelerating scientific progress. Such methods often take the form of agentic-style iterative workflows that repeatedly propose and revise candidate models by imitating human discovery processes. However, existing LLM-based approaches typically implement such workflows via hand-crafted heuristic procedures, without an explicit probabilistic formulation. We recast model discovery as probabilistic inference, i.e., as sampling from an unknown distribution over mechanistic models capable of explaining the data. This perspective provides a unified way to reason about model proposal, refinement, and selection within a single inference framework. As a concrete instantiation of this view, we introduce ModelSMC, an algorithm based on Sequential Monte Carlo sampling. ModelSMC represents candidate models as particles which are iteratively proposed and refined by an LLM, and weighted using likelihood-based criteria. Experiments on real-world scientific systems illustrate that this formulation discovers models with interpretable mechanisms and improves posterior predictive checks. More broadly, this perspective provides a probabilistic lens for understanding and developing LLM-based approaches to model discovery.

</details>


### [171] [Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering](https://arxiv.org/abs/2602.18348)
*Matheus Camilo da Silva,Leonardo Arrighi,Ana Carolina Lorena,Sylvio Barbon Junior*

Main category: cs.LG

TL;DR: 本文研究了AutoClustering元模型的可解释性，通过分析22种现有方法的元特征，应用全局和局部可解释性技术来揭示元特征对聚类算法选择的影响，为提高无监督学习自动化的决策透明度提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前AutoClustering系统虽然性能良好，但其推荐结果难以解释：数据集元特征对算法和超参数选择的影响通常不透明，这限制了系统的可靠性、偏差诊断和元特征工程效率，阻碍了进一步改进。

Method: 1) 回顾22种现有方法并构建元特征结构化分类法；2) 应用全局可解释性技术（决策谓词图）评估元模型中特征重要性；3) 使用局部可解释性工具（如SHAP）分析特定聚类决策。

Result: 研究发现了元特征相关性的稳定模式，识别了当前元学习策略中可能扭曲推荐结果的结构性弱点，并为设计更可解释的AutoML系统提供了实用指导。

Conclusion: 本研究为提高无监督学习自动化的决策透明度提供了实践基础，通过增强元模型的可解释性来改善AutoClustering系统的可靠性、诊断能力和设计效率。

Abstract: AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.

</details>


### [172] [PRISM-FCP: Byzantine-Resilient Federated Conformal Prediction via Partial Sharing](https://arxiv.org/abs/2602.18396)
*Ehsan Lari,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: PRISM-FCP是一个拜占庭鲁棒的联邦保形预测框架，通过部分模型共享和统计边界机制，在训练和校准阶段都能抵御拜占庭攻击，保持标称覆盖保证并减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法只在校准阶段处理对抗行为，导致学习模型容易受到中毒更新的攻击。需要一种端到端的拜占庭鲁棒联邦保形预测框架，既能抵御训练阶段的攻击，又能保持校准阶段的鲁棒性。

Method: 1. 训练阶段：客户端通过每轮只传输D个参数中的M个来部分共享模型更新，将对手扰动的预期能量衰减M/D倍；2. 校准阶段：客户端将非保形分数转换为特征向量，计算基于距离的恶意分数，在估计保形分位数前对可疑的拜占庭贡献进行降权或过滤。

Result: 在合成数据和UCI超导数据集上的实验表明，PRISM-FCP在拜占庭攻击下能保持标称覆盖保证，避免了标准FCP中观察到的区间膨胀，同时减少了通信开销。

Conclusion: PRISM-FCP提供了一个鲁棒且通信高效的联邦不确定性量化方法，通过端到端的拜占庭鲁棒机制，在训练和校准阶段都能有效抵御攻击，实现了更好的MSE和更紧的预测区间。

Abstract: We propose PRISM-FCP (Partial shaRing and robust calIbration with Statistical Margins for Federated Conformal Prediction), a Byzantine-resilient federated conformal prediction framework that utilizes partial model sharing to improve robustness against Byzantine attacks during both model training and conformal calibration. Existing approaches address adversarial behavior only in the calibration stage, leaving the learned model susceptible to poisoned updates. In contrast, PRISM-FCP mitigates attacks end-to-end. During training, clients partially share updates by transmitting only $M$ of $D$ parameters per round. This attenuates the expected energy of an adversary's perturbation in the aggregated update by a factor of $M/D$, yielding lower mean-square error (MSE) and tighter prediction intervals. During calibration, clients convert nonconformity scores into characterization vectors, compute distance-based maliciousness scores, and downweight or filter suspected Byzantine contributions before estimating the conformal quantile. Extensive experiments on both synthetic data and the UCI Superconductivity dataset demonstrate that PRISM-FCP maintains nominal coverage guarantees under Byzantine attacks while avoiding the interval inflation observed in standard FCP with reduced communication, providing a robust and communication-efficient approach to federated uncertainty quantification.

</details>


### [173] [Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study](https://arxiv.org/abs/2602.18403)
*Orfeas Bourchas,George Papalambrou*

Main category: cs.LG

TL;DR: 提出混合建模框架，结合物理知识与数据驱动残差学习，提升船舶主机功率预测精度与泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在船舶主机功率预测中难以遵循螺旋桨定律的功率-速度关系，导致训练范围外泛化能力差

Method: 混合建模框架：物理基线组件（P=cV^n）捕捉主要功率-速度关系，非线性回归器预测残差功率（环境与操作条件偏差）

Result: 混合模型在稀疏数据区域优于纯数据驱动基线，在数据丰富区域保持相似性能，XGBoost、简单神经网络和PINN均表现良好

Conclusion: 混合框架简化学习、提升泛化、保持物理一致性，为船舶性能监控提供实用高效工具，适用于天气航线、配平优化和能效规划

Abstract: Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.

</details>


### [174] [Assigning Confidence: K-partition Ensembles](https://arxiv.org/abs/2602.18435)
*Aggelos Semoglou,John Pavlopoulos*

Main category: cs.LG

TL;DR: CAKE框架通过聚类集成评估每个点的分配稳定性与局部几何拟合一致性，提供可解释的置信度评分，帮助识别模糊点和稳定核心成员。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法缺乏对个体分配可靠性的评估，诊断指标通常反映全局质量而非特定实例的置信度。分配层面的不稳定性会影响聚类精度和鲁棒性，而集成方法虽然能改善全局一致性，但缺乏结合跨运行一致性和学习到的聚类结构几何支持的点级置信度量化工具。

Method: 提出CAKE框架，通过聚类集成计算两个互补统计量：分配稳定性（跨运行的一致性）和局部几何拟合一致性（与学习到的聚类结构的几何支持）。将这些统计量结合为[0,1]范围内的单一可解释评分。

Result: 理论分析表明CAKE在噪声下仍保持有效，并能区分稳定点与不稳定点。在合成和真实数据集上的实验表明，CAKE能有效突出模糊点和稳定核心成员，提供可用于指导过滤或优先级排序以改善聚类质量的置信度排名。

Conclusion: CAKE框架为聚类分配提供了点级置信度评估，结合了跨运行一致性和几何支持，能够识别模糊点和稳定核心成员，从而提升聚类质量和鲁棒性。

Abstract: Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [175] [The microscopic origin of droplet line tension](https://arxiv.org/abs/2602.17707)
*Franziska Aurbach,Fei Wang,Britta Nestler*

Main category: physics.flu-dyn

TL;DR: 该论文揭示了重力效应和压力诱导的界面张力变化对线张力的贡献，为纳米到毫米尺度液滴中线张力符号和量级变化提供了统一物理解释。


<details>
  <summary>Details</summary>
Motivation: 传统线张力理论难以解释实验中观察到的线张力符号和量级在纳米到毫米尺度液滴中的巨大变化，需要新的物理机制来解释这种多尺度现象。

Method: 通过考虑重力效应和压力诱导的吸附层内体积分数依赖性界面张力变化，建立了线张力的理论模型，分析了表面润湿性、吸附层初始体积分数和液滴尺寸对线张力的影响。

Result: 该机制能够解释线张力在纳米到毫米尺度液滴中符号变化和跨越数个数量级的量级变化，与实验和模拟结果一致。表观线张力的符号由表面润湿性、吸附层初始体积分数和液滴尺寸共同控制。

Conclusion: 该研究为实验观察到的线张力符号和量级变化提供了统一的物理解释，揭示了重力效应和压力诱导界面张力变化在决定线张力行为中的关键作用。

Abstract: The size dependence of the equilibrium droplet contact angle is governed by line tension. In this work, we identify a contribution to line tension arising from gravitational effects and pressure-induced changes in volume-fraction-dependent interfacial tensions within an adsorption layer. This mechanism addresses a multiscale problem of line tension in droplets ranging from nanometric to millimetric sizes that change sign and span several orders of magnitude, in agreement with experimental and simulation results. The sign of the apparent line tension is controlled by surface wettability, the initial volume fraction in the adsorption layer, and the droplet size, which also strongly influences its magnitude. Our results provide a unified physical interpretation of the experimentally observed variability in both the sign and magnitude of line tensions.

</details>


### [176] [On the turbulent wake of the actuated fluidic pinball: dynamics, bifurcations and control authority](https://arxiv.org/abs/2602.17713)
*Alicia Rodríguez-Asensio,Luigi Marra,Ignacio Andreu-Angulo,Andrea Meilán-Vila,Juan Alfaro Moreno,Guy Y. Cornejo Maceda,Bernd R. Noack,Andrea Ianiro,Stefano Discetti*

Main category: physics.flu-dyn

TL;DR: 该研究首次对流体弹球湍流尾迹进行了全面的实验和数值研究，探索了对称作动下的湍流状态，揭示了三维作动流形结构和新的低频脱落状态。


<details>
  <summary>Details</summary>
Motivation: 流体弹球作为控制导向降阶建模、非线性控制设计和多种减阻机制的标准基准，现有文献主要关注层流二维雷诺数区域，而对对称作动下的湍流状态研究不足。本研究旨在填补这一空白。

Method: 采用时间分辨粒子图像测速技术和空气动力测量，结合雷诺平均Navier-Stokes模拟，对Re=9100的湍流状态下流体弹球尾迹进行了大范围基座排气和船尾作动参数研究。

Result: 发现流体弹球湍流尾迹可近似为由两个反向叉形分岔组成的三维作动流形；在船尾作动极限下，观察到控制能力降低并出现新的低频脱落状态。

Conclusion: 该研究首次系统探索了流体弹球在湍流状态下的作动特性，揭示了其复杂的流场结构，为湍流控制提供了新的理论基础和实验验证。

Abstract: We present the first comprehensive experimental and numerical study featuring the turbulent wake of the fluidic pinball for a large actuation range. The fluidic pinball is a cluster of three equal circular cylinders centered on the vertices of an equilateral triangle, pointing upstream in uniform flow. This configuration has become a canonical benchmark for control-oriented reduced-order modeling, for nonlinear control design and for a large kaleidoscope of drag reduction mechanisms. While the literature covers well the laminar two-dimensional Reynolds number regime, we focus on unexplored terra incognita: experiments of the symmetrically actuated turbulent regime at a Reynolds number of Re=9100. In other words, the upstream cylinder is kept stationary, while the two downstream cylinders rotate with equal and opposite angular velocities. A large range of base-bleeding and boat-tailing actuation parameters is investigated with time-resolved particle image velocimetry and aerodynamic force measurement with a companion Reynolds-averaged Navier-Stokes simulation. Our results indicate that the turbulent wake of the fluidic pinball can be approximated by a three-dimensional actuation manifold comprising two inverse pitchfork bifurcations. In the boat-tailing limit, a reduced control authority with a new low-frequency shedding state is observed.

</details>


### [177] [Learning Flow Distributions via Projection-Constrained Diffusion on Manifolds](https://arxiv.org/abs/2602.17773)
*Noah Trupin,Rahul Ghosh,Aadi Jangid*

Main category: physics.flu-dyn

TL;DR: 提出一个生成建模框架，用于在任意障碍几何和边界条件下合成物理可行的二维不可压缩流，通过结合扩散模型、物理约束训练目标和投影约束反向扩散过程来保证精确的不可压缩性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的流生成方法要么忽略物理约束，要么仅施加不保证可行性的软惩罚，要么局限于固定几何形状。需要一种能够生成物理可行、满足精确不可压缩性约束的流场的方法。

Method: 方法包含三个互补组件：(1) 边界条件扩散模型处理速度场；(2) 包含散度惩罚的物理信息训练目标；(3) 通过几何感知的Helmholtz-Hodge算子强制执行精确不可压缩性的投影约束反向扩散过程。该方法被推导为在无散向量场流形上约束Langevin采样的离散近似。

Result: 在解析Navier-Stokes数据和障碍物边界流配置上的实验表明，相对于无约束、仅投影和仅惩罚的基线方法，该方法在散度、谱精度、涡量统计和边界一致性方面有显著改进。

Conclusion: 该框架在扩散模型中统一了软硬物理结构，为机器人、图形学和科学计算中的不可压缩场生成建模提供了基础。

Abstract: We present a generative modeling framework for synthesizing physically feasible two-dimensional incompressible flows under arbitrary obstacle geometries and boundary conditions. Whereas existing diffusion-based flow generators either ignore physical constraints, impose soft penalties that do not guarantee feasibility, or specialize to fixed geometries, our approach integrates three complementary components: (1) a boundary-conditioned diffusion model operating on velocity fields; (2) a physics-informed training objective incorporating a divergence penalty; and (3) a projection-constrained reverse diffusion process that enforces exact incompressibility through a geometry-aware Helmholtz-Hodge operator. We derive the method as a discrete approximation to constrained Langevin sampling on the manifold of divergence-free vector fields, providing a connection between modern diffusion models and geometric constraint enforcement in incompressible flow spaces. Experiments on analytic Navier-Stokes data and obstacle-bounded flow configurations demonstrate significantly improved divergence, spectral accuracy, vorticity statistics, and boundary consistency relative to unconstrained, projection-only, and penalty-only baselines. Our formulation unifies soft and hard physical structure within diffusion models and provides a foundation for generative modeling of incompressible fields in robotics, graphics, and scientific computing.

</details>


### [178] [Impact of Structure-Preserving Discretizations on Compressible Wall-Bounded Turbulence of Thermally Perfect Gases](https://arxiv.org/abs/2602.17781)
*Alessandro Aiello,Andrea Palumbo,Carlo De Michele,Gennaro Coppola*

Main category: physics.flu-dyn

TL;DR: 该研究通过直接数值模拟评估了在高焓超音速和高超音速湍流通道流动中，保持结构特性的离散格式（特别是熵守恒和动能守恒）对模拟可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 评估在高焓超音速和高超音速湍流通道流动中，保持结构特性的离散格式（特别是熵守恒和动能守恒）对模拟可靠性的影响，研究数值公式与热力学模型一致性对高焓可压缩湍流模拟的重要性。

Method: 使用CO₂的热完全气体模型，对超音速和高超音速湍流通道流动进行直接数值模拟，比较不同对流项离散格式（包括熵守恒、动能守恒格式）在多个马赫数下的表现。

Result: 不同格式的差异主要源于热力学变量的处理方式，随着可压缩性效应增强，这些差异逐渐影响动力学场。熵一致性与压力离散的耦合会影响高马赫数下的雷诺应力和平均流动特性。

Conclusion: 数值公式与热力学模型的一致性对高焓可压缩湍流的可靠模拟至关重要。研究系统评估了热完全气体的熵守恒离散格式在壁面流动中的应用，并考察了它们在高马赫数下对热力学-动力学耦合的影响。

Abstract: Direct numerical simulations of compressible turbulent channel flow at supersonic and hypersonic Mach numbers are performed using a thermally perfect gas model for CO$_2$. The objective is to assess the role of structure-preserving discretizations of the convective terms in high-enthalpy regimes, with particular emphasis on entropy conservation, kinetic-energy preservation, and consistency with the thermodynamic closure.
  The comparative analysis of various formulations examines their impact on robustness, thermodynamic fluctuations, and turbulence statistics across a range of Mach numbers. Differences among formulations are found to originate primarily in the treatment of thermodynamic variables and progressively influence the dynamical fields as compressibility effects intensify. In particular, the coupling between entropy consistency and pressure discretization is shown to affect Reynolds stresses and mean flow properties in high-speed regimes.
  Overall, the results indicate that consistency between the numerical formulation and the thermodynamic model contributes significantly to the reliable simulation of high-enthalpy compressible turbulence. The study systematically assesses entropy-conservative discretizations for thermally perfect gases in wall-bounded flows and examines their impact on thermodynamic-dynamic coupling at high Mach numbers.

</details>


### [179] [The influence of body anisotropy on wake characteristics and enstrophy production for prolate ellipsoids at $ \mathrm{Re}_{D} = 10,000 $](https://arxiv.org/abs/2602.17804)
*Sartaj Tanweer,Mukesh Sharma,Aditya R. Nayak,Edwin Malkiel,Michael Twardowski,Siddhartha Verma*

Main category: physics.flu-dyn

TL;DR: 通过大涡模拟研究不同长宽比椭球体在Re=10,000下的流动特性，发现高各向异性导致边界层提前分离、尾流变宽、阻力增加，并在近尾迹区产生持续的负涡量生成。


<details>
  <summary>Details</summary>
Motivation: 研究椭球体各向异性对流动分离、剪切层行为、涡量生成和局部流动拓扑的影响，理解几何形状如何改变湍流流动特性。

Method: 采用大涡模拟(LES)方法，在雷诺数Re_D=10,000下研究5种不同长宽比(AR=5:1到1:1)的椭球体，长轴垂直于来流方向。

Result: 高各向异性导致赤道面边界层提前分离，尾流变宽，压力阻力和总阻力单调增加；正涡量生成在椭球体下游约2.5D处达到最大值；高各向异性在近尾迹区产生持续的负涡量生成，特别是在5:1椭球体的极区附近。

Conclusion: 椭球体各向异性显著影响流动分离和涡量生成机制，高各向异性在极区附近产生负涡量生成，这与应变率张量中间特征向量与涡量矢量的相互作用以及不稳定焦点/压缩拓扑结构有关。

Abstract: The flow around prolate ellipsoids is investigated using Large Eddy Simulation (LES) at a Reynolds number of $ \mathrm{Re}_{D} = 10,000 $. Five different aspect ratios are considered, with $ \mathit{AR} = H/D $ varying from 5:1 to 1:1, where $D$ and $H$ represent the minor- and major- axes, respectively. The major axes of the ellipsoids are set perpendicular to the freestream, and the influence of body anisotropy on boundary layer separation, shear layer behaviour, enstrophy production, and local flow topology is examined. Higher body anisotropy leads to early separation of the boundary layer in the equatorial plane, resulting in a wider wake and a monotonic increase in pressure drag and total drag. Positive enstrophy production reaches a maximum approximately $2.5D$ downstream of the ellipsoids independently of body anisotropy. High body anisotropy leads to sustained negative enstrophy production in the near-wake, specifically near the poles of the 5:1 ellipsoid. Negative production occurs due to the distinct behaviour of streamlines near the high curvature pole, where they undergo strong anisotropic contraction in the cross-stream plane. Interactions between the vorticity vector and the intermediate eigenvector of the strain rate tensor are shown to be the primary source of enstrophy production close to the pole, and the intermediate eigenvalue exhibits negative values in this region. The negative production region is shown to be dominated by the unstable focus / compressing (UF/C) topology, which is consistent with findings from other studies that report negative enstrophy production in turbulent flows.

</details>


### [180] [Optimization of Higher-Order Harmonic Surface Tessellations for Additively Manufactured Air-to-Air Heat Exchangers](https://arxiv.org/abs/2602.17824)
*Patrick Adegbaye,Aigbe E. Awenlimobor,Justin An,Zhang Xiao,Jiajun Xu*

Main category: physics.flu-dyn

TL;DR: 该研究提出了一种通过优化框架开发的高阶谐波传热表面镶嵌结构，旨在提高空气-空气换热器的整体热工水力性能，在湍流工况下相比gyroid TPMS结构具有更高的效率和更低的压降。


<details>
  <summary>Details</summary>
Motivation: 传统空气-空气换热器存在效率降低、压力损失高和泵送功率增加的问题。虽然增材制造技术使得晶格和TPMS等仿生几何结构成为可能，但这些结构通常会导致过高的压降。因此需要开发既能增强传热又能控制压降的新型换热表面。

Method: 采用结合解析和数值方法的优化框架，开发了优化的高阶谐波传热表面镶嵌结构。通过敏感性分析确定关键控制参数，并比较了谐波结构与gyroid TPMS结构在不同流态下的性能。

Result: 敏感性分析显示，这种二次表面改性可使换热效率提升高达70%，但伴随压降增加。表面波频率比振幅对热工水力性能影响更大。在Re≥7000的湍流工况下，优化的二阶谐波结构相比gyroid结构具有更高的效率和更低的压降。gyroid结构在层流和弱湍流工况下效率较高，但压降显著高于谐波结构。

Conclusion: 优化的高阶谐波传热表面在湍流工况下表现出优于gyroid TPMS结构的综合热工水力性能，为高效低阻换热器设计提供了新思路。表面波频率是控制性能的关键参数，二次表面改性可显著提升换热效率。

Abstract: Air-to-air heat exchangers are vital for energy recovery and thermal management but often suffer from reduced effectiveness, high pressure losses, and increased pumping power in conventional designs. Advances in additive manufacturing have enabled nature-inspired geometries, such as lattice and triply periodic minimal surface (TPMS) structures, which enhance heat transfer through complex first-order surfaces but frequently cause excessive pressure drops. This study proposes an optimized higher-order harmonic heat-transfer surface tessellation developed through an optimization framework integrating analytical and numerical methods. The goal is to improve the overall thermal-hydraulic performance of the heat exchanger over a range of operating conditions. Results of sensitivity analysis show that secondary surface modification of this type can yield significant increase in the effectiveness reaching up to 70% although with associated increase in the pressure drop. The secondary surface wave frequency was found to be a more important control parameter than the amplitude in achieving high thermal-hydraulic performance. Additionally, we show that the optimized second order harmonic-type structure achieved relatively higher effectiveness and lower pressure-drop than the gyroid structure in the turbulent flow regime for Re>=7000. Although the gyroid TPMS structure had relatively higher effectiveness in the laminar and weakly turbulent flow regime, the associated pressure drop was found to be significantly higher than that of the harmonic-type structure.

</details>


### [181] [Manifestation of spurious currents and interface regularization in wind turbulence over fast-propagating waves](https://arxiv.org/abs/2602.18024)
*Hanul Hwang,Catherine Gorle*

Main category: physics.flu-dyn

TL;DR: 评估界面捕捉方法在风浪模拟中的数值误差机制，重点关注曲率估计和通量离散化对高波龄条件下湍流统计的影响。


<details>
  <summary>Details</summary>
Motivation: 快速传播波浪上的风湍流精确模拟需要抑制数值伪影的界面捕捉方法。在高波龄条件下，气水界面的数值误差可能达到与物理流动相当的量级，直接影响湍流统计预测。

Method: 使用静态和平移液滴基准测试，以及孤立波和单色波案例，系统评估广泛使用的界面捕捉技术，通过曲率估计和通量离散化分析虚假电流和界面正则化的影响。

Result: 识别并量化了主导的数值误差机制，与实验测量对比揭示了这些主要误差源在耦合风浪模拟中的表现，阐明了观测差异的数值起源。

Conclusion: 在高波龄条件下，精确的曲率和通量处理至关重要，这些发现强调了界面捕捉方法中数值精度对风浪模拟准确性的重要性。

Abstract: Accurate simulation of wind turbulence over fast-propagating waves requires interface-capturing methods that suppress numerical artifacts while accurately resolving momentum transfer across the interface. In high wave-age regimes, numerical errors at the air-water interface can reach magnitudes comparable to the physical flow, directly affecting predicted turbulence statistics. This study examines widely used interface-capturing techniques to evaluate how spurious currents and interface regularization influence wind-wave simulations through curvature estimation and flux discretization. A systematic assessment is performed using static and translating droplet benchmarks, together with solitary and monochromatic wave cases, to identify and quantify the dominant numerical error mechanisms. In addition, comparison with experimental measurements reveals how these primary error sources manifest in coupled wind-wave simulations. These findings clarify the numerical origin of the observed discrepancies and underscore the importance of accurate curvature and flux treatment in high wave-age regimes.

</details>


### [182] [Measured multiple flow states in turbulent thermal convection with aspect ratio 10](https://arxiv.org/abs/2602.18236)
*Yi-Zhen Li,Jun-Jie Huo,Xin Chen,Heng-Dong Xi*

Main category: physics.flu-dyn

TL;DR: 实验研究了大长宽比矩形腔中湍流Rayleigh-Bénard对流的多流态现象，发现流动自组织成水平堆叠的多个对流卷，卷数在3-7之间变化，且初始条件和Prandtl数会影响最终湍流状态和输运特性。


<details>
  <summary>Details</summary>
Motivation: 研究大长宽比矩形腔中湍流Rayleigh-Bénard对流的自组织行为，探索流动结构的多态性、Prandtl数对流动结构转变的影响，以及初始条件对最终湍流状态的作用。

Method: 在长宽比Γ=10的矩形腔中进行实验，覆盖Rayleigh数范围5.4×10⁷-7.2×10⁹和Prandtl数范围4.3-67.3。使用平面粒子图像测速技术测量流动结构，通过控制初始条件和扰动来研究多流态现象。

Result: 流动自组织成3-7个水平堆叠的对流卷，最常见为6个。当Pr增至67.3时，卷状结构数量显著增加，表明从卷主导流向羽流主导流的转变。全局动量输运在Pr≤18.3时遵循Re∼Ra^{0.58}Pr^{-0.97}，Pr=67.3时变为Re∼Ra^{0.72}。初始条件影响最终湍流状态，卷数越多，垂直动量和热输运越强。

Conclusion: 大长宽比矩形腔中的湍流Rayleigh-Bénard对流存在多流态现象，流动结构受Prandtl数影响发生从卷主导到羽流主导的转变，初始条件对最终湍流状态有显著影响，卷数调节全局输运特性。

Abstract: We report an experimental investigation of turbulent Rayleigh-Benard convection in a rectangular cell of large aspect ratio ($Γ= 10$) over the Rayleigh number range $5.4\times10^7 \le Ra \le 7.2\times10^9$ and Prandtl number range $4.3 \le Pr \le 67.3$. Planar particle image velocimetry measurements show that the flow self organises into several horizontally stacked convection rolls, and repeated experiments under identical parameters (both $Ra$ and $Pr$) reveal that the number of rolls varies within the range of 3 to 7 with 6 being the most probable, which demonstrates the presence of multiple flow states. When $Pr$ is increased to 67.3, the number of roll like structures increases significantly, indicating a structural transition from a roll dominated to a plume dominated flow. This transition is reflected in the global momentum transport, for $Pr \leq 18.3$ the Reynolds number scales as $Re \sim Ra^{0.58}Pr^{-0.97}$, whereas the scaling is changed to $Re \sim Ra^{0.72}$ when $Pr$ reaches 67.3. Within individual rolls, we further examine the Reynolds numbers based on horizontal and vertical velocity components, $Re_{u,\text{roll}}$ and $Re_{w,\text{roll}}$, and find that the former increases while the latter decreases with roll size (quantified as the aspect ratio of the roll $Γ_\text{roll}$) due to continuity constraints, with their ratio following $Re_{w,\text{roll}}/Re_{u,\text{roll}} \sim Γ_\text{roll}^{-0.61}$. We impose different initial flow conditions (roll structures) with controlled perturbations, and demonstrate that the initial condition can influence the final turbulent state. We show that the number of horizontally stacked rolls regulates the global transport, larger number of rolls induces greater vertical momentum and heat transfer.

</details>


### [183] [Vortex breakdown in a hydro turbine draft tube swirling jet](https://arxiv.org/abs/2602.18278)
*Artur Gesla,Eunok Yim*

Main category: physics.flu-dyn

TL;DR: 该研究在简化层流条件下，将混流式水轮机中的涡旋绳视为涡破裂相关的不稳定模态，揭示了其在尾水管中的超临界Hopf分岔特性，并发现了亚临界解范围及迟滞现象。


<details>
  <summary>Details</summary>
Motivation: 混流式水轮机中的涡旋绳（vortex rope）现象对涡轮机运行稳定性有重要影响。传统上将其视为涡破裂相关的不稳定模态，但缺乏在简化层流条件下的系统性分岔分析，特别是在考虑壁面摩擦效应时的动力学行为。

Method: 采用简化层流流动设置，在涡轮尾水管中分析涡旋绳模态。忽略壁面摩擦效应，研究轴对称基流中的螺旋涡旋绳模态分岔行为。通过分析亚临界解范围和迟滞环，探讨轴向流动变化对分岔结构的影响。

Result: 1. 螺旋涡旋绳模态在Hopf分岔中从轴对称基流超临界分岔；2. 忽略壁面摩擦时，轴线上形成大回流区，在涡轮部分负荷工况下存在亚临界解范围；3. 亚临界解促进迟滞环形成；4. 描述了轴线回流泡形成及其被外围螺旋涡旋绳破坏的规律动力学；5. 增加轴向流量至额定负荷工况时，稳态解分支在有限雷诺数下经历跨临界分岔。

Conclusion: 该研究在有限雷诺数下发现了涡旋流动的跨临界分岔，补充了先前仅在无粘极限下报告的涡旋射流跨临界分岔证据。揭示了涡轮机不同负荷工况下涡旋绳形成的分岔机制和迟滞行为，为理解水轮机流动不稳定性和优化运行提供了理论基础。

Abstract: The swirling flow in a Francis type hydropower turbine is known to be susceptible to the formation of a large helical structure, commonly referred to as a vortex rope. This vortex rope can be interpreted as an unstable mode associated with vortex breakdown. This perspective is adopted here in a simplified laminar flow setting. The helical vortex rope mode is shown to bifurcate supercritically from an axisymmetric baseflow in a Hopf bifurcation within a turbine draft tube. When wall friction effects are neglected, a large recirculation region at the axis can form and a range of subcritical solutions is identified for a flow regime corresponding to partial load of the turbine. The existence of these subcritical solutions promotes the emergence of a hysteresis loop. We further describe a regular dynamics of a formation of recirculation bubble at the axis and its destruction due to the emergence of a helical vortex rope at its periphery. Increasing the axial flow discharge towards the regime corresponding to nominal turbine load leads to an unfolding of the steady solutions branch in a transcritical bifurcation. This bifurcation takes place at finite Reynolds number and complements existing evidence of transcritical bifurcation of the swirling jet flows, previously reported only in the inviscid limit.

</details>


### [184] [Solutocapillary bubble centering in a confined ethanol plume in water](https://arxiv.org/abs/2602.18349)
*Tobias Baier,Steffen Bisswanger,Sebastian Dehe,Steffen Hardt*

Main category: physics.flu-dyn

TL;DR: 研究乙醇气泡在垂直毛细管中通过溶质毛细力（Marangoni效应）实现径向中心定位的机制


<details>
  <summary>Details</summary>
Motivation: 探索在微流体、反应器和相分离过程中实现无接触气泡聚焦的可靠机制，利用溶质毛细力控制气泡位置

Method: 实验研究乙醇注入共流水鞘流在垂直毛细管中的气泡行为，建立简化模型分析气泡径向定位机制

Result: 不同尺寸气泡可靠地沿羽流中心线排列，大气泡的轴向Marangoni效应可调节上升速度甚至诱导反向迁移

Conclusion: 溶质毛细迁移为无接触气泡聚焦提供了可靠机制，对微流体、反应器和相分离过程的气泡操控具有重要意义

Abstract: This study investigates the radial centering of gas bubbles within a buoyant plume of ethanol injected into a co-flowing water sheath flow in a vertical capillary. Bubbles nucleate in the ethanol stream due to CO$_2$ supersaturation and rapidly migrate toward the plume axis via solutocapillary (Marangoni) forces driven by interfacial tension gradients in the ethanol-water mixture. Experiments reveal that bubbles of varying sizes reliably align along the plume centerline, facilitated by steep radial concentration gradients near the plume boundary. A reduced-order model supports robust centering across a wide range of bubble radii. For larger bubbles, axial Marangoni effects modulate ascent velocities and can even induce upstream migration under transient conditions, highlighting the complex feedback between bubble dynamics and plume distortion. The results demonstrate that solutocapillary migration provides a reliable mechanism for contact-free bubble focusing, with implications for bubble manipulation in microfluidics, reactors, and phase-separation processes.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [185] [Solving and learning advective multiscale Darcian dynamics with the Neural Basis Method](https://arxiv.org/abs/2602.17776)
*Yuhe Wang,Min Wang*

Main category: math.NA

TL;DR: 提出Neural Basis Method，一种基于投影的物理信息机器学习方法，通过预定义的物理符合神经基空间和算子诱导的残差度量，实现稳定可靠的求解和参数推断。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息机器学习将控制方程作为惩罚损失，其尺度和意义由启发式平衡设定，这模糊了算子结构，混淆了近似误差与方程执行误差，使得求解和学习过程难以解释和控制。

Method: Neural Basis Method：结合预定义的物理符合神经基空间和算子诱导的残差度量，形成确定性的最小化问题。残差不仅是优化目标，还是与近似和执行相关的可计算证书，在基函数丰富时保持稳定，并产生可学习的降维坐标。

Result: 该方法在单次求解中产生准确稳健的解，并能够通过算子学习实现快速有效的参数推断。以对流多尺度Darcian动力学作为具体演示。

Conclusion: Neural Basis Method提供了一种稳定可靠的物理信息机器学习框架，通过投影方法和算子诱导度量解决了传统惩罚方法的局限性，实现了可解释和可控的求解过程。

Abstract: Physics-governed models are increasingly paired with machine learning for accelerated predictions, yet most "physics--informed" formulations treat the governing equations as a penalty loss whose scale and meaning are set by heuristic balancing. This blurs operator structure, thereby confounding solution approximation error with governing-equation enforcement error and making the solving and learning progress hard to interpret and control. Here we introduce the Neural Basis Method, a projection-based formulation that couples a predefined, physics-conforming neural basis space with an operator-induced residual metric to obtain a well-conditioned deterministic minimization. Stability and reliability then hinge on this metric: the residual is not merely an optimization objective but a computable certificate tied to approximation and enforcement, remaining stable under basis enrichment and yielding reduced coordinates that are learnable across parametric instances. We use advective multiscale Darcian dynamics as a concrete demonstration of this broader point. Our method produce accurate and robust solutions in single solves and enable fast and effective parametric inference with operator learning.

</details>


### [186] [Variational optimization approach for reconstruction of dielectric permittivity and conductivity functions using partial boundary measurements](https://arxiv.org/abs/2602.17819)
*Eric Lindström,Larisa Beilina*

Main category: math.NA

TL;DR: 提出了一种变分优化方法，用于同时重建时变麦克斯韦系统中介电常数和电导率函数的系数反问题，使用有限的电场边界观测数据。


<details>
  <summary>Details</summary>
Motivation: 解决在有限边界观测条件下，同时重建时变麦克斯韦系统中介电常数和电导率函数的系数反问题，这在电磁成像、无损检测等领域有重要应用价值。

Method: 采用变分优化方法，构建拉格朗日函数的弱形式，使用有限元重建算法。推导了拉格朗日函数的最优性条件、伴随问题的稳定性估计，以及正则化Tikhonov泛函的Frechét可微性。

Result: 通过二维和三维数值研究验证了理论分析的有效性，证明了该方法能够同时重建介电常数和电导率函数。

Conclusion: 提出的变分优化方法为解决时变麦克斯韦系统的系数反问题提供了有效的理论框架和数值实现，在有限边界观测条件下能够同时重建多个物理参数。

Abstract: We present a variational optimization approach for the solution of a coefficient inverse problem of simultaneous reconstruction of the dielectric permittivity and conductivity functions in time-dependent Maxwell's system using limited boundary observations of the electric field.
  The variational optimization approach is based on constructing a weak form of a Lagrangian which allows to use finite element based reconstruction algorithms.
  The optimality conditions for the Lagrangian and stability estimate for the adjoint problem are derived, as well as Frechét differentiability of it and of the regularized Tikhonov functional are also presented. Two- and three-dimensional numerical studies confirm our theoretical investigations.

</details>


### [187] [A finite-difference summation-by-parts, conditionally stable partitioned algorithm for conjugate heat transfer problems](https://arxiv.org/abs/2602.17843)
*Sarah Nataj,David C. Del Rey Fernández,David Brown,Rajeev Jaiman*

Main category: math.NA

TL;DR: 提出一种可证明条件稳定的弱耦合分区方案，用于求解共轭传热问题，采用高阶SBP-SAT方法在曲线坐标下实现空间离散，通过精心选择界面SAT参数保证能量稳定性。


<details>
  <summary>Details</summary>
Motivation: 开发适用于一般几何形状的高阶条件稳定分区求解器，解决共轭传热问题中不同物理域耦合的数值稳定性挑战。

Method: 使用高阶求和分部有限差分算子结合同时近似项在曲线坐标下进行空间离散，采用一阶和二阶时间离散化及界面时间外推，通过精心选择界面SAT参数确保能量稳定性。

Result: 通过探索一系列耦合参数确定了能产生稳定方案的参数范围，给出了确保稳定性的SAT参数选择逐步方法，在二维矩形域曲线网格模型问题上验证了方法的有效性。

Conclusion: 该方法能够开发适用于一般几何形状的高阶条件稳定分区求解器，为共轭传热问题的数值模拟提供了有效的解决方案。

Abstract: In this work, we design and analyze a novel, provably conditionally stable, weakly coupled partitioned scheme to solve the conjugate heat transfer (CHT) problem. We consider a model CHT problem consisting of linear advection-diffusion and heat equations, coupled at an interface through continuity of temperature and heat flux. We employ high-order summation-by-parts finite-difference operators in conjunction with simultaneous-approximation-terms (SATs) in curvilinear coordinates for spatial derivatives, combined with first- and second-order time discretizations and temporal extrapolation at the interface. Energy stability is maintained by carefully selecting SAT parameters at the interface. A range of coupling parameters are explored to identify those that yield a stable scheme, and a stepwise approach for choosing SAT parameters that ensure stability is given. The effectiveness of the method is demonstrated through numerical experiments in a two-dimensional model problem on a rectangular domain with curvilinear grids. The proposed approach enables the development of high-order, conditionally stable partitioned solvers suitable for general geometries.

</details>


### [188] [Hybrid ABBA-GMRES for Unmatched Backprojectors in Large Scale X-Ray Computerized Tomography](https://arxiv.org/abs/2602.17892)
*Ryan Bentley,Mirjeta Pasha,Malena Sabaté Landman,Luisa Yang,Jeffery Zhang*

Main category: math.NA

TL;DR: 提出混合AB-GMRES和BA-GMRES方法，将Tikhonov正则化直接融入Krylov子空间迭代，处理CT中不匹配的正向投影和反向投影算子对，改善重建质量并缓解半收敛问题。


<details>
  <summary>Details</summary>
Motivation: 大规模CT中，矩阵自由迭代方法因系统矩阵显式构建成本过高而必需。实践中，正向投影和反向投影算子常采用不同离散化或加速实现，导致不匹配的算子对。这种不匹配违反了经典最小二乘求解器的伴随假设，使迭代不再对应真正的最小二乘问题，可能表现出非对称或不一致行为。

Method: 开发混合AB-GMRES和BA-GMRES方法，将Tikhonov正则化直接融入Krylov子空间迭代。研究该方法与LSQR和LSMR混合变体的关系，考虑匹配和不匹配的反向投影算子。提出基于L曲线和广义交叉验证(GCV)的自动正则化参数选择策略。

Result: 在二维CT问题上使用GPU加速投影算子进行数值实验，结果表明：提出的混合AB-和BA-GMRES方法缓解了半收敛问题，产生更高质量的重建结果，并表现出比非混合对应方法更稳定的停止行为。

Conclusion: 混合AB-和BA-GMRES方法通过将正则化直接融入Krylov子空间迭代，有效处理CT中不匹配的投影算子对问题，改善了重建质量和算法稳定性，为大规模CT重建提供了更可靠的解决方案。

Abstract: In large-scale X-ray computed tomography (CT), matrix-free iterative methods are essential due to the prohibitive cost of explicitly forming the system matrix. In practice, forward projectors and backprojectors are often implemented with different discretizations or accelerations, leading to unmatched projector pairs. This mismatch violates the adjointness assumptions underlying classical least-squares solvers, so the resulting iterations no longer correspond to a true least-squares problem and can exhibit non-symmetric or inconsistent behavior. Prior work has explored Krylov subspace solvers such as AB-GMRES and BA-GMRES to handle unmatched projector pairs, where these methods exhibit semi-convergent regularizing behavior. Under matched conditions, AB-GMRES and BA-GMRES reduce to LSQR and LSMR, respectively. However, in the presence of unmatched projectors, AB- and BA-GMRES have been observed to yield improved reconstruction quality compared to classical least-squares solvers. In this paper, we develop hybrid AB- and BA-GMRES methods that incorporate Tikhonov regularization directly into the Krylov subspace iterations. We also examine the relationship between the proposed methods and hybrid variants of LSQR and LSMR, considering both matched and unmatched backprojectors. We propose automatic strategies for selecting regularization parameters, including approaches based on the L-curve and generalized cross validation (GCV), and analyze their effect on convergence behavior and image quality. Numerical experiments on two-dimensional CT problems using GPU-accelerated projectors demonstrate that the proposed hybrid AB- and BA-GMRES methods mitigate semi-convergence, produce higher-quality reconstructions, and exhibit more stable stopping behavior than their non-hybrid counterparts.

</details>


### [189] [Optimal error estimate of an isoparametric upwind discontinuous Galerkin method for radiation transport equation on curved domains](https://arxiv.org/abs/2602.17936)
*Changhui Yao,Yunpan Ma,Lingxiao Li*

Main category: math.NA

TL;DR: 该研究分析了等参迎风间断伽辽金方法在求解具有分段C^{k+1}光滑曲边界的辐射输运方程中的应用，通过构造辅助映射近似原始弯曲域，获得了高阶最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在具有复杂曲边界的域上求解辐射输运方程的数值方法问题。传统方法在处理弯曲边界时可能面临几何近似误差与数值离散误差的平衡问题，需要开发能够同时处理这两种误差的高阶收敛方法。

Method: 采用等参迎风间断伽辽金方法，通过构造辅助映射来近似原始弯曲域。该方法在DG范数下进行分析，全面平衡了数值离散误差和几何近似误差。

Result: 理论分析表明该方法在DG范数下具有高阶最优收敛率。二维和三维数值实验验证了理论结果的正确性，证明了该方法在处理曲边界问题时的有效性。

Conclusion: 等参迎风间断伽辽金方法能够有效处理具有分段C^{k+1}光滑曲边界的辐射输运方程，在平衡数值离散误差和几何近似误差方面表现出色，具有高阶最优收敛特性。

Abstract: This work investigates the isoparametric upwind discontinuous Galerkin method for solving the radiation transport equation defined on a bounded domain $D$ with a piecewise $C^{k+1}$ smooth curved boundary. An auxiliary mapping is constructed to approximate the original curved domain. The analysis delineates a high-order optimal convergence rate under the DG norm, which comprehensively balances the errors stemming from the numerical discretization and the geometric approximation. Two- and three-dimensional numerical experiments validate the theoretical results.

</details>


### [190] [Mathematical and numerical study on the ground states of rotating spin-orbit coupled spin-1 Bose-Einstein condensates](https://arxiv.org/abs/2602.17950)
*Jing Wang,Wei Yang,Yongjun Yuan,Yong Zhang*

Main category: math.NA

TL;DR: 研究三组分旋转自旋轨道耦合自旋-1玻色-爱因斯坦凝聚体的基态，通过理论证明基态存在性，提出高效预条件非线性共轭梯度算法进行数值计算，并分析各种物理参数对基态的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旋转自旋轨道耦合多组分玻色-爱因斯坦凝聚体的基态性质，这类系统具有丰富的物理现象（如涡旋结构），但理论分析和数值计算都面临挑战，需要发展有效的数学方法和计算算法。

Method: 1. 理论分析：严格证明基态存在性，推导维里恒等式和SOC能量负性等解析性质；2. 数值算法：提出预条件非线性共轭梯度算法，采用有界矩形域截断和傅里叶谱方法，结合自适应步长控制和级联多重网格、快速傅里叶变换优化效率。

Result: 1. 成功证明基态存在性并建立解析性质；2. 开发的高效算法能够遍历各种初始猜测，验证空间谱精度；3. 数值实验揭示了局域相互作用、旋转、自旋轨道耦合和外势对基态的影响，观测到巨涡旋和U形涡旋线等有趣物理现象。

Conclusion: 本文为三组分旋转SOC自旋-1 BEC系统提供了完整的理论分析和高效数值框架，不仅证明了基态存在性，还开发了高性能计算算法，揭示了丰富的物理现象，为相关量子多体系统的研究提供了有力工具。

Abstract: In this article, we study mathematically and numerically the ground states of three-component rotating spin-orbit coupled (SOC) spin-1 Bose-Einstein condensates modeled by the coupled Gross-Pitaevskii equations (CGPEs). Firstly, we rigorously prove existence result of the ground state and derive some analytical properties, including the virial identity and negativity of SOC energy. Secondly, we propose an efficient and accurate preconditioned nonlinear conjugate gradient (PCG) algorithm to compute the ground states. We truncate the whole space into a bounded rectangular domain and readily apply the Fourier spectral method to approximate the wave function. The PCG method is successfully adapted with appropriate modifications to the adaptive step size control strategy for the one-parameter energy minimization problem and to the choice of preconditioners, achieving great performance in terms of accuracy and efficiency. Lastly, we carry out extensive numerical experiments to verify the existence and property results of the ground states, confirm the spatial spectral accuracy by traversing the most commonly-used initial guesses for each component thanks to its great efficiency, which is also attributed to a utilization of cascadic multigrid and discrete Fast Fourier Transform (FFT). Moreover, we investigate the effects of local interaction, rotation and spin-orbit coupling and external trapping potential on the ground state, and unveil some interesting physical phenomena, such as giant vortex and U-shape vortex line.

</details>


### [191] [Strong convergence of finite element schemes for the stochastic Landau--Lifshitz--Bloch equation](https://arxiv.org/abs/2602.18021)
*Agus L. Soenjaya*

Main category: math.NA

TL;DR: 本文针对高维有界铁磁体中的随机Landau-Lifshitz-Bloch方程，建立了多种全离散有限元格式的强收敛性，并给出了显式收敛率，同时改进了概率收敛结果。


<details>
  <summary>Details</summary>
Motivation: 随机Landau-Lifshitz-Bloch方程是描述有界铁磁体高温磁化动力学的关键模型，但现有数值方法的收敛性分析不够完善，需要建立严格的收敛理论并改进现有结果。

Method: 采用半隐式和隐式全离散有限元格式，结合局部化误差估计和新的指数矩界分析，对随机偏微分方程进行数值分析，并进行了数值实验验证。

Result: 在初始数据充分正则的假设下，证明了数值格式在L^2(Ω)意义下的强收敛性，给出了显式收敛率，改进了概率收敛结果，并在小噪声条件下得到了一维情形的均方指数稳定性和不变测度唯一性。

Conclusion: 本文为随机Landau-Lifshitz-Bloch方程提供了严格的数值分析框架，建立了多种有限元格式的收敛理论，改进了现有结果，并通过数值实验验证了理论发现。

Abstract: The dynamics of magnetisation in a bounded ferromagnet in $\mathbb{R}^d$ ($d=1,2$) at high temperatures can be described by the stochastic Landau--Lifshitz--Bloch (sLLB) equation, which is a vector-valued quasilinear stochastic partial differential equation. In this paper, assuming adequate regularity of the initial data, we establish strong convergence in $L^2(Ω)$ of several semi-implicit and implicit fully discrete finite element schemes for the sLLB equation, together with explicit convergence rates. The analysis relies on localised error estimates and new exponential moment bounds for the exact solution. As a by-product, these moment bounds yield mean-square exponential stability of solutions and uniqueness of the invariant measure in one spatial dimension under a small noise assumption. We also sharpen existing convergence-in-probability results for the numerical schemes. Numerical experiments are presented to illustrate and support the theoretical findings.

</details>


### [192] [Computing accurate singular values using a mixed-precision one-sided Jacobi algorithm](https://arxiv.org/abs/2602.18134)
*Zhengbo Zhou,Françoise Tisseur,Marcus Webb*

Main category: math.NA

TL;DR: 提出一种混合精度预条件单边Jacobi算法，通过低精度计算预条件子、高精度应用，获得比标准SVD算法更小的奇异值相对前向误差


<details>
  <summary>Details</summary>
Motivation: 传统SVD算法在病态矩阵上误差较大，需要更精确的奇异值计算方法。混合精度方法可以利用低精度计算预条件子来加速收敛，同时保持高精度计算以获得更准确的结果

Method: 1. 使用低精度计算预条件子；2. 在高精度下应用预条件子；3. 在工作精度下使用单边Jacobi算法计算SVD；4. 提出并分析两种构造有效预条件子的方法

Result: 1. 理论分析得到比标准SVD算法更小的奇异值相对前向误差界；2. 数值实验显示比LAPACK的DGESVJ、DGEJSV和MATLAB的svd函数更准确，尤其在病态矩阵上；3. 加速Jacobi迭代收敛，主要计算成本来自一次高精度矩阵乘法

Conclusion: 该混合精度预条件单边Jacobi算法在精度上显著优于现有方法，如果硬件或软件能优化高精度矩阵乘法瓶颈，其速度可与DGEJSV相当，但精度更高

Abstract: We present a relative forward error analysis of a mixed-precision preconditioned one-sided Jacobi algorithm, analogous to a two-sided version introduced in [N. J. Higham, F. Tisseur, M. Webb and Z. Zhou, SIAM J. Matrix Anal. Appl. 46 (2025), pp. 2423-2448], which uses low precision to compute the preconditioner, applies it in high precision, and computes the singular value decomposition using the one-sided Jacobi algorithm at working precision. Our analysis yields smaller relative forward error bounds for the computed singular values than those of standard SVD algorithms. We present and analyse two approaches for constructing effective preconditioners. Our numerical experiments support the theoretical results and demonstrate that our algorithm achieves smaller relative forward errors than the LAPACK routines $\texttt{DGESVJ}$ and $\texttt{DGEJSV}$, as well as the MATLAB function $\texttt{svd}$, particularly for ill-conditioned matrices. Timing tests show that our approach accelerates the convergence of the Jacobi iterations and that the dominant cost arises from a single high-precision matrix-matrix multiplication. With improved software or hardware support for this bottleneck, our algorithm would be faster than the LAPACK one-sided Jacobi algorithm $\texttt{DGESVJ}$ and comparable in speed to the state-of-the-art preconditioned one-sided Jacobi algorithm $\texttt{DGEJSV}$, but much more accurate.

</details>


### [193] [Theoretical insights on the residual transformation from bi-conjugate gradient into bi-conjugate residual via a smoothing scheme](https://arxiv.org/abs/2602.18159)
*Arisa Kawase,Kensuke Aihara*

Main category: math.NA

TL;DR: 本文证明了Bi-CG方法通过残差平滑技术转换得到的算法具有与原Bi-CR方法相同的双正交性质，提供了理论验证和更简洁的转换算法。


<details>
  <summary>Details</summary>
Motivation: 在之前的研究中，作者观察到Bi-CG残差可以通过平滑方案转换为Bi-CR残差，但只提供了启发式和实验观察，缺乏理论支撑。本研究旨在从理论上深入分析这种转换关系。

Method: 采用理论证明方法，证明从Bi-CG方法通过残差平滑技术转换得到的算法具有与原Bi-CR方法相同的双正交性质。同时提出了更简洁的转换算法。

Result: 成功证明了转换算法的双正交性质与原始Bi-CR方法相同，提供了理论验证。通过数值例子展示了转换算法的有效性。

Conclusion: 本研究为Bi-CG和Bi-CR方法之间的残差转换提供了理论有效性，补充了先前研究的不足，建立了两种方法之间转换的理论基础。

Abstract: Bi-conjugate gradient (Bi-CG) and bi-conjugate residual (Bi-CR) methods are underlying iterative solvers for linear systems with nonsymmetric matrices. Residual smoothing is a standard technique for obtaining smooth convergence behavior of residual norms; additionally, it represents the transformation between iterative methods. For example, the residuals of the CR method can be obtained by applying a smoothing scheme to those of the CG method for symmetric linear systems. Based on this relationship, the transformation from Bi-CG residuals to Bi-CR residuals using a smoothing scheme was examined in our previous study [Kawase, A., Aihara, K.: Transformation from Bi-CG into Bi-CR Using a Residual Smoothing-like Scheme. AIP Conference Proceedings (2026)]; however, we only provided heuristic and experimental observations. In the present study, we provide a detailed discussion on the theoretical aspects of these transformations. Specifically, we prove that the resulting algorithm transformed from the Bi-CG method using the residual smoothing technique has the same bi-orthogonal properties as those of the original Bi-CR method. We also present a more concise transformation algorithm and its numerical example. These analyses complement our previous study and provide theoretical validity of the residual transformation between the Bi-CG and Bi-CR methods.

</details>


### [194] [A Parametric Finite Element Approach for an Anisotropic Multi-Phase Mullins-Sekerka Problem with Kinetic Undercooling](https://arxiv.org/abs/2602.18226)
*Tokuhiro Eto,Harald Garcke,Robert Nürnberg*

Main category: math.NA

TL;DR: 提出了一种用于各向异性多相Mullins-Sekerka问题的尖锐界面公式，并开发了无条件稳定的非拟合有限元方法，可模拟多个冰晶的演化。


<details>
  <summary>Details</summary>
Motivation: 研究各向异性多相Mullins-Sekerka问题，该问题涉及具有动力学过冷的相界面演化，特别是多个冰晶的生长和相互作用。

Method: 推导了变分公式，引入了完全离散的非拟合有限元方法，其中移动界面的近似与体方程使用的三角剖分无关。

Result: 该方法被证明是无条件稳定的，数值示例展示了该方法的能力，特别是能够模拟具有连接点的多个冰晶的演化。

Conclusion: 所提出的非拟合有限元方法为各向异性多相Mullins-Sekerka问题提供了有效的数值求解框架，能够处理复杂的多相界面演化问题。

Abstract: We consider a sharp interface formulation for an anisotropic multi-phase Mullins-Sekerka problem with kinetic undercooling. The flow is characterized by a cluster of surfaces evolving such that the total surface energy plus a weighted sum of the volumes of the enclosed phases decreases in time. Upon deriving a suitable variational formulation, we introduce a fully discrete unfitted finite element method. In this approach, the approximations of the moving interfaces are independent of the triangulations used for the equations in the bulk. Our method can be shown to be unconditionally stable. Several numerical examples demonstrate the capabilities of the introduced method. In particular, it is demonstrated that the evolution of multiple ice crystals with junctions can be modeled using the proposed approach.

</details>


### [195] [Well-posedness and time stepping adaptivity for a class of collocation discretisations of time-fractional subdiffusion equations](https://arxiv.org/abs/2602.18404)
*Sebastian Franz,Natalia Kopteva*

Main category: math.NA

TL;DR: 论文研究了使用配置方法离散时间分数阶抛物方程，分析了任意阶配置解的存在唯一性条件，并探讨了后验误差估计和自适应时间步长算法的应用性能。


<details>
  <summary>Details</summary>
Motivation: 时间分数阶抛物方程在科学工程中广泛应用，但数值求解面临挑战。配置方法假设Caputo导数分段多项式，但高阶配置解的存在唯一性条件尚不明确，且后验误差估计和自适应步长算法在分数阶方程中的应用需要深入研究。

Method: 采用配置方法离散时间分数阶抛物方程，假设Caputo导数分段多项式。对任意阶m≥0和任意配置点选择，推导了配置解存在唯一的充分条件。在此基础上，研究了后验误差估计和自适应时间步长算法在分数阶方程中的应用。

Result: 建立了任意阶配置解存在唯一的充分条件，验证了配置方法在时间分数阶抛物方程中的适用性。成功将后验误差估计和自适应时间步长算法扩展到分数阶方程，提高了数值求解的效率和精度。

Conclusion: 配置方法为时间分数阶抛物方程提供了有效的数值求解框架，建立的存在唯一性条件保证了算法的可靠性，后验误差估计和自适应步长算法进一步提升了求解效率，为分数阶偏微分方程的数值计算提供了重要工具。

Abstract: Time-fractional parabolic equations with a Caputo time derivative of order $α\in(0,1)$ are discretised in time using collocation methods, which assume that the Caputo derivative of the computed solution is piecewise-polynomial. For such discretisations of any order $m\ge 0$, with any choice of collocation points, we give sufficient conditions for existence and uniqueness of collocation solutions. Furthermore, we investigate the applicability and performance of such schemes in the context of the a-posteriori error estimation and adaptive time stepping algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [196] [Epistemic Traps: Rational Misalignment Driven by Model Misspecification](https://arxiv.org/abs/2602.17676)
*Xingcheng Xu,Jingjing Qu,Qiaosheng Zhang,Chaochao Lu,Yanqing Yang,Na Zou,Xia Hu*

Main category: cs.AI

TL;DR: 论文提出AI安全问题的根源在于模型误设，而非训练缺陷。通过经济学理论建立框架，证明不安全行为是结构必然，安全是离散相而非奖励连续函数，需要转向主观模型工程。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全范式将大语言模型和AI代理的行为病理（如奉承、幻觉、策略性欺骗）视为训练缺陷，缺乏统一理论框架解释其出现和稳定性。需要从根本理论上理解这些行为为何持续存在。

Method: 将经济学中的Berk-Nash理性化理论适配到人工智能领域，建立严格框架，将代理建模为在错误主观世界模型下优化。通过六个最先进模型家族的行为实验验证理论预测，生成相图精确映射安全行为的拓扑边界。

Result: 证明广泛观察到的失败是结构必然：不安全行为要么作为稳定错位均衡出现，要么作为振荡循环出现；策略性欺骗作为"锁定"均衡或通过认知不确定性持续存在。安全是离散相，由代理的认知先验决定而非奖励大小。

Conclusion: 安全问题的根源在于模型误设，不安全行为是数学上可理性化的。需要从操纵环境奖励转向塑造代理对现实的解释，即主观模型工程，这是实现稳健对齐的必要条件，标志着范式转变。

Abstract: The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a "locked-in" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.

</details>


### [197] [Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge](https://arxiv.org/abs/2602.17826)
*Marcelo Labre*

Main category: cs.AI

TL;DR: 本文研究利用形式化领域本体（OpenMath）通过检索增强生成提升语言模型在数学推理中的可靠性，发现高质量本体检索能提升性能，但无关上下文会降低性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型存在幻觉、脆弱性和缺乏形式化基础等根本限制，这在需要可验证推理的高风险专业领域（如数学）中尤为成问题。作者旨在探索形式化领域本体是否能通过检索增强生成提升语言模型的可靠性。

Method: 采用神经符号管道，利用OpenMath本体，结合混合检索和交叉编码器重排序技术，将相关定义注入模型提示中。在MATH基准上使用三个开源模型进行评估。

Result: 评估显示：当检索质量高时，本体引导的上下文能提升模型性能；但当检索到无关上下文时，反而会主动降低性能。这突显了神经符号方法的潜力和挑战。

Conclusion: 形式化领域本体在提升语言模型可靠性方面具有潜力，但检索质量至关重要。无关的上下文会损害性能，表明神经符号方法需要精心设计的检索机制来确保有效性。

Abstract: Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.

</details>


### [198] [The Token Games: Evaluating Language Model Reasoning with Puzzle Duels](https://arxiv.org/abs/2602.17831)
*Simon Henniger,Gabriel Poesia*

Main category: cs.AI

TL;DR: TTG是一个基于编程谜题的自挑战评估框架，模型通过互相出题进行对决，无需人工标注即可评估推理能力


<details>
  <summary>Details</summary>
Motivation: 现有评估方法面临两个主要问题：1）人工标注高质量难题成本高昂，尤其是需要博士级领域知识的题目；2）难以区分模型是真正推理还是见过类似训练数据。需要一种无法被设计饱和的评估范式

Method: 受16世纪数学决斗启发，设计TTG框架：模型通过创建编程谜题互相挑战（给定返回布尔值的Python函数，找到使函数返回True的输入）。通过两两对决计算Elo评分，相对比较模型能力

Result: 评估了10个前沿模型，TTG排名与Humanity's Last Exam等现有基准高度一致，且无需人工创建谜题。发现创建优质谜题对当前模型仍是极具挑战的任务，这是先前基准未测量的能力

Conclusion: TTG提出了一种新的评估范式，既不会被设计饱和，又能同时测试模型的创造力、任务创建能力和问题解决能力，为推理评估开辟了新方向

Abstract: Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.

</details>


### [199] [El Agente Gráfico: Structured Execution Graphs for Scientific Agents](https://arxiv.org/abs/2602.17902)
*Jiaru Bai,Abdulrahman Aldossary,Thomas Swanick,Marcel Müller,Yeonghun Kang,Zijian Zhang,Jin Won Lee,Tsz Wai Ko,Mohammad Ghazi Vakili,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: 提出了El Agente Gráfico框架，通过类型安全执行环境和动态知识图谱，将LLM决策嵌入到结构化科学概念抽象中，实现可扩展的科学自动化


<details>
  <summary>Details</summary>
Motivation: 当前LLM与异构计算工具的集成方式临时且脆弱，基于非结构化文本的代理方法会产生大量信息，模糊决策来源并阻碍可审计性

Method: 开发单代理框架，将科学概念结构化抽象，使用对象-图映射器将计算状态表示为类型化Python对象，存储在内存或外部知识图谱中，通过类型化符号标识符而非原始文本管理上下文

Result: 在量子化学任务基准测试中，单代理结合可靠执行引擎能稳健执行复杂、多步骤和并行计算；进一步应用于构象集合生成和金属有机框架设计，知识图谱同时作为记忆和推理基础

Conclusion: 抽象和类型安全为基于代理的科学自动化提供了超越提示中心设计的可扩展基础，展示了结构化方法在科学工作流中的优势

Abstract: Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.

</details>


### [200] [Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems](https://arxiv.org/abs/2602.17910)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.AI

TL;DR: APEMO是一种运行时调度层，通过利用时间-情感信号优化计算资源分配，在固定预算下提升自主智能体在长时程工作流中的轨迹级可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐主要关注单个模型输出，但自主智能体在长时程工作流中需要在整个交互轨迹上保持持续可靠性。现有方法缺乏对轨迹级稳定性的关注，需要新的工程化途径来开发长时程智能体系统。

Method: APEMO是一种基于情感感知的峰值-终点调制编排方法，通过行为代理检测轨迹不稳定性，并在关键段（如峰值时刻和终点）进行针对性修复。该方法不修改模型权重，而是作为运行时调度层优化计算资源分配。

Result: 在多智能体模拟和基于LLM的规划-执行流程中的评估表明，APEMO相比结构化编排器能持续提升轨迹级质量和重用概率。

Conclusion: 研究结果将AI对齐重新定义为时间控制问题，为长时程智能体系统的开发提供了有弹性的工程化途径。

Abstract: Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.

</details>


### [201] [WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics](https://arxiv.org/abs/2602.17990)
*Madhav Kanda,Pedro Las-Casas,Alok Gautam Kumbhare,Rodrigo Fonseca,Sharad Agarwal*

Main category: cs.AI

TL;DR: 提出了WorkflowPerturb基准，通过可控扰动评估工作流评估指标，包含4,973个黄金工作流和44,757个扰动变体，分析指标敏感性和校准性


<details>
  <summary>Details</summary>
Motivation: LLM生成的结构化工作流自动评估困难，因为指标分数通常未校准，且分数变化不能直接反映工作流退化的严重程度

Method: 引入WorkflowPerturb基准，通过对黄金工作流应用现实可控扰动（缺失步骤、压缩步骤、描述变化），在10%、30%、50%严重级别上创建扰动变体，分析多种指标家族的敏感性和校准性

Result: 基准包含4,973个黄金工作流和44,757个扰动变体，揭示了不同指标家族的系统性差异，支持基于严重程度的工作流评估分数解释

Conclusion: WorkflowPerturb为工作流评估指标提供了受控研究基准，有助于理解指标敏感性和校准性，支持更准确的工作流质量评估

Abstract: LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.

</details>


### [202] [Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets](https://arxiv.org/abs/2602.18025)
*Haruki Abe,Takayuki Osa,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.AI

TL;DR: 该研究结合离线强化学习与跨具身学习，通过分析16种机器人平台的运动数据集，发现该方法在子优轨迹丰富时优于行为克隆，但随子优数据和机器人类型增加会出现梯度冲突，提出基于形态相似性的分组策略来缓解冲突。


<details>
  <summary>Details</summary>
Motivation: 解决机器人策略预训练中高质量演示数据收集成本高的问题，通过结合离线强化学习（利用专家和大量子优数据）与跨具身学习（聚合异构机器人轨迹）来获取通用控制先验。

Method: 构建包含16种机器人平台的运动数据集，系统分析离线RL与跨具身学习范式，提出基于形态相似性的分组策略（将机器人按形态聚类，使用组梯度更新模型）来减少跨机器人梯度冲突。

Result: 实验表明：1）在子优轨迹丰富的数据集上，该方法优于纯行为克隆；2）随着子优数据比例和机器人类型增加，跨形态梯度冲突会阻碍学习；3）提出的静态分组策略能显著减少机器人间冲突，优于现有冲突解决方法。

Conclusion: 离线RL与跨具身学习结合是有效的机器人策略预训练方法，但需解决跨形态梯度冲突问题；基于形态相似性的分组策略是简单有效的解决方案，为大规模机器人策略学习提供了新方向。

Abstract: Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.

</details>


### [203] [Neurosymbolic Language Reasoning as Satisfiability Modulo Theory](https://arxiv.org/abs/2602.18095)
*Hyunseok Oh,Sam Stern,Youngki Lee,Matthai Philipose*

Main category: cs.AI

TL;DR: Logitext：一种将自然语言文本约束与SMT求解结合的新方法，用于处理部分逻辑结构的文档推理


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在文本与逻辑交织的推理任务上表现不佳，现有神经符号系统仅限于完全形式化的任务（如数学、程序合成），无法处理自然文档中部分逻辑结构的问题

Method: 提出Logitext神经符号语言，将文档表示为自然语言文本约束（NLTCs），使部分逻辑结构显式化；开发算法将基于LLM的约束评估与可满足性模理论（SMT）求解相结合

Result: 在内容审核新基准、LegalBench和Super-Natural Instructions上的实验表明，Logitext提高了准确性和覆盖范围

Conclusion: 这是首个将基于LLM的推理视为SMT理论的工作，将神经符号方法扩展到完全形式化领域之外

Abstract: Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.

</details>


### [204] [SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps](https://arxiv.org/abs/2602.18201)
*Joseph Bingham,Netanel Arussy,Dvir Aran*

Main category: cs.AI

TL;DR: SOMtime方法在无监督表示中揭示了被排除的敏感属性（如年龄、收入）作为潜在主导轴，表明"公平通过不知情"在表示层面失效


<details>
  <summary>Details</summary>
Motivation: 挑战无监督表示对敏感属性保持中性的假设，证明即使明确排除敏感属性，它们仍会在表示中显现

Method: 使用SOMtime（基于高容量自组织映射的拓扑保持表示方法），在两个大规模真实数据集（世界价值观调查和人口普查收入数据集）上进行实验

Result: SOMtime恢复了与被排除敏感属性对齐的单调排序，斯皮尔曼相关性高达0.85，而PCA、UMAP、t-SNE和自编码器通常低于0.34；无监督分割产生人口统计学偏斜的聚类

Conclusion: "公平通过不知情"在序数敏感属性的表示层面失效，公平审计必须扩展到机器学习流水线的无监督组件

Abstract: Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime

</details>


### [205] [Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies](https://arxiv.org/abs/2602.18291)
*Zhuoran Li,Hai Zhong,Xun Wang,Qingxin Xia,Lihua Zhang,Longbo Huang*

Main category: cs.AI

TL;DR: OMAD：首个基于扩散策略的在线多智能体强化学习框架，通过松弛策略目标最大化联合熵，实现高效探索与协调，在多样任务中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和离线设置中表现出卓越的表达能力和多模态表示能力，但在在线多智能体强化学习中的应用尚未充分探索。主要障碍是扩散模型的不可处理似然性阻碍了基于熵的探索和协调。

Method: 提出OMAD框架：1）松弛策略目标最大化缩放联合熵，实现无需可处理似然的探索；2）在CTDE范式下，使用联合分布值函数优化去中心化扩散策略；3）利用可处理的熵增强目标指导扩散策略同步更新，确保稳定协调。

Result: 在MPE和MAMuJoCo的10个多样化任务上进行评估，OMAD成为新的SOTA方法，样本效率显著提高2.5倍到5倍。

Conclusion: OMAD成功将扩散策略应用于在线多智能体强化学习，通过创新的松弛策略目标和联合分布值函数，克服了扩散模型不可处理似然性的障碍，实现了高效的探索与协调。

Abstract: Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \underline{O}nline off-policy \underline{MA}RL framework using \underline{D}iffusion policies (\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\times$ to $5\times$ improvement in sample efficiency.

</details>
